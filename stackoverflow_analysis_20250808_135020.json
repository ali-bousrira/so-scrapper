[
  {
    "id":6694,
    "title":"JSON column mapping – navigation property throws NullReferenceException",
    "link":"https:\/\/stackoverflow.com\/questions\/79722731\/json-column-mapping-navigation-property-throws-nullreferenceexception",
    "text":"I have a model with a ``` WorkFlow ``` column in the database, defined as ``` nvarchar(MAX) ``` and storing JSON data. Previously, we treated it as a plain string in our C# entity, but I'm now trying to take advantage of EF Core 8’s JSON column mapping feature to deserialize it directly into a strongly typed object. Here's the ``` WorkFlow ``` model class: ``` public class WorkFlow { public WorkFlow() { Blocks = new List<Block>(); Connections = new List<PortConnection>(); } [JsonPropertyName(\"workFlowId\")] public Guid WorkFlowId { get; set; } [JsonPropertyName(\"blocks\")] public List<Block> Blocks { get; set; } [JsonPropertyName(\"connections\")] public List<PortConnection> Connections { get; set; } } ``` EF Core 8 mapping configuration: ``` public class LogicPointWorkFlowDetailConfiguration : IEntityTypeConfiguration<LogicPointWorkFlowDetail> { public void Configure(EntityTypeBuilder<LogicPointWorkFlowDetail> entity) { entity.OwnsOne(w => w.WorkFlow, navBuilder => { navBuilder.ToJson(); navBuilder.OwnsMany(x => x.Connections, connectionsBuilder => { connectionsBuilder.ToJson(); }); navBuilder.OwnsMany(x => x.Blocks, blocksBuilder => { blocksBuilder.ToJson(); blocksBuilder.OwnsOne(b => b.View, viewBuilder => { viewBuilder.ToJson(); viewBuilder.OwnsMany(x => x.PortInputs, portBuilder => { portBuilder.ToJson(); }); viewBuilder.OwnsMany(x => x.PortOuts, portBuilder => { portBuilder.ToJson(); }); }); }); }); } } ``` The Problem When I attempt to query the ``` Blocks ``` property (for example, using ``` .Any() ``` in a LINQ expression), I encounter the following exception: System.NullReferenceException: Object reference not set to an instance of an object. What I've verified The JSON stored in the database is valid and deserializes correctly outside EF Core Every ``` WorkFlow ``` JSON includes a non-null ``` blocks ``` array with valid elements The model and EF mapping seem to align with the structure of the JSON Questions Is this a known limitation or bug with EF Core 8's JSON column support? Is there an issue with my usage of ``` .OwnsMany() ``` or how I'm structuring the nested JSON mapping? Should I be initializing the collections in some special way for EF to handle them correctly? Any help or insights would be greatly appreciated! I tried to query the database table ``` LogicPointWorkFlowDetails ``` where the ``` WorkFlow ``` column is, and it does work. I can see the WorkFlow json deserilized correctly. ``` var activeDetails = await ctx.LogicPointWorkFlowDetails .Where(x => x.Status == Lms.UnifiedContextDb.Models.Enums.WorkFlowStatus.Active) .ToListAsync(); ``` If i try to execute ``` .Any() ``` on the ``` Blocks ``` property of the Workflow i get the exception. I tried to filter the Blocks with not null but i still get the exception: ``` var activeDetails1 = await ctx.LogicPointWorkFlowDetails .Where(x => x.Status == Lms.UnifiedContextDb.Models.Enums.WorkFlowStatus.Active) .Where(x => x.WorkFlow != null && x.WorkFlow.Blocks != null && x.WorkFlow.Blocks.Any(g => g.LogicPointWorkFlowBlockId != null && blockIds.Contains(g.LogicPointWorkFlowBlockId))) .Select(x => x.WorkFlow) .ToListAsync(); ```",
    "author_id":6130,
    "publication_date":1754070782000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"ludovicoilgrande",
    "author_reputation":11.0,
    "tags":"c#, json, entity-framework-core, ef-core-8.0, sql-server-2019",
    "text_length":3148,
    "title_length":71,
    "num_tags":5
  },
  {
    "id":6693,
    "title":"How to join public channel?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722733\/how-to-join-public-channel",
    "text":"I'm trying to join a public Slack channel that I'm not currently in via the Slack API. Using the conversations.join method. The request appears to run fine, returns a 200 with a success and metadata about the channel, with is_member=true, among other fields. However, upon inspecting Slack, my user account is not added to the channel (even though it says \"already_in_channel\") and I am unable to perform any actions (such as inviting other user accounts to the channel) that would work if I were actually in the channel. Below is a picture of me looking at the channel I'm supposedly a member of. ``` { \"ok\": true, \"channel\": { \"id\": \"[redacted]\", \"name\": \"[redacted]\", \"is_channel\": true, \"is_group\": false, \"is_im\": false, \"is_mpim\": false, \"is_private\": false, \"created\": 1754057960, \"is_archived\": false, \"is_general\": false, \"unlinked\": 0, \"name_normalized\": \"[redacted]\", \"is_shared\": false, \"is_org_shared\": false, \"is_pending_ext_shared\": false, \"pending_shared\": [], \"context_team_id\": \"[redacted]\", \"updated\": 1754057962121, \"parent_conversation\": null, \"creator\": \"[redacted]\", \"is_moved\": 0, \"is_ext_shared\": false, \"shared_team_ids\": [\"[redacted]\"], \"internal_team_ids\": [\"[redacted]\"], \"pending_connected_team_ids\": [], \"is_member\": true, \"topic\": { \"value\": \"\", \"creator\": \"\", \"last_set\": 0 }, \"purpose\": { \"value\": \"\", \"creator\": \"\", \"last_set\": 0 }, \"properties\": { \"tabs\": [{ \"type\": \"bookmarks\", \"label\": \"\", \"id\": \"bookmarks\" }], \"tabz\": [{ \"type\": \"bookmarks\" }] }, \"previous_names\": [], \"priority\": 0 }, \"warning\": \"already_in_channel\", \"response_metadata\": { \"warnings\": [\"already_in_channel\"] } } ```",
    "author_id":6129,
    "publication_date":1754070837000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"m4nw17h4pl4n",
    "author_reputation":149.0,
    "tags":"slack, slack-api, slack-commands",
    "text_length":1625,
    "title_length":27,
    "num_tags":3
  },
  {
    "id":6692,
    "title":"Want VS Code to stop on any exception in my code, but not in external libraries",
    "link":"https:\/\/stackoverflow.com\/questions\/79722734\/want-vs-code-to-stop-on-any-exception-in-my-code-but-not-in-external-libraries",
    "text":"In VS Code, running C#. I want it to stop on any exceptions in my own code, but not in libraries or in .NET itself or nuget packages. But I cannot get this to work, even with Copilot helping. In my launch task I set ``` \"justMyCode\": true ``` . And then in the debug exceptions screen, I have two choices: All exceptions User-unhandled exceptions All seems to make all, even in libraries. And I want to break on exceptions in my own libraries. And my code does handle all exceptions, but that's not the point, I need to break when my own code throws an exception. Visual Studio does this well, but I cannot get VS Code to behave the same. Has anyone out there, using VS Code, running C#, solved this issue? Per request, here is the launch task. Does this help? ``` { \"name\": \"Run Runner\", \"type\": \"coreclr\", \"request\": \"launch\", \"program\": \"${workspaceFolder}\/dimensionx\/Runner\/bin\/Debug\/net8.0\/Runner.dll\", \"args\": [ \"${input:runner_args}\" ], \"cwd\": \"${workspaceFolder}\/dimensionx\/Runner\", \"console\": \"internalConsole\", \"stopAtEntry\": false, \"preLaunchTask\": \"build runner\", \"justMyCode\": true, \"requireExactSource\": true, \"enableStepFiltering\": true, \"suppressJITOptimizations\": true, \"symbolOptions\": { \"searchPaths\": [ \"${workspaceFolder}\/dimensionx\/Runner\/bin\/Debug\/net8.0\" ] }, \"env\": { \"DOTNET_ENVIRONMENT\": \"Development\" } } ```",
    "author_id":6128,
    "publication_date":1754070885000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Daniel Williams",
    "author_reputation":9430.0,
    "tags":"c#, visual-studio-code",
    "text_length":1336,
    "title_length":79,
    "num_tags":2
  },
  {
    "id":6691,
    "title":"Why does mypy error on returning list[str, int] + list[int]?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722736\/why-does-mypy-error-on-returning-liststr-int-listint",
    "text":"From mypy's point of view ``` l1 + l2 ``` is OK. But returning ``` l1 + l2 ``` isn't OK. Why? I'm using Python 3.11 and mypy 1.16. ``` def test() -> list[str | int]: l1: list[str | int] l2: list[int] l1 + l2 # OK return l1 + l2 # error: Unsupported operand types for + (\"list[str | int]\" and \"list[int]\") [operator] ```",
    "author_id":6127,
    "publication_date":1754071046000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Stas Stepanov",
    "author_reputation":155.0,
    "tags":"python, mypy, python-typing",
    "text_length":319,
    "title_length":60,
    "num_tags":3
  },
  {
    "id":6690,
    "title":"kubernetes nodes&#39;s IP address is not matching with any of the available network interfaces",
    "link":"https:\/\/stackoverflow.com\/questions\/79722742\/kubernetes-nodess-ip-address-is-not-matching-with-any-of-the-available-network",
    "text":"I am trying to setup kubernetes cluster using kind. I am following this video. After deplying the Metallb when i was about to start configuring it, i noticed that my nodes shows IP 172.18.0.x where as my docker0 network interface shows CIDR 172.17.0.1\/16. I understand that \"172.18.0.x\" belongs to internal IP ofkubernetes network, but there must be some interface which has this CIDR. I do not have any such CIDR in my host machine. below is O\/P of ``` ip a s ``` command scriptchess@scriptchess:\/work\/devops\/kube-dashboard$ ip a s 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link\/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1\/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1\/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link\/ether d8:5e:d3:d4:e8:75 brd ff:ff:ff:ff:ff:ff inet 192.168.1.7\/24 brd 192.168.1.255 scope global dynamic noprefixroute enp4s0 valid_lft 44289sec preferred_lft 44289sec inet6 2402:e280:2013:a0:df30:3faf:c413:6059\/64 scope global temporary deprecated dynamic valid_lft 86182sec preferred_lft 0sec inet6 2402:e280:2013:a0:45a8:4567:c8c9:1a72\/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 86182sec preferred_lft 86182sec inet6 fe80::cda0:bf3c:7b3c:4088\/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link\/ether 52:54:00:5a:58:11 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1\/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever 4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default link\/ether aa:bc:77:49:55:52 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1\/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever Also, I am unable to understand why docker0 is DOWN when my metallb pods are running.",
    "author_id":6126,
    "publication_date":1754071390000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Kumar Gaurav",
    "author_reputation":1331.0,
    "tags":"docker, kubernetes, kubectl, kind, metallb",
    "text_length":2055,
    "title_length":94,
    "num_tags":5
  },
  {
    "id":6689,
    "title":"Extracting Non-Settlement Days from Bloomberg Using xbbg and Calendar Overrides",
    "link":"https:\/\/stackoverflow.com\/questions\/79722748\/extracting-non-settlement-days-from-bloomberg-using-xbbg-and-calendar-overrides",
    "text":"I would like to find the non trading days on a certain period for a certain ticker. The thing is that I cannot wait to see if I can retrieve a price through blp.bdh because I need to find the non trading days before they happen. I need to find non trading days, for each index in the country_currency_code dictionary. so I use ``` 'CALENDAR_NON_SETTLEMENT_DATES' ``` linked to a country code because holidays depend on the country. I tried with different tickers, different country codes, none of it worked. I only get empty dataframes. ``` import pandas as pd import numpy as np from xbbg import blp from datetime import datetime, timedelta country_currency_code = { 'SOFRRATE Index': ('USD Curncy', 'US'), 'BISTTREF Index': ('TRY Curncy', 'TU'), 'MUTKCALM Index': ('JPY Curncy','JN'), 'RUONIA Index': ('RUB Cunrcy','R$'), 'SIBCSORA Index': ('SGD Curncy','SI'), 'SONIO\/N Index': ('GBP Curncy','GB'), 'SRFXON3 Index': ('CHF Curncy','SZ'), 'TTHORON Index': ('TWD Curncy','T+') } def shift_month(year: int, month: int, offset: int): new_month = month + offset new_year = year + (new_month - 1) \/\/ 12 new_month = ((new_month - 1) % 12) + 1 return new_year, new_month def get_off_days(year: int, month: int): start_year, start_month = shift_month(year, month, -1) end_year, end_month = shift_month(year, month, +1) start_date = datetime(start_year, start_month, 10).strftime('%Y%m%d') end_date = datetime(end_year, end_month, 15).strftime('%Y%m%d') all_off_days = {} for index, (_, country_code) in country_currency_code.items(): try: result = blp.bds( index, 'CALENDAR_NON_SETTLEMENT_DATES', [ f'SETTLEMENT_CALENDAR_CODE={country_code}', f'CALENDAR_START_DATE={start_date}', f'CALENDAR_END_DATE={end_date}' ] ) off_days = result.get('calendar_non_settlement_dates', []) all_off_days[index] = off_days except Exception as e: print(f\"Erreur pour {index} : {e}\") all_off_days[index] = [] return all_off_days ```",
    "author_id":6125,
    "publication_date":1754072048000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Jstne",
    "author_reputation":1.0,
    "tags":"python, dataframe, calendar, bloomberg, xbbg",
    "text_length":1905,
    "title_length":79,
    "num_tags":5
  },
  {
    "id":6688,
    "title":"Hibernate validator ClassNotFoundException in glassfish 7 app when its a provided module?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722757\/hibernate-validator-classnotfoundexception-in-glassfish-7-app-when-its-a-provide",
    "text":"Glassfish 7 ships with ``` hibernate-validator.jar ``` in version 8.0.2 since its in the glassfish module folder. In my code i am using hibernate ``` org.hibernate.validator.internal.constraintvalidators.bv.time.pastorpresent.PastOrPresentValidatorForDate ``` and when deploying my EAR application it says the class is not found? How can it be, if the class is in the module path? I must add the exact jar file to the EAR aswell, effectively having this jar twice. But this creates another problem with a class of this jar when casting ``` ConstraintValidatorContext ``` to ``` ConstraintValidatorContextImpl ``` at runtime since the JVM does not know which jar to load the class from (the provided module or the bundled one in the ear). Did not had these problems with glassfish 5 and javax.",
    "author_id":6124,
    "publication_date":1754072813000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"djmj",
    "author_reputation":5574.0,
    "tags":"jakarta-ee, glassfish",
    "text_length":792,
    "title_length":89,
    "num_tags":2
  },
  {
    "id":6687,
    "title":"How does Django support asynchronous operations",
    "link":"https:\/\/stackoverflow.com\/questions\/79722758\/how-does-django-support-asynchronous-operations",
    "text":"Environment django = \"==3.2.15\" python3 Mysql Uwsgi + Nginx Demo Code The request is returned immediately, and the database operation is executed by the background thread (I don't need to pay attention to the thread execution result, as long as it can be executed correctly). ``` \/\/ views function @classmethod def _op_function_view(cls, request): task_params = get_request_value(request) task_params['user'] = request.user thread = threading.Thread(target=Task.function, args=(task_params,)) thread.start() return \"Task run success\" \/\/ modle calss class Tast: \/\/some mysql filed create_username = models.CharField(max_length=50, default='system') update_username = models.CharField(max_length=50, default='system') .... \/\/ task function def function(cls,params): \/\/ ORM logical ...... ``` Problem It happens when I execute the code: (2006, 'MySQL server has gone away') What should I do to maintain the current logic (keep the task function running in the background and return requests immediately), and ensure that the background threads do not lose sql I try ``` def function(cls,params): \/\/ django.db.connection.ensure_connection() ORM logical ...... django.db.connection.close() ``` But it will still happen：(2006, 'MySQL server has gone away')",
    "author_id":5901,
    "publication_date":1754072861000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Dylan",
    "author_reputation":43.0,
    "tags":"python, django-3.2",
    "text_length":1250,
    "title_length":47,
    "num_tags":2
  },
  {
    "id":6686,
    "title":"Database Design for attendance managment app",
    "link":"https:\/\/stackoverflow.com\/questions\/79722759\/database-design-for-attendance-managment-app",
    "text":"I am making an app to manage attendance at an office, the most crucial part is the database design. i'll start by explaining what would be done on the app and then what database designs I came up with. Application requirements: Log when each employee came in and left the office. Employees can have permitted leaves for a couple of hours a day either for personal or work reasons, for instance an employee's day could look like this (Arrive at office at 9AM work until 11AM and meet with a client from 11AM to 1PM, and come back to the office to work until 5PM) all of this should be logged these part-time leaves shouldn't be logged as normal work hours because we need to know when everyone was in office Standard vacation or sick days that could last multiple days. query what someone did over a certain period The problem I am trying to figure out is how to log these part-time and multiple-day leaves I came up with the following design: Each employee is related to many logs, and each log represents either work, vacation, leave, etc.. each log has a start and end attributes (datetime), but i'm afraid that this will be buggy and I will end up with a lot of edge\/impossible cases, for example: should a vacation that lasts 3 days be 3 seperate logs or a single one, and at which time should it start at (9AM or 00:00) if in the future, I need to add specific attributes related to one of the log types they'd have to exist for every type, creating possible impossible cases.",
    "author_id":6123,
    "publication_date":1754072902000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Yusuf Bouzekri",
    "author_reputation":357.0,
    "tags":"database, database-design, system-design",
    "text_length":1481,
    "title_length":44,
    "num_tags":3
  },
  {
    "id":6685,
    "title":"Harmless SQL clutter on startup with no tables",
    "link":"https:\/\/stackoverflow.com\/questions\/79722762\/harmless-sql-clutter-on-startup-with-no-tables",
    "text":"I have no data.sql and configured for MariaDB. Upon startup every missing table causes a hibernate error because the table does not exist. Is there a way to add a command to silence this and not lose any SQL output? ``` spring.jpa.show-sql = true spring.jpa.hibernate.ddl-auto = create-drop spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.MariaDBDialect spring.jpa.properties.hibernate.format-sql = true spring.jpa.defer-datasource-initialization=true hibernate.cache.use_second_level_cache=true hibernate.cache.region.factory_class=org.hibernate.cache.ehcache.EhCacheRegionFactory ```",
    "author_id":6122,
    "publication_date":1754073097000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"cp.",
    "author_reputation":1271.0,
    "tags":"hibernate",
    "text_length":601,
    "title_length":46,
    "num_tags":1
  },
  {
    "id":6684,
    "title":"Polars bug using windowed aggregate functions on Decimal type columns",
    "link":"https:\/\/stackoverflow.com\/questions\/79722764\/polars-bug-using-windowed-aggregate-functions-on-decimal-type-columns",
    "text":"Windowed aggregate functions on Decimal-types move decimals to integers I found a bug in ``` polars ``` (version 1.21.0 in a Python 3.10.8 environment) using windowed aggregate functions. They are not properly handling the decimal, essentially multiplying the result by 100. Here is a minimum reproducible example: ``` import polars as pl pl.__version__ # 1.21.0 pl.DataFrame( { 'People':['John', 'John', 'John', 'John', 'Jane', 'Jane', 'Jane', 'Jane'], 'Balance': [0.00, 10.59, 11.29, 0.00, 12.34, 23.45, 34.56, 45.67] } , schema={'People':pl.String, 'Balance':pl.Decimal(10, 2)} ).with_columns( Bal_max = pl.col(\"Balance\").max(), Bal_max_person_wrong = pl.col(\"Balance\").max().over(\"People\"), Bal_max_person_right = pl.col(\"Balance\").cast(float).max().over(\"People\"), Bal_min_person_wrong = pl.col(\"Balance\").min().over(\"People\"), Bal_sum_person_wrong = pl.col(\"Balance\").sum().over(\"People\") ) ``` The results are below: What should I do about this? I'm tempted to hack it a bit and divide by 100, but that seems unwise. I will probably treat the data as floats, but I prefer limiting the decimals for values that will always be treated as dollars and cents. Any advice you can give would be appreciated!",
    "author_id":6121,
    "publication_date":1754073271000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"jpm_phd",
    "author_reputation":935.0,
    "tags":"python, python-polars, polars, window-functions",
    "text_length":1207,
    "title_length":69,
    "num_tags":4
  },
  {
    "id":6683,
    "title":"Nextjs DYNAMIC_SERVER_USAGE error when using generateMetadata",
    "link":"https:\/\/stackoverflow.com\/questions\/79722766\/nextjs-dynamic-server-usage-error-when-using-generatemetadata",
    "text":"I have migrated my nextjs website from pages router to app router and I am getting a ``` DYNAMIC_SERVER_USAGE ``` error when I attempt to add page metadata using the ``` generateMetadata ``` function. I know that ``` export const dynamic = \"force-dynamic\"; ``` solves the error but I want to generate all of the pages at build time so that is not an option. Why is ``` generateMetadata ``` causing this error? Every reference I can find refers to pages that cannot be statically generated but all of the pages are built fine when I remove ``` generateMetadata ``` and I am not calling any functions inside ``` generateMetadata ``` . Thanks ``` export const dynamic = \"force-static\"; import fs from \"fs\"; import path from \"path\"; import { getPostData, IPost } from \"..\/..\/..\/lib\/posts\"; import Date from \"..\/..\/..\/components\/date\"; import { Metadata } from \"next\"; const postsDirectory = path.join(process.cwd(), \"src\/posts\"); interface IPageProps { id: string; } export async function generateMetadata(): Promise<Metadata> { return { title: \"title\", description: \"description\", }; } export default async function Page({ params, }: { params: Promise<IPageProps>; }): Promise<JSX.Element> { const { id } = await params; const { title, date, contentHtml } = await getPostData(id); return ( <div> <article className=\"prose\"> <h1 className=\"font-Inter text-3xl sm:text-4xl\">{title}<\/h1> <div className=\"text-2xl text-slate-500 md:text-xl\"> <Date dateString={date} \/> <\/div> <div className=\"text-mob md:text-lg lg:text-lg\" dangerouslySetInnerHTML={{ __html: contentHtml }} \/> <\/article> <\/div> ); } export async function generateStaticParams(): Promise<IPost[]> { const fileNames = fs.readdirSync(postsDirectory); return fileNames.map((fileName) => { return { params: { id: fileName.replace(\/\\.md$\/, \"\"), }, }; }); } ```",
    "author_id":6120,
    "publication_date":1754073336000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Lucas Amos",
    "author_reputation":1209.0,
    "tags":"next.js, javascript",
    "text_length":1814,
    "title_length":61,
    "num_tags":2
  },
  {
    "id":6682,
    "title":"When removing a ToolbarItem from the navigation bar, how do I make the remaining ToolbarItems resize correctly?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722770\/when-removing-a-toolbaritem-from-the-navigation-bar-how-do-i-make-the-remaining",
    "text":"Because .searchable does not allow for customizing buttons in the search bar, I've manually had to recreate the search bar as shown below. However, when removing one of the items in the search bar, the TextField does not resize correctly and effectively inserts padding on the leading edge. When the TextField is focused, it resizes and fills the entire space. If the \"Compose\" button was already hidden when the search bar is presented, it lays out correctly. How do I resize the TextField after removing the \"Compose\" button automatically? Thanks, jjp ``` struct ContentView: View { @State var isSearchBarVisible = false @State var isComposingMessage = false @State var searchText = \"\" let items: [String] = [\"hey\", \"there\", \"how\", \"are\", \"you\"] var searchItems: [String] { items.filter { item in item.lowercased().contains(searchText.lowercased()) } } var body: some View { NavigationStack { VStack { List { if !searchText.isEmpty { ForEach(searchItems, id: \\.self) { item in Text(item) } } else { ForEach(items, id: \\.self) { item in Text(item) } } } } .toolbar { if isSearchBarVisible { ToolbarItem(placement: .principal) { TextField(\"Search\", text: $searchText) .padding(8) .background(Color.gray.opacity(0.2)) } ToolbarItem(placement: .topBarTrailing) { Button(action: { isSearchBarVisible = false },[![enter image description here][1]][1] label: { Text(\"Cancel\") }) } if !isComposingMessage { ToolbarItem(placement: .topBarTrailing) { Button(action: { isComposingMessage.toggle() }, label: { Text(\"Compose\") }) } } } else { ToolbarItem(placement: .topBarLeading) { Button(action: { isSearchBarVisible = true }, label: { Text(\"Search\") }) } ToolbarItem(placement: .principal) { Text(\"Title\") } ToolbarItem(placement: .topBarTrailing) { Button(action: { isComposingMessage.toggle() }, label: { Text(\"Compose\") }) } } } } } } ```",
    "author_id":6119,
    "publication_date":1754073544000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"price",
    "author_reputation":115.0,
    "tags":"ios, swiftui",
    "text_length":1834,
    "title_length":111,
    "num_tags":2
  },
  {
    "id":6681,
    "title":"Using a model property (list of dictionaries) as an input to django&#39;s format_html_join() yields KeyError",
    "link":"https:\/\/stackoverflow.com\/questions\/79722771\/using-a-model-property-list-of-dictionaries-as-an-input-to-djangos-format-htm",
    "text":"I am attempting to use Django's ``` format_html_join() ``` util to return an html formatted version history for one of my models. But I cannot get ``` format_html_join() ``` to accept my list of dictionaries. Here is what the documentation suggests: ``` format_html_join( \"\\n\", '<li data-id=\"{id}\">{id} {title}<\/li>', ({\"id\": b.id, \"title\": b.title} for b in books), ) ``` That third argument is intended to be: args_generator should be an iterator that yields arguments to pass to format_html(), either sequences of positional arguments or mappings of keyword arguments. I have tried different ways to get this to work and I'm not getting it, so I'm asking for help. I thought a list of dictionaries is iterable. I'm also thinking there has to be a way to use a list of dictionaries in a util that is expecting a list of dictionaries without having to re-create the list of dictionaries. Here is the model method I have to get the version history: ``` @property # I have tried this as a property and not as a property, neither works def get_version_history(self): versions = Version.objects.get_for_object(self) version_history = [] for version in versions: history_fields = version.field_dict hdict = {\"question\": history_fields['question'], \"answer\": history_fields['answer'], \"user\": version.revision.user.username, \"timestamp\": version.revision.date_created.strftime(\"%Y-%m-%d %H:%M\"), } version_history.append(hdict) return version_history ``` That method returns something like this: ``` [{'question': \"I'm out of questions\", 'answer': 'bye', 'user': 'test.supervisor', 'timestamp': '2025-07-31 20:19'}, {'question': \"I'm out of questions\", 'answer': 'me too', 'user': 'test.supervisor', 'timestamp': '2025-07-31 20:18'}, {'question': \"I'm out of questions\", 'answer': '', 'user': 'test.supervisor', 'timestamp': '2025-07-31 20:18'}] ``` Now I am trying to return an html formatted version of that list of dictionaries: ``` def version_html(self): html = format_html_join( \"\\n\", \"\"\"<tr> <td>{question}<\/td> <td>{answer}<\/td> <td>{user}<\/td> <td>{timestamp}<\/td> <\/tr>\"\"\", self.get_version_history ) return html ``` The above code returns this error: ``` File ~\/.virtualenvs\/cep\/lib\/python3.12\/site-packages\/django\/utils\/html.py:112, in format_html(format_string, *args, **kwargs) 110 args_safe = map(conditional_escape, args) 111 kwargs_safe = {k: conditional_escape(v) for (k, v) in kwargs.items()} --> 112 return mark_safe(format_string.format(*args_safe, **kwargs_safe)) KeyError: 'question' ``` I have tried various things for the third argument, all with various errors: ``` self.get_version_history self.get_version_history() **self.get_version_history **self.get_version_history() {\"question\": v.question, \"answer\": v.answer, \"user\": v.user, \"timestamp\": v.timestamp,} for v in self.get_version_history()) ({\"question\": v['question'], \"answer\": v['answer'], \"user\": v['user'], \"timestamp\": v['timestamp']} for v in self.get_version_history()) {\"question\": v.question, \"answer\": v.answer, \"user\": v.user, \"timestamp\": v.timestamp,} for v in self.get_version_history) ({\"question\": v['question'], \"answer\": v['answer'], \"user\": v['user'], \"timestamp\": v['timestamp']} for v in self.get_version_history) (d for d in self.get_version_history) (d for d in self.get_version_history()) [d for d in self.get_version_history] [d for d in self.get_version_history()] ``` Now I'm just thrashing.",
    "author_id":6118,
    "publication_date":1754073622000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"robline",
    "author_reputation":504.0,
    "tags":"python, django, html, django-reversion",
    "text_length":3398,
    "title_length":108,
    "num_tags":4
  },
  {
    "id":6680,
    "title":"Why does TS infer return type differently between `reduce` and `map` for `unknown[]`?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722772\/why-does-ts-infer-return-type-differently-between-reduce-and-map-for-unknow",
    "text":"``` const array: unknown[] = [] const a = array.map(() => '') \/\/ a is string const b = array.reduce((acc) => acc, 'initial') \/\/ b is unknown (why?) ``` For the ``` map ``` call above, TS inferred the following signature: ``` map<U>(callbackfn: (value: T, index: number, array: T[]) => U, thisArg?: any): U[]; ``` which makes sense ( ``` U ``` is ``` string ``` , ``` T ``` is ``` unknown ``` ). But for the ``` reduce ``` call, TS inferred: ``` reduce(callbackfn: (previousValue: T, currentValue: T, currentIndex: number, array: T[]) => T, initialValue: T): T; ``` (generalizing the return type to ``` T = unknown ``` ) instead of inferring ``` \/\/ This signature is also available in lib.es5.d.ts reduce<U>(callbackfn: (previousValue: U, currentValue: T, currentIndex: number, array: T[]) => U, initialValue: U): U; ``` Why didn't TS narrowed the inferred types of the ``` reduce ``` call to ``` T = unknown ``` and ``` U = string ``` ?",
    "author_id":6117,
    "publication_date":1754073677000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Ricardo Baptista",
    "author_reputation":115.0,
    "tags":"typescript, generics",
    "text_length":936,
    "title_length":85,
    "num_tags":2
  },
  {
    "id":6679,
    "title":"Handling concurrent state updates on a distributed system",
    "link":"https:\/\/stackoverflow.com\/questions\/79722776\/handling-concurrent-state-updates-on-a-distributed-system",
    "text":"My system includes horizontally scaled microservices named Consumers that reads from a RabbitMQ queue. Each message contains state update on resources (claims) that triggers an expensive enrichment computation (like 2 minutes) based on the fields updates. To race conditions on the claims I implemented a status field in the MongoDB documents, so everytime I am updating a claim, I put it in the WORKING state. Whenever a Consumer receives a message for a claim in a WORKING state, it saves the message in a dedicated Mongo collection and then those messages are requeued by a Cronjob that reads from that collection. I know that I cannot rely on the order in which messages are saved in Mongo and so it can happen that a newer update is overwritten by an older one (stale update). Is there a way to make the updates idempotent? I am not in control of the service that publishes the messages into the queue as one potential solution is to attach a timestamp that mark the moment the message is published. Another possible solution could be to use a dedicated microservice that reads from the queue and mark them without horizontally scale it. Are there any elegant solution? Any book recommendation that deals with this kind of problems?",
    "author_id":6116,
    "publication_date":1754074167000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Antonio Santoro",
    "author_reputation":921.0,
    "tags":"microservices, mongodb, rabbitmq, distributed-system, optimistic-concurrency",
    "text_length":1237,
    "title_length":57,
    "num_tags":5
  },
  {
    "id":6678,
    "title":"Mtcnn not Being Detected in Jupyter Notebook",
    "link":"https:\/\/stackoverflow.com\/questions\/79722786\/mtcnn-not-being-detected-in-jupyter-notebook",
    "text":"I am using Juypter Notebook and a virtual environment. Mtcnn is not being detected for some reason even when I install it. I don't know what the issue is.",
    "author_id":6115,
    "publication_date":1754075081000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"JackLalane1",
    "author_reputation":179.0,
    "tags":"python-3.x, jupyter-notebook, virtual-environment",
    "text_length":154,
    "title_length":44,
    "num_tags":3
  },
  {
    "id":6677,
    "title":"Can I add annotation on the same line as a conref call to help footnoting",
    "link":"https:\/\/stackoverflow.com\/questions\/79722788\/can-i-add-annotation-on-the-same-line-as-a-conref-call-to-help-footnoting",
    "text":"I want to add asterisks to unstructured list items that are called using a conref= statement. In the example below, the list data will not pick up the \"*\" marks, but the \"notes\" asterisks will print. ``` <body id=\"certificationsBody\"> <ul> <li><data conref=\"..\/common_topics\/common_documentation_reuseTopics.dita#bluetooth5.0\" outputclass=\"-dita-use-conref-target\">*<\/data><\/li> <li><data conref=\"..\/common_topics\/common_documentation_reuseTopics.dita#bluetoothNLC\" outputclass=\"-dita-use-conref-target\">**<\/data><\/li> <\/ul> <note>&#xA; &#42; -CB mwConnect Casambi &#xA; &#42; &#42; -SR TruBlu<\/note> <\/body> ```",
    "author_id":6114,
    "publication_date":1754075127000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Syndesis",
    "author_reputation":11.0,
    "tags":"dita",
    "text_length":612,
    "title_length":73,
    "num_tags":1
  },
  {
    "id":6676,
    "title":"Trying to compile Zeek in Ubuntu 20.04 and keep getting CMakeList.txt error",
    "link":"https:\/\/stackoverflow.com\/questions\/79722790\/trying-to-compile-zeek-in-ubuntu-20-04-and-keep-getting-cmakelist-txt-error",
    "text":"I have been working for over an hour on attempting to resolve this persistent error of CMakeList.txt whenever I try to run ``` cmake .. ``` or ``` .\/configure ``` in Ubuntu 20.04 to run compile and finish building Zeek. These are some of the steps I feel I have repeated numerous times already: Step-by-Step Clean Fix Delete the broken clone: ``` rm -rf ~\/zeek-src ``` Clone Zeek with submodules: ``` git clone --recursive https:\/\/github.com\/zeek\/zeek.git ~\/zeek-src git submodule update --init --recursive ``` I ran the ``` submodule ``` command because I did not run it last time see if I get different results because even when I do ``` --recursive ``` I still get the CMakeList.txt:xxx (include): error. Then I run this: ``` cd ~\/zeek-src mkdir build cd build cmake .. -DPython_EXECUTABLE=\/usr\/bin\/python3.10 make -j$(nproc) sudo make install ``` And I still get the same error ``` cmake\/FindPCAP.cmake:50 (find_package_handle_standard_args) CMakeLists.txt:843 (find_package) ``` and now it also cannot find: ``` Could NOT find PCAP (missing: PCAP_LIBRARY PCAP_INCLUDE_DIR) ```",
    "author_id":5706,
    "publication_date":1754075289000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Daniel",
    "author_reputation":15619.0,
    "tags":"cmake, ubuntu-20.04, zeek",
    "text_length":1081,
    "title_length":75,
    "num_tags":3
  },
  {
    "id":6675,
    "title":"How to show my maps website in JavaFX application using WebView?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722792\/how-to-show-my-maps-website-in-javafx-application-using-webview",
    "text":"I need to implement a map for my road management project. Something similar to Google Maps but simpler. It is for an academic project. I don't want to use Google Maps API. I tried using JavaFX WebView. I made an HTML, CSS, and JavaScript code that used MapLibre maps. HTML ``` <!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <title>MapLibre Map<\/title> <meta name=\"viewport\" content=\"initial-scale=1,maximum-scale=1,user-scalable=no\"> <!-- MapLibre GL CSS --> <link href=\".\/node_modules\/maplibre-gl\/dist\/maplibre-gl.css\" rel=\"stylesheet\"> <!-- Custom CSS --> <link rel=\"stylesheet\" href=\"styles.css\"> <\/head> <body> <div id=\"map\"><\/div> <!-- MapLibre GL JS --> <script src=\".\/node_modules\/maplibre-gl\/dist\/maplibre-gl.js\"><\/script> <!-- Your custom JS --> <script src=\"script.js\"><\/script> <\/body> <\/html> ``` CSS ``` body, html { margin: 0; padding: 0; height: 100%; } #map { width: 100%; height: 100vh; } ``` JavaScript ``` const map = new maplibregl.Map({ container: 'map', style: 'https:\/\/api.maptiler.com\/maps\/streets-v2\/style.json?key=uSZHAA0ZFcoLru0ECNJq', \/\/ Sample style center: [90.4125, 23.8103], \/\/ Example: Dhaka zoom: 13 }); \/\/ Example marker const marker = new maplibregl.Marker({ color: 'red' }) .setLngLat([90.4125, 23.8103]) .addTo(map); ``` As you can see, I didnt keep it too complex. But yet my WebView control cannot seem to show this. When I try to see this by running the ``` index.html ``` file, I can see it, but when I use it with WebView I don't. I asked AI, it said that maybe it has something to do with WebGL or that JavaFX is backdated and cant use it or something like that. I have a seperate directory named node_modules as well.",
    "author_id":6113,
    "publication_date":1754075381000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Ahmed abrar",
    "author_reputation":1.0,
    "tags":"java, javascript, html, css, javafx",
    "text_length":1668,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6674,
    "title":"putting looped API Call results into a dataframe in Python",
    "link":"https:\/\/stackoverflow.com\/questions\/79722798\/putting-looped-api-call-results-into-a-dataframe-in-python",
    "text":"i need some help. have got a part of a python script which accesses a url field in a sql database, and then calls an api based using the url in the field. Now i cannot get the data into a dataframe to then flatten and output to csv, can anyone help. I am not very proficient in Python. Have done it for paginated API Calls, but the looping of a call, put into df, go to next call, put in df etc is breaking me The code so far is below; ``` import pyodbc import pandas as pd import requests as re from pathlib import Path import json import os server = 'TestServer' database = 'TestDB' cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes') cursor = cnxn.cursor() # select rows from SQL table to insert in dataframe. websites = \"select TOP 2 URL FROM [dbo].[API_URL_TEST];\" UUID_df = pd.read_sql(websites, cnxn) print(UUID_df.head(10)) #print(df) from urllib.request import urlopen for link in UUID_df['URL']: with urlopen(link) as response: myfile = response.read() print(myfile) data=response.json() df = pd.json_normalize(data['match']) print(df) ``` i get json data from the api, I have been using ``` response.json() ``` but i am only getting the data from the 1st api call from the 2 i am using for testing. When i ``` print(myfile) ``` i can see data from both api calls. i suspect it is me not knowing how to put each call in the dataframe before the next api call. in total once completed will be around 1k api's to call and get data from.",
    "author_id":6112,
    "publication_date":1754075776000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Trevor Turn",
    "author_reputation":1.0,
    "tags":"python, dataframe, pandas",
    "text_length":1500,
    "title_length":58,
    "num_tags":3
  },
  {
    "id":6673,
    "title":"Update Bicep File to Fix Azure SQL Database 2014-04-01 APIs will be retired?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722804\/update-bicep-file-to-fix-azure-sql-database-2014-04-01-apis-will-be-retired",
    "text":"My IT department is getting an e-mail \"Azure SQL Database 2014-04-01 APIs will be retired on 31 October 2025\". I use bicep and have never used any API's that are that old, they're all currently at ``` 2021-11-01 ``` . If I export my Azure SQL Database or Server I get entries like the following: ``` resource servers_myproj_dev1_sql_name_CreateIndex 'Microsoft.Sql\/servers\/advisors@2014-04-01' = { parent: servers_myproj_dev1_sql_name_resource name: 'CreateIndex' properties: { autoExecuteValue: 'Disabled' } } ``` I believe these were implicitly added when I initially deployed. Two questions: On November 1 2025 will I experience any problems due to obsolete API's? How can I stop the e-mail that are unnerving my IT department? I'm fairly certain from running the Graph queries specified in How to react to message from Microsoft regarding updating API's that the answer to 1 is no, it won't be a problem. What I don't know about is #2. If I update my bicep files to specify ``` Microsoft.Sql\/servers\/databases\/advisors@2021-11-01 ``` with ``` autoExecuteValue: 'Disabled' ``` will it make the error messages go away?",
    "author_id":6111,
    "publication_date":1754076246000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Lee Richardson",
    "author_reputation":8895.0,
    "tags":"sql-server, azure-bicep, azure-sql-database",
    "text_length":1120,
    "title_length":76,
    "num_tags":3
  },
  {
    "id":6672,
    "title":"Suitelet error: The page you&#39;re looking for used information that you entered. Returning to that page might cause any action you took to be repeated",
    "link":"https:\/\/stackoverflow.com\/questions\/79722805\/suitelet-error-the-page-youre-looking-for-used-information-that-you-entered-r",
    "text":"I hope you are doing well. I have developed suitelet script to design custom UI to select some checkboxes and show result in a sublist. Evrything works fine but when I reload the page I see this pop-up. \"The page that you're looking for used information that you entered. Returning to that page might cause any action you took to be repeated. Do you want to continue?\" How to avoid this pop-up while reloading the page? Please advice. Thank you! This is client script page init function: function pageInit(context) { ``` try { const rec = context.currentRecord; const deletionSuccessParam = rec.getValue({ fieldId: 'custpage_deletion_success_hidden' }); if (deletionSuccessParam === 'T') { sessionStorage.removeItem('selectedPairs'); \/\/ Clear stored selections } } catch (e) { console.error('Error in pageInit:', e.name + ': ' + e.message); } } ```",
    "author_id":6110,
    "publication_date":1754076360000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Maira S",
    "author_reputation":121.0,
    "tags":"user-interface, client, netsuite, reload, suitescript2.0",
    "text_length":848,
    "title_length":152,
    "num_tags":5
  },
  {
    "id":6671,
    "title":"Changing button&#39;s look by state; Only works while debugging",
    "link":"https:\/\/stackoverflow.com\/questions\/79722812\/changing-buttons-look-by-state-only-works-while-debugging",
    "text":"For some background, I'm displaying my button as an image. What I want to change the look of my button when it's clicked and keep it as that new look until it is clicked again. Here's what I have so far: ``` from tkinter import * from tkinter import ttk def TestLogic(): print(testBtn[\"state\"]) if testBtn[\"state\"] == \"normal\": stgImg = PhotoImage(file=\"test1.png\") testBtn.configure(image=stgImg) testBtn.image = stgImg testBtn.configure(state=\"active\") elif testBtn[\"state\"] == \"active\": stgImg = PhotoImage(file=\"test.png\") testBtn.configure(image=stgImg) testBtn.image = stgImg testBtn.configure(state=\"normal\") root = Tk() root.geometry('600x600') stgImg = PhotoImage(file=\"test.png\") testBtn=ttk.Button(root, text=\"TEST\", image = stgImg, command=TestLogic) testBtn.pack(anchor=\"center\") root.mainloop() ``` The problem is that without the print statement the code doesn't work, however, it works just fine with the print statement. I want to know why, how to fix this issue so I don't have to have the print statement, and possibly some ideas on how to improve this.",
    "author_id":6109,
    "publication_date":1754076779000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Thevin Jayawardena",
    "author_reputation":71.0,
    "tags":"python, tkinter",
    "text_length":1072,
    "title_length":63,
    "num_tags":2
  },
  {
    "id":6670,
    "title":"&quot;The parameter is incorrect. (winmgmt)&quot; while creating a BizTalk host",
    "link":"https:\/\/stackoverflow.com\/questions\/79722814\/the-parameter-is-incorrect-winmgmt-while-creating-a-biztalk-host",
    "text":"I am installing a fresh BizTalk server 2020 on a fresh Windows 2019 server. I followed the following steps: Installed prerequisits from https:\/\/learn.microsoft.com\/en-us\/biztalk\/install-and-config-guides\/set-up-and-install-prerequisites-for-biztalk-server-2020 Installed Biztalk Server 2020 Installed CU6 Configured BizTalk (including BAM, except BAM alerts). For this I also had to install SSIS locally and also SSIS Catalog on the SQL box. Installed and configured the ESB Toolkit Updated the URL on Microsoft.Practices.ESB>Send Port>All.Exceptions to direct to the correct Biztalk instance Deleted the unwanted adapters from the adapters list. Installed WinSCP Ran the script to create Biztalk hosts and host instances. And I get following errors on every host\/host instances specified in the script: ``` CreateBizTalkHost : TestHost host could not be created: Exception calling \"Invoke\" with \"2\" argument(s): \"The parameter is incorrect. \".Exception.ToString() At C:\\Host Instances Creation Scripts for Clustered Environment\\CreateHostScript.ps1:297 char:4 + CreateBizTalkHost $HostName64 1 $BizTalkHostGroupName $fa ... + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : NotSpecified: (:) [Write-Error], WriteErrorException + FullyQualifiedErrorId : Microsoft.PowerShell.Commands.WriteErrorException,CreateBizTalkHost CreateBizTalkHostInstance : TestHost host instance on server could not be created: Exception calling \"Map\" : \"Instance of the WMI class is not found. No instance was found with the specified key. This could be the result of the instance being deleted by another BizTalk Admin session.\".Exception.ToString() At C:\\Host Instances Creation Scripts for Clustered Environment\\CreateHostScript.ps1:303 char:5 + CreateBizTalkHostInstance $HostName64 $BizTalkServerName $Service ... + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : NotSpecified: (:) [Write-Error], WriteErrorException + FullyQualifiedErrorId : Microsoft.PowerShell.Commands.WriteErrorException,CreateBizTalkHostInstance ``` I also tried to create a host manually and got the following error: ``` The parameter is incorrect. (WinMgmt) ``` I googled this error and tried following but nothing worked: restart WMI service reboot BizTalk server Ran winmgmt \/verifyrepository. Result: \"wmi repository is consistent\" Ran wbemtest (but I could not figure out how to use it honestly) I cannot think of anything else apart from uninstalling whole biztalk and a fresh new install. Please suggest something that resolves this issue and saves me from uninstall & reinstall.",
    "author_id":6108,
    "publication_date":1754077038000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"manibest",
    "author_reputation":13.0,
    "tags":"installation, wmi, biztalk, hosts, biztalk-2020",
    "text_length":2605,
    "title_length":79,
    "num_tags":5
  },
  {
    "id":6669,
    "title":"Can I enforce the BigQuery table schema when transfering from PostgreSQL with Google Cloud Dataflow?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722820\/can-i-enforce-the-bigquery-table-schema-when-transfering-from-postgresql-with-go",
    "text":"I'm trying to create a job to mirror a view that I have in my PostgreSQL DB to a BigQuery table in my Google Cloud Project through Dataflow, I created the job using the \"Job builder\", and I've got the following YAML generated from it: ``` pipeline: transforms: - name: otto-bq-sbx-postgresql-source type: ReadFromPostgres config: jdbc_url: 'jdbc:postgresql:\/\/host:5432\/otto' read_query: SELECT * FROM public.\"Carlos_PowerBi\" username: user password: pass type: postgres - name: otto-bq-sbx-bigquery-update-param type: ReadFromBigQuery config: table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi' fields: - lastMessage - name: otto-bq-sbx-incremental-transform type: Sql config: query: |- SELECT * FROM input1 AS source WHERE source.\"lastMessage\" > ( SELECT COALESCE(MAX(lastMessage), TIMESTAMP '1970-01-01') FROM input0 ) input: input0: otto-bq-sbx-bigquery-update-param input1: otto-bq-sbx-postgresql-source - name: otto-bq-sbx-sink type: WriteToBigQuery input: otto-bq-sbx-incremental-transform config: table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi' num_streams: 1 ``` So, in summary, I want the job to incrementally update the BQ table's data based on the lastMessage column. Another point is that I'm connecting to the DB through a VPC, which apparently is working fine (I've correctly configured the connection in the job parameters). However, when I run the job I run into an error where JDBC tells me that it couldn't figure out the correct type of a field: ``` ValueError: Failed to decode schema due to an issue with Field proto: \"name: \\\"proposalSent\\\" type { nullable: true logical_type { urn: \\\"beam:logical_type:javasdk_bit:v1\\\" payload: \\\"\\\\202SNAPPY\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\002a\\\\301\\\\007\\\\360@\\\\254\\\\355\\\\000\\\\005sr\\\\000*org.apache.beam.sdk.io.jdbc.LogicalTypes$1%\\\\263\\\\224\\\\246%\\\\031\\\\260\\\\355\\\\002\\\\000\\\\000xr\\\\000?N9\\\\000 schemas.l\\\\t9Dtypes.PassThroughL\\\\t\\\\030\\\\001Q\\\\270\\\\210\\\\324\\\\331\\\\211\\\\313P\\\\033\\\\263\\\\002\\\\000\\\\004L\\\\000\\\\010argumentt\\\\000\\\\022Ljava\/lang\/Object;L\\\\000\\\\014a\\\\r \\\\001:\\\\034t\\\\000.Lorg\/\\\\t\\\\266\\\\000\/\\\\001\\\\266\\\\020\/sdk\/\\\\r}\\\\004\/S\\\\005\\\\205\\\\024$Field\\\\0010\\\\020;L\\\\000\\\\tf\\\\021\\\\rDq\\\\000~\\\\000\\\\003L\\\\000\\\\nidentifier6s\\\\000\\u003cString;xpt\\\\000\\\\000sr\\\\0006n\\\\346\\\\000$AutoValue_\\\\ts\\\\000_\\\\025sh9\\\\304m\\\\364S\\\\243\\\\227P\\\\002\\\\000\\\\010L\\\\000\\\\025collectionEle\\\\001\\\\346\\\\001\\\\226\\\\r\\\\211\\\\000\\\\013-+\\\\001\\\\023\\\\010t\\\\0000\\\\216\\\\331\\\\000\\\\000L9E$;L\\\\000\\\\nmapKey\\\\001@\\\\rS\\\\014\\\\014map\\\\005\\\\227\\\\035\\\\024,\\\\010metadatat\\\\000\\\\017)aXutil\/Map;L\\\\000\\\\010nullablet\\\\000\\\\023\\\\t\\\\035%~\\\\030Boolean!?\\\\010row\\\\t\\\\343\\\\010t\\\\000$\\\\212\\\\243\\\\000\\\\001T!\\\\374\\\\030Namet\\\\000-\\\\2122\\\\000\\\\000$\\\\001\\\\254\\\\001\/\\\\020;xr\\\\000,nu\\\\001\\\\t\\\\2109\\\\3360\\\\013PLl[\\\\357\\\\3103\\\\002\\\\000\\\\000xp\\\\001\\\\001\\\\014sr\\\\000\\\\036AC\\\\000.\\\\001\\\\342\\\\004.C5|Ds$EmptyMapY6\\\\024\\\\205Z\\\\334\\\\347\\\\320\\\\0053\\\\014sr\\\\000\\\\021\\\\005\/\\\\020lang.\\\\r\\\\3648\\\\315 r\\\\200\\\\325\\\\234\\\\372\\\\356\\\\002\\\\000\\\\001Z\\\\000\\\\005v!\\\\344\\\\034xp\\\\000p~r\\\\000+\\\\212\\\\234\\\\000\\\\021\\\\314\\\\000\\\\000\\\\r\\\\001\\\\000\\\\022e1\\\\000\\\\016\\\\031f\\\\014Enum\\\\r\\\\034\\\\005\\\\035(pt\\\\000\\\\006STRINGs!\\\\304\\\\020\\\\007pppp\\\\001\\\\t\\\\000\\\\020\\\\001\\\\005\\\\010\\\\022p~\\\\001\\\\007@\\\\023t\\\\000\\\\007BOOLEANt\\\\000\\\\003BIT\\\" representation { atomic_type: BOOLEAN } argument_type { atomic_type: STRING } argument { atomic_value { string: \\\"\\\" } } } } ``` I've created the BQ table matching the types of my PostgreSQL view, so I have no clue why this is happening. Also, my BQ table is currently empty, could this be a problem too? Just to make sure I'm not missing anything, here are the view types: And here are my BQ table types, matching the ones of the view: Here is my most recent attempt as a YAML: ``` pipeline: transforms: - name: otto-bq-sbx-postgresql-source type: ReadFromPostgres config: jdbc_url: 'jdbc:postgresql:\/\/host:5432\/otto' read_query: >- SELECT \"idConversa\"::INTEGER AS \"idConversa\", \"dataConversa\"::TIMESTAMP AS \"dataConversa\", \"estadoFunil\"::TEXT AS \"estadoFunil\", \"tags\"::TEXT AS \"tags\", \"propostaEnviada\"::BOOLEAN AS \"propostaEnviada\", \"termoEnviado\"::BOOLEAN AS \"termoEnviado\", \"termoAssinado\"::BOOLEAN AS \"termoAssinado\", \"idCanal\"::INTEGER AS \"idCanal\", \"nomeCanal\"::TEXT AS \"nomeCanal\", \"nomeCliente\"::TEXT AS \"nomeCliente\", \"numeroCliente\"::TEXT AS \"numeroCliente\", \"ultimaMensagem\"::TIMESTAMP AS \"ultimaMensagem\", \"infoExtra\"::TEXT AS \"infoExtra\" FROM public.\"Carlos_PowerBi\" username: user password: pass type: postgres - name: otto-bq-sbx-bigquery-update-param type: ReadFromBigQuery config: table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi' fields: - ultimaMensagem - name: otto-bq-sbx-incremental-transform type: Sql config: query: >- SELECT source.\"idConversa\"::INTEGER AS \"idConversa\", source.\"dataConversa\"::TIMESTAMP AS \"dataConversa\", source.\"estadoFunil\"::TEXT AS \"estadoFunil\", source.\"tags\"::TEXT AS \"tags\", source.\"propostaEnviada\"::BOOLEAN AS \"propostaEnviada\", source.\"termoEnviado\"::BOOLEAN AS \"termoEnviado\", source.\"termoAssinado\"::BOOLEAN AS \"termoAssinado\", source.\"idCanal\"::INTEGER AS \"idCanal\", source.\"nomeCanal\"::TEXT AS \"nomeCanal\", source.\"nomeCliente\"::TEXT AS \"nomeCliente\", source.\"numeroCliente\"::TEXT AS \"numeroCliente\", source.\"ultimaMensagem\"::TIMESTAMP AS \"ultimaMensagem\", source.\"infoExtra\"::TEXT AS \"infoExtra\" FROM input1 AS source WHERE source.\"ultimaMensagem\" ( SELECT COALESCE(MAX(\"ultimaMensagem\"), TIMESTAMP '1970-01-01') FROM input0 ) input: input0: otto-bq-sbx-bigquery-update-param input1: otto-bq-sbx-postgresql-source - name: otto-bq-sbx-sink type: WriteToBigQuery input: otto-bq-sbx-incremental-transform config: table: 'migracao-zrp:otto_bq_sbx.Carlos_PowerBi' num_streams: 1 ``` It's still a mystery to me why this is still struggling with the types, since they are all well-defined.",
    "author_id":4528,
    "publication_date":1754077341000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Gustavo Trivelatto",
    "author_reputation":29.0,
    "tags":"postgresql, google-bigquery, jdbc, google-cloud-dataflow",
    "text_length":5746,
    "title_length":100,
    "num_tags":4
  },
  {
    "id":6668,
    "title":"Flutter inspector forward me to source codes of flutter instead of my codes",
    "link":"https:\/\/stackoverflow.com\/questions\/79722827\/flutter-inspector-forward-me-to-source-codes-of-flutter-instead-of-my-codes",
    "text":"I use vs code. ``` [✓] Flutter (Channel stable, 3.32.8, on macOS 15.1.1 24B91 darwin-arm64, locale en-GB) [734ms] • Flutter version 3.32.8 on channel stable at \/Users\/me\/Documents\/flutter • Upstream repository https:\/\/github.com\/flutter\/flutter.git • Framework revision edada7c56e (7 days ago), 2025-07-25 14:08:03 +0000 • Engine revision ef0cd00091 • Dart version 3.8.1 • DevTools version 2.45.1 ``` I open my app and enable the inspector. I clicked a text, Inspector goes to text.dart instead of my component like ``` Text(\"something\") ``` or I click on a card from my code, it goes card.dart.",
    "author_id":6107,
    "publication_date":1754077711000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"senabdulgani",
    "author_reputation":11.0,
    "tags":"flutter, flutter-devtools",
    "text_length":595,
    "title_length":75,
    "num_tags":2
  },
  {
    "id":6667,
    "title":"On passing json stringify data to controller method shows null",
    "link":"https:\/\/stackoverflow.com\/questions\/79722828\/on-passing-json-stringify-data-to-controller-method-shows-null",
    "text":"I am trying to post my form data using ajax. I am serializing the form data before posting. But at the controller method, it shows null. There is no problem with the form data because I can pass data to controller when I do, ``` var formData_ = $(this).serialize(); ``` ``` $('#i_finish_form').submit(function(e) { e.preventDefault(); let formData = { name: $('#i_finish_name').val(), remarks1: $('#i_bun_num').val() }; $.ajax({ url: \"{{ route('admin_finish_str.store') }}\", type: \"POST\", contentType: 'application\/json; charset=utf-8', dataType: 'JSON', data: JSON.stringify(formData), success: function(res) { if (res.status == 200) { $('#i_add_finish_hid_div_form').toggle(); $('#i_finish_form').trigger(\"reset\"); fetchData(); } } }); }); ``` My controller method, ``` public function store(Request $request) { return response()->json(['status'=>'error', 'error'=> $request->n_finish_name]); } ``` Json response is null in this case. Why is it so?",
    "author_id":5355,
    "publication_date":1754077715000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"user4221591",
    "author_reputation":2238.0,
    "tags":"javascript, ajax, laravel-12, jquery, ajaxform",
    "text_length":950,
    "title_length":62,
    "num_tags":5
  },
  {
    "id":6666,
    "title":"Kotlin Bluetooth connection - no up to date tutorial",
    "link":"https:\/\/stackoverflow.com\/questions\/79722829\/kotlin-bluetooth-connection-no-up-to-date-tutorial",
    "text":"I’ve tried working through the current tutorial for Bluetooth Classic integration. The problem is that nothing works. Some of the methods listed are marked as “deprecated,” then it seems that code is missing in places, but the biggest issue is that I can’t even compile the examples because, since a certain Android version, Bluetooth connections require explicit runtime permission requests—which the tutorial doesn’t mention at all. I’ve scoured the web, but most other tutorials and videos still target older Android releases, so they use the same outdated examples. On GitHub there’s an official reference implementation—and even that warns it’s outdated and directs you to the “Samples” page. I did that, but it just loops me back to the same outdated tutorial. I’m completely stuck. Where can I find a solid, up-to-date guide? Tutorial I am working with: https:\/\/developer.android.com\/develop\/connectivity\/bluetooth\/find-bluetooth-devices Sample implementation by Google (also deprecated): https:\/\/github.com\/android\/connectivity-samples\/tree\/main\/BluetoothChat",
    "author_id":6106,
    "publication_date":1754077731000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"CodeCannibal",
    "author_reputation":338.0,
    "tags":"android, kotlin, bluetooth",
    "text_length":1067,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":6665,
    "title":"Disable WooCommerce REST API product write permission only",
    "link":"https:\/\/stackoverflow.com\/questions\/79722834\/disable-woocommerce-rest-api-product-write-permission-only",
    "text":"I need help to disable WooCommerce REST API product write permission only. I have generated REST API for my software, but I want to remove only product write permission. It will be read only for that REST API and use all other permission will be default. Please help me to do this. ``` add_filter('woocommerce_rest_check_permissions', 'custom_restrict_product_api_write', 10, 4); function custom_restrict_product_api_write($permission, $context, $object_id, $post_type) { \/\/ Restrict only for products and write operations (create\/update\/delete) if ($post_type === 'product' && in_array($context, ['edit', 'delete'])) { \/\/ You can customize this condition to restrict only certain API users\/roles return false; \/\/ Deny permission } \/\/ Allow all others return $permission; } ```",
    "author_id":6105,
    "publication_date":1754077985000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Rifat Rahman",
    "author_reputation":179.0,
    "tags":"woocommerce, woocommerce-rest-api",
    "text_length":777,
    "title_length":58,
    "num_tags":2
  },
  {
    "id":6664,
    "title":"Exchange Web Services Email Search Not Filtering (returns all emails)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722835\/exchange-web-services-email-search-not-filtering-returns-all-emails",
    "text":"Querying with EWS against an Office 365 mailbox, I have the following search: ``` var ewsSearchString = \"Sent:0001\/01\/01..\" + DateTime.Now.AddYears(-1).ToString(\"yyyy\/MM\/dd\"); sentItems = service.FindItems(WellKnownFolderName.SentItems, ewsSearchString, view); ``` Up until this week, that would get every email more than a year old. This week, it returns every email in the mailbox. I've tried changing it to all kinds of different permutations (Sent:< for example, I even tried Body:\"asdf\" which shouldn't find anything), and even filters that shouldn't return anything. They all return every email in the mailbox. All I can really guess at this point is that Microsoft broke something in a recent update to the cloud servers, but is anyone else running into this? Any suggestions aside from \"migrate to Graph\", which is something I'm pushing with management to get going on already. Thanks.",
    "author_id":6104,
    "publication_date":1754078035000,
    "scraped_at":1754660261000,
    "scrape_method":"api",
    "author_name":"Tridus",
    "author_reputation":5401.0,
    "tags":"c#, exchangewebservices",
    "text_length":893,
    "title_length":69,
    "num_tags":2
  },
  {
    "id":6663,
    "title":"Count(*) query returns empty when using Tez, but works with MapReduce",
    "link":"https:\/\/stackoverflow.com\/questions\/79722840\/count-query-returns-empty-when-using-tez-but-works-with-mapreduce",
    "text":"I have hadoop + hive setup using docker, however when I try to run count(*) on my table it gives me an empty return when using Tez and the correct one when using MapReduce, the table is an external table stored as parquets, the parquets were created with PyArrow and I've made sure the schema matches the format of the table on Hive, below is my query plus some of the logs ``` jdbc:hive2:\/\/localhost:10000\/default> SELECT COUNT(*) FROM market_data WHERE ric = 'WINc1' AND year = 2024 AND month = 1 AND day = 2; INFO : File System Counters: INFO : FILE_BYTES_READ: 108 INFO : FILE_BYTES_WRITTEN: 228 INFO : FILE_READ_OPS: 0 INFO : FILE_LARGE_READ_OPS: 0 INFO : FILE_WRITE_OPS: 0 INFO : HDFS_BYTES_READ: 1440097729 INFO : HDFS_BYTES_WRITTEN: 0 INFO : HDFS_READ_OPS: 1096 INFO : HDFS_LARGE_READ_OPS: 0 INFO : HDFS_WRITE_OPS: 0 INFO : org.apache.tez.common.counters.TaskCounter: INFO : SPILLED_RECORDS: 0 INFO : NUM_SHUFFLED_INPUTS: 0 INFO : NUM_FAILED_SHUFFLE_INPUTS: 0 INFO : GC_TIME_MILLIS: 520 INFO : CPU_MILLISECONDS: 44910 INFO : WALL_CLOCK_MILLISECONDS: 120244 INFO : PHYSICAL_MEMORY_BYTES: 16314269696 INFO : VIRTUAL_MEMORY_BYTES: 70690332672 INFO : COMMITTED_HEAP_BYTES: 16314269696 INFO : INPUT_RECORDS_PROCESSED: 255910 INFO : INPUT_SPLIT_LENGTH_BYTES: 1440098825 INFO : OUTPUT_RECORDS: 12 INFO : APPROXIMATE_INPUT_RECORDS: 12 INFO : OUTPUT_LARGE_RECORDS: 0 INFO : OUTPUT_BYTES: 54 INFO : OUTPUT_BYTES_WITH_OVERHEAD: 150 INFO : OUTPUT_BYTES_PHYSICAL: 486 INFO : ADDITIONAL_SPILLS_BYTES_WRITTEN: 0 INFO : ADDITIONAL_SPILLS_BYTES_READ: 0 INFO : ADDITIONAL_SPILL_COUNT: 0 INFO : SHUFFLE_BYTES: 0 INFO : SHUFFLE_BYTES_DECOMPRESSED: 0 INFO : SHUFFLE_BYTES_TO_MEM: 0 INFO : SHUFFLE_BYTES_TO_DISK: 0 INFO : SHUFFLE_BYTES_DISK_DIRECT: 0 INFO : SHUFFLE_PHASE_TIME: 3220 INFO : FIRST_EVENT_RECEIVED: 36 INFO : LAST_EVENT_RECEIVED: 3220 INFO : DATA_BYTES_VIA_EVENT: 198 INFO : HIVE: INFO : CREATED_FILES: 1 INFO : DESERIALIZE_ERRORS: 0 INFO : RECORDS_IN_Map_1: 261916407 INFO : RECORDS_OUT_0: 1 INFO : RECORDS_OUT_INTERMEDIATE_Map_1: 18 INFO : RECORDS_OUT_INTERMEDIATE_Reducer_2: 0 INFO : RECORDS_OUT_OPERATOR_FIL_8: 11171207 INFO : RECORDS_OUT_OPERATOR_FS_13: 1 INFO : RECORDS_OUT_OPERATOR_GBY_10: 12 INFO : RECORDS_OUT_OPERATOR_GBY_12: 1 INFO : RECORDS_OUT_OPERATOR_MAP_0: 0 INFO : RECORDS_OUT_OPERATOR_RS_11: 18 INFO : RECORDS_OUT_OPERATOR_SEL_9: 11171207 INFO : RECORDS_OUT_OPERATOR_TS_0: 261916407 INFO : org.apache.hadoop.hive.ql.exec.tez.HiveInputCounters: INFO : GROUPED_INPUT_SPLITS_Map_1: 12 INFO : INPUT_DIRECTORIES_Map_1: 22 INFO : INPUT_FILES_Map_1: 274 INFO : RAW_INPUT_SPLITS_Map_1: 274 INFO : Completed executing command(queryId=hive_20250730142019_de067ec2-e26a-4085-8bde-0b98eb7e80fd); Time taken: 27.824 seconds +------+ | _c0 | +------+ +------+ No rows selected (27.926 seconds) ``` What's really interesting to me is the line 'RECORDS_OUT_OPERATOR_SEL_9: 11171207', this is the correct number of rows, so Tez actually did the count, but still returned nothing (???) I'm sorry if the question is too long, also I'm quite new to Hive and so it could be that I'm making a silly mistake. If more information is needed I'd be happy to provide and thanks for anyone for reading so far. Versions: Hive 4.0.0 Hadoop 3.4.1 Tez 0.10.3",
    "author_id":6103,
    "publication_date":1754078540000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Pedro Novaes",
    "author_reputation":21.0,
    "tags":"hdfs, hive, apache-tez",
    "text_length":3243,
    "title_length":69,
    "num_tags":3
  },
  {
    "id":6662,
    "title":"Usage of method onUpdateReceived from org.telegram.telegrambots.bots.TelegramWebhookBot",
    "link":"https:\/\/stackoverflow.com\/questions\/79722846\/usage-of-method-onupdatereceived-from-org-telegram-telegrambots-bots-telegramweb",
    "text":"Can anyone explain what is ``` BotApiMethod<?> onWebhookUpdateReceived(Update update) ``` method used for? I am developing a telegram webhook bot in java spring boot and i have been using ``` @RestController ``` successfully to process ``` Update ``` data. Now i am interested in wether i can process ``` Update ``` data just by using ``` BotApiMethod<?> onWebhookUpdateReceived(Update update) ``` without a controller? If not, what is the purpose of that method at all? I have tryed removing the ``` @RestController ``` and responding to updates in ``` onWebhookUpdateReceived ``` but ngrok logs show that responce is always 404. Details: I have set webhook adress using https:\/\/api.telegram.org\/botmy-token\/setWebhook?url=https:\/\/my-url.ngrok-free.app\/webhook request I have been able to successfully proceed updates using: ``` @RestController @RequestMapping(\"\/webhook\") public class BotController { @PostMapping public void onUpdateReceived(@RequestBody Update update){ \/\/ more code here } } ``` I have tried several pathes for the return value of ``` public String getBotPath() ``` in attempts of using ``` onUpdateReceived ``` method in the bot class. Like: \/webhook , https:\/\/my-url.ngrok-free.app\/webhook and so on but noone succeed",
    "author_id":6102,
    "publication_date":1754079061000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Анатолий",
    "author_reputation":3.0,
    "tags":"java, spring-boot, telegram-bot, telegram-api",
    "text_length":1240,
    "title_length":87,
    "num_tags":4
  },
  {
    "id":6661,
    "title":"Chart.js chart looks blurry even after setting devicePixelRatio: 4 and resizing\/zooming in editor",
    "link":"https:\/\/stackoverflow.com\/questions\/79722849\/chart-js-chart-looks-blurry-even-after-setting-devicepixelratio-4-and-resizing",
    "text":"I'm using Chart.js in a custom editor where users can resize the chart and zoom in\/out on the canvas. I'm noticing that the chart appears blurry or pixelated, especially after resizing actions or zoom interactions. To try and fix this, I've explicitly set devicePixelRatio: 4 in the chart config, but it doesn’t seem to help. What I’ve tried: Setting devicePixelRatio: 4 in chart options Ensuring the canvas has correct CSS width and height Key Observations: Blur happens more after zooming in or resizing the chart within the editor. Question: Has anyone else experienced blurry charts in Chart.js despite setting a high devicePixelRatio? What’s the correct way to handle resizing and zooming in a dynamic editor without losing clarity? Are there best practices or internal canvas tricks I might be missing?",
    "author_id":6101,
    "publication_date":1754079303000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Prince kumar",
    "author_reputation":9.0,
    "tags":"reactjs, javascript, chart.js, canvas",
    "text_length":808,
    "title_length":97,
    "num_tags":4
  },
  {
    "id":6660,
    "title":"excel vba how to enter the Unicode Character “Δ” (U+0394)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722853\/excel-vba-how-to-enter-the-unicode-character-%ce%94-u0394",
    "text":"I am struggling to enter this character into a find string eg ``` Call sf_replace(sf_rg1, \"Tightness \" & Chr(349) & \"P\", Chr(349) & \"P\") ``` and found this page https:\/\/stackoverflow.com\/questions\/57742453\/utf-troubleshooting-inequality-signs-%e2%89%a0\/57742825#57742825 So I wonder if I have to hack the registry or is there a simpler way?",
    "author_id":6100,
    "publication_date":1754079511000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"theHeatingEngineer",
    "author_reputation":19.0,
    "tags":"string, vba, excel, character",
    "text_length":340,
    "title_length":57,
    "num_tags":4
  },
  {
    "id":6659,
    "title":"Using CommunityToolkit.Camera, how can I save picture in a particular folder?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722863\/using-communitytoolkit-camera-how-can-i-save-picture-in-a-particular-folder",
    "text":"I'm using VS2022 and SQLite to develop an app to take pictures of employees. I'm using CommunityToolkit.Camera. I tried an example on YouTube, It functions very well, but I don't know how to get the captured image's path, and save the image in a specified folder. ``` <ContentPage x:Class=\"MauiAppSyncTutor.Pages.PageEmployes\" xmlns=\"http:\/\/schemas.microsoft.com\/dotnet\/2021\/maui\" xmlns:x=\"http:\/\/schemas.microsoft.com\/winfx\/2009\/xaml\" xmlns:toolkit=\"http:\/\/schemas.microsoft.com\/dotnet\/2022\/maui\/toolkit\" Title=\"PageEmployes\"> <StackLayout> <Image x:Name=\"imgEmp\" Aspect=\"AspectFit\" HeightRequest=\"150\" \/> <toolkit:CameraView x:Name=\"MyCamera\" HeightRequest=\"150\" MediaCaptured=\"MyCamera_MediaCaptured\" \/> <Button x:Name=\"butTakePic\" Margin=\"60,0,60,0\" BackgroundColor=\"#edd72f\" BorderColor=\"#8ac4fc\" BorderWidth=\"5\" Clicked=\"butTakePic_Clicked\" FontAttributes=\"Bold\" Text=\"Smille\" TextColor=\"Black\" \/> <\/StackLayout> <\/ContentPage> ``` Code Behind ``` private void MyCamera_MediaCaptured(object sender, CommunityToolkit.Maui.Core.MediaCapturedEventArgs e) { if (Dispatcher.IsDispatchRequired) { Dispatcher.Dispatch(() => imgEmp.Source = ImageSource.FromStream(() => e.Media)); return; } imgEmp.Source = ImageSource.FromStream(() => e.Media); } private async void butTakePic_Clicked(object sender, EventArgs e) { await MyCamera.CaptureImage(CancellationToken.None); } ``` My question: How save the image in a specified folder? Is there a way to get the captured image's path? Then I could copy that file to the desired location.",
    "author_id":6099,
    "publication_date":1754080987000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"formula",
    "author_reputation":21.0,
    "tags":"c#, android, .net, visual-studio-2022, maui",
    "text_length":1529,
    "title_length":77,
    "num_tags":5
  },
  {
    "id":6658,
    "title":"Avoiding &quot;...is not a valid element name (element-name)&quot; error in Phoenix Code?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722864\/avoiding-is-not-a-valid-element-name-element-name-error-in-phoenix-code",
    "text":"In Phoenix Code I get dozens of flagged errors like \"'my-customTag' is not a valid element name (element-name)\". The code runs just fine, with CSS and JS interactions, but I'd like to 'clean up' my error list of clutter... I understood that in HTML5 adding a hyphen into the name clears this up, but at least Phoenix Code doesn't accept it. Is there something I can add into the header that will clear this issue up?",
    "author_id":6098,
    "publication_date":1754081126000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"URi613",
    "author_reputation":41.0,
    "tags":"element",
    "text_length":416,
    "title_length":89,
    "num_tags":1
  },
  {
    "id":6657,
    "title":"TYPO3 Fluid: Condition, if a link target is an active page?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722872\/typo3-fluid-condition-if-a-link-target-is-an-active-page",
    "text":"I have a TYPO3 Fluid code like this: ``` <f:if condition=\"{data.header_link.url}\"> <f:then> <f:link.typolink parameter=\"{data.header_link}\">My Link<\/f:link.typolink> <\/f:then> <f:else> Sorry, no link <\/f:else> <\/f:if> ``` Problem: When a link is set, but the target page is disabled, the condition is true and I get \"My Link\" (just text without link) as output. Is there a condition to address that issue? TYPO3 13.4.15 Thanks!",
    "author_id":4820,
    "publication_date":1754081575000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"t3fan",
    "author_reputation":82.0,
    "tags":"typo3, typo3-13.x",
    "text_length":427,
    "title_length":59,
    "num_tags":2
  },
  {
    "id":6656,
    "title":"How to Setup Python Scripts so other users can run it?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722879\/how-to-setup-python-scripts-so-other-users-can-run-it",
    "text":"I'm wondering what would be the best structure\/setup to run my python scripts for other users? I have python scripts that modify\/automate excel files using xlwings. For learning purposes and to get better I tried setting it up using FastAPI. So currently, I manually launch uvicorn through cmd and it launches a fastapi app that I can access via localhost. The site just accepts a file path and some extra information and when I hit run it runs a subprocess that runs another python script that actually does all the xlwings\/excel work. This works fine for myself but I'm now trying to get my teammates to use it too. If I manually launch uvicorn they can access the site and run it but I want to decouple having to manually launch it myself (and also not have excel launch under my user every time someone uses it) I have tried setting it to run as a windows service but apparently that runs it on \"Session 0\" and excel can't run there because it doesn't have desktop access. I'm looking for ideas to make this work for my team, and if I need to completely change the architecture I don't mind, just looking for any ideas.",
    "author_id":6097,
    "publication_date":1754082109000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dugan",
    "author_reputation":1.0,
    "tags":"python, fastapi, uvicorn",
    "text_length":1123,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6655,
    "title":"403 from https:\/\/pdftron-maven.s3.amazonaws.com during Android build",
    "link":"https:\/\/stackoverflow.com\/questions\/79722880\/403-from-https-pdftron-maven-s3-amazonaws-com-during-android-build",
    "text":"``` Android Gradle Plugin: 8.8.2 Gradle: 8.10.2 React Native: 0.79.5 (Expo SDK 53) > Configure project :react-native-reanimated FAILURE: Build failed with an exception. * What went wrong: Could not determine the dependencies of task ':app:buildReleasePreBundle'. > Could not resolve all dependencies for configuration ':app:releaseRuntimeClasspath'. > Could not resolve com.facebook.android:facebook-android-sdk:16.+. Required by: project :app > project :react-native-fbsdk-next > Unable to load Maven meta-data from https:\/\/pdftron-maven.s3.amazonaws.com\/release\/com\/facebook\/android\/facebook-android-sdk\/maven-metadata.xml (HTTP 403 Forbidden) ``` Is the pdftron-maven S3 repository currently restricted or misconfigured? Gradle can’t fetch facebook-android-sdk (and any other transitive deps) because every request to that URL returns 403 Forbidden. Do I need to update the repository URL, add credentials, or switch to a different mirror?",
    "author_id":6096,
    "publication_date":1754082187000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Giorgio Arlandi",
    "author_reputation":19.0,
    "tags":"android, react-native, pdftron",
    "text_length":942,
    "title_length":68,
    "num_tags":3
  },
  {
    "id":6654,
    "title":"Azure Synapse SQL Merge is not updating records, instead of that it inserts matching records using spark.sql",
    "link":"https:\/\/stackoverflow.com\/questions\/79722887\/azure-synapse-sql-merge-is-not-updating-records-instead-of-that-it-inserts-matc",
    "text":"I have the below code where the Id is a 36 character GUID. The code gets executed but when a matching record is found , instead of updating it inserts the entire records again. What could be the root cause for this? ``` delta_table.alias(\"target\").merge( deduped_df.alias(\"source\"), \"trim(upper(target.Id)) = trim(upper(source.dId)) \" ).whenMatchedUpdate( set={ \"Id\" : \"source.dId\", \"EntityId\" : \"source.EntityId\", \"PropertyName\" : \"source.PropertyName\", \"ValueString\":\"source.ValueString\", \"ValueInt\" : \"source.ValueInt\", \"ValueDecimal\" : \"source.ValueDecimal\", \"ValueBit\" : \"source.ValueBit\", \"ValidFrom\" : \"source.ValidFrom\", \"ValidTo\" : \"source.ValidTo\", \"Description\" : \"source.Description\", \"ModifiedBy\" : \"source.ModifiedBy\", \"CreatedAt\" : \"source.CreatedAt\", \"CreatedBy\" : \"source.CreatedBy\", \"Active\" : \"source.Active\", \"Saved\" : \"source.Saved\", \"ETL_UpdateDate\" : \"source.ETL_UpdateDate\", \"ETL_Source\" : \"source.ETL_Source\" }).whenNotMatchedInsert(values={ \"Id\" : \"source.dId\", \"EntityId\" : \"source.EntityId\", \"PropertyName\" : \"source.PropertyName\", \"ValueString\":\"source.ValueString\", \"ValueInt\" : \"source.ValueInt\", \"ValueDecimal\" : \"source.ValueDecimal\", \"ValueBit\" : \"source.ValueBit\", \"ValidFrom\" : \"source.ValidFrom\", \"ValidTo\" : \"source.ValidTo\", \"Description\" : \"source.Description\", \"ModifiedBy\" : \"source.ModifiedBy\", \"CreatedAt\" : \"source.CreatedAt\", \"CreatedBy\" : \"source.CreatedBy\", \"Active\" : \"source.Active\", \"Saved\" : \"source.Saved\", \"ETL_UpdateDate\" : \"source.ETL_UpdateDate\", \"ETL_LoadDate\" : \"source.ETL_LoadDate\", \"ETL_Source\" : \"source.ETL_Source\" }).execute() ```",
    "author_id":6095,
    "publication_date":1754082763000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Sandeep T",
    "author_reputation":451.0,
    "tags":"pyspark, apache-spark, apache-spark-sql, azure-synapse, delta-lake",
    "text_length":1595,
    "title_length":108,
    "num_tags":5
  },
  {
    "id":6653,
    "title":"Is this the correct way to connect to an AIDL service?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722889\/is-this-the-correct-way-to-connect-to-an-aidl-service",
    "text":"I'm trying to connect to an AIDL service for the first time, and I'm afraid that I might be doing something wrong, even though, on the surface level, everything seems to be working fine. Am I doing any mistake that I'm not aware of? ``` static std::shared_ptr<IMyService> service; void connect_to_service() { constexpr const char* serviceName{\"IMyService\"}; static std::mutex mut{}; std::lock_guard<std::mutex> lock{mut}; ALOGI(\"Connecting to %s...\", serviceName); while (true) { const ndk::SpAIBinder binder{AServiceManager_waitForService(serviceName)}; static const ndk::ScopedAIBinder_DeathRecipient deathRecipient{ AIBinder_DeathRecipient_new([](void*) { const std::uint8_t reconnectionTimeoutS{1}; ALOGW(\"Binder death recipient triggered: service %s has died. Reconnecting in %d seconds\", serviceName, reconnectionTimeoutS); sleep(reconnectionTimeoutS); connect_to_service(); }) }; const binder_status_t linkToDeathStatus = AIBinder_linkToDeath(binder.get(), deathRecipient.get(), nullptr); if (linkToDeathStatus != STATUS_OK) { ALOGW(\"Could not register binder death recipient. Status code: %d\", linkToDeathStatus); } else { ALOGI(\"Successfully registered binder death recipient\"); service = IMyService::fromBinder(binder); break; } } ALOGI(\"Connected to %s\", serviceName); } ```",
    "author_id":6094,
    "publication_date":1754082972000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"136",
    "author_reputation":1227.0,
    "tags":"c++, android, android-ndk, aidl",
    "text_length":1285,
    "title_length":54,
    "num_tags":4
  },
  {
    "id":6652,
    "title":"Not all files have gcno with bazel --coverage",
    "link":"https:\/\/stackoverflow.com\/questions\/79722891\/not-all-files-have-gcno-with-bazel-coverage",
    "text":"NB I have the custom targets which run by python scripts. So I couldn't use ``` bazel coverage ``` directly for this one. 1. I defined in ``` .bazelrc ``` ``` build:coverage --collect_code_coverage build:coverage --instrumentation_filter=\"^\/\/src[\/:]\" coverage --instrumentation_filter=\"^\/\/src[\/:]\" ``` In the root BUILD ``` config_setting( name = \"Coverage\", flag_values = { \":coverage\": \"true\", }, ) bool_flag( name = \"coverage\", build_setting_default = False, ) ``` In the ``` cc_binary ``` wrapper I use ``` + select({\"\/\/:Coverage\": coverageCopts, \"\/\/conditions:default\": [], }), \/\/ and select({\"\/\/:Coverage\": coverageLopts, \"\/\/conditions:default\": [], }), ``` where ``` coverageCopts = [\"--coverage\", \"-fprofile-abs-path\", \"-fprofile-arcs\", \"-ftest-coverage\"] coverageLopts = [\"--coverage\", \"-lgcov\"] ``` and run this python script (from bazel) like this: ``` bazel run --subcommands --\/\/\\:coverage=true script:runner ``` But for some mystical reasons I have a lack of ``` gcno ``` files. although there are these ``` gcno ``` 's for neighboring files from the same library. Grepping for ``` bazel --subcommands ``` indicates the presences of ``` --coverage ``` flag for this cpp file. I did ``` bazel clean --expunge ``` - same result.",
    "author_id":6093,
    "publication_date":1754083086000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ulrich Von Rekkenin",
    "author_reputation":372.0,
    "tags":"code-coverage, bazel",
    "text_length":1240,
    "title_length":45,
    "num_tags":2
  },
  {
    "id":6651,
    "title":"Easier navigation for a single dynamic sheet in Google Sheets",
    "link":"https:\/\/stackoverflow.com\/questions\/79722896\/easier-navigation-for-a-single-dynamic-sheet-in-google-sheets",
    "text":"I have a Google Sheets worksheet that is produced by a Google Form that is constantly being updated. Rows are added constantly as the form is filled and there is a script that sorts the sheet when it is opened. I added a number of rows manually that serve as section headers and the sorting places them at the beginning of the respective rows depending on the input of one of the fields that is a radio button list. The sheet can become quite long and I'm looking for an easy way to navigate this sheet. The sheet contains section header rows and I would like to create something like a Table of contents similar to Google Docs to quickly jump to the respective header. The section rows change when data is entered so I cannot create links to particular rows. The following is an example sheet structure: Field 1 Field 2, etc. First header row Data row Data row Data row Second header row Data row Data row Third header row Data row Data row The problem is that with the increase in sheet length it becomes increasingly painstaking to scroll down to one particular section. Is there a way to simplify sheet navigation?",
    "author_id":6092,
    "publication_date":1754083491000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"To Do",
    "author_reputation":137.0,
    "tags":"google-sheets",
    "text_length":1118,
    "title_length":61,
    "num_tags":1
  },
  {
    "id":6650,
    "title":"Calculate Median in KQL when number of values is even",
    "link":"https:\/\/stackoverflow.com\/questions\/79722897\/calculate-median-in-kql-when-number-of-values-is-even",
    "text":"I am trying to calculate a median using the percentile KQL function like so: ``` datatable(number:real) [30.5,35.2,36.9,40.6] | summarize Median=toreal(percentile(number, 50)) ``` The above results in 35.2. However, my expectation would be 36.05 (average of two center values). Is there an easy way to achieve my expected outcome?",
    "author_id":6091,
    "publication_date":1754083604000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"PorgtheEaten",
    "author_reputation":194.0,
    "tags":"azure-data-explorer, kql, kusto-explorer",
    "text_length":330,
    "title_length":53,
    "num_tags":3
  },
  {
    "id":6649,
    "title":"Cannot link to just libglfw.so (Have to include libglfw.so.3 as well)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722901\/cannot-link-to-just-libglfw-so-have-to-include-libglfw-so-3-as-well",
    "text":"I am making a small application with glfw. I'm not using CMake and instead opted to make a custom build system for more control, but I'm getting an error when trying to run my program: error while loading shared libraries: libglfw.so.3: cannot open shared object file: No such file or directory I know that shared objects are typically used for system packages that multiple applications will be relying on, and thus they have versions at the end (i.e. ``` .so.3 ``` or ``` .so.3.5 ``` in glfw), but I will be distributing my application with GLFW built in. For example, when I build my app, I have the following binaries built in the ``` dist\/ ``` folder: ``` dist\/ app libengine.so libglfw.so libimgui.so libbackend.so ``` As I'd rather not force users to install glfw system-wide just to try out my app. For this reason, I'd like to avoid having both ``` libglfw.so ``` and ``` libglfw.so.3 ``` . When I try and replace ``` libglfw.so ``` with ``` libglfw.so.3 ``` in the ``` dist\/ ``` folder, I get a new compile-time error saying \/usr\/bin\/ld: cannot find -lglfw: No such file or directory Having both ``` libglfw.so ``` and ``` libglfw.so.3 ``` obviously increases the total app size, which is also undesirable. And making a symlink makes it much more difficult to distribute. For this reason, I'd like to just keep my current setup with ``` dist\/ ``` including only ``` libglfw.so ``` . But I can't get it to work for some reason (my ``` -L ``` path is just my ``` dist\/ ``` directory, so I don't know why it's even looking for a ``` libglfw.so.3 ``` ). Does anyone know how to prevent my app from looking for a ``` libglfw.so.3 ``` file when being run?",
    "author_id":6090,
    "publication_date":1754083897000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Joe",
    "author_reputation":53.0,
    "tags":"c, linker, linker-errors, glfw, shared-libraries",
    "text_length":1659,
    "title_length":69,
    "num_tags":5
  },
  {
    "id":6648,
    "title":"Why can a trivially relocatable type not be an implicit lifetime type?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722904\/why-can-a-trivially-relocatable-type-not-be-an-implicit-lifetime-type",
    "text":"Consider the following code: ``` #include <type_traits> struct A trivially_relocatable_if_eligible final { A() {} A(A const&) = delete; A& operator=(A const&) = delete; ~A() {} }; static_assert(std::is_implicit_lifetime_v<A>); \/\/ true in clang-21.1.0-rc2 ``` However, as per the latest C++ standard n5008 ( 11.2\/p16 [class.prop] ): A class S is an implicit-lifetime class if (16.1) it is an aggregate whose destructor is not user-provided or (16.2) it has at least one trivial eligible constructor and a trivial, non-deleted destructor. ``` std::is_implicit_lifetime_v<A> ``` should be ``` false ``` , because ``` A ``` doesn't satisfy ``` (16.1) ``` or ``` (16.2) ``` , and thus is not an implicit lifetime type! Note that it is necessary to call ``` std::start_lifetime_as ``` 1 after a trivial relocation in some cases 2 , so I think trivially relocatable types and replaceable types should both be defined as implicit lifetime types (as clang-21.1.0-rc2 does), but the C++ standard seems not to do so. Is it an oversight by the C++ standard? or just my misunderstanding? 1 ``` std::start_lifetime_as ``` requires the related type must be an implicit lifetime type. 2 https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2025\/p2786r13.html",
    "author_id":5016,
    "publication_date":1754084326000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"xmllmx",
    "author_reputation":44278.0,
    "tags":"c++, c++26, language-lawyer, lifetime, type-traits",
    "text_length":1241,
    "title_length":70,
    "num_tags":5
  },
  {
    "id":6647,
    "title":"Why does this LibraryImport fail to compile in .NET?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722905\/why-does-this-libraryimport-fail-to-compile-in-net",
    "text":"The following code will compile in Visual Studio, but if I try to compile it on dotnetfiddle.net (which presumably uses dotnet as it's a Linux execution environment), I get the following error: Compilation error (line 61, col 42): Partial method 'Win32.ReOpenFile(nint, uint, uint, uint)' must have an implementation part because it has accessibility modifiers The only way I can get it to compile on dotnetfiddle is to remove ``` partial ``` and mark ReOpenFile as ``` extern ``` . However, that does not compile in Visual Studio, producing the following error: Method 'ReOpenFile' should be 'static', 'partial', and non-generic when marked with 'LibraryImportAttribute'. P\/Invoke source generation will ignore method 'ReOpenFile' In both environments I am using .NET 9. ``` internal static partial class Win32 { [LibraryImport(\"kernel32.dll\", SetLastError = true)] public static partial SafeFileHandle ReOpenFile( IntPtr hOriginalFile, uint dwDesiredAccess, uint dwShareMode, uint dwFlagsAndAttributes ); public const uint GENERIC_READ = 0x80000000; public const uint GENERIC_WRITE = 0x40000000; public const uint FILE_SHARE_READ = 0x00000001; public const uint FILE_SHARE_WRITE = 0x00000002; public const uint FILE_FLAG_OVERLAPPED = 0x40000000; } ```",
    "author_id":5887,
    "publication_date":1754084500000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Chris_F",
    "author_reputation":5731.0,
    "tags":"c#, .net-9.0, .net-fiddle",
    "text_length":1253,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":6646,
    "title":"How do I get CLion to prefer T const&amp; over const T&amp;?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722910\/how-do-i-get-clion-to-prefer-t-const-over-const-t",
    "text":"CLion supports a \"code inspection\" named \"pass value parameters by const&\", which suggests you prefer passing by const& rather than by-value with a copy. When you accept the suggestion, your parameter type ``` T ``` is replaced by ``` const T& ``` . However - I prefer the \"east const\" to the \"const west\" style, so I write ``` T const& ``` (or ``` T const & ``` ). How can I get CLion to make the correction the way I prefer? I found the inspection under File > Settings > Editor > Inspection > C\/C++ - but I could not find a UI control for doing what I want.",
    "author_id":4608,
    "publication_date":1754085054000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"einpoklum",
    "author_reputation":135725.0,
    "tags":"c++, clion, constants, code-inspection",
    "text_length":560,
    "title_length":60,
    "num_tags":4
  },
  {
    "id":6645,
    "title":"MySQL select data from multiple tables with table names as reference",
    "link":"https:\/\/stackoverflow.com\/questions\/79722911\/mysql-select-data-from-multiple-tables-with-table-names-as-reference",
    "text":"I want to retrieve all data from 3 tables with common column names. But I want the table names as reference too in another column. All the tables have same column names, i.e., Name , Address , Mobile and Email . I tried this code which gave me error. ``` SELECT *, TABLE_NAME AS id FROM table1 UNION SELECT *, TABLE_NAME AS id FROM table2 UNION SELECT *, TABLE_NAME AS id FROM table3 ``` I had expected this output to show: Name Address Mobile Email id Name1 address1 mobile1 email1 table1 Name2 address2 mobile2 email2 table1 Name3 address3 mobile3 email3 table2 Name4 address4 mobile4 email4 table3 But instead, I got an error: ``` #1054 - Unknown column 'TABLE_NAME' in 'field list' ``` Please guide me on how to achieve the result.",
    "author_id":6089,
    "publication_date":1754085286000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Saumya Prakash Mishra",
    "author_reputation":153.0,
    "tags":"mysql, multi-table",
    "text_length":735,
    "title_length":68,
    "num_tags":2
  },
  {
    "id":6644,
    "title":"How to filter on missing field in jsonpath (Jayway)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722912\/how-to-filter-on-missing-field-in-jsonpath-jayway",
    "text":"In Jsonpath (Jayway Java implementation), how do I find array elements where a certain field is missing? For example, in the below JSON, how do I find books without isbn? In theory, empty(true) predicate should do the trick: ``` $.store.book[?(@.isbn empty true)] ``` . This doesn't work though (bug?). Only empty(false) returns results. Searching for null or zero-length values also does not produce the desired result. ``` { \"store\" : { \"book\" : [ { \"category\" : \"reference\", \"author\" : \"Nigel Rees\", \"title\" : \"Sayings of the Century\", \"price\" : 8.95 }, { \"category\" : \"fiction\", \"author\" : \"Evelyn Waugh\", \"title\" : \"Sword of Honour\", \"price\" : 12.99 }, { \"category\" : \"fiction\", \"author\" : \"Herman Melville\", \"title\" : \"Moby Dick\", \"isbn\" : \"0-553-21311-3\", \"price\" : 8.99 }, { \"category\" : \"fiction\", \"author\" : \"J. R. R. Tolkien\", \"title\" : \"The Lord of the Rings\", \"isbn\" : \"0-395-19395-8\", \"price\" : 22.99 } ], \"bicycle\" : { \"color\" : \"red\", \"price\" : 19.95 } }, \"expensive\" : 10 } ```",
    "author_id":6088,
    "publication_date":1754085345000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dmitry E.",
    "author_reputation":103.0,
    "tags":"jsonpath, jayway",
    "text_length":994,
    "title_length":51,
    "num_tags":2
  },
  {
    "id":6643,
    "title":"Apache Flink: kafkaSource with idle works bad when connecting a broadcastStream",
    "link":"https:\/\/stackoverflow.com\/questions\/79722917\/apache-flink-kafkasource-with-idle-works-bad-when-connecting-a-broadcaststream",
    "text":"When kafkaSource connected a broadcastStream is set with idleness, the watermark of downStream is abnormal. My question is how to make the watermark normal to use in window. Here's a case. KafkaSource is set with idleness, it's necessary in my system for some empty topics are subscribed zkWatcherSource is set with watermarkStrategy that always send Watermark.MAX_WATERMARK, to advance the downStream watermark. ``` public static class MaxWatermarkGenerator<T> implements WatermarkGenerator<T> { @Override public void onEvent(T event, long eventTimestamp, WatermarkOutput output) { } @Override public void onPeriodicEmit(WatermarkOutput output) { output.emitWatermark(Watermark.MAX_WATERMARK); } } ``` As follow, watermark of ``` operator1 ``` is ``` Long.MIN_VALUE ``` at first, but after some time, it becomes to ``` Long.MAX_VALUE ``` Watermark of ``` window operator2 ``` is ``` Long.MAX_VALUE ``` forever, window cannot work normally Watermark is get by ``` ctx.currentWatermark() ``` ``` kafkaSource →→ operator1 →→ window operator2 →→ operator3 connect ↑ zkWatcherSource(broadcast) ``` How should i do to make watermark of downStream only follow kafkaSource? Use ``` assignTimestampsAndWatermarks ``` instead of set watermarkStrategy in ``` fromSource ``` function can avoid it, but i want to set watermark in ``` fromSouce ``` for its feature Here's code for checking if you need ``` public class KafkaSourceWithBroadcastConnectDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration()); env.setParallelism(2); Configuration conf = FlinkUtils.loadConfig(args); \/\/ kafka source DataStreamSource<PerformanceMessage> kafkaSourceStream = env.fromSource( Sources.<PerformanceMessage>kafkaSourceBuilder() .setBootstrapServers(conf.get(JobOptions.MAIN_KAFKA_BOOTSTRAP_SERVERS)) .setTopicPattern(Pattern.compile(conf.get(JobOptions.KAFKA_PERFORMANCE_TOPIC_PATTERN))) .setGroupId(conf.get(JobOptions.KAFKA_CONSUMER_GROUP_ID)) .setValueOnlyDeserializer(new JacksonDeserializationSchema<>(PerformanceMessage.class)) .build(), WatermarkStrategy .<PerformanceMessage>forBoundedOutOfOrderness(Duration.ofSeconds(conf.get(JobOptions.WATERMARK_MAX_OUT_OF_ORDERNESS))) .withIdleness(Duration.ofSeconds(conf.get(JobOptions.KAFKA_SOURCE_IDLE_TIMEOUT))) .withTimestampAssigner((msg, timestamp) -> NumberUtils.parseLong(msg.getTime())), \"kafka-source\") .setParallelism(1); \/\/ broadcast source BroadcastStream<String> broadcastStream = env.fromSource( new DataGeneratorSource<>((GeneratorFunction) value -> \"number:\" + value, Long.MAX_VALUE, RateLimiterStrategy.perSecond(1), Types.STRING), WatermarkStrategy .forGenerator(ctx -> new MaxWatermarkGenerator<>()), \"max-set\").broadcast(CONTROL_SIGNAL_BROADCAST); \/\/ connect and process kafkaSourceStream .connect(broadcastStream) .process(new BroadcastProcessFunction<PerformanceMessage, String, PerformanceMessage>() { @Override public void processElement(PerformanceMessage value, BroadcastProcessFunction<PerformanceMessage, String, PerformanceMessage>.ReadOnlyContext ctx, Collector<PerformanceMessage> out) throws Exception { System.err.println(ctx.currentWatermark() + \"--->\" + value); out.collect(value); } @Override public void processBroadcastElement(String value, BroadcastProcessFunction<PerformanceMessage, String, PerformanceMessage>.Context ctx, Collector<PerformanceMessage> out) throws Exception { \/\/ do nothing } }).setParallelism(2) .keyBy(msg -> msg.getDevice().getId()) .window(TumblingEventTimeWindows.of(Duration.ofMinutes(1L))) .process(new ProcessWindowFunction<PerformanceMessage, String, String, TimeWindow>() { @Override public void process(String deviceNo, ProcessWindowFunction<PerformanceMessage, String, String, TimeWindow>.Context context, Iterable<PerformanceMessage> elements, Collector<String> out) throws Exception { System.out.println(context.currentWatermark()); for (PerformanceMessage message : elements) { out.collect(message.getDevice().getId() + \":\" + message.getTime()); } } }).print().setParallelism(4); env.execute(); } public static final MapStateDescriptor<String, Void> CONTROL_SIGNAL_BROADCAST = new MapStateDescriptor<>( \"control-signal-broadcast\", BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.VOID_TYPE_INFO ); public static class MaxWatermarkGenerator<T> implements WatermarkGenerator<T> { @Override public void onEvent(T event, long eventTimestamp, WatermarkOutput output) { } @Override public void onPeriodicEmit(WatermarkOutput output) { output.emitWatermark(Watermark.MAX_WATERMARK); } } } ```",
    "author_id":6087,
    "publication_date":1754085830000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"pre5T",
    "author_reputation":1.0,
    "tags":"java, apache-flink, watermark",
    "text_length":4604,
    "title_length":79,
    "num_tags":3
  },
  {
    "id":6642,
    "title":"Python Selenium: How can I specify a &quot;no timeout&quot; WebDriverWait?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722918\/python-selenium-how-can-i-specify-a-no-timeout-webdriverwait",
    "text":"I'm using Selenium WebDriverWait in a Python function decorated with ``` @timeout() ``` . Since timeout is handled at the function level, I really don't need WebDriverWait to timeout. Is there a way to have a WebDriverWait instance with no timeout?",
    "author_id":6086,
    "publication_date":1754085858000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Geoff Alexander",
    "author_reputation":524.0,
    "tags":"selenium-webdriver, python, webdriverwait",
    "text_length":248,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":6641,
    "title":"MaterialAlertDialog max height to 90% of screen",
    "link":"https:\/\/stackoverflow.com\/questions\/79722921\/materialalertdialog-max-height-to-90-of-screen",
    "text":"I want to set MaterialAlertDialog max height to 90% of the screen so it can be dynamically change up to 90% of the screen If i use AlertDialog.Builder->works fine If i use MaterialAlertDialogBuilder ->No I want to know is there a way to make MaterialAlertDialog (M3) dynamically change his height up to 90% of the screen",
    "author_id":6085,
    "publication_date":1754086246000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Layla Sghaier",
    "author_reputation":3.0,
    "tags":"android, kotlin, material-design",
    "text_length":320,
    "title_length":47,
    "num_tags":3
  },
  {
    "id":6640,
    "title":"Extract value from nested Box&lt;T&gt;",
    "link":"https:\/\/stackoverflow.com\/questions\/79722926\/extract-value-from-nested-boxt",
    "text":"In my project I have recursive enum that holds various constraints. One of the variants implies that constraint is negated. ``` #[derive(PartialEq, Eq, PartialOrd, Ord, Debug)] enum A { NOT(Box<A>), B } ``` I need to implement optimization function that will get rid of double negation : ``` let mut x = A::NOT(Box::new( A::NOT(Box::new( A::B )) )); x.optimize(); \/\/ x == A::B ``` I'm struggling with proper extraction of Boxed values. My implementation so far: ``` impl A { fn optimize (&mut self) { let replacement = match self { A::NOT(a1) => { match &**a1 { A::NOT(a2) => Some(a2), _ => None } }, _ => None }; if let Some(a) = replacement { *self = **a; } } } ``` Throws ``` error[E0507]: cannot move out of **a which is behind a shared reference ``` . My feeling is that I have to destroy Boxes first. After a brief research I've stumbled upon into_raw . But I have no idea how to handle raw pointers it returns and how to address \"the caller should properly destroy T\" requirement of this function.",
    "author_id":6084,
    "publication_date":1754086879000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Pawel Pabian bbkr",
    "author_reputation":1331.0,
    "tags":"rust",
    "text_length":1004,
    "title_length":38,
    "num_tags":1
  },
  {
    "id":6639,
    "title":"Object to Array Issue",
    "link":"https:\/\/stackoverflow.com\/questions\/79722929\/object-to-array-issue",
    "text":"I'm trying to retrieve data from an EVV platform to SharePoint via API. The output presents data as a dictionary\/object with numeric keys, not a clean array. Attempts to convert to array using select and compose steps have been unsuccessful so far. I have also done a double Parse JSON step with no luck. What is your recommended solution? I'm fairly new to Power Automate",
    "author_id":6083,
    "publication_date":1754087299000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"SuperJiga",
    "author_reputation":9.0,
    "tags":"power-automate",
    "text_length":372,
    "title_length":21,
    "num_tags":1
  },
  {
    "id":6638,
    "title":"Model Context is not passing through",
    "link":"https:\/\/stackoverflow.com\/questions\/79722936\/model-context-is-not-passing-through",
    "text":"I'm trying to switch a project over to SwiftData from CoreData, in order to familiarize myself with SwiftData, I started a new project and started playing around. I can't seem to figure out why my model context is not getting to a descendant view. I'm passing it in, and keep gettin ad error in my console... Set a .modelContext in view's environment to use Query I'm passing it through, and in fact, just to be safe, I added even more passes. Here's the final code that's producing this error... Main app ``` @Main struct TestSwiftDataApp: App { var sharedModelContainer: ModelContainer = { let schema = Schema([FoodItem.self]) let modelConfiguration = ModelConfiguration(schema: schema, isStoredInMemoryOnly: false) do { return try ModelContainer(for: schema, configurations: [modelConfiguration]) } catch { fatalError(\"Could not create ModelContainer: \\(error)\") } }() var body: some Scene { WindowGroup { ContentView() } .modelContainer(sharedModelContainer) } } ``` Then in my ContentView ``` struct ContentView: View { @Environment(\\.modelContext) var modelContext @State var selection: Int = 0 var body: some View { TabView(selection: $selection) { SavedMealsController() .modelContext(modelContext) \/\/I added this after the error started, still getting it .tabItem { Label(\"Saved\", systemImage: \"square.and.arrow.down.on.square\") } .tag(0) \/\/ rest of my views } .modelContext(modelContext) \/\/this was the original passing in when I got the error } } ``` Now in SavedMealsController ``` struct SavedMealsController: View { @Environment(\\.modelContext) var context @State var filter: String = \"\" var body: some View { NavigationStack { VStack { HStack(alignment: .leading) { FilterTextField(filter: $filter) \/\/ a text field to allow filtering by name string } FilteredMeals(filter: filter) .modelContext(context) } } } } ``` And finally in the FilteredMeals ``` struct FilteredMeals: View { @Environment(\\.modelContext) var modelContext @Query() var items: [FoodItem] \/\/.. all of my init and displaying of food items } ``` Now, whenever I run the app, in the console I get the error Set a .modelContext in view's environment to use Query I've set the model context, many, many times, in many, many places. I've even tried removing a couple of them in order minimize the number of times it's being passed through and nothing works. I always get that same error and the page (although it works) doesn't show any items, so the query isn't being run. Not sure why it thinks there is no model context in that view.",
    "author_id":6082,
    "publication_date":1754087715000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Neglected Sanity",
    "author_reputation":1460.0,
    "tags":"swiftui, swiftdata, swift-data-modelcontext",
    "text_length":2515,
    "title_length":36,
    "num_tags":3
  },
  {
    "id":6637,
    "title":"How to resolve a mismatch in binding an object in a ListView in MAUI",
    "link":"https:\/\/stackoverflow.com\/questions\/79722938\/how-to-resolve-a-mismatch-in-binding-an-object-in-a-listview-in-maui",
    "text":"In my .NET 9 MAUI application, I have a component using a ViewModel defined like that ``` public partial class LanguagePickerViewModel : ObservableObject { [ObservableProperty] private ObservableCollection<LanguageModelGroup>? filteredCulturesGroups; } ``` The ``` LanguageModelGroup ``` is defined as following ``` public class LanguageModelGroup : List<LanguageModel> { public string Name { get; private set; } public string Flag { get; private set; } public LanguageModelGroup(string name, string flag, List<LanguageModel> languages) : base(languages) { Name = name; Flag = flag; } } ``` After creating an instance of the ViewModel I set the ``` BindingContext ``` with this ViewModel. So, I binding the ``` ListView ``` with the ``` <ListView x:Name=\"listLanguages\" ItemsSource=\"{Binding FilteredCulturesGroups}\"> <ListView.ItemTemplate> <DataTemplate x:DataType=\"md:LanguageModel\"> <ViewCell> <Label Text=\"{Binding LanguageName}\" \/> <\/ViewCell> <\/DataTemplate> <\/ListView.ItemTemplate> <\/ListView> ``` The code is working as expected, but I have a warning that I would like to clear. The warning is this one Microsoft.Maui.Controls.Xaml.Diagnostics.BindingDiagnostics: Warning: Mismatch between the specified x:DataType (LanguageInUse.Models.LanguageDropdown.LanguageModel) and the current binding context (LanguageInUse.Components.LanguageCustomPicker.ViewModels.LanguagePickerViewModel). What is the correct way to bind?",
    "author_id":4871,
    "publication_date":1754088026000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Enrico",
    "author_reputation":6592.0,
    "tags":"c#, .net-9.0, maui",
    "text_length":1427,
    "title_length":68,
    "num_tags":3
  },
  {
    "id":6636,
    "title":"Access Google Cloud Storage Bucket Generation 2 Run Cloud Function Firebase App",
    "link":"https:\/\/stackoverflow.com\/questions\/79722939\/access-google-cloud-storage-bucket-generation-2-run-cloud-function-firebase-app",
    "text":"I am updating a version one cloud function to generation 2 cloud run, which is no longer working. I'm getting the error failed to import the @google-cloud\/storage library (version 7.16.0 is installed and I'm in node 22.0.0) and undefined reading cloudStorageURL. I am using a serviceAccount and following the example code provided, but it does not work. MY INDEX.JS FILE ``` \/\/Initialize the app for gen2 function const { initializeApp, cert } = require(\"firebase-admin\/app\"); \/\/get the service account from my .json file in root const serviceAccount = require(\".\/abc.json\"); \/\/initialize the app with my firebase info and service account initializeApp({ credential: cert(serviceAccount), databaseURL: \"https\/\/:abc-app.abc.com\", storageBucket: \"abc-app.appspot.com\", }); \/\/import my utilities file for creating the sitemap file in root const utilities = require(\".\/utilities\") \/\/export the sitemap function from the utilities file exports.siteMaps_Gen2 = utilities.siteMap; ``` MY UTILITIES.JS FILE ``` \/\/bring in the onDocumentCreated Gen2 funtion for firestore const { onDocumentCreated } = require(\"firebase-functions\/v2\/firestore\"); \/\/bring in the getStoage function const { getStorage } = require(\"firebase-admin\/storage\"); \/\/bring in the getFirestore function const { getFirstore } = require(\"firebase-admin\/firestore); \/\/import some node.js utilities const path = require(\"path\"); const os = require(\"os\"); const fse = require(\"fs-extra\"); \/\/get the google cloud bucket const bucket = getStorage().bucket(); \/\/get the firestore database const db = getFirestore().collection(\"sitemaps\").doc(sitemapsId) \/\/call the sitemap function exports.siteMap = onDocumentCreated(\"sitemaps\/{sitemapsId}\", (event) => { \/\/get the id for my sitemap const id = event.data.ref.path.split(\"\/\"); const sitemapsId = id; }); ```",
    "author_id":6081,
    "publication_date":1754088046000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Steve Klock",
    "author_reputation":133.0,
    "tags":"javascript, firebase, node.js, google-cloud-storage, google-cloud-functions",
    "text_length":1812,
    "title_length":79,
    "num_tags":5
  },
  {
    "id":6635,
    "title":"A collection with cascade=&quot;all-delete-orphan&quot; was no longer referenced by the owning entity instance: ProcedureStep.subSteps",
    "link":"https:\/\/stackoverflow.com\/questions\/79722940\/a-collection-with-cascade-all-delete-orphan-was-no-longer-referenced-by-the-ow",
    "text":"My Java code is throwing the following error: \"A collection with cascade='all-delete-orphan' was no longer referenced by the owning entity instance: br.com.unika.pathdetail.entity.ProcedureStep.subSteps.\" This happens when trying to save an entity that comes from the client (frontend). The collection subSteps is not being modified or reassigned in the Java code; it is received as-is and directly passed to be saved. Despite that, Hibernate still triggers this error, likely interpreting that the original reference to the collection has been replaced or lost. I've already tried everything I can think of to resolve this issue, but nothing has worked. ``` @Entity @Table(name = \"procedure_step\", indexes = { @Index(name = \"idx_pstep_code\", columnList = \"code, procedure_id\"), }) @Data public class ProcedureStep extends AbstractEntity<ProcedureStep, ProcedureStepDTO> { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"id\") private Long id; @AuditableObject(mapBy = \"id\") @ManyToOne(fetch = FetchType.EAGER, optional = false) @JoinColumn(name = \"account_id\", updatable = false) private Account account; @AuditableObject(mapBy = \"name\") @ManyToOne(fetch = FetchType.LAZY, optional = false) @JoinColumn(name = \"procedure_id\", updatable = false) private Procedure procedure; @AuditableObject(mapBy = \"name\") @ManyToOne(fetch = FetchType.EAGER) @JoinColumn(name = \"parent_step_id\") private ProcedureStep parentStep; \/\/ ex: 1.1.1 @Auditable @Column(nullable = false, length = 20) private String code; @Auditable @Column(name = \"name\", nullable = false) private String name; @Temporal(TemporalType.TIMESTAMP) @Column(nullable = false, name = \"created_date\", updatable=false) private Date createdDate = new Date(); @OneToMany(mappedBy = \"step\", cascade = CascadeType.ALL, orphanRemoval = true) private List<StepParameter> parameters = new ArrayList<>(); @OneToMany(mappedBy = \"parentStep\", cascade = CascadeType.ALL, orphanRemoval = true) private List<ProcedureStep> subSteps = new ArrayList<>(); ``` ``` @Transactional(rollbackFor = Exception.class) public List<ProcedureStepDTO> saveMany(List<ProcedureStepDTO> dtoList) throws BusinessRuleException { Set<String> newObjs = new HashSet<>(); \/\/ Convert all elements to DTO List<ProcedureStep> toSave = dtoList.stream().map(ProcedureStepDTO::toEntity).toList(); for (ProcedureStep entity : toSave) { validateUpdatePermission(entity); if(entity.getId() == null) { newObjs.add(entity.getUuidCheck()); beforePersist(entity); } else { beforeSave(entity); } commonValidations(entity); } List<ProcedureStep> saved = repository.saveAll(toSave); for (ProcedureStep s : saved) { if(newObjs.contains(s.getUuidCheck())) { this.afterPersist(s); } else { this.afterSave(s); } } return saved.stream().map(ProcedureStep::toDTO).toList(); } ```",
    "author_id":6080,
    "publication_date":1754088129000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Arthur Andrade",
    "author_reputation":35.0,
    "tags":"java, hibernate, spring-data-jpa",
    "text_length":2796,
    "title_length":134,
    "num_tags":3
  },
  {
    "id":6634,
    "title":"Blazor server call keeps returning log in page even after user is authenticated",
    "link":"https:\/\/stackoverflow.com\/questions\/79722941\/blazor-server-call-keeps-returning-log-in-page-even-after-user-is-authenticated",
    "text":"Goal : simple Blazor app, in which, after the user has logged in and their authentication state is confirmed, I call the backend to load some user data. Process : I launch the app locally from Visual Studio and call an Blazor server endpoint with the ``` [Authorized] ``` attribute. Problem : whatever ``` [Authorized] ``` endpoint I call first, no matter what it is, throws an error { \"'<' is an invalid start of a value. Path: $ | LineNumber: 0 | BytePositionInLine: 0.\" } because instead of returning a JSON payload, it's returning a log in page. The funny bit is that if I refresh the page, every endpoint returns the data it's supposed to just fine. Code : the first endpoint it calls is immediately in the ``` MainLayout.razor ``` page, ``` UserService.LoadCurrentUserAsync ``` : ``` @inject AuthenticationStateProvider AuthenticationStateProvider @inject IUserService UserService \/\/ ... protected override async Task OnInitializedAsync() { var authState = await AuthenticationStateProvider.GetAuthenticationStateAsync(); var user = authState.User; if (user.Identity is not null && user.Identity.IsAuthenticated) { try { await UserService.LoadCurrentUserAsync(); } catch (Exception ex) { await Console.Error.WriteLineAsync(\"Error with LoadCurrentUserAsync: \" + ex.Message); } } } ``` ``` UserService.LoadCurrentUserAsync ``` could only be called if the user was already authenticated. Here is LoadCurrentUserAsync: ``` public async Task LoadCurrentUserAsync() { CurrentUser = await apiService.GetAsync<DtoCurrentUser>(\"api\/users\/me\"); } ``` Here's my code for the ``` apiService.GetAsync ``` : ``` public async Task<T?> GetAsync<T>(string relativeUrl, bool? throwOnNoContent = false) { var baseUri = navigationManager.BaseUri; var response = await httpClient.GetAsync(new Uri(new Uri(baseUri), relativeUrl)); if (response.StatusCode == HttpStatusCode.NoContent) { return default; } var content = await response.Content.ReadAsStringAsync(); Console.WriteLine(\"Content: \" + content); if (!response.IsSuccessStatusCode) { throw new HttpRequestException( $\"Request failed with status {response.StatusCode}: {content}\"); } var res = await response.Content.ReadFromJsonAsync<T>(); Console.WriteLine(\"Response:\" + res); return res; } ``` It's on the ``` response.Content.ReadFromJsonAsync ``` line where the exception is thrown. The ReadFromJsonAsync expects to get a JSON payload, but it receives an HTML string for the log in page instead. I've definitely been experiencing \"Observer Effect\" with this. Debugging it in certain spots delays the backend call enough to where the cookies or whatever are loaded in time, so the backend deems authentication to be true and doesn't return with a login screen. It was only because of the ``` Console.WriteLine(\"Content: \" + content) ``` that I could even see it was a login screen being returned. In my client side Program.cs, I have these auth related services being loaded: ``` builder.Services.AddAuthorizationCore(); builder.Services.AddCascadingAuthenticationState(); builder.Services.AddAuthenticationStateDeserialization(); ``` On the server side Program.cs, I have this: ``` builder.Services.ConfigureApplicationCookie(options => { options.Cookie.SameSite = SameSiteMode.Lax; options.Cookie.SecurePolicy = CookieSecurePolicy.Always; }); ... builder.Services.AddCascadingAuthenticationState(); builder.Services.AddIdentity<ApplicationUser, ApplicationRole>(options => options.SignIn.RequireConfirmedAccount = true) .AddEntityFrameworkStores<ApplicationDbContext>() .AddDefaultTokenProviders(); app.UseAuthentication(); app.UseAuthorization(); ```",
    "author_id":6079,
    "publication_date":1754088260000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"LCIII",
    "author_reputation":3776.0,
    "tags":"blazor, blazor-webassembly",
    "text_length":3597,
    "title_length":79,
    "num_tags":2
  },
  {
    "id":6633,
    "title":"A question about Inner join and primary keys in an SQLite Db",
    "link":"https:\/\/stackoverflow.com\/questions\/79722944\/a-question-about-inner-join-and-primary-keys-in-an-sqlite-db",
    "text":"I have an option in my app for the player to add chips to their collection. There are a number of chips each with a different amount value. When a player indicates they want to add chips, a window opens to allow them to choose the desired chip from a displayed list. After the user chooses the desired chip they tap the Add button. This fires the saveTheSupplies function shown below. The saveTheSupplies func writes the ``` PlayerID_Supply_XRef (gPlayerID) ``` and the ``` SupplyID_XRef (supplyID) ``` into the ``` crossRefTable_Player2Supplies ``` which is an inner join between Player Table and the ``` Supplies_Chip_List table ``` . Here's the scenario where the problem occurs. If Player A adds a $50 chip and saves it. Then later Player B tries to add a $50 chip it throws an error \"SQLite error 19: UNIQUE constraint failed: ``` Player_Supplies_Chips.SupplyID_XRef ``` - while executing ``` INSERT INTO Player_Supplies_Chips (SupplyID_XRef, PlayerID_Supply_XRef, Supply_Count, Supply_Type) VALUES (?, ?, ?, ?) ``` \". The problem I'm having is that this will only work if both ``` SupplyID_XRef ``` and ``` PlayerID_Supply_XRef ``` are set to Primary Key. Does an inner join require both to be primary keys or is there something wrong with my code? ``` get_The_Supply_IDs ``` pulls all of the supplyID in the ``` crossRefTable_Player2Supplies ``` table that the current player has already saved. ``` func get_The_Supply_IDs() { the_Supply_IDs_Array.removeAll() let crossRefTable_Player2Supplies = \"Player_Supplies_\\(supplyType)\" do { try Database_GRDB.shared.databaseConnection!.read { db in for theID in try Int64.fetchAll(db, sql: \"SELECT SupplyID_XRef FROM \\(crossRefTable_Player2Supplies) WHERE PlayerID_Supply_XRef = ?\", arguments: [gPlayerID]) { the_Supply_IDs_Array.append(theID) } } } catch { print(\"Couldn't get the_Supply_IDs_Array! \\(VC_String) \\(error)\") } } \/\/ MARK: - Save Supply func saveTheSupplies() { let crossRefTable_Player2Supplies = \"Player_Supplies_\" + supplyType let theCount = Int(counterFld_Outlet.text ?? \"1\") ?? 1 do { try Database_GRDB.shared.databaseConnection!.write { db in try db.execute(sql: \"INSERT INTO \\(crossRefTable_Player2Supplies) (SupplyID_XRef, PlayerID_Supply_XRef, Supply_Count, Supply_Type) VALUES (?, ?, ?, ?)\", arguments: [supplyID, gPlayerID, theCount, supplyType]) } applySnapshot() } catch { let theString = \"\\(error)\" print(\"error\", theString) get_The_Supply_IDs() if the_Supply_IDs_Array.contains(supplyID) { sendConfirmationAlert(theTitle: K.Titles.itemAlreadyExists, theMessage: K.Titles.editChangeQuanity, buttonTitle: K.Titles.ok) } } } ```",
    "author_id":6078,
    "publication_date":1754088485000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user11004336",
    "author_reputation":null,
    "tags":"swift, sqlite, grdb",
    "text_length":2603,
    "title_length":60,
    "num_tags":3
  },
  {
    "id":6632,
    "title":"How do I change the indent of a YAML sequence in goccy\/go-yaml?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722945\/how-do-i-change-the-indent-of-a-yaml-sequence-in-goccy-go-yaml",
    "text":"I have a YAML file which starts out like this: ``` additional_hostnames: [] ``` I use goccy\/go-yaml to parse it and add a new value like this: ``` newHostnameNode := ast.String(token.New(hostname, hostname, &token.Position{})) additionalHostnames.IsFlowStyle = false additionalHostnames.Values = append(additionalHostnames.Values, newHostnameNode) ``` writing out I get: ``` additional_hostnames: - foobar ``` which is really really close to what I wanted which is: ``` additional_hostnames: - foobar ``` any way to change the indent to be so?",
    "author_id":6077,
    "publication_date":1754088558000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"chx",
    "author_reputation":11832.0,
    "tags":"go",
    "text_length":543,
    "title_length":63,
    "num_tags":1
  },
  {
    "id":6631,
    "title":"Shrink a matrix (i.e. remove borders lines filled with 0)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722947\/shrink-a-matrix-i-e-remove-borders-lines-filled-with-0",
    "text":"Using NumPy and according a matrix is equal to an array … I want to shrink a matrix. I mean by that remove all the borders empty lines (filled with 0) Example: Matrix to shrink: ``` array([[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8) ``` Result expected: ``` array([[1, 0, 1], [0, 0, 0], [1, 0, 1]], dtype=uint8) ``` To do that, I wrote this function: ``` import numpy as np def shrink(mat): while mat[0].any()==0 or mat[-1].any()==0 or mat[:,0].any()==0 or mat[:,-1].any()==0: if mat[0].any()==0: mat = np.delete(mat, 0, 0) if mat[-1].any()==0: mat = np.delete(mat, -1, 0) if mat[:,0].any()==0: mat = np.delete(mat, 0, 1) if mat[:,-1].any()==0: mat = np.delete(mat,-1, 1) return mat ``` It works but I'm pretty sure there is a better method …",
    "author_id":6076,
    "publication_date":1754089005000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Tawal",
    "author_reputation":111.0,
    "tags":"python-3.x, numpy-ndarray, numpy-slicing",
    "text_length":894,
    "title_length":57,
    "num_tags":3
  },
  {
    "id":6630,
    "title":"Search filter not working Vue.js\/Django(DRF)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722955\/search-filter-not-working-vue-js-djangodrf",
    "text":"I'm learning DRF from video, I can't figure out how to get the query parameter from vue.js in django so that the filter works. Github of the project author: Django: https:\/\/github.com\/SteinOveHelset\/djackets_django Vue.js: https:\/\/github.com\/SteinOveHelset\/djackets_vue Link to video: https:\/\/www.youtube.com\/watch?v=Yg5zkd9nm6w&t=4867s I don't understand anything about vue.js, I'm just repeating the code. On the vue.js side, the page doesn't change, but there are no errors either. There are errors on the django side: \"GET \/api\/v1\/products\/search\/ HTTP\/1.1\" 405 5711 \"POST \/api\/v1\/products\/search\/ HTTP\/1.1\" 400 5714 views.py ``` @api_view(['POST']) def search(request): query = request.data.get('query', '') if query: products = Product.objects.filter(Q(name__icontains=query) | Q(description__icontains=query)) serializer = ProductSerializer(products, many=True) return Response(serializer.data) else: return Response({\"products\": []}) ``` App.vue ``` <form method=\"get\" action=\"\/search\"> <div class=\"field has-addons\"> <div class=\"control\"> <input type=\"search\" class=\"input\" placeholder=\"What are you looking for?\" name=\"query\"> <\/div> <div class=\"control\"> <button class=\"button is-success\"> <span class=\"icon\"> <i class=\"fas fa-search\"><\/i> <\/span> <\/button> <\/div> <\/div> <\/form> ``` Search.vue ``` <template> <div class=\"page-search\"> <div class=\"columns is-multiline\"> <div class=\"column is-12\"> <h1 class=\"title\">Search<\/h1> <h2 class=\"is-size-5 has-text-grey\">Search term: \"{{ query }}\"<\/h2> <\/div> <ProductBox v-for=\"product in products\" v-bind:key=\"product.id\" v-bind:product=\"product\" \/> <\/div> <\/div> <\/template> <script> import axios from 'axios' import ProductBox from '@\/components\/ProductBox.vue' export default { name: 'Search', components: { ProductBox }, data() { return { products: [], query: '' } }, mounted() { document.title = 'Search' let uri = window.location.search.substring(1) let params = new URLSearchParams(uri) if (params.get('query')) { this.query = params.get('query') this.performSearch() } }, methods: { async performSearch() { this.$store.commit('setIsLoading', true) await axios .post('\/api\/v1\/products\/search\/', {'query': this.query}) .then(response => { this.products = response.data }) .catch(error => { console.log(error) }) this.$store.commit('setIsLoading', false) } } } <\/script> ``` My github: https:\/\/github.com\/Viktoriya472\/Shop.git The comments mentioned the same error, and the answer was: make sure the url in search endpoint(django product\/url.py ), and vue route as well as axios post request is consistent. check the trailing slashes. if you put a trailing slash in api endpoint and you're missing one in axios request in Search component, you will get this error. I looked at the paths, compared them with the author's code on github, nothing has changed.",
    "author_id":6075,
    "publication_date":1754089610000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Bobby Loner",
    "author_reputation":1.0,
    "tags":"django-rest-framework, django, vue.js, django-filter",
    "text_length":2817,
    "title_length":44,
    "num_tags":4
  },
  {
    "id":6629,
    "title":"Docker Mounts from WSL Does Not Show Files But Mounts from Windows Do",
    "link":"https:\/\/stackoverflow.com\/questions\/79722958\/docker-mounts-from-wsl-does-not-show-files-but-mounts-from-windows-do",
    "text":"My setup Windows 11 and WSL with Ubuntu 24 instance. I would like to have a typical Linux experience inside WSL. However when I have a directory structure like this ``` % tree 130 ↵ . ├── Dockerfile ├── dir1 │ └── nested │ └── test1.txt ├── dir2 │ └── nested │ └── test2.txt ├── docker-compose.test.yml ├── tcp-volume-test.txt ├── test-docker-tcp.sh └── test.txt 5 directories, 7 files ``` Running ``` % docker run --rm -v $(pwd):\/test alpine ls -la \/test ``` brings nothing ``` total 4 drwxr-xr-x 2 root root 40 Aug 1 20:39 . drwxr-xr-x 1 root root 4096 Aug 1 20:58 .. ``` Doing the same in Powershell on Windows has totally different result ``` PS C:\\Users\\sharape\\docker-test> ls Directory: C:\\Users\\sharape\\docker-test Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 8\/1\/2025 4:52 PM dir1 -a---- 8\/1\/2025 4:52 PM 14 test.txt -a---- 8\/1\/2025 4:52 PM 14 test2.txt PS C:\\Users\\sharape\\docker-test> docker run --rm -v ${PWD}:\/test alpine ls -al \/test total 5 drwxrwxrwx 1 root root 0 Aug 1 20:52 . drwxr-xr-x 1 root root 4096 Aug 1 20:55 .. drwxrwxrwx 1 root root 0 Aug 1 20:52 dir1 -rwxr-xr-x 1 root root 14 Aug 1 20:52 test.txt -rwxr-xr-x 1 root root 14 Aug 1 20:52 test2.txt ``` Checking the versions and info for docker in Windows ``` PS C:\\Users\\sharape\\docker-test> docker info Client: Version: 28.3.0 Context: desktop-linux Debug Mode: false Plugins: ai: Docker AI Agent - Ask Gordon (Docker Inc.) Version: v1.6.0 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-ai.exe buildx: Docker Buildx (Docker Inc.) Version: v0.25.0-desktop.1 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe cloud: Docker Cloud (Docker Inc.) Version: v0.4.2 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-cloud.exe compose: Docker Compose (Docker Inc.) Version: v2.38.1-desktop.1 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe debug: Get a shell into any image or container (Docker Inc.) Version: 0.0.41 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-debug.exe desktop: Docker Desktop commands (Docker Inc.) Version: v0.1.11 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-desktop.exe extension: Manages Docker extensions (Docker Inc.) Version: v0.2.29 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe init: Creates Docker-related starter files for your project (Docker Inc.) Version: v1.4.0 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe mcp: Docker MCP Plugin (Docker Inc.) Version: v0.9.3 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-mcp.exe model: Docker Model Runner (EXPERIMENTAL) (Docker Inc.) Version: v0.1.32 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-model.exe sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.) Version: 0.6.0 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe scout: Docker Scout (Docker Inc.) Version: v1.18.1 Path: C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe Server: Containers: 2 Running: 1 Paused: 0 Stopped: 1 Images: 17 Server Version: 28.3.0 Storage Driver: overlayfs driver-type: io.containerd.snapshotter.v1 Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 2 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog CDI spec directories: \/etc\/cdi \/var\/run\/cdi Discovered Devices: cdi: docker.com\/gpu=webgpu Swarm: inactive Runtimes: io.containerd.runc.v2 runc Default Runtime: runc Init Binary: docker-init containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da runc version: v1.2.5-0-g59923ef init version: de40ad0 Security Options: seccomp Profile: builtin cgroupns Kernel Version: 6.10.14-linuxkit Operating System: Docker Desktop OSType: linux Architecture: x86_64 CPUs: 28 Total Memory: 1.913GiB Name: docker-desktop ID: 209bc38d-c9ca-4696-8672-e79db5e295c5 Docker Root Dir: \/var\/lib\/docker Debug Mode: false HTTP Proxy: http.docker.internal:3128 HTTPS Proxy: http.docker.internal:3128 No Proxy: hubproxy.docker.internal Labels: com.docker.desktop.address=npipe:\/\/\\\\.\\pipe\\docker_cli Experimental: false Insecure Registries: hubproxy.docker.internal:5555 ::1\/128 127.0.0.0\/8 Live Restore Enabled: false WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set ``` and in Linux ``` % docker info Client: Docker Engine - Community Version: 28.3.2 Context: default Debug Mode: false Plugins: WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-buildx\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-buildx: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-compose\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-compose: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-debug\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-debug: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-dev\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-dev: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-extension\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-extension: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-feedback\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-feedback: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-init\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-init: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-sbom\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-sbom: no such file or directory WARNING: Plugin \"\/usr\/local\/lib\/docker\/cli-plugins\/docker-scout\" is not valid: failed to fetch metadata: fork\/exec \/usr\/local\/lib\/docker\/cli-plugins\/docker-scout: no such file or directory Server: Containers: 2 Running: 1 Paused: 0 Stopped: 1 Images: 17 Server Version: 28.3.0 Storage Driver: overlayfs driver-type: io.containerd.snapshotter.v1 Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 2 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog CDI spec directories: \/etc\/cdi \/var\/run\/cdi Discovered Devices: cdi: docker.com\/gpu=webgpu Swarm: inactive Runtimes: io.containerd.runc.v2 runc Default Runtime: runc Init Binary: docker-init containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da runc version: v1.2.5-0-g59923ef init version: de40ad0 Security Options: seccomp Profile: builtin cgroupns Kernel Version: 6.10.14-linuxkit Operating System: Docker Desktop OSType: linux Architecture: x86_64 CPUs: 28 Total Memory: 1.913GiB Name: docker-desktop ID: 209bc38d-c9ca-4696-8672-e79db5e295c5 Docker Root Dir: \/var\/lib\/docker Debug Mode: false HTTP Proxy: http.docker.internal:3128 HTTPS Proxy: http.docker.internal:3128 No Proxy: hubproxy.docker.internal Labels: com.docker.desktop.address=npipe:\/\/\\\\.\\pipe\\docker_cli Experimental: false Insecure Registries: hubproxy.docker.internal:5555 ::1\/128 127.0.0.0\/8 Live Restore Enabled: false WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set ``` The docker in Linux is installed using ``` sudo apt install -y docker-ce-cli docker-compose-plugin ``` and it's using host's Docker through TCP ``` DOCKER_HOST=tcp:\/\/localhost:2375 ``` I use Docker without WSL integration and it is deliberate because otherwise otherwise there's whole host of issues with networking in WSL instance, that lead to WSL instance freezing and becoming unresponsive. What could be potential solution for this mounting problem ?",
    "author_id":6074,
    "publication_date":1754089707000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"EvgeniySharapov",
    "author_reputation":3566.0,
    "tags":"docker, windows-subsystem-for-linux",
    "text_length":7844,
    "title_length":69,
    "num_tags":2
  },
  {
    "id":6628,
    "title":"In Racket, why does parameterize not work lexically with shift\/reset?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722959\/in-racket-why-does-parameterize-not-work-lexically-with-shift-reset",
    "text":"Consider the following piece of code: ``` (define p (make-parameter 'outer)) (reset (parameterize ([p 'inner]) (displayln (p)) ; prints 'inner (shift k (begin (displayln (p)) ; prints 'outer (!!) (k 'done))))) ``` I would expect it to print out \"inner\" twice, because both calls to ``` (displayln (p)) ``` are inside the ``` parameterize ``` form. However, the output I got was actually ``` inner outer 'done. ``` Why is this the case? Is there a way to get the expected behavior?",
    "author_id":6073,
    "publication_date":1754089845000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Joseph Camacho",
    "author_reputation":197.0,
    "tags":"racket, control-flow, dynamic-variables, delimited-continuations",
    "text_length":480,
    "title_length":69,
    "num_tags":4
  },
  {
    "id":6627,
    "title":"cache-efficient partitioning for multithreaded processing in arm",
    "link":"https:\/\/stackoverflow.com\/questions\/79722961\/cache-efficient-partitioning-for-multithreaded-processing-in-arm",
    "text":"Suppose you are processing a large data set using several cores in parallel. I am looking for the most memory-efficient way to break up the data among the processors. Specifically, this would be for arm processors on a Mac (Apple Silicon), and the algorithm is processing-light, memory-bound, and the data are independent from each other (e.g. simple statistics, like a histogram or an average.) Suppose there are ``` n ``` cores available, and so you are splitting the proceessing among ``` n ``` threads, and there are ``` L ``` items (bytes\/ints\/etc.) to process. There are several possibilities (pseudo-code is simplified and ignores edge cases, etc.): Sectioned partitioning: Split the data into ``` n ``` blocks of size ~ ``` L\/n ``` , and have each thread\/core process each block in parallel. Each thread ``` k ``` will have a loop looking like: ``` blockSize = L\/n; for (i = k*blockSize; i < (k+1)*blockSize; i++) { \/\/ Process data[i] } ``` Interleaved partitioning: Have each thread read the same data, with a stride of n items, each with a different offset, so each thread ``` k ``` reads and processes the ``` k, k+n, k+2n... ``` 'th items. Each thread ``` k ``` will have a loop looking like: ``` for (i = k; i < L; i += n) { \/\/ Process data[i] } ``` Now, Apple Silicon has a common memory and cache (L3), and each core has its own L1 and L2 cache. With sectioned partitioning, the central memory will have to feed n times more data at a time to the common cache, from n non-adjacent memory locations. With interleved partitioning, each core will utilize only 1\/n of the data read from the central cache, and therefore will need to be fed n times as much from the central cache. There's also a more complex possibility, of which the above two are end cases: Hybrid partitioning: Each thread\/core processes a small amount of data at a time, between 1 item and L\/n items, let's say (arbitrarily) 256 items. So each thread ``` k ``` will have a loop looking like: ``` chunkSize = 256; assert (chunkSize >= 1 && chunkSize <= L\/n); for (i = k*chunkSize; i<L; i += n*chunkSize) { for (j=0; j<chunkSize; j++) { \/\/ process data[i+j] } } ``` Here the M3 cache reads one stream from the main memory in sequence, and the L3 cache feeds separate streams to n cores at the same time. Do any of the three schemes have a significant performance advantage over the others, with regards to the memory system?",
    "author_id":6072,
    "publication_date":1754090050000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Zzyzx",
    "author_reputation":53.0,
    "tags":"arm, multithreading, parallel-processing, apple-m1, cpu-cache",
    "text_length":2403,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6626,
    "title":"Open e-mail client view with attachment and editable fields",
    "link":"https:\/\/stackoverflow.com\/questions\/79722962\/open-e-mail-client-view-with-attachment-and-editable-fields",
    "text":"I need some java code to open a mail client view with an attachment and the user is able to edit field like to: and body-text. I tried a lot, Destop.open does not support attachments. MimeMessage looks fine, see , but opening the created file the fields are not editable. Are there any ideas? Regards",
    "author_id":6071,
    "publication_date":1754090082000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Thorsten",
    "author_reputation":1.0,
    "tags":"java, client, email, view",
    "text_length":300,
    "title_length":59,
    "num_tags":4
  },
  {
    "id":6625,
    "title":"iOS Facebook SDK Graph Request",
    "link":"https:\/\/stackoverflow.com\/questions\/79722963\/ios-facebook-sdk-graph-request",
    "text":"I'm trying to get the user's Facebook id and email. I'm able to login without error but the GraphRequest fails. ``` { manager.logIn(permissions: [\"public_profile\", \"email\"], from: nil) { (result, err) in if err != nil { print(err!.localizedDescription) return } } logged = true let request = GraphRequest(graphPath: \"me\", parameters: [\"fields\": \"email\"]) request.start { (connection, result, error) in if let error = error { \/\/ Handle the error print(\"Error fetching email: \\(error.localizedDescription)\") } else if let userInfo = result as? [String: Any], let email = userInfo[\"email\"] as? String { \/\/ Successfully retrieved the email print(\"User email: \\(email)\") } } } ``` Error fetching email: The operation couldn’t be completed. (com.facebook.sdk.core error 8.)",
    "author_id":6070,
    "publication_date":1754090207000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"sonics876",
    "author_reputation":957.0,
    "tags":"ios, swift, facebook-ios-sdk, facebook-ios-sdk-4.0",
    "text_length":767,
    "title_length":30,
    "num_tags":4
  },
  {
    "id":6624,
    "title":"Not able to connect Arduino UNO R3 to MacBook (No Serial Port Detected)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722964\/not-able-to-connect-arduino-uno-r3-to-macbook-no-serial-port-detected",
    "text":"I'm a newbie to IoT and Arduino and recently got an Arduino UNO R3 to start learning. I'm trying to connect it to my MacBook using a USB-C dongle, but I'm not seeing any serial port like \/dev\/cu.usbmodemXXXX show up. I'm trying to connect an Arduino UNO R3 to my MacBook, but I'm not seeing any serial port like \/dev\/cu.usbmodemXXXX show up. I'm using a USB-C dongle to connect the Arduino via USB. When I run l ``` s \/dev\/cu.* ``` , I only see: ``` bashCopyEdit\/dev\/cu.wlan-debug \/dev\/cu.debug-console \/dev\/cu.Bluetooth-Incoming-Port ``` Nothing changes when I plug or unplug the Arduino. Here's what I've tried so far: Different USB-C hub Different USB cable Restarted Mac Is there a specific driver I need for the UNO R3 on macOS? Or could the board\/cable be faulty? Any help or suggestions would be appreciated!",
    "author_id":6069,
    "publication_date":1754090300000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aswanth",
    "author_reputation":1.0,
    "tags":"arduino-uno",
    "text_length":815,
    "title_length":71,
    "num_tags":1
  },
  {
    "id":6623,
    "title":"run powershell script with specific AppUserModelID from node.js",
    "link":"https:\/\/stackoverflow.com\/questions\/79722965\/run-powershell-script-with-specific-appusermodelid-from-node-js",
    "text":"I have some powershell script\/command I wish to run form a node.js environment. But since the command is dependent on the 'AppUserModelID' it seems to fail since the command spawns a separate program (powershell.exe) with its own 'AppUserModelID'. For example: ``` import { exec } from \"child_process\"; return exec(\"Powershell.exe [Windows.Security.Cryptography.CryptographicBuffer, Windows.Security.Cryptography, ContentType = WindowsRuntime].GetMethod('EncodeToHexString', [type[]]@([Windows.Storage.Streams.IBuffer, Windows.Storage.Streams, ContentType = WindowsRuntime])).Invoke($null, [Windows.System.Profile.SystemIdentification, Windows.System.Profile, ContentType = WindowsRuntime]::GetSystemIdForPublisher().Id)\", {}, (err, stdout, stderr) => { if (err) console.error(err); else console.log(stdout.toString()); }); ``` Will result in the exact same output as running the exact same command in cmd while it shouldn't, as the node.js application and cmd should be considered different programs with each their unique 'AppUserModelID'. So is there a way to either run powershell as a direct sub-process of the node.js application, or set the 'AppUserModelID' within the powershell script somehow? Would love to know if there is someting I have missed, or can try.",
    "author_id":6068,
    "publication_date":1754090554000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"n247s",
    "author_reputation":1922.0,
    "tags":"powershell, node.js, subprocess, child-process",
    "text_length":1269,
    "title_length":63,
    "num_tags":4
  },
  {
    "id":6622,
    "title":"Will I need to use a different library than WINSOCK2 if I want to interface with Unix like systems",
    "link":"https:\/\/stackoverflow.com\/questions\/79722970\/will-i-need-to-use-a-different-library-than-winsock2-if-i-want-to-interface-with",
    "text":"I'm going to be building a network scanner in Windows. Will I need to select a different library if I'm going to be planning on SSHing in to unix-like systems? I.E. if my network scanner will also be able to SSH in to a group of unifi devices to update their set-inform address, or if I'm going to be building network maps with snmp and lldp information gathered by the program? Note, this will be in C++",
    "author_id":6067,
    "publication_date":1754090932000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Charles Staal",
    "author_reputation":347.0,
    "tags":"c++, windows, linux, network-programming",
    "text_length":404,
    "title_length":98,
    "num_tags":4
  },
  {
    "id":6621,
    "title":"How can a process be terminated if not by signal, normal termination (exit, _exit), or system calls to exit\/exit-group on Linux?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722973\/how-can-a-process-be-terminated-if-not-by-signal-normal-termination-exit-exi",
    "text":"Debug environment: Ubuntu Linux 24.04, VSCode, a c++ app being debugged using the VSCODE-hosted version of gdb (the default Microsoft-supplied C\/C++ debugger). I am trying to debug a problem in my app. But it has been exceedingly difficult to debug because the app (and the debugger) terminate abruptly, without giving any warning or message to indicate why the process being debugged was terminated. No breaking to the debugger on a SEGV. Just straight to the command prompt. (The prompt in VSCODE's terminal window, in this case). This app is NOT terminating due to a signal (as far as I can tell). It is not being terminated by system calls to exit or exit_group, or a normal termination via exit or return from main. There are no journalctl entries indicating that the app has been terminated in some special or extraordinary way. How else can a process be terminated by linux if not those? The triggering condition is a round of high-intensity file I\/O and computing going on in an RT priority 5 thread (a \"background\" thread as far as the application is concerned since is providing real-time audio). I strongly suspect that the actual fatal error occurs on a super-high-priorty (RT priority 88) realtime audio service thread that is driving ALSA devices. If I actually had to guess, I would guess \"bug in an an ALSA device driver\", but there are no system log messages that indicate a concrete problem. Also infuriatingly, the issue occurs when I am using a VSCODE hosted gdb debugger (the official Microsoft C\/C++ debugger). But the bug in the app does not occur at all when I use a non-VSCODE gdb session. The bug itself is extremely timing sensitive. Setting breakpoints in the background thread will usually prevent the crash from happening, for example. What I have tried: installing custom signal handlers for SIGSEGV and SIGILL, SIGKILL, on the theory that there are problems with flushing a message to the VSCODE debug console that has actually been written. I cannot catch a breakpoint in the handlers, when the issue occurs. setting breakpoints on exit, and _exit, and system call catchpoints on exit and exit_group. The app terminates without hitting any of those breakpoints. Whoof. Straightt to comammnd proint. \"All C++ Exceptions\" is checked (on). What am I missing? Stack overflow maybe? (I can't remember whether gdb is one of those debuggers that can TELL you that you stack-overflowed or not).",
    "author_id":6066,
    "publication_date":1754091172000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Robin Davies",
    "author_reputation":7887.0,
    "tags":"c++, linux, vscode-debugger, gdb",
    "text_length":2419,
    "title_length":128,
    "num_tags":4
  },
  {
    "id":6620,
    "title":"How to delete files in deliveryoptimization\\cache Windows 10",
    "link":"https:\/\/stackoverflow.com\/questions\/79722976\/how-to-delete-files-in-deliveryoptimization-cache-windows-10",
    "text":"All articles say you are supposed to use the DiskCleanup utility and select the \"Delivery Optimization\" check box. Except that check box does not exist on my system. This is regardless as to whether I run the utility as administrator or not, even though I'm logged in as administrator. Windows says that the files are in use by another (unnamed) process if I attempt to delete the files with the command prompt. How can I delete these rather large and useless files?",
    "author_id":6065,
    "publication_date":1754091410000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"DumbestGuyOnSaturn",
    "author_reputation":61.0,
    "tags":"windows, optimization",
    "text_length":466,
    "title_length":60,
    "num_tags":2
  },
  {
    "id":6619,
    "title":"Google Drive API: gapi.client.drive.files.copy returns 404 for existing file (Slides template)",
    "link":"https:\/\/stackoverflow.com\/questions\/79722982\/google-drive-api-gapi-client-drive-files-copy-returns-404-for-existing-file-sl",
    "text":"I'm trying to copy a Google Slides template using the Google Drive API with gapi.client.drive.files.copy, but I always get a 404 error, even though the file exists and is owned by the same account I'm authenticated with. Here’s the relevant code: ``` const response = await gapi.client.drive.files.copy({ fileId: templateId, resource: { name: newTitle } }); ``` Scopes: ``` const SCOPES = [ 'https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/presentations', ]; const DISCOVERY_DOCS = [ 'https:\/\/slides.googleapis.com\/$discovery\/rest?version=v1', 'https:\/\/www.googleapis.com\/discovery\/v1\/apis\/drive\/v3\/rest', ]; ``` GAPI Initialization: ``` async function initializeGapiClient() { await gapi.client.init({ apiKey: API_KEY, discoveryDocs: DISCOVERY_DOCS, }); gapiInited = true; maybeEnableButtons(); } function gisLoaded() { tokenClient = google.accounts.oauth2.initTokenClient({ client_id: CLIENT_ID, scope: SCOPES.join(' '), callback: '', \/\/ defined later }); gisInited = true; maybeEnableButtons(); } ``` Notes: templateId is a valid Google Slides presentation ID. The file was manually created in the same Google account I'm logged in with. The file is not in the trash, and I have full ownership and sharing permissions. I’ve confirmed the file ID is correct and not corrupted (no extra dots or spaces). I'm using the correct scopes (Drive + Slides). Problem: Despite all this, I consistently receive the following error: ``` { \"error\": { \"code\": 404, \"message\": \"File not found: [templateId]\", \"errors\": [ { \"message\": \"File not found: [templateId]\", \"domain\": \"global\", \"reason\": \"notFound\", \"location\": \"fileId\", \"locationType\": \"parameter\" } ] } } ``` Question: What could be causing gapi.client.drive.files.copy to return 404 when the file definitely exists and is owned by the authenticated user? Am I missing a specific scope or permission step when using gapi.client in the browser?",
    "author_id":6064,
    "publication_date":1754091871000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jaime G&#243;mez",
    "author_reputation":59.0,
    "tags":"oauth-2.0, google-api, google-drive-api, google-api-js-client, google-slides-api",
    "text_length":1916,
    "title_length":94,
    "num_tags":5
  },
  {
    "id":6618,
    "title":"Inside a dll file",
    "link":"https:\/\/stackoverflow.com\/questions\/79722987\/inside-a-dll-file",
    "text":"i'm using visual studio 2022, and i openned a .dll file (aticfx64.dll) (i want to know what is in that file), can someone explain me what each column means and how to read it properly? Look. I'm expecting to understand the insides of this .dll file and maybe other .dll files. basically it looks like this link",
    "author_id":6063,
    "publication_date":1754092434000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Green_Apple",
    "author_reputation":1.0,
    "tags":"windows, driver, dll",
    "text_length":310,
    "title_length":17,
    "num_tags":3
  },
  {
    "id":6617,
    "title":"Intercept API 35+ system undo and redo",
    "link":"https:\/\/stackoverflow.com\/questions\/79722988\/intercept-api-35-system-undo-and-redo",
    "text":"My editing app handles undo and redo by itself. Having the system handle it on top is just weird: the system undo is perceived as a text change and the user can then undo the system's undoes & redoes using the in-app undo button... even worse, I'm just adding undo support, so my users were used to using the system's undo. Can I intercept the system undo and redo actions, block them, and then use my own? More details: To be clear, I'm talking about the undo feature that was added in API 35, not the undo manager added in API 24 which can be disabled with ``` android:allowUndo=\"false\" ``` . The system undo and redo are accessible from input methods (i.e. keyboard apps) or by hitting Ctrl+Z or Ctrl+Shift+Z . Keyboard apps basically just send these keys to trigger it, since as far as I know there is no actual API to do so. I added a ``` TextWatcher ``` to my ``` EditText ``` . When looking at the stacktrace in ``` beforeTextChanged ``` , there seems to be really no way to differentiate a regular edit from an edit generated by a system undo. The stacktrace is identical. One way I found to completely block undo\/redo is overriding ``` EditText.onCreateInputConnection ``` and returning ``` null ``` . The system can't make changes that way, it has no connection. But then I lose all features like auto correct, auto capitalization, etc. so this is obviously not a solution. The system completely intercepts keys. Even the activity doesn't receive Ctrl+Z events at all. EDIT: It turns out this was just GBoard undo\/redo, which happens to be available only after API 35, probably because it uses the new InputConnection APIs. If I intercept undo\/redo shortcuts it still works with other keyboards.",
    "author_id":6062,
    "publication_date":1754092527000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nicolas",
    "author_reputation":7190.0,
    "tags":"android, android-edittext",
    "text_length":1705,
    "title_length":38,
    "num_tags":2
  },
  {
    "id":6616,
    "title":"How to set HTTP header after writing of response body but before submitting it?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722990\/how-to-set-http-header-after-writing-of-response-body-but-before-submitting-it",
    "text":"If I call ``` setHeader ``` after calling ``` write ``` on native Node.js ``` ServerReponse ``` , it will fail with \"Cannot set headers after they are sent to the client\". ``` response.write(\"<h1>Test<\/h1>\"); response.setHeader(\"Set-Cookie\", []); ``` Which method do I need to use instead of ``` write ``` if want to set some headers after writing? From this viewpoint \"write\" does not mean \"submit\", I want to split these actions.",
    "author_id":6061,
    "publication_date":1754092571000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Takeshi Tokugawa YD",
    "author_reputation":1062.0,
    "tags":"node.js",
    "text_length":431,
    "title_length":79,
    "num_tags":1
  },
  {
    "id":6615,
    "title":"Cocos creator: Failed to preload &#39;bundle-a&#39;, Please load bundle &#39;bundle-b&#39; first bug",
    "link":"https:\/\/stackoverflow.com\/questions\/79722991\/cocos-creator-failed-to-preload-bundle-a-please-load-bundle-bundle-b-first",
    "text":"Suppose that I have these two bundles with the specified priorities: ``` bundle-a ``` -> ``` Bundle priority: 1 ``` ``` bundle-b ``` -> ``` Bundle priority: 2 ``` And ``` bundle-b ``` is dependent on ``` bundle-a ``` . So it means that I always have to load them in this order: ``` bundle-a ``` then ``` bundle-b ``` And the point is that when I try to preload the ``` bundle-a ``` , I get this error: ``` cc.js:1161 Failed to preload 'bundle-a', Please load bundle bundle-b first ``` It seems there's a mistake I made that ``` bundle-a ``` uses something from ``` bundle-b ``` (in the wrong order). But after 5 hours, I couldn't find anything. Is there any cache that I can clear? Is there any method I can debug the ``` bundle-a ``` dependencies to other bundles? Any idea? I'm using Cocos Creator 3.8.6",
    "author_id":6060,
    "publication_date":1754092599000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"imaN NeoFighT",
    "author_reputation":580.0,
    "tags":"cocos2d-x, cocoscreator",
    "text_length":805,
    "title_length":100,
    "num_tags":2
  },
  {
    "id":6614,
    "title":"podman ps takes a long time (5+ minutes) to detect a killed container &amp; its &#39;conmon&#39; OCI runtime wrapper, can it be tweaked to be more responsive?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722997\/podman-ps-takes-a-long-time-5-minutes-to-detect-a-killed-container-its-con",
    "text":"I am running podman version 5.4.0 on Rocky Linux 9.6. I notice that when a container is killed along with its ' conmon ' OCI runtime wrapper, say by issuing a kill -9, the podman ps command does not detect the dead container for a good 5 minutes+. In the intervening time, the command lists the container as being up even as other commands like podman stats , podman exec all fail pointing correctly to the container as being dead in the error message! ``` $ podman ps -a | grep kafka 7fd65b99d2a0 localhost\/****\/cp-kafka:*.*.* \/etc\/confluent\/do... 39 hours ago Up 37 hours 9092\/tcp kafka $ podman exec -it 7fd65b99d2a0 bash Error: OCI runtime error: crun: the container `7fd65b99d2a06252078fc85d3c9832d4c1410e0d185bb9cde08c6641aca31334` is not running $ podman stats 7fd65b99d2a0 Error: cannot get cgroup path unless container 7fd65b99d2a06252078fc85d3c9832d4c1410e0d185bb9cde08c6641aca31334 is running: container is stopped ``` I understand the parent runtime monitor is also killed but i am not sure if that justifies reporting an incorrect status in the ``` podman ps ``` command. Is that the expected behavior? Can this be tweaked in some way to be more responsive? Thanks!",
    "author_id":6059,
    "publication_date":1754093335000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"lmk",
    "author_reputation":768.0,
    "tags":"redhat, podman, rocky-os, podman-networking",
    "text_length":1178,
    "title_length":158,
    "num_tags":4
  },
  {
    "id":6613,
    "title":"C++ MSB8041 Error: MFC libraries are required for this project",
    "link":"https:\/\/stackoverflow.com\/questions\/79722998\/c-msb8041-error-mfc-libraries-are-required-for-this-project",
    "text":"I installed every available MFC option, which is not out of support, restarted Visual Studio 2022, and I still get this error. MFC libraries are required for this project. Install them from the Visual Studio installer (Individual Components tab) for any toolsets and architectures being used. I initially had the default ISO C++ 14 Standard, but tried switching to the ISO C++ 17 Standard, but that yielded the exact same error. I have v143 as the target platform toolset. Here are the settings: Although you cannot see the entire list, I did select and install all available MFC options, and definitely for the v143 option, as you can see. Also, how can I tell exactly which C++vXX.XX that it wants? Here is the offending line: ``` <VCMessage Code=\"MSB8041\" Type=\"Error\" Arguments=\"MFC\" Condition=\"'$(CheckMFCInstalled)' == 'true' and !Exists($(MFC_KeyFile)) and '$(SpectreLibs)' == ''\" \/> ``` That means Spectre is not used. That leaves the value of ``` $(MFC_KeyFile) ``` and ``` $(CheckMFCInstalled) ``` . A VS does terminal does not help me, as I cannot just do ``` ? $(MFC_KeyFile) ``` . A terminal window is not a debugger. Thoughts? Relevant Structured Log Viewer Output Assembly Output ``` Assembly = C:\\Program Files\\Microsoft Visual Studio\\2022\\Professional\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.Build.CppTasks.Common.dll ``` Parameters ``` Arguments: MFC Code = MSB8041 Type = Error ``` Error C:\\Program Files\\Microsoft Visual Studio\\2022\\Professional\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(504,5): error MSB8041: MFC libraries are required for this project. Install them from the Visual Studio installer (Individual Components tab) for any toolsets and architectures being used.",
    "author_id":6058,
    "publication_date":1754093507000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Sarah Weinberger",
    "author_reputation":15695.0,
    "tags":"c++, visual-studio, build, mfc",
    "text_length":1703,
    "title_length":62,
    "num_tags":4
  },
  {
    "id":6612,
    "title":"What does the message &quot;Merge is not an allowed merge method in this repository&quot; indicate in a GitHub PR?",
    "link":"https:\/\/stackoverflow.com\/questions\/79722999\/what-does-the-message-merge-is-not-an-allowed-merge-method-in-this-repository",
    "text":"After introducing a branch Ruleset in my GitHub repo, PRs are blocked from merging with this confusing message: Merge is not an allowed merge method in this repository. In the ruleset I specified Merge as the only \"Allowed merge methods\" (note the confusing UI wording there). So why are PRs blocked when I've explicitly allowed \"Merge\" as the PR merge method?",
    "author_id":6057,
    "publication_date":1754093508000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"E-Riz",
    "author_reputation":33397.0,
    "tags":"github, github-rulesets",
    "text_length":360,
    "title_length":114,
    "num_tags":2
  },
  {
    "id":6611,
    "title":"Why can&#39;t a (CUDA) process be unlocked from the &#39;checkpointed&#39; state?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723001\/why-cant-a-cuda-process-be-unlocked-from-the-checkpointed-state",
    "text":"NVIDIA has recently introduced a process-level GPU state checkpoint mechanism into its CUDA driver. Basically, it seems one \"locks\" a process, so that no more CUDA API calls are accepted; then one \"checkpoints\" that process, then finally, one \"unlocks\" the process so it can resume using CUDA. However - the documentation for the unlock function, ``` cuCheckpointProcessUnlock() ``` , says it can only be used on a process in the \"locked\" state, not the \"checkpointed\" state it enters after one performs the checkpoint. Why is that?",
    "author_id":4608,
    "publication_date":1754093569000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"einpoklum",
    "author_reputation":135725.0,
    "tags":"checkpoint, cuda-driver",
    "text_length":532,
    "title_length":81,
    "num_tags":2
  },
  {
    "id":6610,
    "title":"How to PyInstaller a non-pure Python library?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723003\/how-to-pyinstaller-a-non-pure-python-library",
    "text":"I'm trying to use PyInstaller to package my Python application that uses PyMuPDF (fitz) to convert PDF pages into images and display them in a Tkinter UI. The script works perfectly when run normally with Python, but when I build it with PyInstaller, it fails to PyInstaller. I know PyMuPDF is not a pure Python library, and I'm not sure what extra steps I need to take to get PyInstaller to include all the necessary components (DLLs, fonts, resources, etc.). How do I properly package this application with PyInstaller so that it works like the original script? ``` import fitz import os import tkinter as tk from tkinter import filedialog from PIL import Image, ImageTk class PDFViewerApp: def __init__(self, root): self.root = root self.root.title(\"PDF to Image Viewer\") self.label = tk.Label(root) self.label.pack() self.btn_frame = tk.Frame(root) self.btn_frame.pack() self.prev_btn = tk.Button(self.btn_frame, text=\"← Prev\", command=self.show_prev) self.prev_btn.grid(row=0, column=0) self.next_btn = tk.Button(self.btn_frame, text=\"Next →\", command=self.show_next) self.next_btn.grid(row=0, column=1) self.load_btn = tk.Button(root, text=\"select file PDF\", command=self.load_pdf) self.load_btn.pack(pady=10) self.image_paths = [] self.current_index = 0 def load_pdf(self): file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")]) if file_path: output_folder = \"output_images\" self.convert_pdf_to_images(file_path, output_folder) self.image_paths = sorted([ os.path.join(output_folder, f) for f in os.listdir(output_folder) if f.endswith(\".png\") ]) self.current_index = 0 self.show_image() def convert_pdf_to_images(self, pdf_path, output_folder, dpi=150): os.makedirs(output_folder, exist_ok=True) doc = fitz.open(pdf_path) for page in doc: pix = page.get_pixmap(dpi=dpi) fname = os.path.join(output_folder, f\"page_{page.number+1}.png\") pix.save(fname) def show_image(self): if not self.image_paths: return img = Image.open(self.image_paths[self.current_index]) img = img.resize((600, 800)) photo = ImageTk.PhotoImage(img) self.label.config(image=photo) self.label.image = photo def show_next(self): if self.current_index < len(self.image_paths) - 1: self.current_index += 1 self.show_image() def show_prev(self): if self.current_index > 0: self.current_index -= 1 self.show_image() # Chạy ứng dụng if __name__ == \"__main__\": root = tk.Tk() app = PDFViewerApp(root) root.mainloop() ``` I tried to create an .exe file from my Python script using this command: bash: ``` pyinstaller --onefile --windowed my_script.py ``` I expected it to create an .exe file that runs the GUI application and opens the PDF file, converts each page to images and displays them. But PyInstaller can't build the executable. The build takes a very long time and fails I also tried using ``` --collect-all fitz ``` and ``` --hidden-import ``` , but still no luck",
    "author_id":6056,
    "publication_date":1754093591000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"level-up",
    "author_reputation":1.0,
    "tags":"python, pdf, pymupdf, pyinstaller",
    "text_length":2864,
    "title_length":45,
    "num_tags":4
  },
  {
    "id":6609,
    "title":"Can a process checkpoint itself with CUDA?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723004\/can-a-process-checkpoint-itself-with-cuda",
    "text":"NVIDIA has recently introduced a process-level GPU state checkpoint mechanism into its CUDA driver. The API calls take a process ID, which is something we don't really see in other CUDA driver API calls (and perhaps better fits NVML?) ... be that as it may - can checkpointing be applied to the current process? i.e. if the calling process provides its own pid?",
    "author_id":4608,
    "publication_date":1754093705000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"einpoklum",
    "author_reputation":135725.0,
    "tags":"checkpoint, cuda-driver",
    "text_length":361,
    "title_length":42,
    "num_tags":2
  },
  {
    "id":6608,
    "title":"makefile calls a shell script but it doesn&#39;t recognize the first argument passed",
    "link":"https:\/\/stackoverflow.com\/questions\/79723013\/makefile-calls-a-shell-script-but-it-doesnt-recognize-the-first-argument-passed",
    "text":"I have the following script and makefile: Makefile ``` WORKING_DIR := \/home\/user1\/working_dir outdir := \/home\/user1\/working_dir\/outdir script := .\/myscript.sh my_target: file1.ini file2.ini file3.ini file4.ini file5.ini $(call print_vars,\"$@ $^\") echo $(.SHELLSTATUS) $(shell ${script}) echo $(.SHELLSTATUS) $(shell ${script} -t \"$@\" -s \"$^\" -w \"${WORKING_DIR}\" -o \"${outdir}\" -x ) echo $(.SHELLSTATUS) %.ini: printf $@ print_vars = \\ printf \"print_vars $@\" ``` myscript.sh ``` #!\/bin\/bash OPTSTRING=\":s:t:w:o:h:xav:p:l:czb\" while getopts ${OPTSTRING} option; do case $option in s) printf \"s\\n\" printf \"Sources: option $option OPTARG ${OPTARG}\\n\" SOURCES=${OPTARG} ;; t) printf \"t\\n\" printf \"Target: option $option OPTARG ${OPTARG}\\n\" TARGET=${OPTARG} ;; w) printf \"w\\n\" printf \"WORKING_DIR: option $option OPTARG ${OPTARG}\\n\" WORKING_DIR=${OPTARG} ;; o) printf \"o\\n\" printf \"outdir: option $option OPTARG ${OPTARG}\\n\" printf \"OUTDIR ${OPTARG}\\n\" OUTDIR=${OPTARG} ;; x) printf \"x\\n\" printf \"option $option OPTARG ${OPTARG}\\n\" printf \"xml SOURCES ${SOURCES} TARGET ${TARGET} WORKING_DIR ${WORKING_DIR} OUTDIR ${OUTDIR}\" ;; *) printf \"Invalid option errro !!! -$option\\n\" ;; esac done ``` I run it by using make and the output is something like this: ``` printf file1.ini file1.iniprintf file2.ini file2.iniprintf file3.ini file3.iniprintf file4.ini file4.iniprintf file5.ini file5.iniprintf \"print_vars my_target\" print_vars my_targetecho echo 0 0 t Target: option t OPTARG my_target s Sources: option s OPTARG file1.ini file2.ini file3.ini file4.ini file5.ini w WORKING_DIR: option w OPTARG \/home\/user1\/working_dir o outdir: option o OPTARG OUTDIR x option x OPTARG xml SOURCES file1.ini file2.ini file3.ini file4.ini file5.ini TARGET my_target WORKING_DIR \/home\/user1\/working_dir OUTDIR make: t: Command not found make: *** [Makefile:8: my_target] Error 127 ``` It seems like everything is ok except that it does not recognize the -t argument. If I change it to be the second argument and write -s to be the first argument, then it does not recognize -s . Any help on this is appreciated.",
    "author_id":6055,
    "publication_date":1754095314000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ulises V.",
    "author_reputation":13.0,
    "tags":"bash, makefile",
    "text_length":2089,
    "title_length":84,
    "num_tags":2
  },
  {
    "id":6607,
    "title":"Stunnel and the Android CAStore (Openssl 3.3.1)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723017\/stunnel-and-the-android-castore-openssl-3-3-1",
    "text":"I have build stunnel with a few changes to run in a QThread (qtcreator Qt 6.9.1) for Android. My Qt App starts stunnel at the start of execution and stops it when my Qt application exits. This all works fine (I need to review and clean up my stunnel changes before I use my solution for real) but first I cannot get certification to work, here are two test runs one with verify set to 0 (works) and verify set to 2 (fails). I have a private certificate in the Android CAStore. Do I need to change my stunnel.conf file ? (obfuscated details) ``` stunnel.conf foreground = yes debug = 6 [mariadb] CAstore = \/data\/misc\/user\/0\/cacerts-added CApath = \/data\/misc\/user\/0\/cacerts-added client = yes accept = 127.0.0.1:3307 connect = remote_server:3307 verify = 0 Works: I\/stunnel : LOG6[ui]: Initializing inetd mode configuration I\/stunnel : LOG5[ui]: stunnel 5.75 on android-mobile-device platform I\/stunnel : LOG5[ui]: Compiled\/running with OpenSSL 3.1.1 30 May 2023 I\/stunnel : LOG5[ui]: Threading:PTHREAD Sockets:SELECT,IPv4 TLS:ENGINE,OCSP,PSK,SNI I\/stunnel : LOG6[ui]: Initializing inetd mode configuration I\/stunnel : LOG5[ui]: Reading configuration from buffer 0x7280e409b640 I\/stunnel : LOG5[ui]: UTF-8 byte order mark not detected I\/stunnel : LOG6[ui]: Compression disabled I\/stunnel : LOG6[ui]: Initializing service [mariadb] I\/stunnel : LOG6[ui]: stunnel default security level set: 2 I\/stunnel : LOG6[ui]: Session resumption enabled I\/stunnel : LOG6[ui]: Configured trusted server CA: C=AU, ST=Florida, L=York, O=Home, OU=Whiskey, CN=Michael, emailAddress=eric.admin@intheether.com W\/stunnel : LOG4[ui]: Service [mariadb] needs authentication to prevent MITM attacks I\/stunnel : LOG6[ui]: DH initialization skipped: client section I\/stunnel : LOG5[ui]: Configuration successful I\/stunnel : LOG6[ui]: Service [mariadb] (FD=117) bound to 127.0.0.1:3307 I\/stunnel : LOG6[ui]: Accepting new connections I\/stunnel : LOG6[per-day]: Executing per-day jobs I\/stunnel : LOG6[per-day]: Per-day jobs completed in 0 seconds I\/stunnel : LOG5[0]: Service [mariadb] accepted connection from 127.0.0.1:36802 I\/stunnel : LOG6[0]: s_connect: connecting 192.168.0.1:3307 I\/stunnel : LOG5[0]: s_connect: connected 192.168.0.1:3307 I\/stunnel : LOG5[0]: Service [mariadb] connected remote server from 10.0.2.16:58660 I\/stunnel : LOG6[0]: SNI: sending servername: remote_server I\/stunnel : LOG6[0]: Peer certificate not required I\/stunnel : LOG6[0]: Received trusted client CA: C=AU, ST=Florida, L=York, O=Home, OU=Whiskey, CN=Michael, emailAddress=eric.admin@intheether.com I\/stunnel : LOG6[0]: CERT: Certificate verification disabled I\/stunnel : LOG6[0]: CERT: Certificate verification disabled I\/stunnel : LOG6[0]: OCSP: Certificate chain verification disabled I\/stunnel : LOG6[0]: TLS connected: new session negotiated I\/stunnel : LOG6[0]: TLSv1.3 ciphersuite: TLS_AES_256_GCM_SHA384 (256-bit encryption) I\/stunnel : LOG6[0]: Peer temporary key: X25519, 253 bits I\/stunnel : LOG6[0]: Session id: 90DD6325CCBC47C3F8F560C36F76755B4959E6A8421DD97B751E37FDD1DCCAC7 I\/stunnel : LOG6[0]: Session id: 24345E21753FE3B254CA089FB7621C1481B523E72EB9571DDF7A366B5E89E834 stunnel.conf foreground = yes debug = 6 [mariadb] CAstore = \/data\/misc\/user\/0\/cacerts-added CApath = \/data\/misc\/user\/0\/cacerts-added client = yes accept = 127.0.0.1:3307 connect = remote_server:3307 verify = 2 Fails: I\/stunnel : LOG6[ui]: Initializing inetd mode configuration I\/stunnel : LOG5[ui]: stunnel 5.75 on android-mobile-device platform I\/stunnel : LOG5[ui]: Compiled\/running with OpenSSL 3.1.1 30 May 2023 I\/stunnel : LOG5[ui]: Threading:PTHREAD Sockets:SELECT,IPv4 TLS:ENGINE,OCSP,PSK,SNI I\/stunnel : LOG6[ui]: Initializing inetd mode configuration I\/stunnel : LOG5[ui]: Reading configuration from buffer 0x7280e409b0a0 I\/stunnel : LOG5[ui]: UTF-8 byte order mark not detected I\/stunnel : LOG6[ui]: Compression disabled I\/stunnel : LOG6[ui]: Initializing service [mariadb] I\/stunnel : LOG6[ui]: stunnel default security level set: 2 I\/stunnel : LOG6[ui]: Session resumption enabled I\/stunnel : LOG6[ui]: Configured trusted server CA: C=AU, ST=Florida, L=York, O=Home, OU=Whiskey, CN=Michael, emailAddress=eric.admin@intheether.com W\/stunnel : LOG4[ui]: Service [mariadb] uses \"verifyChain\" without subject checks W\/stunnel : LOG4[ui]: Use \"checkHost\" or \"checkIP\" to restrict trusted certificates I\/stunnel : LOG6[ui]: DH initialization skipped: client section I\/stunnel : LOG5[ui]: Configuration successful I\/stunnel : LOG6[ui]: Service [mariadb] (FD=114) bound to 127.0.0.1:3307 I\/stunnel : LOG6[ui]: Accepting new connections I\/stunnel : LOG6[per-day]: Executing per-day jobs I\/stunnel : LOG6[per-day]: Per-day jobs completed in 0 seconds I\/stunnel : LOG5[0]: Service [mariadb] accepted connection from 127.0.0.1:39238 I\/stunnel : LOG6[0]: s_connect: connecting 192.168.0.1:3307 I\/stunnel : LOG5[0]: s_connect: connected 192.168.0.1:3307 I\/stunnel : LOG5[0]: Service [mariadb] connected remote server from 10.0.2.16:59154 I\/stunnel : LOG6[0]: SNI: sending servername: remote_server I\/stunnel : LOG6[0]: Peer certificate required I\/stunnel : LOG6[0]: Received trusted client CA: C=AU, ST=Florida, L=York, O=Home, OU=Whiskey, CN=Michael, emailAddress=eric.admin@intheether.com W\/stunnel : LOG4[0]: CERT: Pre-verification error: certificate not found in local repository: self-signed certificate W\/stunnel : LOG4[0]: Rejected by CERT at depth=0: C=PL, ST=Mazovia Province, L=Warsaw, O=Stunnel Developers, OU=Provisional CA, CN=localhost E\/stunnel : LOG3[0]: SSL_connect: tls_post_process_server_certificate@ssl\/statem\/statem_clnt.c:1890: error:0A000086:SSL routines::certificate verify failed I\/stunnel : LOG5[0]: Connection closed\/reset: 0 byte(s) sent to TLS, 0 byte(s) sent to socket I\/stunnel : LOG6[ui]: Child process 17669 finished with code 0 ```",
    "author_id":6054,
    "publication_date":1754096104000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Stuart",
    "author_reputation":51.0,
    "tags":"android, stunnel",
    "text_length":5809,
    "title_length":47,
    "num_tags":2
  },
  {
    "id":6606,
    "title":"Updating values in JSON array using jq",
    "link":"https:\/\/stackoverflow.com\/questions\/79723019\/updating-values-in-json-array-using-jq",
    "text":"Using jq, I am trying to update the value of each object in an array, the new value calculated from the value of other keys in the same array, and returning the whole updated JSON. Test data saved in \"cardata.json\": ``` { \"data\": { \"cars\": [ { \"type\": \"limo\", \"rating\": null, \"comfort\": 1, \"specs\": { \"seats\": 6, \"heat\": true, \"color\": \"black\" } }, { \"type\": \"sedan\", \"rating\": null, \"comfort\": 2, \"specs\": { \"seats\": 4, \"heat\": true, \"color\": \"blue\" } }, { \"type\": \"open\", \"rating\": null, \"comfort\": 3, \"specs\": { \"seats\": 2, \"heat\": true, \"color\": \"red\" } }, { \"type\": \"sport\", \"rating\": null, \"comfort\": 5, \"specs\": { \"seats\": 1, \"heat\": false, \"color\": \"yellow\" } } ] } } ``` So this command ``` jq '( .data.cars | map(.rating = (if .specs.seats > 3 then \"OK\" else \"Too small\" end)) )' cardata.json ``` does the right thing, sort of, but only returns the updated cars object ``` [ { \"type\": \"limo\", \"rating\": \"OK\", \"comfort\": 1, \"specs\": { \"seats\": 6, \"heat\": true, \"color\": \"black\" } }, { \"type\": \"sedan\", \"rating\": \"OK\", \"comfort\": 2, \"specs\": { \"seats\": 4, \"heat\": true, \"color\": \"blue\" } }, { \"type\": \"open\", \"rating\": \"Too small\", \"comfort\": 3, \"specs\": { \"seats\": 2, \"heat\": true, \"color\": \"red\" } }, { \"type\": \"sport\", \"rating\": \"Too small\", \"comfort\": 5, \"specs\": { \"seats\": 1, \"heat\": false, \"color\": \"yellow\" } } ] ``` but I cannot get the whole JSON. What do I do wrong?",
    "author_id":6053,
    "publication_date":1754096487000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"HJensen",
    "author_reputation":113.0,
    "tags":"json, jq, sh",
    "text_length":1385,
    "title_length":38,
    "num_tags":3
  },
  {
    "id":6605,
    "title":"NestJS @Public() decorator works on other routes but returns 401 Unauthorized on one route with @Res() response",
    "link":"https:\/\/stackoverflow.com\/questions\/79723025\/nestjs-public-decorator-works-on-other-routes-but-returns-401-unauthorized-on",
    "text":"I have a NestJS app with global JWT and Roles guards applied in main.ts. I use a custom @Public() decorator to mark public (unauthenticated) routes. The decorator works fine for most routes, but one specific route using @Res() to redirect always returns 401 Unauthorized, even though it's decorated with @Public(). The route decorated with @Public() should bypass authentication and authorization guards and allow public access. But When I hit this route, I get a 401 Unauthorized response. main.ts (global guards applied manually) ``` import { NestFactory, Reflector } from '@nestjs\/core'; import { AppModule } from '.\/app.module'; import { ValidationPipe } from '@nestjs\/common'; import * as dotenv from 'dotenv'; import { RolesGuard } from '.\/common\/guards\/roles.guard'; import { JwtAuthGuard } from '.\/common\/guards\/jwt\/jwt-auth.guard'; async function bootstrap() { dotenv.config(); const app = await NestFactory.create(AppModule); app.enableCors({ origin: process.env.CLIENT_BASE_URL, credentials: true, methods: ['GET', 'HEAD', 'PUT', 'PATCH', 'POST', 'DELETE', 'OPTIONS'], }); \/\/ Apply global validation pipe app.useGlobalPipes( new ValidationPipe({ transform: true, whitelist: true, forbidNonWhitelisted: false, }), ); \/\/ Get the Reflector instance for guards that use metadata (e.g. @Roles) const reflector = app.get(Reflector); \/\/ Apply global guards app.useGlobalGuards(new JwtAuthGuard(reflector), new RolesGuard(reflector)); const port = process.env.PORT || 8080; await app.listen(port); console.log(`🚀 Application is running on: http:\/\/localhost:${port}`); } void bootstrap(); } ``` @Public() decorator ``` import { SetMetadata } from '@nestjs\/common'; export const IS_PUBLIC_KEY = 'isPublic'; export const Public = () => SetMetadata(IS_PUBLIC_KEY, true); } ``` JwtAuthGuard ``` @Injectable() export class JwtAuthGuard extends AuthGuard('jwt') { constructor(private reflector: Reflector) { super(); } canActivate(context: ExecutionContext) { const isPublic = this.reflector.getAllAndOverride<boolean>(IS_PUBLIC_KEY, [ context.getHandler(), context.getClass(), ]); if (isPublic) { return true; } return super.canActivate(context); } } ``` Controller route that fails ``` @Public() @Get('stripe-payment-callback') async membershipStripePaymentCallback( @Query('session_id') sessionId: string, @Res() res: Response, ) { const response = await this.paymentService.stripePaymentCallback(sessionId); return res.redirect(response.url); } ``` I have other routes decorated with @Public() that do not use @Res(), and those routes work as expected without authorization. What I tried so far: 1.Tried returning plain data (no @Res()), but still 401 on this route. 2.Confirmed the @Public() metadata is set properly. 3.Added console logs inside JwtAuthGuard to verify isPublic is true for this route. 4.Checked that RolesGuard also respects @Public() (it does). 5.Considered switching to APP_GUARD instead of manual useGlobalGuards. Why is this route still getting blocked by the auth guard when it is marked as @Public()? Could using @Res() be interfering with guard execution? What are best practices for combining @Public() and @Res()-based routes in NestJS? Thanks in advance for any suggestions or insights!",
    "author_id":6052,
    "publication_date":1754098293000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Analyzer",
    "author_reputation":11.0,
    "tags":"nestjs, node.js, express, nestjs-passport, nestjs-jwt",
    "text_length":3214,
    "title_length":111,
    "num_tags":5
  },
  {
    "id":6604,
    "title":"How can I create a draggable, auto-sizing speech bubble with an in-place TextEditor?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723026\/how-can-i-create-a-draggable-auto-sizing-speech-bubble-with-an-in-place-textedi",
    "text":"My comic book editor app in SwiftUI has a user-manageable speech bubble. I want to create a ``` BubbleView ``` that a user can drag freely on a canvas. Dynamic sizing: The bubble's frame must automatically resize to fit the text inside it. Real-time growth: When editing text inside an in-place ``` TextEditor ``` , the bubble's shape should fluently resize in real-time with every character typed and new line wrapped. Stable interaction: The user should be able to drag the bubble, select it with a single tap, and enter edit mode via a button on a selection frame. These interactions must not conflict with each other. Dynamically sizing a bubble with a static ``` Text ``` view is straightforward. ``` Text(...).background(Shape()) ``` works but breaks when I introduce a ``` TextEditor ``` . The greedy sizing behavior of ``` TextEditor ``` , if not given an explicit frame, tries to occupy all available space, leading to the bubble suddenly expanding to fill the entire screen when entering edit mode. Giving the ``` TextEditor ``` a fixed frame breaks the real-time growth feature. When using ``` PreferenceKey ``` and ``` GeometryReader ``` to measure the content size in real-time I either got \"the compiler is unable to type-check this expression in reasonable time\" or the app would freeze in an infinite layout loop. My code uses a selection frame with an \"Edit\" button. However, the bubble only resizes after the user finishes editing, not while typing: ``` import SwiftUI \/\/ MARK: - 1. MODEL & SHAPE struct BubbleItem: Identifiable { let id: UUID = UUID() var text: String = \"Tap to edit...\" var position: CGPoint var size: CGSize = .zero \/\/ Used to store the measured size of the text } struct WigglyBubbleShape: Shape { func path(in rect: CGRect) -> Path { \/\/ A custom shape drawing code that draws a comic-style bubble var path = Path() let innerRect = rect.inset(by: .init(top: 15, left: 15, bottom: 15, right: 15)) path.addRoundedRect(in: innerRect, cornerSize: CGSize(width: 25, height: 25)) let tailStartPoint = CGPoint(x: innerRect.minX + innerRect.width * 0.25, y: innerRect.maxY) let tailEndPoint = CGPoint(x: tailStartPoint.x - 20, y: tailStartPoint.y + 20) path.move(to: tailStartPoint) path.addLine(to: tailEndPoint) path.addLine(to: CGPoint(x: tailStartPoint.x + 20, y: tailStartPoint.y)) return path } } \/\/ Helper for measuring view size struct SizePreferenceKey: PreferenceKey { static var defaultValue: CGSize = .zero static func reduce(value: inout CGSize, nextValue: () -> CGSize) { value = nextValue() } } \/\/ MARK: - 2. MAIN VIEW struct ContentView: View { @State private var bubbles: [BubbleItem] = [] @State private var selectedBubbleID: UUID? @FocusState private var focusedBubbleID: UUID? var body: some View { ZStack { Color(UIColor.systemGray6).ignoresSafeArea() .onTapGesture { selectedBubbleID = nil focusedBubbleID = nil } ForEach($bubbles) { $bubble in BubbleView( bubble: $bubble, isSelected: selectedBubbleID == bubble.id, focusedID: $focusedBubbleID, onSelect: { selectedBubbleID = bubble.id }, onDelete: { bubbles.removeAll { $0.id == bubble.id }; selectedBubbleID = nil } ) } } .safeAreaInset(edge: .bottom) { Button(action: addBubble) { Label(\"Add New Bubble\", systemImage: \"plus.bubble.fill\") .font(.headline).padding() .foregroundColor(.white).background(Color.blue, in: Capsule()) .shadow(radius: 5) } .padding() } } private func addBubble() { let newBubble = BubbleItem(position: .init(x: 200, y: 300)) bubbles.append(newBubble) selectedBubbleID = newBubble.id } } \/\/ MARK: - 3. THE PROBLEMATIC BUBBLEVIEW struct BubbleView: View { @Binding var bubble: BubbleItem var isSelected: Bool var focusedID: FocusState<UUID?>.Binding var onSelect: () -> Void var onDelete: () -> Void @State private var isEditing: Bool = false @GestureState private var dragOffset: CGSize = .zero var body: some View { let dragGesture = DragGesture() .updating($dragOffset) { value, state, _ in state = value.translation } .onEnded { value in bubble.position.x += value.translation.width bubble.position.y += value.translation.height } ZStack { WigglyBubbleShape() .fill(isEditing ? Color.yellow.opacity(0.8) : Color.white) .shadow(radius: 4) \/\/ THE PROBLEM AREA: This resizes only after editing is done. \/\/ How to make it resize IN REAL TIME while typing in TextEditor? if isEditing { TextEditor(text: $bubble.text) .font(.system(size: 20, weight: .medium)) .foregroundColor(.black) .multilineTextAlignment(.center) .padding(30) .scrollContentBackground(.hidden) .focused(focusedID, equals: bubble.id) } else { Text(bubble.text) .font(.system(size: 20, weight: .medium)) .foregroundColor(.black) .multilineTextAlignment(.center) .padding(30) .background(GeometryReader { geometry in Color.clear.preference(key: SizePreferenceKey.self, value: geometry.size) }) } } .frame(width: bubble.size.width, height: bubble.size.height) .overlay( Group { if isSelected && !isEditing { HStack(spacing: 12) { Button(action: onDelete) { Image(systemName: \"xmark.circle.fill\").foregroundColor(.red) } Button(action: { isEditing = true }) { Image(systemName: \"pencil.circle.fill\").foregroundColor(.blue) } } .font(.title).padding(8) .background(Color.white.opacity(0.7), in: Capsule()) .offset(y: -50) } }, alignment: .top ) .position(bubble.position) .offset(dragOffset) .gesture(dragGesture) .onTapGesture(perform: onSelect) .onPreferenceChange(SizePreferenceKey.self) { newSize in if !isEditing && newSize.width > 0 { bubble.size = newSize } } .onAppear { if bubble.size == .zero { bubble.size = CGSize(width: 180, height: 120) } } .onChange(of: isEditing) { _, isNowEditing in if isNowEditing { DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { focusedID.wrappedValue = bubble.id } } } .onChange(of: isSelected) { _, isNowSelected in if !isNowSelected { isEditing = false } } .onChange(of: focusedID.wrappedValue) { _, newValue in if newValue != bubble.id { isEditing = false } } .animation(.spring(response: 0.4, dampingFraction: 0.6), value: bubble.size) } } ```",
    "author_id":5444,
    "publication_date":1754098338000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Murat &#199;i&#231;ek",
    "author_reputation":21.0,
    "tags":"swiftui, text-editor",
    "text_length":5988,
    "title_length":84,
    "num_tags":2
  },
  {
    "id":6603,
    "title":"What is this design pattern?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723033\/what-is-this-design-pattern",
    "text":"What is this design pattern where a client makes API calls in a system? Here's how the code is called ``` const extractor = new Extractor(); extractor?.setClient(new ExtractorClient(clientConfig)); ``` Extractor Client has functions that makes API calls to third parties. What is this design pattern called? This looks like strategy design pattern because there could be multiple clients. Right?",
    "author_id":6051,
    "publication_date":1754099367000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Chris Hansen",
    "author_reputation":8743.0,
    "tags":"javascript, design-patterns",
    "text_length":395,
    "title_length":28,
    "num_tags":2
  },
  {
    "id":6602,
    "title":"I am having an issue with an entry form with Swiftui. When I enter the data from the form, the fields do not reset to the defaults",
    "link":"https:\/\/stackoverflow.com\/questions\/79723036\/i-am-having-an-issue-with-an-entry-form-with-swiftui-when-i-enter-the-data-from",
    "text":"``` import Foundation import SwiftData import SwiftUI struct EntryView: View { @Environment(\\.modelContext) var modelContext @Query private var tests: [Test] @State private var a: Date = .init() @State private var b: Int = 0 @State private var c: Int = 0 @State private var d: Int = 0 @State private var e: String = \"\" var body: some View { Form { Text(\"Enter your information here.\") .font(.system(size: 18, weight: .bold, design: .default)) .padding(.top) DatePicker(\"Date and time\", selection: $a, displayedComponents: [.date, .hourAndMinute]) TextField(\"b\", value: $b, format: .number) .frame(width: 200, height: 40) TextField(\"c\", value: $c, format: .number) .frame(width: 200, height: 40) TextField(\"d\", value: $d, format: .number) .frame(width: 150, height: 40) TextField(\"e\", text: $e) .frame(width: 600, height: 40) Button(action: { self.saveReading() }) { Text(\"Save Test\") .padding(10) } Text(\"\\(test.count) readings saved\") .font(.system(size: 18, weight: .bold, design: .default)) .foregroundStyle(.red) } } func saveReading() { let test = Test() test.a = a test.b = Int(Int16(b)) test.c = Int(Int16(c)) test.d = Int(Int16(d)) test.e = e modelContext.insert(reading) try? modelContext.save() } } ``` I tried adding to the call to save a set of instructions to reset the field values to the default. It failed. I tried adding a func to reset the values after data entry. It failed. What is required to get the fields in the form to reset to default after data is entered",
    "author_id":6050,
    "publication_date":1754099766000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Clay",
    "author_reputation":1.0,
    "tags":"swiftui",
    "text_length":1482,
    "title_length":130,
    "num_tags":1
  },
  {
    "id":6601,
    "title":"Why am I getting a ModuleNotFoundError when trying to quickstart the Google Docs API in Python?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723038\/why-am-i-getting-a-modulenotfounderror-when-trying-to-quickstart-the-google-docs",
    "text":"I am trying to use Python to read a Google Doc, and I have been following the tutorial found here: https:\/\/developers.google.com\/workspace\/docs\/api\/quickstart\/python However, when I run quickstart.py, I get the following error message: ``` ModuleNotFoundError: No module named 'google.auth'; 'google' is not a package ``` I have tried reinstalling google.auth multiple times, but it has not fixed the error. Is there anything else I can do to fix this or some other method of reading a Google Doc in Python?",
    "author_id":6049,
    "publication_date":1754099803000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Crimson Jester",
    "author_reputation":3.0,
    "tags":"python, google-docs, google-docs-api",
    "text_length":507,
    "title_length":95,
    "num_tags":3
  },
  {
    "id":6600,
    "title":"Find roman numerals case-insensitively in regular text",
    "link":"https:\/\/stackoverflow.com\/questions\/79723041\/find-roman-numerals-case-insensitively-in-regular-text",
    "text":"The regexes found in the answers to this SO question have various problems with finding only valid roman numbers that are within regular text (not on a line by themselves). (To be clear, that wasn't a requirement of the original question, but a couple of the answers did try to address it). The core issue is that, although various methods are used to not match an empty string, none of them handle an empty match . That is, it's possible for a match itself to be empty, even if the string isn't. I've searched quite a bit for other possible solutions, and they're either marked as duplicate and closed due to the one above (SO), or the answers aren't any better (SO and external). Small amount of test data to demonstrate the problem: ``` Charles I was a bad king, I was not. Charles X was a good one. Who was Louis XVI? The year is MCMXCIX, the month is June. Do you need an X-ray, do you think? My friends Cil and Cleo met me for coffee. MCMLXIX ``` This is the best regex I could identify of the group for this particular problem, improved slightly (it doesn't work without the changes, either, so I don't believe they're the problem). ``` (?=\\b[MCDXLVI]+\\b)M{0,4}(?:CM|CD|D?C{0,3})(?:XC|XL|L?X{0,3})(?:IX|IV|V?I{0,3})(?!-)\\b ``` This regex (in case-insensitive mode) finds all of the valid roman numbers in the above test data (including the false-positive \"I\" in \"I was not\", but that is to be expected and not an issue), but has two empty matches: the empty string prior to the ``` X ``` in ``` X-ray ``` in the 5th line. the empty string prior to ``` Cil ``` in the next-to-last-line. This occurs because both \"X\" and \"Cil\" pass the lookahead since they contain solely roman numeral characters, but the rest of the regex doesn't match anything due to the trailing dash in the first case and Cil not being a valid roman number in the second case. Thus in each case there's a match, but it's empty. The question: is it possible to modify the regex to not allow an empty match? (No empty match at all, not just on this test data.) Ultimately this will be in python 3, but I've also been testing in a PCRE2 editor for convenience. (And in regex101 with both PCRE2 and python for a sanity check.) For completeness, although it's in python, the solution needs to be a single regex, not programmatic, e.g. looping through matches.",
    "author_id":6048,
    "publication_date":1754100130000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"vr8ce",
    "author_reputation":614.0,
    "tags":"python, regex, roman-numerals",
    "text_length":2331,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6599,
    "title":"Change value of a single prop in a Zod object in JS",
    "link":"https:\/\/stackoverflow.com\/questions\/79723042\/change-value-of-a-single-prop-in-a-zod-object-in-js",
    "text":"Using Zod with Javascript, how do I change the value of a single prop in my Zod object while still taking advantage of validation? Examples (vanilla JS): ``` import * as z from 'zod'; const MyObjectSchema = z.strictObject({ prop1: z.number().gte(5), \/\/ Any number >= 5. prop2: z.string(), \/\/ Any string. prop3: z.array(z.number()) \/\/ Array of any numbers. }); \/\/ The following executes with no errors, and sets the props as specified. const myThing = MyObjectSchema.parse({ prop1: 99, prop2: 'Slartibartfast', prop3: [1, 43, 1] }); \/\/ The following is legal, with no error thrown, despite the \"gte(5)\" above; \/\/ because this assignment doesn't use Zod's parse() for validation. myThing.prop1 = 1; \/\/ The following throws an error, because Zod's parse() wants the entire object \/\/ submitted, not a single prop. myThing = MyObjectSchema.parse({ prop1: 40 }); ``` My searching suggests that I need to parse the entire object every time I want to change a single prop, if I want validation; or else write wrapper functions. Can this be true? Am I a uniquely bad programmer who just wants to do myObject.prop2 = 4; without having to set the 100 properties of my big object? TIA.",
    "author_id":6047,
    "publication_date":1754100245000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Bill",
    "author_reputation":33.0,
    "tags":"javascript, zod",
    "text_length":1173,
    "title_length":51,
    "num_tags":2
  },
  {
    "id":6598,
    "title":"Can not add tmcd stream using libavcodec to replicate behavior of ffmpeg -timecode option",
    "link":"https:\/\/stackoverflow.com\/questions\/79723044\/can-not-add-tmcd-stream-using-libavcodec-to-replicate-behavior-of-ffmpeg-timeco",
    "text":"I'm trying to replicate option of command line ffmpeg -timecode in my C\/C++ code. For some reasons the tcmd stream is not written to the output file. However the av_dump_format shows it in run time Here is my minimal test ``` #include <iostream> extern \"C\" { #include <libavcodec\/avcodec.h> #include <libavformat\/avformat.h> #include <libavutil\/avutil.h> #include <libswscale\/swscale.h> #include <libavutil\/opt.h> #include <libavutil\/imgutils.h> #include <libavutil\/samplefmt.h> } bool checkProResAvailability() { const AVCodec* codec = avcodec_find_encoder_by_name(\"prores_ks\"); if (!codec) { std::cerr << \"ProRes codec not available. Please install FFmpeg with ProRes support.\" << std::endl; return false; } return true; } int main(){ av_log_set_level(AV_LOG_INFO); const char* outputFileName = \"test_tmcd.mov\"; AVFormatContext* formatContext = nullptr; AVCodecContext* videoCodecContext = nullptr; if (!checkProResAvailability()) { return -1; } std::cout << \"Creating test file with tmcd stream: \" << outputFileName << std::endl; \/\/ Allocate the output format context if (avformat_alloc_output_context2(&formatContext, nullptr, \"mov\", outputFileName) < 0) { std::cerr << \"Failed to allocate output context!\" << std::endl; return -1; } if (avio_open(&formatContext->pb, outputFileName, AVIO_FLAG_WRITE) < 0) { std::cerr << \"Failed to open output file!\" << std::endl; avformat_free_context(formatContext); return -1; } \/\/ Find ProRes encoder const AVCodec* videoCodec = avcodec_find_encoder_by_name(\"prores_ks\"); if (!videoCodec) { std::cerr << \"Failed to find the ProRes encoder!\" << std::endl; avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } \/\/ Video stream setup AVStream* videoStream = avformat_new_stream(formatContext, nullptr); if (!videoStream) { std::cerr << \"Failed to create video stream!\" << std::endl; avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } videoCodecContext = avcodec_alloc_context3(videoCodec); if (!videoCodecContext) { std::cerr << \"Failed to allocate video codec context!\" << std::endl; avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } videoCodecContext->width = 1920; videoCodecContext->height = 1080; videoCodecContext->pix_fmt = AV_PIX_FMT_YUV422P10; videoCodecContext->time_base = (AVRational){1, 30}; \/\/ Set FPS: 30 videoCodecContext->bit_rate = 2000000; if (avcodec_open2(videoCodecContext, videoCodec, nullptr) < 0) { std::cerr << \"Failed to open ProRes codec!\" << std::endl; avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } if (avcodec_parameters_from_context(videoStream->codecpar, videoCodecContext) < 0) { std::cerr << \"Failed to copy codec parameters to video stream!\" << std::endl; avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } videoStream->time_base = videoCodecContext->time_base; \/\/ Timecode stream setup AVStream* timecodeStream = avformat_new_stream(formatContext, nullptr); if (!timecodeStream) { std::cerr << \"Failed to create timecode stream!\" << std::endl; avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } timecodeStream->codecpar->codec_type = AVMEDIA_TYPE_DATA; timecodeStream->codecpar->codec_id = AV_CODEC_ID_TIMED_ID3; timecodeStream->codecpar->codec_tag = MKTAG('t', 'm', 'c', 'd'); \/\/ Timecode tag timecodeStream->time_base = (AVRational){1, 30}; \/\/ FPS: 30 if (av_dict_set(&timecodeStream->metadata, \"timecode\", \"00:00:30:00\", 0) < 0) { std::cerr << \"Failed to set timecode metadata!\" << std::endl; avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } \/\/ Write container header if (avformat_write_header(formatContext, nullptr) < 0) { std::cerr << \"Failed to write file header!\" << std::endl; avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } \/\/ Encode a dummy video frame AVFrame* frame = av_frame_alloc(); if (!frame) { std::cerr << \"Failed to allocate video frame!\" << std::endl; avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } frame->format = videoCodecContext->pix_fmt; frame->width = videoCodecContext->width; frame->height = videoCodecContext->height; if (av_image_alloc(frame->data, frame->linesize, frame->width, frame->height, videoCodecContext->pix_fmt, 32) < 0) { std::cerr << \"Failed to allocate frame buffer!\" << std::endl; av_frame_free(&frame); avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); return -1; } \/\/ Fill frame with black memset(frame->data[0], 0, frame->linesize[0] * frame->height); \/\/ Y plane memset(frame->data[1], 128, frame->linesize[1] * frame->height \/ 2); \/\/ U plane memset(frame->data[2], 128, frame->linesize[2] * frame->height \/ 2); \/\/ V plane \/\/ Encode the frame AVPacket packet; av_init_packet(&packet); packet.data = nullptr; packet.size = 0; if (avcodec_send_frame(videoCodecContext, frame) == 0) { if (avcodec_receive_packet(videoCodecContext, &packet) == 0) { packet.stream_index = videoStream->index; av_interleaved_write_frame(formatContext, &packet); av_packet_unref(&packet); } } av_frame_free(&frame); \/\/ Write a dummy packet for the timecode stream AVPacket tmcdPacket; av_init_packet(&tmcdPacket); tmcdPacket.stream_index = timecodeStream->index; tmcdPacket.flags |= AV_PKT_FLAG_KEY; tmcdPacket.data = nullptr; \/\/ Empty packet for timecode tmcdPacket.size = 0; tmcdPacket.pts = 0; \/\/ Set necessary PTS tmcdPacket.dts = 0; av_interleaved_write_frame(formatContext, &tmcdPacket); \/\/ Write trailer if (av_write_trailer(formatContext) < 0) { std::cerr << \"Failed to write file trailer!\" << std::endl; } av_dump_format(formatContext, 0, \"test.mov\", 1); \/\/ Cleanup avcodec_free_context(&videoCodecContext); avio_close(formatContext->pb); avformat_free_context(formatContext); std::cout << \"Test file with timecode created successfully: \" << outputFileName << std::endl; return 0; } ``` The code output is: ``` Creating test file with tmcd stream: test_tmcd.mov [prores_ks @ 0x11ce05790] Autoselected HQ profile to keep best quality. It can be overridden through -profile option. [mov @ 0x11ce04f20] Timestamps are unset in a packet for stream 0. This is deprecated and will stop working in the future. Fix your code to set the timestamps properly [mov @ 0x11ce04f20] Encoder did not produce proper pts, making some up. Output #0, mov, to 'test.mov': Metadata: encoder : Lavf61.7.100 Stream #0:0: Video: prores (HQ) (apch \/ 0x68637061), yuv422p10le, 1920x1080, q=2-31, 2000 kb\/s, 15360 tbn Stream #0:1: Data: timed_id3 (tmcd \/ 0x64636D74) Metadata: timecode : 00:00:30:00 Test file with timecode created successfully: test_tmcd.mov ``` The ffprobe output is: ``` $ ffprobe test_tmcd.mov ffprobe version 7.1.1 Copyright (c) 2007-2025 the FFmpeg developers built with Apple clang version 16.0.0 (clang-1600.0.26.6) configuration: --prefix=\/opt\/homebrew\/Cellar\/ffmpeg\/7.1.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon libavutil 59. 39.100 \/ 59. 39.100 libavcodec 61. 19.101 \/ 61. 19.101 libavformat 61. 7.100 \/ 61. 7.100 libavdevice 61. 3.100 \/ 61. 3.100 libavfilter 10. 4.100 \/ 10. 4.100 libswscale 8. 3.100 \/ 8. 3.100 libswresample 5. 3.100 \/ 5. 3.100 libpostproc 58. 3.100 \/ 58. 3.100 Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'test_tmcd.mov': Metadata: major_brand : qt minor_version : 512 compatible_brands: qt encoder : Lavf61.7.100 Duration: N\/A, start: 0.000000, bitrate: N\/A Stream #0:0[0x1]: Video: prores (HQ) (apch \/ 0x68637061), yuv422p10le, 1920x1080, 15360 tbn (default) Metadata: handler_name : VideoHandler vendor_id : FFMP $ ``` Spent hours with all AI models, no help. Appeal to the human intelligence now",
    "author_id":6046,
    "publication_date":1754100847000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Sailor Jerry",
    "author_reputation":335.0,
    "tags":"c++, ffmpeg, video",
    "text_length":8850,
    "title_length":89,
    "num_tags":3
  },
  {
    "id":6597,
    "title":"Updating future custom posts",
    "link":"https:\/\/stackoverflow.com\/questions\/79723046\/updating-future-custom-posts",
    "text":"I've set up a Custom Post Type called \"Classes\" in the CPT UI. The Post Type Slug is \"class\". I've added the following code to the functions.php file in my child theme. The code should change the post_status from \"future\" to \"published\" ``` \/\/ Function to include future posts in the query \/\/ changes the status of all future \"class\" posts from \"scheduled\" to \"published\"function custom_publish_scheduled_posts() { $args = array( 'post_type' => 'class', \/\/ **IMPORTANT: Change this to your actual custom post type slug** 'post_status' => 'future', \/\/ 'future' is the status for scheduled posts 'posts_per_page' => -1, \/\/ Retrieve all posts of this type 'fields' => 'ids' \/\/ Only retrieve post IDs for efficiency ); $scheduled_posts = get_posts( $args ); if ( $scheduled_posts ) { foreach ( $scheduled_posts as $post_id ) { wp_update_post( array( 'ID' => $post_id, 'post_status' => 'publish' ) ); } error_log( 'Successfully published ' . count( $scheduled_posts ) . ' scheduled posts.' ); } else { error_log( 'No scheduled posts found for the specified custom post type.' ); } } add_action( 'init', 'custom_publish_scheduled_posts' ); ``` This code and very similar variations are in several past Stack Overflow postings and all over the internet. It looks like it should work. However, When the code executes I get the following error in the debug.log: ``` No scheduled posts found for the specified custom post type. ``` None of the posts get updated. This despite the fact that I can see scheduled posts in the \"All Classes\" tab. I've tried changing the post_type variable to \"classes\", \"Class\", and \"Classes\" to no effect. Am I missing something?",
    "author_id":6045,
    "publication_date":1754102505000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Antiquated",
    "author_reputation":1.0,
    "tags":"wordpress",
    "text_length":1649,
    "title_length":28,
    "num_tags":1
  },
  {
    "id":6596,
    "title":"infverif.exe error 1273 uses disk id 1, which is not listed under [SourceDisksNames]",
    "link":"https:\/\/stackoverflow.com\/questions\/79723049\/infverif-exe-error-1273-uses-disk-id-1-which-is-not-listed-under-sourcedisksna",
    "text":"While I was trying to install a custom made driver using a self-written ``` .inf ``` file, I decided to use the Microsoft ``` infverif ``` program to check the ``` .inf ``` file. Unfortunately, the program reported an error: ERROR(1273) in filename.inf, line 23: Source file \"driver.sys\" uses disk id 1, which is not listed under [SourceDisksNames]",
    "author_id":6044,
    "publication_date":1754102789000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Thema Guishard",
    "author_reputation":90.0,
    "tags":"windows, driver, inf",
    "text_length":348,
    "title_length":84,
    "num_tags":3
  },
  {
    "id":6595,
    "title":"Basic Authentication from OHIF Viewer to Orthanc (Docker) not working – “Failed to retrieve study data”",
    "link":"https:\/\/stackoverflow.com\/questions\/79723056\/basic-authentication-from-ohif-viewer-to-orthanc-docker-not-working-failed",
    "text":"What I’m looking for: How can I properly pass Basic Auth credentials from OHIF Viewer to Orthanc so the study data is retrieved successfully? Problem I'm running OHIF Viewer (Docker) and Orthanc (Docker) with DicomWeb enabled. My Orthanc instance requires Basic Authentication ( ``` orthanc:orthanc ``` ), and I can successfully ``` curl ``` the ``` \/dicom-web\/studies ``` endpoint with ``` -u orthanc:orthanc ``` from within the OHIF container, which returns a valid JSON list of studies. However, when I open the viewer using a URL like: ``` http:\/\/localhost:3001\/viewer?studyUIDs=1.2.826.0.1.3680043.8.1055.1.20220101123456789.1&url=http:\/\/orthanc:8042\/dicom-web ``` I get \"Failed to retrieve study data\" I’ve tried embedding credentials in the URL (http:\/\/orthanc:orthanc@orthanc:8042\/dicom-web) but it still fails. OHIF logs don’t show any attempt to fetch studies, and Orthanc logs show 401 Unauthorized if no Authorization header is passed. What works: ``` curl -u orthanc:orthanc http:\/\/orthanc:8042\/dicom-web\/studies ``` from inside OHIF container returns expected JSON DICOM file upload works StudyInstanceUIDs are correct and visible via Orthanc REST What fails: OHIF Viewer cannot load study data Passing credentials via URL doesn't work Viewer always shows “Failed to retrieve study data” Tech Stack: Docker Compose OHIF Viewer (Docker image: ``` ohif\/viewer ``` ) Orthanc (Docker image: ``` jodogne\/orthanc-plugins ``` ) DICOMweb enabled Both containers in same Docker network docker-compose.yml ``` version: '3.7' services: orthanc: image: jodogne\/orthanc-plugins container_name: orthanc ports: - \"8042:8042\" environment: - ORTHANC__REGISTERED_USERS=orthanc:orthanc - ORTHANC__DICOMWEB__ENABLE=true - ORTHANC__HTTP_SERVER__ENABLE_CORS=true networks: - ohifnet ohif: image: ohif\/viewer container_name: ohif-viewer ports: - \"3001:80\" environment: - OHIF_VIEWER_PUBLIC_DICOMWEB_ENDPOINT=http:\/\/orthanc:orthanc@orthanc:8042\/dicom-web - REACT_APP_USE_STATIC_WADO=true depends_on: - orthanc networks: - ohifnet networks: ohifnet: ```",
    "author_id":6043,
    "publication_date":1754104090000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"noomywastaken",
    "author_reputation":1.0,
    "tags":"reactjs, docker, basic-authentication, orthanc-server, ohif",
    "text_length":2042,
    "title_length":103,
    "num_tags":5
  },
  {
    "id":6594,
    "title":"OSError: Can&#39;t synchronously read data (can&#39;t open directory) when trying to index retrieve values from a hdf5 file",
    "link":"https:\/\/stackoverflow.com\/questions\/79723058\/oserror-cant-synchronously-read-data-cant-open-directory-when-trying-to-ind",
    "text":"I generated a hdf5 file with the code below: ``` # Create forecast training data file import h5py import hdf5plugin #mmap_array = np.memmap('video_prediction_224.dat', dtype='uint8', mode='w+', shape=(n_images, 224, 224, 3)) sampling_interval = 2 n_valid = 307518 # index 307517 is the last valid index n_trainval = 149680 n_test = 6145 chunk_shape = (1, stack_height+1, *output_img_shape) batch_size = 8000 #for sampling_interval in sampling_interval_all: resume_idx = 0 with h5py.File('video_prediction_224_testing.h5', 'w') as f: #image_log = f.create_dataset('image_log', shape = (n_images, 224, 224, 3), dtype ='uint8', ) if resume_idx and 'image_log' in f: # Resume mode - datasets already exist image_log_ds = f['image_log'] pv_log_ds = f['pv_log'] pv_pred_ds = f['pv_pred'] print(f\"Resuming from existing datasets. Current size: {image_log_ds.shape[0]}\") else: # First run - create new datasets trainval_group = f.create_group('trainval') test_group = f.create_group('test') image_log_trainval_ds = trainval_group.create_dataset( 'image_log', shape=(n_trainval, stack_height+1, *output_img_shape), chunks=chunk_shape, compression=hdf5plugin.Blosc2(cname='zstd', clevel=1, filters=hdf5plugin.Blosc2.SHUFFLE), dtype='uint8' ) pv_log_trainval_ds = trainval_group.create_dataset( 'pv_log', shape=(n_trainval, stack_height+1), dtype='float64' ) pv_pred_trainval_ds = trainval_group.create_dataset( 'pv_pred', shape=(n_trainval,), dtype='float64' ) image_log_test_ds = test_group.create_dataset( 'image_log', shape=(n_test, stack_height+1, *output_img_shape), chunks=chunk_shape, compression=hdf5plugin.Blosc2(cname='zstd', clevel=1, filters=hdf5plugin.Blosc2.SHUFFLE), dtype='uint8' ) pv_log_test_ds = test_group.create_dataset( 'pv_log', shape=(n_test, stack_height+1), dtype='float64' ) pv_pred_test_ds = test_group.create_dataset( 'pv_pred', shape=(n_test,), dtype='float64' ) print(\"Creating new datasets\") #image_log = np.empty([0,stack_height+1]+output_img_shape,dtype = 'uint8') #pv_log = np.empty([0, stack_height+1]) #pv_pred = np.empty([0]) last_valid_index = 0 curr_trainval_size = 0 curr_test_size = 0 tic = time.process_time() for b in range(resume_idx if (resume_idx and 'image_log' in f) else 0, 8000, batch_size): current_batch_size = min(batch_size, n_images-b) # Initialize variables to save pv values #image_log_batch = np.zeros([n_images,stack_height+1]+output_img_shape,dtype = 'uint8') idx_test_batch = get_subarray_between_values(np.asarray(idx_test),b,b+current_batch_size) image_log_batch = np.zeros([current_batch_size,stack_height+1]+output_img_shape,dtype = 'uint8') #all_times_batch = all_times[b*batch_size : b*batch_size + current_batch_size] pv_log_batch = np.zeros((current_batch_size,stack_height+1)) pv_pred_batch = np.zeros(current_batch_size) validity_mask = np.ones(current_batch_size,dtype = bool) sampling_interval_td = datetime.timedelta(minutes = sampling_interval) - datetime.timedelta(seconds=1) for i in range(current_batch_size): count = b+i # See if the specified sampling frequency is met if all_times[count] - all_times[last_valid_index] > sampling_interval_td: # Collecting groud truth for predicted value pred_time = all_times[count]+datetime.timedelta(minutes=forecast_horizon) pv_pred_idx = find_time_within_nparray(pv_data.index,pred_time) if pv_pred_idx is None:# if prediction ground truth not found validity_mask[i] = False #print(all_times[i],'has no PV pred') else: pv_pred_batch[i] = pv_data.iloc[pv_pred_idx] # Collecting image log and PV log for j in range(stack_height+1): log_time = all_times[count] - datetime.timedelta(minutes = j) # Collecting a stack of image log_time_idx = find_time_within_nparray(all_times,log_time) if log_time_idx is not None: image_log_batch[i,j] = all_images[log_time_idx] else: validity_mask[i] = False #print(all_times[count],'has no image log') break # Collecting a stack of PV value pv_log_idx = find_time_within_nparray(pv_data.index,log_time) # Check if PV value present if pv_log_idx is None: validity_mask[i] = False #print(all_times[count],'has no PV log') break else: pv_log_batch[i,j] = pv_data.iloc[pv_log_idx] else: # if this is in between the sampling points, discard validity_mask[i] = False if validity_mask[i]: last_valid_index = count # Prompt progress of current work print('processed {0}\/{1} images'.format(b+current_batch_size,len(all_times))) # Only pick out the valid time points #all_times_batch = all_times_batch[validity_mask] test_mask = np.zeros(current_batch_size, dtype=bool) test_mask[idx_test_batch-b] = True validity_test_mask = validity_mask & test_mask trainval_mask = np.ones(current_batch_size, dtype=bool) trainval_mask[idx_test_batch-b] = False validity_trainval_mask = validity_mask & trainval_mask image_log_trainval_batch = image_log_batch[validity_trainval_mask] pv_log_trainval_batch = pv_log_batch[validity_trainval_mask] pv_pred_trainval_batch = pv_pred_batch[validity_trainval_mask] image_log_test_batch = image_log_batch[validity_test_mask] pv_log_test_batch = pv_log_batch[validity_test_mask] pv_pred_test_batch = pv_pred_batch[validity_test_mask] # Store information print(\"storing data\") #image_log_ds[curr_size:curr_size+validity_mask.sum()] = image_log_batch #pv_log_ds[curr_size:curr_size+validity_mask.sum()] = pv_log_batch #pv_pred_ds[curr_size:curr_size+validity_mask.sum()] = pv_pred_batch image_log_trainval_ds[curr_trainval_size:curr_trainval_size+validity_trainval_mask.sum()] = image_log_trainval_batch #print(\"image_log_trainval_batch: \",image_log_trainval_batch) #print(\"image_log_trainval_ds.shape: \",image_log_trainval_ds.shape) print(\"stored trainval image log\") pv_log_trainval_ds[curr_trainval_size:curr_trainval_size+validity_trainval_mask.sum()] = pv_log_trainval_batch print(\"stored trainval pv log\") pv_pred_trainval_ds[curr_trainval_size:curr_trainval_size+validity_trainval_mask.sum()] = pv_pred_trainval_batch print(\"stored trainval pv pred\") image_log_test_ds[curr_test_size:curr_test_size+validity_test_mask.sum()] = image_log_test_batch print(\"stored test image log\") pv_log_test_ds[curr_test_size:curr_test_size+validity_test_mask.sum()] = pv_log_test_batch pv_pred_test_ds[curr_test_size:curr_test_size+validity_test_mask.sum()] = pv_pred_test_batch curr_trainval_size += validity_trainval_mask.sum() print(\"trainval_size: \",validity_trainval_mask.sum()) curr_test_size += validity_test_mask.sum() print(\"test_size: \",validity_test_mask.sum()) print('For sampling frequency: ',sampling_interval,' minutes') #print('Expected finishing time:', datetime.datetime.now()+ # datetime.timedelta(seconds = (time.process_time() - tic)*(len(all_times)\/(b+batch_size)))) f.flush() del image_log_trainval_batch del pv_log_trainval_batch del pv_pred_trainval_batch del image_log_test_batch del pv_log_test_batch del pv_pred_test_batch pred_folder_child = os.path.join(pred_folder,'frequency_'+str(sampling_interval)) #store_trainval_test(all_times,image_log,pv_log,pv_pred,pred_folder_child) ``` The file is created with no errors, but when I try to access the image datasets: ``` # generate handler for the hdf5 data forecast_dataset = h5py.File(data_path, 'r') # show structure of the hdf5 data def get_all(name): if name!=None: print(forecast_dataset[name]) forecast_dataset.visit(get_all) img = forecast_dataset[\"trainval\"][\"image_log\"][0] print(img) ``` I get the error, ``` data_folder: d:\\PVOutputPrediction\\data data_path: d:\\PVOutputPrediction\\video_prediction_224_second.h5 output_folder: d:\\PVOutputPrediction\\model_output\\UNet_sky_image_PV_mapping <HDF5 group \"\/test\" (3 members)> <HDF5 dataset \"image_log\": shape (6145, 16, 224, 224, 3), type \"|u1\"> <HDF5 dataset \"pv_log\": shape (6145, 16), type \"<f8\"> <HDF5 dataset \"pv_pred\": shape (6145,), type \"<f8\"> <HDF5 group \"\/trainval\" (3 members)> <HDF5 dataset \"image_log\": shape (149680, 16, 224, 224, 3), type \"|u1\"> <HDF5 dataset \"pv_log\": shape (149680, 16), type \"<f8\"> <HDF5 dataset \"pv_pred\": shape (149680,), type \"<f8\"> --------------------------------------------------------------------------- OSError Traceback (most recent call last) Cell In[7], line 3 1 import matplotlib.pyplot as plt ----> 3 img = forecast_dataset[\"trainval\"][\"image_log\"][0] 4 print(img) File h5py\/_objects.pyx:56, in h5py._objects.with_phil.wrapper() File h5py\/_objects.pyx:57, in h5py._objects.with_phil.wrapper() File d:\\PVOutputPrediction\\pytorch_env\\Lib\\site-packages\\h5py\\_hl\\dataset.py:820, in Dataset.__getitem__(self, args, new_dtype) 818 if self._fast_read_ok and (new_dtype is None): 819 try: --> 820 return self._fast_reader.read(args) 821 except TypeError: 822 pass # Fall back to Python read pathway below File h5py\/_selector.pyx:376, in h5py._selector.Reader.read() OSError: Can't synchronously read data (can't open directory) ``` This error is only present when I use Blosc compression, other compression methods work fine. I've tried reinstalling both h5py and hdf5plugin, running chkdsk for both my drives, and re-running the script several times to no avail. I have 300 GB available on my D Drive where I'm storing the file.",
    "author_id":6042,
    "publication_date":1754104209000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aiden Yun",
    "author_reputation":53.0,
    "tags":"python, compression, hdf5, h5py, blosc",
    "text_length":9050,
    "title_length":123,
    "num_tags":5
  },
  {
    "id":6593,
    "title":"Wordpress PODS customized ORDER_BY and Templates",
    "link":"https:\/\/stackoverflow.com\/questions\/79723059\/wordpress-pods-customized-order-by-and-templates",
    "text":"The above image shows what I was hoping to use to make each \"unit\" in my \"couse\" render via its template in the same order the units are set with the \"menu_order\". Yet this doesn't seem to do anything. What am I not understanding about how the \"ORDER_BY\" setting is supposed to work?",
    "author_id":6041,
    "publication_date":1754104351000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Gus Reiber",
    "author_reputation":51.0,
    "tags":"wordpress",
    "text_length":283,
    "title_length":48,
    "num_tags":1
  },
  {
    "id":6592,
    "title":"cmake generating a bad command line option for CUDA in MSVC on Windows",
    "link":"https:\/\/stackoverflow.com\/questions\/79723061\/cmake-generating-a-bad-command-line-option-for-cuda-in-msvc-on-windows",
    "text":"Cmake build is producing this error message, ``` nvcc fatal : A single input file is required for a non-link phase when an outputfile is specified ``` when running this command like that itself generates: ``` \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.9\\bin\\nvcc.exe\" --use-local-env -ccbin \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.44.35207\\bin\\HostX64\\x64\" -x cu -I\"C:\\Gitlab-Runner\\builds\\t3_1sV2uA\\0\\correaa\\boost-multi\\include\" -I\"C:\\vcpkg\\installed\\x64-windows\\include\" -I\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.9\\include\" --keep-dir allocator.cpp.x\\x64\\Debug -use_fast_math -maxrregcount=0 --machine 64 --compile -cudart static -Wno-deprecated-gpu-targets -std=c++17 -arch=native --Werror ext-lambda-captures-this --extra-device-vectorization --restrict -Xcudafe=--display_error_number -Xcudafe=--diag_suppress=20011 -Xcudafe=--diag_suppress=20014 -Xcudafe=--diag_error=incompatible_assignment_operands -Xcudafe=--diag_error=returning_ptr_to_local_variable -Xcudafe=--diag_error=subscript_out_of_range -Xcudafe=--diag_error=used_before_set -Xcudafe=--diag_error=undefined_preproc_id -Xcudafe=--diag_error=implicit_func_decl -Xcudafe=--diag_error=implicit_return_from_non_void_function -Xcudafe=--diag_error=missing_type_specifier \/GS \/Wall \/wd4371 \/wd4514 \/wd4623 \/wd4625 \/wd4626 \/wd4710 \/wd4711 \/wd4820 \/wd4866 \/wd4848 \/wd4868 \/wd5026 \/wd5027 \/wd5045 --expt-relaxed-constexpr --extended-lambda \/EHsc \/permissive- -Werror all-warnings -Xcompiler=\"-Zi -Ob0\" -g -D\"CMAKE_INTDIR=\\\"Debug\\\"\" -D_MBCS -D\"CMAKE_INTDIR=\\\"Debug\\\"\" -Xcompiler \"\/EHsc \/W1 \/nologo \/Od \/FS \/Zi \/RTC1 \/MDd \" -Xcompiler \"\/Fdallocator.cpp.x.dir\\Debug\\vc143.pdb\" -o allocator.cpp.x.dir\\Debug\\allocator.obj \"C:\\Gitlab-Runner\\builds\\t3_1sV2uA\\0\\correaa\\boost-multi\\test\\allocator.cpp\" ``` Many sources online say that this error happens when nvcc.exe (on Windows) can't parse the command line options, but I don't find what is the option generating this problem. What could be the problem with this command line? I don't use Windows command line very much and I am not familiar with all the MSVC options. I am using cmake 3.31.5, MSVC 19.44.35211.0, and nvcc\/cuda 12.9.86.",
    "author_id":6040,
    "publication_date":1754104715000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"alfC",
    "author_reputation":16624.0,
    "tags":"windows, cuda, visual-c++, cmake, nvcc",
    "text_length":2204,
    "title_length":70,
    "num_tags":5
  },
  {
    "id":6591,
    "title":"Why does Laravel 11+ requires curl 7.34.0? What is this specific curl version used for that olders can&#39;t do?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723062\/why-does-laravel-11-requires-curl-7-34-0-what-is-this-specific-curl-version-us",
    "text":"According to the docs ( https:\/\/laravel.com\/docs\/11.x\/upgrade ) it says: Laravel's HTTP client now requires curl 7.34.0 or greater. I've a Laravel application on version 10 in a shared hosting, I'd like to upgrade it to version 11, but the maximum version in the hosting is curl 7.29.0. I'd like to know why curl 7.34.0 is required and for what exactly? I did some tests and I noticed that it works fine with curl 7.29.0, but I'm avoiding any bad surprises. Please, I don't want to know what is curl used for in Laravel, I'd like to know what version 7.34.0 is used for that version 7.29.0 can't do.",
    "author_id":6039,
    "publication_date":1754104800000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jo&#227;o",
    "author_reputation":831.0,
    "tags":"laravel",
    "text_length":599,
    "title_length":112,
    "num_tags":1
  },
  {
    "id":6590,
    "title":"Does Firebase Firestore charge for loading cached documents?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723063\/does-firebase-firestore-charge-for-loading-cached-documents",
    "text":"When I subscribe to a collection that results in 100 documents, a couple of days later, I start my app again. I am offline. I see all 100 documents because Firestore cached them for me. Then I go online. In my collection, 5 documents have changed, and I have received the updated data. So the first query returned 100 documents. The second query also returned 100 documents, but only 5 were sent from the server to the client. Do I have to pay for 105 documents or for 200 documents?",
    "author_id":6038,
    "publication_date":1754105440000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Witek",
    "author_reputation":6510.0,
    "tags":"google-cloud-platform, firebase, google-cloud-firestore",
    "text_length":483,
    "title_length":60,
    "num_tags":3
  },
  {
    "id":6589,
    "title":"Vaadin 23 Flow session not expiring when Spring Session JDBC session expires",
    "link":"https:\/\/stackoverflow.com\/questions\/79723065\/vaadin-23-flow-session-not-expiring-when-spring-session-jdbc-session-expires",
    "text":"I have a multi-project setup with a Spring back-end and a Vaadin 23 Flow front-end. I'm trying to synchronize the session lifecycles so that when the back-end session expires, the Vaadin UI correctly displays the default \"Session Expired\" screen. However, my VaadinSession seems to be staying alive even after the back-end session has expired. My goal is for the Vaadin UI to show the default \"Session Expired\" overlay after the backend session timeout is reached. Project Details Back-end project: Java: 11 Spring: 5.3.30 Session Management: spring-session-jdbc to store sessions in a MySQL database. Session Timeout: Configured to 30 seconds. Front-end project: Java: 11 Vaadin: 23.3.11 Problem Description: I start a session by making a request to my backend's \/session\/start endpoint from the Vaadin UI. The backend successfully creates a session with a 30-second timeout. I wait for more than 30 seconds without any user interaction. The backend session expires, as confirmed by my SessionEventListener logging a SessionDestroyedEvent and from the database too. After the session expires, I interact with the Vaadin UI (e.g., by clicking a button that performs a Vaadin-side action like VaadinSession.getCurrent().setAttribute). Example: ``` Button pingButton = new Button(\"Ping UI\", e -> { UI ui = UI.getCurrent(); ui.access(() -> { try { VaadinSession.getCurrent().setAttribute(\"ping\", \"pong\"); Notification.show(\"Ping successful\"); } catch (Exception ex) { Notification.show(\"Session expired\"); } }); }); ``` Expected behavour The Vaadin UI should detect the expired backend session and display the \"Session Expired\" overlay. Actual behavior The Vaadin UI continues to function as if the session is still active. My pingButton's code VaadinSession.getCurrent().setAttribute(\"ping\", \"pong\") executes successfully, and a \"Ping successful\" notification is shown. This indicates that my VaadinSession is not expiring in sync with the spring-session-jdbc managed HttpSession. What I've Tried I understand that Vaadin's heartbeat mechanism can keep the underlying HTTP session alive. I've tried a few approaches to synchronize the timeouts, but none have worked as expected. Attempt 1: Using a custom RequestHandler. I created a VaadinServiceInitListener with a RequestHandler that checks for the existence of the HttpSession and manually closes the VaadinSession if it's not found. The RequestHandler is called, but httpRequest.getSession(false) does not seem to return null even after the Spring Session has expired. This leads me to believe the servlet container's session object is still present, or my handler is not executing at the right time. My CustomServiceInitListener: ``` import com.vaadin.flow.server.*; import javax.servlet.annotation.WebListener; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpSession; @WebListener public class CustomServiceInitListener implements VaadinServiceInitListener { @Override public void serviceInit(ServiceInitEvent event) { event.addRequestHandler((vaadinSession, request, response) -> { VaadinServletRequest servletRequest = (VaadinServletRequest) request; HttpServletRequest httpRequest = servletRequest.getHttpServletRequest(); HttpSession httpSession = httpRequest.getSession(false); if (httpSession == null) { vaadinSession.close(); return true; } return false; }); } } ``` Attempt 2: Configuring Vaadin's session timeout in web.xml. I removed the custom RequestHandler and tried to use servlet initialization parameters to control Vaadin's session timeout, aiming to make it shorter than the Spring session timeout. My web.xml for the Vaadin project: ``` <web-app xmlns=\"http:\/\/xmlns.jcp.org\/xml\/ns\/javaee\" version=\"4.0\"> <servlet> <servlet-name>vaadin<\/servlet-name> <servlet-class>com.vaadin.flow.server.VaadinServlet<\/servlet-class> <init-param> <param-name>session-timeout<\/param-name> <param-value>25<\/param-value> <\/init-param> <init-param> <param-name>closeIdleSessions<\/param-name> <param-value>true<\/param-value> <\/init-param> <load-on-startup>1<\/load-on-startup> <\/servlet> <servlet-mapping> <servlet-name>vaadin<\/servlet-name> <url-pattern>\/*<\/url-pattern> <\/servlet-mapping> <\/web-app> ``` Even with this configuration, the Vaadin session does not expire after 25 seconds of inactivity. I suspect there's a fundamental aspect of how Spring Session and Vaadin's session management interact that I'm missing, especially in a non-Spring-Boot environment using web.xml and spring-session-jdbc. How can I correctly configure my Vaadin 23 Flow application to ensure the VaadinSession expires as soon as the spring-session-jdbc session expires, triggering the default \"Session Expired\" UI?",
    "author_id":6037,
    "publication_date":1754106490000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Va Run",
    "author_reputation":23.0,
    "tags":"java, spring-session, vaadin23, vaadin-session",
    "text_length":4677,
    "title_length":76,
    "num_tags":4
  },
  {
    "id":6588,
    "title":"How to exclude specific REST API endpoints from being cached by WP Rocket?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723066\/how-to-exclude-specific-rest-api-endpoints-from-being-cached-by-wp-rocket",
    "text":"I'm using WP Rocket along with their helper plugin WP Rocket | Cache WP Rest API which allows caching of WordPress REST API endpoints. I’ve added the following filter to allow REST API caching: ``` add_filter( 'rocket_cache_reject_wp_rest_api', '__return_false' ); ``` Now it's caching API routes like: \/wp-json\/wc\/store\/v1\/cart \/wp-json\/custom\/v1\/xyz However, I want to exclude certain endpoints (for example, \/wp-json\/wp\/v2\/posts or \/wp-json\/custom\/v1\/sensitive-data) from being cached.",
    "author_id":6036,
    "publication_date":1754106586000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Vineet SND",
    "author_reputation":9.0,
    "tags":"php, wordpress, woocommerce, wordpress-rest-api, woocommerce-rest-api",
    "text_length":488,
    "title_length":74,
    "num_tags":5
  },
  {
    "id":6587,
    "title":"Polars read function still blocks the process even if PyQt6 Threading\/Runnable is used",
    "link":"https:\/\/stackoverflow.com\/questions\/79723077\/polars-read-function-still-blocks-the-process-even-if-pyqt6-threading-runnable-i",
    "text":"I have a PyQt6 GUI. I made it unblocking so that when a long process runs, users can still use the application to navigate and open some new windows. All is working well. However, when a user inputs a data which is being read using Polars ``` read_excel ``` , the GUI will not function when the data is still being read - although polars is fast, when the data is quite large, the user needs to wait a couple of seconds for the GUI to be usable - which is not what I'm after and considered a bug. Here are some snippets of my code: ``` class WorkerSignals(QObject): finished = pyqtSignal(object) log = pyqtSignal(str) error = pyqtSignal(str) class TransformerRunnable(QRunnable): signals: WorkerSignals def __init__( self, transformer_fn: Callable[..., Any], file_paths: dict[str, str], data_engine=\"polars\", ): super().__init__() self.transformer_fn = transformer_fn self.file_paths = file_paths self.data_engine = data_engine self.signals = WorkerSignals() def run(self): try: self.creds = load_credentials() self.conf = load_config() self.signals.log.emit(\"Reading files...\") self.signals.log.emit(\"Running transformation...\") if self.data_engine == \"polars\": data = read_polars_dfs(**self.file_paths) # this is blocking elif self.data_engine == \"pandas\": data = read_pandas_dfs(**self.file_paths) else: self.signals.error.emit( \"Error Occurred. Engine not supported. Falling back to polars...\" ) data = read_polars_dfs(self.file_paths) result = self.transformer_fn( signals=self.signals, credentials=self.creds, config=self.conf, **data ) self.signals.log.emit(\"Transformation complete.\") self.signals.finished.emit(result) except Exception as e: error_msg = f\"Error: {str(e)}\\n{traceback.format_exc()}\" self.signals.error.emit(error_msg) # Here's my simple polars read function def read_polars_dfs(lazy: bool = True, **kwargs) -> dict[str, PolarsDataFrames]: if len(kwargs) > 0: data = { key: pl.read_excel(path_) if not lazy else pl.read_excel(path_).lazy() for key, path_ in kwargs.items() } return data raise ValueError(\"No input data detected. Check your implementation!\") ``` As you can also see in the code, I tried using pandas, but it's not blocking the GUI. I know the fact that polars use all CPU cores to process tasks efficiently ( https:\/\/docs.pola.rs\/user-guide\/misc\/multiprocessing\/ ) - but still not sure what to do with this. Really appreciate your help.",
    "author_id":6035,
    "publication_date":1754108947000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"jjy",
    "author_reputation":23.0,
    "tags":"python, python-polars, pyqt6",
    "text_length":2376,
    "title_length":86,
    "num_tags":3
  },
  {
    "id":6586,
    "title":"create\/update main menu based on user&#39;s preferred language",
    "link":"https:\/\/stackoverflow.com\/questions\/79723081\/create-update-main-menu-based-on-users-preferred-language",
    "text":"I would like to customize the main menu texts based on the customer language preference, but I have not seen any document about how to create \/ update the main menu using java. I can create Inline-Menu using the ``` InlineKeyboardButton ``` and ``` InlineKeyboardMarkup ``` classes. I can create Keyboard-Buttons using the ``` KeyboardRow ``` , ``` KeyboardButton ``` and ``` ReplyKeyboardMarkup ``` classes. But I can not see any documentation or example hot to create and configure the main Telegram menu button using Java. This is how I create the inline-menu: ``` private void buildInlineMenu(SendMessage.SendMessageBuilder builder, ChatResponse chatResponse) { if (!chatResponse.getInlineMenus().isEmpty()) { List<InlineKeyboardButton> inlineButtons = new ArrayList<>(); chatResponse.getInlineMenus().forEach(menu -> { var next = InlineKeyboardButton.builder() .text(menu.getCaption()) .callbackData(menu.getCommand()) .build(); inlineButtons.add(next); }); InlineKeyboardMarkup markup = InlineKeyboardMarkup .builder() .keyboardRow(inlineButtons) .build(); builder.replyMarkup(markup); } } ``` link to image My code that creates the keyboard: ``` private void buildKeyboard(SendMessage.SendMessageBuilder builder, ChatResponse chatResponse) { if (!chatResponse.getKeyboards().isEmpty()) { List<KeyboardRow> keyboard = new ArrayList<>(); chatResponse.getKeyboards().forEach(b -> { String[] buttons = b.getCaption().split(\";\"); KeyboardRow keyboardRow = new KeyboardRow(); Arrays.stream(buttons).forEach(button -> { keyboardRow.add(new KeyboardButton(button)); }); keyboard.add(keyboardRow); }); ReplyKeyboardMarkup markup = new ReplyKeyboardMarkup(); markup.setSelective(true); markup.setResizeKeyboard(true); markup.setOneTimeKeyboard(false); markup.setKeyboard(keyboard); builder.replyMarkup(markup); } } ``` link to image How I call them: (1) ``` ChatResponse.builder() .textMessage(\"Select a fruit! (inline menu)\") .inlineMenus(List.of( ChatMenu.builder().caption(\"apple\").command(\"apple\").build(), ChatMenu.builder().caption(\"kiwi\").command(\"kiwi\").build(), ChatMenu.builder().caption(\"orange\").command(\"orange\").build())) .build(); ``` (2) ``` ChatResponse.builder() .textMessage(\"What is your favourite color? (keyboard)\") .keyboards(List.of( ChatMenu.builder().caption(\"Red;Blue\").build(), ChatMenu.builder().caption(\"Green;Purple;White\").build())) .build(); ``` Calling: ``` SendMessage.SendMessageBuilder sendMessageBuilder = SendMessage .builder() .chatId(telegramId) .parseMode(\"HTML\") .text(chatResponse.getTextMessage()); buildInlineMenu(sendMessageBuilder, chatResponse); buildKeyboard(sendMessageBuilder, chatResponse); execute(sendMessageBuilder.build()); ``` The code above works like a charm but I want to create and configure the main menu as well from Java. I know that I can add the main menu using the ``` BotFather ``` menu from the Telegram chat, but I would like to use Java for this functionality as well. Is that possible somehow? Or I am wrong and there is no way to configure the main menu using Java?",
    "author_id":6034,
    "publication_date":1754109799000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"zappee",
    "author_reputation":22994.0,
    "tags":"java, telegram, telegram-api",
    "text_length":3036,
    "title_length":62,
    "num_tags":3
  },
  {
    "id":6585,
    "title":"How to store authentication and redirect with React",
    "link":"https:\/\/stackoverflow.com\/questions\/79723087\/how-to-store-authentication-and-redirect-with-react",
    "text":"I am building an application that looks the following: app.com -> serves the react application api.app.com -> serves as the backend for data fetching and login I am using TanStack Router and TanStack Query. I am currently doing an OAuth flow where the router redirects a user to the \/login page if they are not logged in. This is a simple check right now \"if localStorage has user\". On the login page, there is a link to the backend to authenticate (api.app.com\/oauth\/login) which kicks off the OAuth login flow and returns me a code, the backend exchanges that code and sets a cookie with the authentication information. I deemed this the most secure way to store the login and it works as expected. There is now a cookie for an authenticated user for api.app.com. Now after the authentication is done, I redirect the user back to React, e.g. app.com\/loggedin. In order for the React app to know if the user is logged in, it should make a simple request to api.app.com\/user and if it returns a 200, store the userdata in localstorage, including when the cookie expires. By that, the app knows that the authentication on the backend side is present and a valid user is using the app. However, I am struggling to find a spot where to implement this logic. Ideally I would have a ``` isAuthenticated ``` method that a) checks localstorage b) calls api.apps.com\/user and returns whether the user is authenticated. But this is all asynchronous and in the beforeLoad I cannot use that, as it expects synchronous code. I also tried to do it in the beforeLoad method of the \/loggedin route, but then I cannot redirect anymore (throw redirect doesn't work in the Promise and useRouter isn't available). Where and how would I implement this authentication logic with React, TanStack Router and TanStack Query?",
    "author_id":6033,
    "publication_date":1754110711000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mmlac",
    "author_reputation":1111.0,
    "tags":"reactjs, tanstackreact-query, tanstack-router",
    "text_length":1800,
    "title_length":51,
    "num_tags":3
  },
  {
    "id":6584,
    "title":"Is sizeof(pointer) the same as processor&#39;s native word size?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723088\/is-sizeofpointer-the-same-as-processors-native-word-size",
    "text":"Say, is ``` sizeof(void*) ``` the same as the size processor can atomically access per instruction? For example, 32-bit processor can read aligned 4 bytes atomically, 64-bit processor can read aligned 8 bytes atomically. While, if it's a 32-bit OS on 64-bit hardware, the atomic access size fall back to 4 bytes. Note by \"atomic access\" I mean it fetches as a whole, a counterexample would be 32-bit OS would need to fetch ``` uint64_t ``` two-pass.",
    "author_id":6032,
    "publication_date":1754111026000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"PkDrew",
    "author_reputation":2367.0,
    "tags":"c, cpu-architecture, sizeof, cpu-word, processor",
    "text_length":449,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6583,
    "title":"How to create page margins in a ggsave() pdf file",
    "link":"https:\/\/stackoverflow.com\/questions\/79723101\/how-to-create-page-margins-in-a-ggsave-pdf-file",
    "text":"I am trying to print a series of ggplot graphs in a multipage PDF. Everything works fine except I can't figure out how to define the page margins. I want to produce 8.5x11 pages with 1\" margins on all sides. The following code produces a PDF page that is only 6.5x9. When printed on a normal sheet of paper, the margins are fine because the PDF is centered, but when viewed in a PDF viewer like Acrobat, there are no margins. ``` library(ggplot2) library(gridExtra) x <- c(5,7,2,4,9) y <- c(2,10,15,8,14) df <- data.frame(x,y) myfunct <- function(i){ p<-ggplot(df, aes(x=x,y=y*i)) + geom_point() return(p) } myGrobs <- lapply(1:10, myfunct) page <- marrangeGrob(myGrobs, nrow=3, ncol=1) ggsave(\"test pdf.pdf\", plot = page, device = \"pdf\", width = 6.5, height = 9, units=\"in\") ```",
    "author_id":5954,
    "publication_date":1754112874000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"BenW",
    "author_reputation":123.0,
    "tags":"r, ggplot2, pdf, gridextra, ggsave",
    "text_length":779,
    "title_length":49,
    "num_tags":5
  },
  {
    "id":6582,
    "title":"Truncation on Notion rollups",
    "link":"https:\/\/stackoverflow.com\/questions\/79723103\/truncation-on-notion-rollups",
    "text":"I made a project to update some Notion tables automatically. By runing this code: ``` # Get page(s) from given instance def PegarPaginas(index_instancia: int, n_paginas: int = 1): url = f\"https:\/\/api.notion.com\/v1\/databases\/`My-page-ID`\/query\" payload = {\"page_size\": n_paginas} response = requests.post(url, headers=headers, json=payload) dados = response.json() results = dados[\"results\"] return results pagina_acumulos = PegarPaginas(0)[0] n=0 for r in pagina_acumulos[\"properties\"][\"studies\"][\"relation\"]: n+=1 print(r) print(n) ``` This returns 25 rollups, output: ``` [...] {'id': '`my 24th ID`'} 24 {'id': '`my 25th ID`'} 25 ``` But it should return 28 for that table : And it seems that no matter how many items there are more than 25, it only loads 25. By changing: ``` \"studies\" ``` to ``` \"sports\" ``` corresponding to that other table I get this output: ``` {'id': '`my24th ID`'} 24 {'id': '`my 25th ID`'} 25 ``` Why might it be? Is there a way to load all of the linked pages on a rollup inside a table properly?",
    "author_id":6031,
    "publication_date":1754113453000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Matrodsilver",
    "author_reputation":5.0,
    "tags":"python, notion-api, notion",
    "text_length":1025,
    "title_length":28,
    "num_tags":3
  },
  {
    "id":6581,
    "title":"When using logging into Instagram using Instagram API, redirect does not work",
    "link":"https:\/\/stackoverflow.com\/questions\/79723109\/when-using-logging-into-instagram-using-instagram-api-redirect-does-not-work",
    "text":"I am trying to authenticate Instagram with Instagram API. This is my code: ``` class InstagramAPI { private accessToken: string | null = null; private baseUrl = \"https:\/\/graph.instagram.com\"; constructor() { \/\/ Check if we have a stored access token if (typeof window !== \"undefined\") { this.accessToken = localStorage.getItem(\"instagram_access_token\"); } } \/\/ OAuth flow - redirect to Instagram authorization initiateOAuth() { \/\/ Remove any existing access token if (typeof window !== \"undefined\") { localStorage.removeItem(\"instagram_access_token\"); } const clientId = process.env.NEXT_PUBLIC_INSTAGRAM_CLIENT_ID; const redirectUri = encodeURIComponent( window.location.origin + \"\/auth\/instagram\/callback\", ); const scope = \"user_profile,user_media\"; console.log(\"OAuth Params:\", { clientId, redirectUri, scope }); if (!clientId) { throw new Error(\"Instagram Client ID not configured\"); } const authUrl = `https:\/\/api.instagram.com\/oauth\/authorize?client_id=${clientId}&redirect_uri=${redirectUri}&scope=${scope}&response_type=code`; console.log(\"Redirecting to Instagram OAuth URL:\", authUrl); window.location.href = authUrl; } \/\/ Exchange authorization code for access token async exchangeCodeForToken(code: string): Promise<string> { try { const response = await fetch(\"\/api\/instagram\/token\", { method: \"POST\", headers: { \"Content-Type\": \"application\/json\", }, body: JSON.stringify({ code }), }); if (!response.ok) { throw new Error(\"Failed to exchange code for token\"); } const data = await response.json(); this.accessToken = data.access_token; if (typeof window !== \"undefined\") { localStorage.setItem(\"instagram_access_token\", this.accessToken!); } return this.accessToken!; } catch (error) { console.error(\"Error exchanging code for token:\", error); throw error; } } ``` This is the frontend code for how it is handled: ``` useEffect(() => { const urlParams = new URLSearchParams(window.location.search); const code = urlParams.get(\"code\"); if (code && !instagramAPI.isAuthenticated()) { setLoading(true); instagramAPI .exchangeCodeForToken(code) .then(() => { setIsAuthenticated(true); router.replace(\"\/\"); \/\/ Redirect to main page after authentication }) .finally(() => setLoading(false)); } }, [router]); const handleConnectInstagram = () => { instagramAPI.initiateOAuth(); }; ``` This initiateOAuth is called when clicking a button, which brings the user to Instagram login and should redirect back to the original page. However, it's not doing that right now and when I open developer tools and see the browser console output, this is the error I get: DTSG response is not valid: {\"__ar\":1,\"error\":1357004,\"errorSummary\":\"Sorry, something went wrong\",\"errorDescription\":\"Please try closing and re-opening your browser window.\",\"isNotCritical\":1,\"rid\":\"Abzoh9kM1lZ197XHXJWOVJd\",\"payload\":null,\"lid\":\"7533722085775331078\"}' My app is configured correctly, the redirect URI is in my settings, I am using the Instagram App ID, not Facebook App ID. I have no idea how to work around this. And sometimes, I get this output: Invalid request: Request parameters are invalid: Invalid platform app I have tried checking my Instagram App configurations, clearing cookies, using an incognito tab, everything you can think of but I still cannot get this to work.",
    "author_id":6030,
    "publication_date":1754114179000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nelson Ng",
    "author_reputation":1.0,
    "tags":"next.js, oauth-2.0, rest, instagram-api, facebook",
    "text_length":3264,
    "title_length":77,
    "num_tags":5
  },
  {
    "id":6580,
    "title":"React Native image not updating properly",
    "link":"https:\/\/stackoverflow.com\/questions\/79723138\/react-native-image-not-updating-properly",
    "text":"I'm building a React Native app (using Expo) where I display user-selected images using the default Image component. Problem: The image component still seems to use a cached version and does not reflect the updated image. When I navigate away from the component after uploading a new image and then return, it displays the previously uploaded image —even though the image should have changed. I am loading the image from Cloudinary, which overwrites the file, meaning the image at the same URL is correctly updated on Cloudinary’s end. Using expo-image-picker to select an image. I have tried using Image from expo-image: ``` import { Image } from 'expo-image' ``` ``` <View className=\"items-center\" onLayout={handleImageContainerLayout} > {imageUri ? ( <Image style={{ width: 160, height: 160, borderRadius: 9999 }} key={1} className=\"w-40 h-40 rounded-full border-4 border-yellow-400 bg-white\" source={{ uri: imageUri }} contentFit=\"cover\" cachePolicy=\"none\" \/> ) : ( <Image className=\"w-40 h-40 rounded-full border-4 border-yellow-400 bg-white\" source={profileImg} style={{ width: 160, height: 160, borderRadius: 9999 }} \/> )} <View className=\"button-container mt-2 flex-row gap-4\"> <Button title=\"Change\" color=\"green\" onPress={pickImage} \/> {imageUri && ( <Button title=\"Delete\" color=\"red\" onPress={handleDeleteButtonPress} \/> )} <\/View> <\/View> ``` I have also tried using Image from react-native: ``` import { Image } from 'react-native' ``` ``` <Image className='w-40 h-40 rounded-full border-4 border-yellow-400 bg-white' resizeMode='contain' source={{ uri: imageUri || '' }} \/> ``` The image does not update properly even after: Changing the URI by appending a timestamp: ``` source={{ uri: imageUri + `?ts=${Date.now()}` }} ``` Trying cache: 'reload' (with expo-image) Using external remote URLs",
    "author_id":6029,
    "publication_date":1754119938000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Romjan Ali",
    "author_reputation":11.0,
    "tags":"android, react-native, expo",
    "text_length":1807,
    "title_length":40,
    "num_tags":3
  },
  {
    "id":6579,
    "title":"CMake Qt project suddenly failing: “Cannot find source file” with duplicated file path after Git reset \/ vcpkg changes",
    "link":"https:\/\/stackoverflow.com\/questions\/79723141\/cmake-qt-project-suddenly-failing-cannot-find-source-file-with-duplicated-fil",
    "text":"Context: I have a CMake project that was working for building a Qt6 app using MSVC 2022, CMake, and vcpkg (manifest mode). I made some changes to try and get the project running fully via vcpkg, didn’t succeed, so I checked out my previously working branch (and even downloaded a fresh copy from GitHub as a zip). Now, out of nowhere, CMake refuses to generate build files. ``` Error: CMake Error at C:\/Users\/Fabian\/dev\/vcpkg\/scripts\/buildsystems\/vcpkg.cmake:600 (_add_executable): Cannot find source file: C:\/Users\/Fabian\/dev\/repos\/Users\/Fabian\/dev\/repos\/InvokeInvoiceSystem\/apps\/src\/main.cpp ``` Notice the file path is duplicated (my repo is at C:\/Users\/Fabian\/dev\/repos\/InvokeInvoiceSystem). There is no such path as \/Users\/Fabian\/dev\/repos\/Users\/Fabian\/dev\/repos\/.... Things I’ve tried: Cleaned build directory (out\/build\/*), deleted CMake cache. Fresh clone from GitHub (confirmed main.cpp exists at src\/app\/main.cpp). Switching between branches, hard reset, etc. Manually checked all CMakeLists.txt for obvious path errors. Project structure (partial): ``` ├── src\/ │ ├── InvoiceSystem\/ │ │ └── ... [core logic] │ └── app\/ │ └── main.cpp ├── CMakeLists.txt ├── src\/app\/CMakeLists.txt Root CMakeLists.txt (snippet): ``` ``` cmake_minimum_required(VERSION 3.24) project(InvokeInvoiceSystem LANGUAGES CXX) set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_PREFIX_PATH \"C:\/Users\/Fabian\/dev\/vcpkg\/installed\/x64-windows\/share\" \"C:\/Qt\/6.9.1\/msvc2022_64\/lib\/cmake\" CACHE STRING \"Paths to vcpkg installed\/share and Qt6 install prefix\" ) # ...find_package\/fmt\/bsoncxx\/etc... add_subdirectory(src\/app) src\/app\/CMakeLists.txt (snippet): file(GLOB_RECURSE APP_SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}\/*.cpp\" # ...other globs for .ui, .qrc, headers ) if(WIN32) qt_add_executable(InvokeInvoiceSystem WIN32 .\/main.cpp # ...other sources, all as relative paths ) endif() ``` What I’m seeing: CMake seems to concatenate paths in a weird way when trying to resolve main.cpp. I changed the folder structure trying to make it more like the qt CMake example but and this is when the error started. But it didn't even work when trying a previously working push. Worked fine before trying out vcpkg. Questions: What could cause CMake to generate these broken\/duplicated file paths out of nowhere? Is there something wrong with the way I’m specifying sources in qt_add_executable? Is this a known problem when switching between vcpkg\/non-vcpkg builds, or with CMake path caching? Any advice would be appreciated! I’ve tried deleting build\/cache, confirming paths, and even a clean clone but nothing fixes the path duplication issue.",
    "author_id":6028,
    "publication_date":1754120069000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"FatherFabian",
    "author_reputation":11.0,
    "tags":"windows, qt, visual-c++, cmake, vcpkg",
    "text_length":2633,
    "title_length":118,
    "num_tags":5
  },
  {
    "id":6578,
    "title":"Is LR(0) GOTO different from LR(1) GOTO?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723143\/is-lr0-goto-different-from-lr1-goto",
    "text":"I'm writing an LALR(1) parser generator, and am getting a bit confused about the use of the GOTO function in creating the parsing table. I've been following the dragon book. The grammar I have been working with is this one: ``` S -> E E -> E + T | T T -> T * F | F F -> ( E ) | id ``` The LR(0) starting state is the closure of ``` {[S -> .E]} ``` . Call this state 0. In LR(0), we have ``` GOTO(0, T) = {[E -> T.], [T -> T .* F]} ``` . Call this state 1. In constructing the LALR(1) automaton, we attach lookaheads to the kernels of the LR(0) sets. Upon doing this, we get the kernel ``` {[S -> .E, $]} ``` for state 0 and ``` {[E -> T., $\/+\/)], [T -> T .* F, $\/+\/*\/)]} ``` for state 1. Now, when I put this grammar into a standard parser generator, it says that in the LALR(1) parsing table, ``` GOTO(0, T) = 1 ``` , which makes sense since that's what it was in LR(0). However, this is not what I get. The relevant part of the LR(1) closure in state 0 is ``` {[E -> .T, $\/+], [T -> .T * F, $\/+\/*]} ``` . ``` GOTO(0, T) ``` is then defined as the closure of ``` {[E -> T., $\/+], [T -> T .* F, $\/+\/*]} ``` . Since there are no nonterminals after the dots in these items, the closure does nothing, making ``` GOTO(0, T) = {E -> T., $\/+], [T -> T .* F, $\/+\/*]} ``` . Note that this is not equal to state 1, which is ``` {[E -> T., $\/+\/)], [T -> T .* F, $\/+\/*\/)]} ``` ! So using the LR(1) ``` GOTO ``` , we have ``` GOTO(0, T) ≠ 1 ``` , even though with the LR(0) ``` GOTO ``` , we have ``` GOTO(0, T) = 1 ``` . As I said above, all the parser generators I've tested say that in LALR(1), ``` GOTO(0, T) = 1 ``` . So what am I doing wrong? Is there a mistake in my calculation of the LR(1) ``` GOTO ``` ? Am I supposed to use the LR(0) ``` GOTO ``` when constructing the LALR(1) parsing table? Or maybe I'm making some other mistake?",
    "author_id":6027,
    "publication_date":1754120295000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"sudgy",
    "author_reputation":571.0,
    "tags":"parsing, compiler-construction, lalr",
    "text_length":1830,
    "title_length":40,
    "num_tags":3
  },
  {
    "id":6577,
    "title":"AllUsers have Storage Viewer Permission but removing it creates error",
    "link":"https:\/\/stackoverflow.com\/questions\/79723151\/allusers-have-storage-viewer-permission-but-removing-it-creates-error",
    "text":"I have a bucket that has AllUsers permission set to Storage Object Viewer. There's an exclamation mark on GCP about how this is not safe. I have Firebase rules that govern how images\/videos are uploaded, deleted and updated. When i remove the Storage Object Viewer from AllUsers, then some of the images are not visible with errors, \"file not found\" even when the images are in the same folders and have the same storage rules. Users do not need authentication to view the images and videos. There are some folders in the bucket that shouldn't be read but don't have firebase rules because users don't interact with them. ``` match \/images\/{userId}\/{image} { allow read: if true; allow create, update: if request.auth != null && request.auth.uid == userId && request.resource.size < 3 * 1024 * 1024 && request.resource.contentType.matches('image\/.*'); allow delete: if request.auth != null && request.auth.uid == userId } ``` The code that pulls a specific users images: ``` useEffect(() => { const colRef = collection(db, \"images\"); const q = query( colRef, where(\"user\", \"==\", profile.user), where(\"adminPublish\", \"==\", true) ); onSnapshot(q, (snapshot) => { const docs = snapshot.docs; \/\/ setImageIDs(docs.map((doc) => doc.id)); setImageList(docs.map((doc) => doc.data())); setLightBox(docs.map((doc) => doc.data().imageLinks.imageKitUrl50)); }); }, [profile]) return ( <> <div className=\"relative md:grid md:grid-cols-2 gap-2\"> <div className=\"left-2 top-2 absolute z-10\"> <button className=\"bg-orange-100 border border-orange-200 mx-auto p-2 shadow-lg motion-preset-seesaw\" onClick={() => setToggler(!toggler)} > View Gallery ({profile.images}) <\/button> <FsLightbox toggler={toggler} type=\"image\" sources={lightBox} \/> <\/div> <div className=\"grid grid-cols-2 gap-1\"> {imageList && imageList.slice(1, 5).map((image, index) => { return ( <div key={index} data-cy=\"portfolios-div\" className=\"w-full h-full p-0 m-0\" > <img src={ image.imageDetails.resolution < 25000000 ? image.imageLinks.imageKitUrl50 : image.imageURL } alt={`photographer gallery-${index}`} fetchPriority=\"high\" className=\"motion-translate-y-in-100 h-48 w-96 object-cover rounded-md border border-orange-500\" \/> <\/div> ); })} <\/div> <\/div> <\/> ``` How do i remove allUsers allowing Storage Object Viewer and still get images\/videos to be viewable? Should i create storage rules for storage folder in the same bucket that users don't have access to?",
    "author_id":6026,
    "publication_date":1754121243000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Coolkid",
    "author_reputation":587.0,
    "tags":"google-cloud-platform, firebase, google-cloud-storage, firebase-security",
    "text_length":2419,
    "title_length":69,
    "num_tags":4
  },
  {
    "id":6576,
    "title":"Why the maze i build the left and bottom outer walls are doubled instead single wall? and how to make it a single outer wall like in the right and top",
    "link":"https:\/\/stackoverflow.com\/questions\/79723159\/why-the-maze-i-build-the-left-and-bottom-outer-walls-are-doubled-instead-single",
    "text":"this screenshot show the maze and in this example the maze size is 30,30 and the left and bottom outer wall is two walls instead one. for example wallprefab at position 28,1,13: and the next to him on the left a wallprefab at position 29,1,13: this is the script i use to generate the maze: ``` using UnityEngine; public class MazeGenerator : MonoBehaviour { public int width = 10; public int height = 10; public GameObject wallPrefab; public GameObject floorPrefab; private bool[,] maze; void Start() { GenerateMaze(); BuildMaze(); } void GenerateMaze() { maze = new bool[width, height]; for (int x = 0; x < width; x++) { for (int y = 0; y < height; y++) { maze[x, y] = true; } } int startX = 1, startY = 1; maze[startX, startY] = false; CarvePath(startX, startY); } void CarvePath(int x, int y) { int[] directions = { 0, 1, 2, 3 }; ShuffleArray(directions); foreach (int dir in directions) { int nx = x, ny = y; switch (dir) { case 0: ny -= 2; break; \/\/ up case 1: ny += 2; break; \/\/ down case 2: nx -= 2; break; \/\/ left case 3: nx += 2; break; \/\/ right } if (nx > 0 && nx < width - 1 && ny > 0 && ny < height - 1 && maze[nx, ny]) { maze[nx, ny] = false; maze[(x + nx) \/ 2, (y + ny) \/ 2] = false; CarvePath(nx, ny); } } } void BuildMaze() { for (int x = 0; x < width; x++) { for (int y = 0; y < height; y++) { if (maze[x, y]) { Vector3 position = new Vector3(x, 1, y); Instantiate(wallPrefab, position, Quaternion.identity, transform); } } } Vector3 floorPosition = new Vector3(width \/ 2f, 0, height \/ 2f); GameObject floor = Instantiate(floorPrefab, floorPosition, Quaternion.identity, transform); floor.transform.localScale = new Vector3(width \/ 10f, 1, height \/ 10f); } void ShuffleArray(int[] array) { for (int i = array.Length - 1; i > 0; i--) { int randomIndex = Random.Range(0, i + 1); int temp = array[i]; array[i] = array[randomIndex]; array[randomIndex] = temp; } } } ``` if i will do here if i add - 1 ``` void BuildMaze() { for (int x = 0; x < width - 1; x++) { for (int y = 0; y < height - 1; y++) ``` then now on the left and bottom there will be a single wall but then the maze is not at the size 30,30 as i set it in the inspector. the maze itself should be at indexes 1 and 28 including and the outer walls should be at indexes 0 and 29. so, what am i missing here?",
    "author_id":4873,
    "publication_date":1754121796000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Daniel Lip",
    "author_reputation":11455.0,
    "tags":"c#, unity-game-engine",
    "text_length":2284,
    "title_length":150,
    "num_tags":2
  },
  {
    "id":6575,
    "title":"Public swagger endpoints giving UNAUTHORIZED error in Spring Security",
    "link":"https:\/\/stackoverflow.com\/questions\/79723164\/public-swagger-endpoints-giving-unauthorized-error-in-spring-security",
    "text":"I have a Spring Boot project and am using Firebase Auth with Spring Security. I am also configuring Swagger for API documentation. My code is: ``` package com.ayushsingh.doc_helper.commons.config.security; import java.time.Instant; import com.ayushsingh.doc_helper.commons.constants.AuthConstants; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.security.config.annotation.method.configuration.EnableMethodSecurity; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity; import org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer; import org.springframework.security.config.http.SessionCreationPolicy; import org.springframework.security.web.AuthenticationEntryPoint; import org.springframework.security.web.SecurityFilterChain; import org.springframework.security.web.access.AccessDeniedHandler; import org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter; import jakarta.servlet.http.HttpServletResponse; @Configuration @EnableWebSecurity @EnableMethodSecurity(prePostEnabled = true) public class SecurityConfig { private final FirebaseAuthFilter firebaseAuthFilter; private final FirebaseAuthenticationProvider firebaseAuthenticationProvider; public SecurityConfig(FirebaseAuthFilter firebaseAuthFilter, FirebaseAuthenticationProvider firebaseAuthenticationProvider) { this.firebaseAuthFilter = firebaseAuthFilter; this.firebaseAuthenticationProvider = firebaseAuthenticationProvider; } @Bean public SecurityFilterChain apiFilterChain(HttpSecurity http) throws Exception { http .csrf(AbstractHttpConfigurer::disable) .sessionManagement(session -> session.sessionCreationPolicy(SessionCreationPolicy.STATELESS)) .authorizeHttpRequests(auth -> auth .requestMatchers(AuthConstants.AUTH_API_PATTERN, \"\/swagger-ui\/**\", \"\/swagger-ui.html\", \"\/webjars\/**\", \"\/configuration\/security\", \"\/swagger-resources\/**\", \"\/swagger-resources\", \"\/v3\/api-docs\/**\", \"\/v3\/api-docs\" ) .permitAll() .anyRequest().authenticated()) .authenticationProvider(firebaseAuthenticationProvider) .addFilterBefore(firebaseAuthFilter, UsernamePasswordAuthenticationFilter.class) .exceptionHandling(ex -> ex .authenticationEntryPoint(firebaseAuthenticationEntryPoint()) .accessDeniedHandler(accessDeniedHandler())); return http.build(); } @Bean public AuthenticationEntryPoint firebaseAuthenticationEntryPoint() { return (request, response, authException) -> { response.setStatus(HttpServletResponse.SC_UNAUTHORIZED); response.setContentType(\"application\/json\"); response.setCharacterEncoding(\"UTF-8\"); String jsonResponse = \"\"\" { \"success\": false, \"error\": { \"code\": \"UNAUTHORIZED\", \"message\": \"Authentication required. Please provide a valid Firebase token.\", \"timestamp\": \"%s\" } } \"\"\".formatted(Instant.now()); response.getWriter().write(jsonResponse); }; } @Bean public AccessDeniedHandler accessDeniedHandler() { return (request, response, accessDeniedException) -> { response.setStatus(HttpServletResponse.SC_FORBIDDEN); response.setContentType(\"application\/json\"); response.setCharacterEncoding(\"UTF-8\"); String jsonResponse = \"\"\" { \"success\": false, \"error\": { \"code\": \"FORBIDDEN\", \"message\": \"Insufficient privileges to access this resource.\", \"timestamp\": \"%s\" } } \"\"\".formatted(Instant.now()); response.getWriter().write(jsonResponse); }; } } ``` I have also created a AuthFilter for Firebase- ``` package com.ayushsingh.doc_helper.commons.config.security; import java.io.IOException; import com.ayushsingh.doc_helper.commons.constants.AuthConstants; import org.springframework.security.core.context.SecurityContextHolder; import org.springframework.stereotype.Component; import org.springframework.web.filter.OncePerRequestFilter; import com.ayushsingh.doc_helper.features.auth.domain.AuthUser; import com.ayushsingh.doc_helper.features.user.domain.User; import com.ayushsingh.doc_helper.features.user.service.UserService; import com.google.firebase.auth.FirebaseAuth; import com.google.firebase.auth.FirebaseAuthException; import com.google.firebase.auth.FirebaseToken; import jakarta.servlet.FilterChain; import jakarta.servlet.ServletException; import jakarta.servlet.http.HttpServletRequest; import jakarta.servlet.http.HttpServletResponse; import lombok.extern.slf4j.Slf4j; @Component @Slf4j public class FirebaseAuthFilter extends OncePerRequestFilter { private final FirebaseAuth firebaseAuth; private final UserService userService; public FirebaseAuthFilter(FirebaseAuth firebaseAuth, UserService userService) { this.firebaseAuth = firebaseAuth; this.userService = userService; } @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { String token = getTokenFromRequest(request); if (token != null) { try { FirebaseToken decodedToken = firebaseAuth.verifyIdToken(token); String firebaseUid = decodedToken.getUid(); User user = userService.findByFirebaseUid(firebaseUid); if (user != null) { AuthUser authUser = new AuthUser(user); FirebaseAuthenticationToken authentication = new FirebaseAuthenticationToken(authUser, authUser.getAuthorities()); SecurityContextHolder.getContext().setAuthentication(authentication); } else { log.warn(\"User not found for Firebase UID: {}\", firebaseUid); SecurityContextHolder.clearContext(); } } catch (FirebaseAuthException e) { log.error(\"Firebase token verification failed: {}\", e.getMessage()); SecurityContextHolder.clearContext(); } } filterChain.doFilter(request, response); } private String getTokenFromRequest(HttpServletRequest request) { String bearerToken = request.getHeader(AuthConstants.AUTHORIZATION_HEADER); if (bearerToken != null && bearerToken.startsWith(AuthConstants.BEARER)) { return bearerToken.substring(7); } return null; } @Override protected boolean shouldNotFilter(HttpServletRequest request) { String path = request.getServletPath(); String method = request.getMethod(); \/\/ Skip filter for public endpoints var skipFilter = (path.startsWith(AuthConstants.AUTH_API_PREFIX) && \"POST\".equals(method)) || path.startsWith(\"\/api\/auth\/\") || path.startsWith(\"\/swagger-ui\") || path.startsWith(\"\/swagger-resources\") || path.startsWith(\"\/v3\/api-docs\") || path.startsWith(\"\/webjars\") || path.equals(\"\/swagger-ui.html\") || path.endsWith(\".js\") || path.endsWith(\".css\") || path.endsWith(\".html\") || path.endsWith(\".png\") || path.endsWith(\".ico\") || path.endsWith(\".map\"); System.out.println(\"Skip filter: \" + skipFilter+\" request: \"+request.getRequestURI()); return skipFilter; } } ``` When I hit any of the swagger endpoint, I get the response: ``` { \"success\": false, \"error\": { \"code\": \"UNAUTHORIZED\", \"message\": \"Authentication required. Please provide a valid Firebase token.\", \"timestamp\": \"2025-08-02T06:12:36.329330400Z\" } } ``` I checked my public endpoint config using a ``` \/test ``` endpoint and it works fine, but I am unable to access the swagger endpoints.",
    "author_id":5935,
    "publication_date":1754122575000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"ayush",
    "author_reputation":694.0,
    "tags":"java, spring, spring-boot, spring-security, swagger-ui",
    "text_length":7087,
    "title_length":69,
    "num_tags":5
  },
  {
    "id":6574,
    "title":"Why does the spring boot maven’s build-image goal implements significant portions of the pack cli",
    "link":"https:\/\/stackoverflow.com\/questions\/79723173\/why-does-the-spring-boot-maven-s-build-image-goal-implements-significant-portion",
    "text":"The Spring Boot Maven plugin appears to replicate functionality already available in the pack CLI. Specifically, it: Extracts the repackaged JAR and prepares it for buildpacks. Contains logic for resolving the appropriate builder image. Constructs and executes container-related commands. This design introduces the need for Docker daemon configuration within the Maven plugin itself. I’m trying to understand the rationale behind this approach. Was there a technical limitation or use case that required reimplementing this logic within the plugin rather than invoking the pack CLI directly (e.g., via a wrapper or Docker container)? Are there historical or architectural reasons documented anywhere that explain why the plugin doesn’t simply delegate to pack? References to documentation, GitHub issues, or historical context around this design would be very helpful.",
    "author_id":6025,
    "publication_date":1754124284000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"buskila",
    "author_reputation":254.0,
    "tags":"spring-boot, buildpack",
    "text_length":869,
    "title_length":97,
    "num_tags":2
  },
  {
    "id":6573,
    "title":"Maui, Osx, CapturePhotoAsync gives me a black window",
    "link":"https:\/\/stackoverflow.com\/questions\/79723178\/maui-osx-capturephotoasync-gives-me-a-black-window",
    "text":"Calling ``` MediaPicker.Default.CapturePhotoAsync() ``` gives me a black screen when running on OSX (works fine in android emulator) --- TL;DR --- My code is ``` var photo = await MediaPicker.Default.CapturePhotoAsync(); ``` and my Info.plist file contain ``` <key>NSCameraUsageDescription<\/key> <string>This app needs access to the camera to take photos.<\/string> <key>NSMicrophoneUsageDescription<\/key> <string>This app needs access to microphone for taking videos.<\/string> <key>NSPhotoLibraryAddUsageDescription<\/key> <string>This app needs access to the photo gallery for picking photos and videos.<\/string> <key>NSPhotoLibraryUsageDescription<\/key> <string>This app needs access to photos gallery for picking photos and videos.<\/string> ``` as per documentation I run on OSX and when I take a photograph in the android emulator it works as expected - I get a simulated camera view. But when I do the same in OSX (I believe it runs Maccatalyst) all I get is a black screen with a redish, unclickable, camera button. My Maui dependencies are 9.0.90 (latest at the time of writing (I have also tried an older 9.0.x version))",
    "author_id":6024,
    "publication_date":1754125066000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"LosManos",
    "author_reputation":7772.0,
    "tags":"maui, maui-community-toolkit, maui-blazor, mac-catalyst",
    "text_length":1127,
    "title_length":52,
    "num_tags":4
  },
  {
    "id":6572,
    "title":"Configure Kafka with multiple Consumers reading the same message",
    "link":"https:\/\/stackoverflow.com\/questions\/79723185\/configure-kafka-with-multiple-consumers-reading-the-same-message",
    "text":"I need to use Kafka where there will be multiple Consumers reading from the same Topic but each Consumer will need to read the same message. From what I have read, this can be achieved by having Consumer Groups. Unfortunately, I have not found any examples of how to configure Kafka to achieve this. Can someone please point me to a detailed example",
    "author_id":6023,
    "publication_date":1754126826000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Andy5",
    "author_reputation":2431.0,
    "tags":"apache-kafka",
    "text_length":349,
    "title_length":64,
    "num_tags":1
  },
  {
    "id":6571,
    "title":"How can I prevent console errors when embedding Calendly in Next.js app (inline, popup, or link)?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723192\/how-can-i-prevent-console-errors-when-embedding-calendly-in-next-js-app-inline",
    "text":"When embedding Calendly into my Next.js app using the react-calendly package , whether as a link, popup, or inline widget, I consistently get the following errors in the browser console: Failed to load resource: net::ERR_BLOCKED_BY_CLIENT [Violation] Potential permissions policy violation: payment is not allowed in this document. Event {isTrusted: true, type: 'error', target: script, currentTarget: null, eventPhase: 0, …} From what I’ve read and tested: These warnings seem to come from third-party scripts Calendly uses (like Facebook Pixel, Stripe, or Recaptcha). The widget still works as expected, meeting booking functions normally. These errors can usually be ignored in development, but they still show up in production too. That said, I’d really prefer to keep the console clean, especially for QA or when sharing with clients. Is there any official way to suppress these errors or load a minimal embed that avoids the extra scripts? ``` \"use client\"; import { InlineWidget } from \"react-calendly\"; export default function ContactPage() { return ( <div className=\"p-8\"> <InlineWidget url=\"https:\/\/calendly.com\/traezeeofor\/new-meeting\" \/> <\/div> ); } ```",
    "author_id":6022,
    "publication_date":1754127623000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Trae Zeeofor",
    "author_reputation":56.0,
    "tags":"next.js, calendly",
    "text_length":1165,
    "title_length":97,
    "num_tags":2
  },
  {
    "id":6570,
    "title":"How to enable Spring AI observability labels - spring.ai.tool.call.arguments?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723194\/how-to-enable-spring-ai-observability-labels-spring-ai-tool-call-arguments",
    "text":"In the documentation, in the observability section, in the \"Tool Calling\" section, in the \"High Cardinality Keys\" table, it says that the labels ``` spring.ai.tool.call.arguments ``` ``` spring.ai.tool.call.result ``` Only appear in traces when enabled. My question is, how do I enable them? Is this a Spring project property? I made a simple project, an mcp client and another mcp server, and uploaded a stack to docker compose with observability to view traces and metrics.",
    "author_id":6021,
    "publication_date":1754127867000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Thales Sant&#39;Anna",
    "author_reputation":1.0,
    "tags":"java, spring, artificial-intelligence, spring-ai",
    "text_length":475,
    "title_length":77,
    "num_tags":4
  },
  {
    "id":6569,
    "title":"Running GitLab pipeline when there is no commit tag",
    "link":"https:\/\/stackoverflow.com\/questions\/79723199\/running-gitlab-pipeline-when-there-is-no-commit-tag",
    "text":"I'd like to only run a GitLab pipeline job when the commit is on master and there is no tag on that commit (to avoid triggering both a regular check and the build pipeline on the same commit). I'm unsure how to do that exactly. I was thinking about ``` if: $CI_COMMIT_BRANCH=='master' && $CI_COMMIT_TAG=='' ``` The branch parts works but I'm unsure about the tag check. What value does ``` CI_COMMIT_TAG ``` have if there is no tag?",
    "author_id":6020,
    "publication_date":1754128778000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dacid99",
    "author_reputation":1.0,
    "tags":"gitlab-ci",
    "text_length":432,
    "title_length":51,
    "num_tags":1
  },
  {
    "id":6568,
    "title":"How to set filter for lookup column in Canvas app?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723204\/how-to-set-filter-for-lookup-column-in-canvas-app",
    "text":"I'm trying to set filter on the list from single Dataverse table for rows where SuperVisor = 'Mike*'. And It doesn't work. You can see that Supervisor is lookup column for another table ``` Contacts ``` and another column linked in the same way to very same ``` Contact ``` . I tried to use ``` ThisItem.Contact.'Full Name' ``` it didn't work, and even if it will work how I can tell that lookup ID coming from Supervisor column not from Provided column. Too many confusions. Unfortunately I don't have control over my Dataverse table, it was already there and I can not modify it. Looking at the row table I could not see what is real ID value for Contact.FullName. How it can be handled? Is it possible at all? Please refer to pic below where you can see all configuration. My Fx line look like this, there are 1 more filter here for normal field Title which works fine ``` Items Fx::: Filter( && StartsWith( Title,\"d\") && StartsWith(Supervisor,\"M\")) ```",
    "author_id":6019,
    "publication_date":1754129296000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mich28 ",
    "author_reputation":500.0,
    "tags":"powerapps",
    "text_length":956,
    "title_length":50,
    "num_tags":1
  },
  {
    "id":6567,
    "title":"What does `d@:&gt;.:&#39;` do in the K language?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723207\/what-does-d-do-in-the-k-language",
    "text":"I'm trying to understand the ``` .Q.chk ``` function. The definition of ``` .Q.chk ``` is: ``` k){if[x~(::);'\"missing dir argument\"];f:{`\/:'x,'d@&(d:!x)like\"[0-9]*\"};d@:>.:'$last'`\\:'d:$[`par.txt in!x;,\/f'-1!'`$0:`\/:x,`par.txt;f x] ``` ``` {[e;u;d]u[i]{.[x;(y;`);:;?[z;();0b;()]]}[d]'e i:&~u in!d}[d[(+u in\/:t)?\\:1b](0#.)'u,'`;u:?,\/t:!:'d]'d} ``` Note that this appears to be a two-line function, with separation of instructions by newline rather than semicolon. For an input like ``` `:c:\/db\/ ``` or other variants of the path ``` :c:\/db ``` ``` `$\":c:\\\\db\" ``` ``` `$\":c:\\\\db\\\\\" ``` the part of the code processing the path fails: ``` d@:>.:'$last'`\\:'d:$[`par.txt in!x;,\/f'-1!'`$0:`\/:x,`par.txt;f x] ``` It partially runs fine for ``` $last'`\\:'d:$[`par.txt in!x;,\/f'-1!'`$0:`\/:x,`par.txt;f x] ``` yielding ``` (\":c:\/db\";\"\") ``` for input ``` :c:\/db\/ ``` But I don't understand the rest of the line though: ``` d@:>.:' ``` . What does this do?",
    "author_id":5861,
    "publication_date":1754129475000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Gabi",
    "author_reputation":455.0,
    "tags":"kdb+, k",
    "text_length":946,
    "title_length":48,
    "num_tags":2
  },
  {
    "id":6566,
    "title":"Is there an example of an implementation based on ardalis clean architecture?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723213\/is-there-an-example-of-an-implementation-based-on-ardalis-clean-architecture",
    "text":"I see that ardalis updates the ewebshop example but that does not implement the full ardalis framework. The example of contributor is very limited. I would like to see a full implementation of ewebshop using ardalis. So including ardalis repositories, specification, smart enum etc ...",
    "author_id":6018,
    "publication_date":1754130360000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ren&#233; Titulaer",
    "author_reputation":23.0,
    "tags":"ardalis-cleanarchitecture",
    "text_length":285,
    "title_length":77,
    "num_tags":1
  },
  {
    "id":6565,
    "title":"Pylance in devcontainer is loading forever",
    "link":"https:\/\/stackoverflow.com\/questions\/79723215\/pylance-in-devcontainer-is-loading-forever",
    "text":"I am trying to set up a devcontainer on my desktop, but functions like autocomplete or go to function don't work. When I hover variable or function it shows never-ending load information Loading information . I was trying use different Pylance versions and different images but results were the same. I have this problem with Jupyter notebooks and regular python files. Only noticeable thing that I saw was progressive usage of RAM throught the resource monitor in VS Code. When RAM hits the threshold of 6.5GB, then it suddenly decrease to 2.5GB and Pylance switch port to a different one. This cycle repeats infinitely. Also, I got this message Enumeration of workspace even if workspace directory contains only test file like this. ``` test = 'test' print(test) ``` Notebooks and files are executing normally. I was using Pylance 2025.7.1, VS Code 1.102.3, Docker 4.43.1 with WSL 2.5.9.0 and Windows 11. Dockerfile ``` FROM pytorch\/pytorch:2.7.1-cuda12.8-cudnn9-devel RUN apt-get update && apt-get upgrade -y RUN apt-get -y install git RUN pip install jupyter ipykernel packaging ninja mlflow tqdm torchinfo matplotlib RUN pip install flash-attn --no-build-isolation RUN pip install \"unsloth[cu128-torch271] @ git+https:\/\/github.com\/unslothai\/unsloth.git\" RUN pip install triton --extra-index-url \"https:\/\/download.pytorch.org\/whl\/cu128\" ``` Docker-compose.yaml ``` services: pytorch: container_name: pytorch build: context: . dockerfile: Dockerfile ports: - \"8888:8888\" environment: - JUPYTER_TOKEN=easy working_dir: \/ volumes: - ..\/:\/workspace deploy: resources: reservations: devices: - driver: nvidia capabilities: [\"gpu\"] device_ids: [\"0\"] command: sleep infinity ``` devcontainer.json ``` { \"name\": \"Pytorch_devcontainer\", \"dockerComposeFile\": \".\/docker-compose.yaml\", \"workspaceFolder\": \"\/\", \"shutdownAction\": \"stopCompose\", \"service\": \"pytorch\", \"customizations\": { \"vscode\": { \"extensions\": [ \"ms-toolsai.jupyter\", \"ms-toolsai.vscode-jupyter-cell-tags\", \"ms-toolsai.jupyter-renderers\", \"ms-toolsai.vscode-jupyter-slideshow\", \"christian-kohler.path-intellisense\", \"ms-python.vscode-pylance\", \"ms-python.python\", \"mutantdino.resourcemonitor\", \"redhat.vscode-yaml\" ] } } } ```",
    "author_id":6017,
    "publication_date":1754130502000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"I&#39;mStuckOnLine911",
    "author_reputation":19.0,
    "tags":"python, visual-studio-code, pylance, vscode-devcontainer, devcontainer",
    "text_length":2185,
    "title_length":42,
    "num_tags":5
  },
  {
    "id":6564,
    "title":"Unity UI is different in game view and scene view",
    "link":"https:\/\/stackoverflow.com\/questions\/79723222\/unity-ui-is-different-in-game-view-and-scene-view",
    "text":"The game window aspect ratio is 1920 by 1080, I use a sprite graph on the image component, and the actual image shown is 20x5 pixels, the sprite graph modifies the alpha value, the alpha value doesn't seem to carry over to the game window, it also doesn't show in the material preview (alpha part is blue here), In scene view everything is fine. I do use the compatibility mode in unity 6 (6000.0.29f1) Scene View Game View",
    "author_id":6016,
    "publication_date":1754131309000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"igr2020",
    "author_reputation":3.0,
    "tags":"unity-game-engine, alpha, unity-ui",
    "text_length":423,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":6563,
    "title":"ERROR [org.keycloak.services.error.KeycloakErrorHandler] No identifier provider for identity",
    "link":"https:\/\/stackoverflow.com\/questions\/79723226\/error-org-keycloak-services-error-keycloakerrorhandler-no-identifier-provider",
    "text":"I was using Twitter\/X as a identity provider of KeyCloak(keycloak:26.3.1). I was using Oauth2, some config is following: config of keycloak ]( https:\/\/i.sstatic.net\/cW4zuHAg.png ) config of keycloak ]( https:\/\/i.sstatic.net\/tCdaIAry.png ) config of keycloak ]( https:\/\/i.sstatic.net\/oJwlFrlA.png ) config of keycloak ]( https:\/\/i.sstatic.net\/f5OFrgK6.png ) succeed to recall keycloak ]( https:\/\/i.sstatic.net\/Bwi2TGzu.png ) failed ]( https:\/\/i.sstatic.net\/ESToh0ZP.png ) But when I click the X to sign in, everything is OK, even X gives the message to keycloak. However, the keycloak can not extract the id, username. ``` 2025-08-02 08:37:35,839 DEBUG [org.keycloak.social.user_profile_dump] (executor-thread-26) User Profile JSON Data for provider X: {\"data\":{\"confirmed_email\":\"mygmail@gmail.com\",\"name\":\"myname\",\"username\":\"username\",\"id\":\"id\"}} 2025-08-02 08:37:35,840 DEBUG [org.hibernate.orm.sql.ast.create] (executor-thread-26) Created new SQL alias : ipme1_0 2025-08-02 08:37:35,840 DEBUG [org.hibernate.orm.sql.ast.create] (executor-thread-26) Registration of TableGroup [StandardTableGroup(org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375))] with identifierForTableGroup [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity] for NavigablePath [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity] 2025-08-02 08:37:35,841 DEBUG [org.hibernate.query.sqm.sql.BaseSqmToSqlAstConverter] (executor-thread-26) Determining mapping-model type for SqmParameter : org.hibernate.query.sqm.tree.expression.SqmJpaCriteriaParameterWrapper@386a06e 2025-08-02 08:37:35,841 DEBUG [org.hibernate.query.sqm.sql.BaseSqmToSqlAstConverter] (executor-thread-26) Determining mapping-model type for SqmPath : SqmBasicValuedSimplePath(org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).realmId) 2025-08-02 08:37:35,841 DEBUG [org.hibernate.query.sqm.sql.BaseSqmToSqlAstConverter] (executor-thread-26) Determining mapping-model type for SqmParameter : org.hibernate.query.sqm.tree.expression.SqmJpaCriteriaParameterWrapper@11dbb523 2025-08-02 08:37:35,841 DEBUG [org.hibernate.query.sqm.sql.BaseSqmToSqlAstConverter] (executor-thread-26) Determining mapping-model type for SqmPath : SqmBasicValuedSimplePath(org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).identityProviderAlias) 2025-08-02 08:37:35,841 DEBUG [org.hibernate.orm.results.graph.AST] (executor-thread-26) DomainResult Graph: -EntityResultImpl [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375)] | +-DelayedCollectionFetch [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).config] | +-BasicFetch [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).identityProviderAlias] | +-BasicFetch [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).identityProviderMapper] | +-BasicFetch [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).name] | -BasicFetch [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).realmId] 2025-08-02 08:37:35,841 DEBUG [org.hibernate.orm.sql.ast.tree] (executor-thread-26) SQL AST Tree: SelectStatement { FromClause { StandardTableGroup (ipme1 : org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375)) { primaryTableReference : IDENTITY_PROVIDER_MAPPER as ipme1_0 } } } 2025-08-02 08:37:35,841 DEBUG [org.hibernate.orm.sql.exec] (executor-thread-26) Skipping reading Query result cache data: cache-enabled = false, cache-mode = NORMAL 2025-08-02 08:37:35,841 DEBUG [org.hibernate.SQL] (executor-thread-26) select ipme1_0.ID,ipme1_0.IDP_ALIAS,ipme1_0.IDP_MAPPER_NAME,ipme1_0.NAME,ipme1_0.REALM_ID from IDENTITY_PROVIDER_MAPPER ipme1_0 where ipme1_0.REALM_ID=? and ipme1_0.IDP_ALIAS=? order by ipme1_0.ID 2025-08-02 08:37:35,844 DEBUG [org.hibernate.orm.results] (executor-thread-26) Initializer list: org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).config -> DelayedCollectionInitializer(org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375).config)@43029102 (PluralAttribute(org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config)) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375) -> EntityJoinedFetchInitializer(org.keycloak.models.jpa.entities.IdentityProviderMapperEntity(375))@685293576 (SingleTableEntityPersister(org.keycloak.models.jpa.entities.IdentityProviderMapperEntity)) 2025-08-02 08:37:35,845 DEBUG [org.hibernate.orm.loader.multi] (executor-thread-26) Batch fetching collection: org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config.1a448657-0987-4717-a523-68c5ea49e974 2025-08-02 08:37:35,845 DEBUG [org.hibernate.orm.sql.exec] (executor-thread-26) Skipping reading Query result cache data: cache-enabled = false, cache-mode = IGNORE 2025-08-02 08:37:35,845 DEBUG [org.hibernate.SQL] (executor-thread-26) select c1_0.IDP_MAPPER_ID,c1_0.NAME,c1_0.VALUE from IDP_MAPPER_CONFIG c1_0 where c1_0.IDP_MAPPER_ID=? 2025-08-02 08:37:35,846 DEBUG [org.hibernate.sql.results.internal.ResultsHelper] (executor-thread-26) Collection fully initialized: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#1a448657-0987-4717-a523-68c5ea49e974] 2025-08-02 08:37:35,847 DEBUG [org.hibernate.orm.loader.multi] (executor-thread-26) Batch fetching collection: org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config.be7e59bd-23a4-462c-bc4b-660bb0d2356b 2025-08-02 08:37:35,847 DEBUG [org.hibernate.orm.sql.exec] (executor-thread-26) Skipping reading Query result cache data: cache-enabled = false, cache-mode = IGNORE 2025-08-02 08:37:35,847 DEBUG [org.hibernate.SQL] (executor-thread-26) select c1_0.IDP_MAPPER_ID,c1_0.NAME,c1_0.VALUE from IDP_MAPPER_CONFIG c1_0 where c1_0.IDP_MAPPER_ID=? 2025-08-02 08:37:35,848 DEBUG [org.hibernate.sql.results.internal.ResultsHelper] (executor-thread-26) Collection fully initialized: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#be7e59bd-23a4-462c-bc4b-660bb0d2356b] 2025-08-02 08:37:35,848 DEBUG [org.hibernate.orm.loader.multi] (executor-thread-26) Batch fetching collection: org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config.ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9 2025-08-02 08:37:35,848 DEBUG [org.hibernate.orm.sql.exec] (executor-thread-26) Skipping reading Query result cache data: cache-enabled = false, cache-mode = IGNORE 2025-08-02 08:37:35,848 DEBUG [org.hibernate.SQL] (executor-thread-26) select c1_0.IDP_MAPPER_ID,c1_0.NAME,c1_0.VALUE from IDP_MAPPER_CONFIG c1_0 where c1_0.IDP_MAPPER_ID=? 2025-08-02 08:37:35,850 DEBUG [org.hibernate.sql.results.internal.ResultsHelper] (executor-thread-26) Collection fully initialized: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9] 2025-08-02 08:37:35,850 DEBUG [org.hibernate.orm.loader.multi] (executor-thread-26) Batch fetching collection: org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config.f307d7a8-aded-4a79-922f-00d3e3071645 2025-08-02 08:37:35,850 DEBUG [org.hibernate.orm.sql.exec] (executor-thread-26) Skipping reading Query result cache data: cache-enabled = false, cache-mode = IGNORE 2025-08-02 08:37:35,850 DEBUG [org.hibernate.SQL] (executor-thread-26) select c1_0.IDP_MAPPER_ID,c1_0.NAME,c1_0.VALUE from IDP_MAPPER_CONFIG c1_0 where c1_0.IDP_MAPPER_ID=? 2025-08-02 08:37:35,851 DEBUG [org.hibernate.sql.results.internal.ResultsHelper] (executor-thread-26) Collection fully initialized: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#f307d7a8-aded-4a79-922f-00d3e3071645] 2025-08-02 08:37:35,851 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Processing flush-time cascades 2025-08-02 08:37:35,852 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Dirty checking collections 2025-08-02 08:37:35,852 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#1a448657-0987-4717-a523-68c5ea49e974], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#1a448657-0987-4717-a523-68c5ea49e974] (initialized) 2025-08-02 08:37:35,852 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#be7e59bd-23a4-462c-bc4b-660bb0d2356b], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#be7e59bd-23a4-462c-bc4b-660bb0d2356b] (initialized) 2025-08-02 08:37:35,852 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9] (initialized) 2025-08-02 08:37:35,852 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#f307d7a8-aded-4a79-922f-00d3e3071645], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#f307d7a8-aded-4a79-922f-00d3e3071645] (initialized) 2025-08-02 08:37:35,852 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Flushed: 0 insertions, 0 updates, 0 deletions to 4 objects 2025-08-02 08:37:35,852 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Flushed: 0 (re)creations, 0 updates, 0 removals to 4 collections 2025-08-02 08:37:35,852 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) Listing entities: 2025-08-02 08:37:35,852 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Name, id=ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9, config=[${brokerContext.profile.data.name}, INHERIT, firstName], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,852 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Username, id=1a448657-0987-4717-a523-68c5ea49e974, config=[${brokerContext.profile.data.username}, INHERIT, username], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,852 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Email, id=f307d7a8-aded-4a79-922f-00d3e3071645, config=[${brokerContext.profile.data.confirmed_email}, INHERIT, email], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,852 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map X ID, id=be7e59bd-23a4-462c-bc4b-660bb0d2356b, config=[${brokerContext.profile.data.id}, FORCE, broker.id], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,852 DEBUG [org.hibernate.orm.sql.exec] (executor-thread-26) Skipping reading Query result cache data: cache-enabled = false, cache-mode = NORMAL 2025-08-02 08:37:35,852 DEBUG [org.hibernate.SQL] (executor-thread-26) select u1_0.ID,u1_0.CREATED_TIMESTAMP,u1_0.EMAIL,u1_0.EMAIL_CONSTRAINT,u1_0.EMAIL_VERIFIED,u1_0.ENABLED,u1_0.FEDERATION_LINK,u1_0.FIRST_NAME,u1_0.LAST_NAME,u1_0.NOT_BEFORE,u1_0.REALM_ID,u1_0.SERVICE_ACCOUNT_CLIENT_LINK,u1_0.USERNAME from FEDERATED_IDENTITY fie1_0 join USER_ENTITY u1_0 on u1_0.ID=fie1_0.USER_ID where fie1_0.REALM_ID=? and fie1_0.IDENTITY_PROVIDER=? and fie1_0.FEDERATED_USER_ID=? 2025-08-02 08:37:35,862 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Processing flush-time cascades 2025-08-02 08:37:35,862 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Dirty checking collections 2025-08-02 08:37:35,862 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#1a448657-0987-4717-a523-68c5ea49e974], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#1a448657-0987-4717-a523-68c5ea49e974] (initialized) 2025-08-02 08:37:35,862 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#be7e59bd-23a4-462c-bc4b-660bb0d2356b], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#be7e59bd-23a4-462c-bc4b-660bb0d2356b] (initialized) 2025-08-02 08:37:35,862 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9] (initialized) 2025-08-02 08:37:35,862 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#f307d7a8-aded-4a79-922f-00d3e3071645], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#f307d7a8-aded-4a79-922f-00d3e3071645] (initialized) 2025-08-02 08:37:35,862 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Flushed: 0 insertions, 0 updates, 0 deletions to 4 objects 2025-08-02 08:37:35,862 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Flushed: 0 (re)creations, 0 updates, 0 removals to 4 collections 2025-08-02 08:37:35,862 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) Listing entities: 2025-08-02 08:37:35,862 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Name, id=ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9, config=[${brokerContext.profile.data.name}, INHERIT, firstName], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,862 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Username, id=1a448657-0987-4717-a523-68c5ea49e974, config=[${brokerContext.profile.data.username}, INHERIT, username], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,862 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Email, id=f307d7a8-aded-4a79-922f-00d3e3071645, config=[${brokerContext.profile.data.confirmed_email}, INHERIT, email], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,862 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map X ID, id=be7e59bd-23a4-462c-bc4b-660bb0d2356b, config=[${brokerContext.profile.data.id}, FORCE, broker.id], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,862 DEBUG [org.hibernate.orm.sql.exec] (executor-thread-26) Skipping reading Query result cache data: cache-enabled = false, cache-mode = NORMAL 2025-08-02 08:37:35,862 DEBUG [org.hibernate.SQL] (executor-thread-26) select ble1_0.USER_ID from BROKER_LINK ble1_0 where ble1_0.REALM_ID=? and ble1_0.IDENTITY_PROVIDER=? and ble1_0.BROKER_USER_ID=? 2025-08-02 08:37:35,866 DEBUG [org.keycloak.services.resources.IdentityBrokerService] (executor-thread-26) Federated user not found for provider 'X' and broker username 'null' 2025-08-02 08:37:35,866 DEBUG [org.keycloak.services.resources.IdentityBrokerService] (executor-thread-26) Redirecting to flow for firstBrokerLogin 2025-08-02 08:37:35,866 DEBUG [org.keycloak.authentication.AuthenticationProcessor] (executor-thread-26) RESET FLOW 2025-08-02 08:37:35,866 DEBUG [org.keycloak.transaction.JtaTransactionWrapper] (executor-thread-26) JtaTransactionWrapper commit. Request Context: HTTP GET \/realms\/{REALM}\/broker\/X\/endpoint 2025-08-02 08:37:35,866 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Processing flush-time cascades 2025-08-02 08:37:35,866 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Dirty checking collections 2025-08-02 08:37:35,866 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#1a448657-0987-4717-a523-68c5ea49e974], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#1a448657-0987-4717-a523-68c5ea49e974] (initialized) 2025-08-02 08:37:35,866 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#be7e59bd-23a4-462c-bc4b-660bb0d2356b], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#be7e59bd-23a4-462c-bc4b-660bb0d2356b] (initialized) 2025-08-02 08:37:35,866 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9] (initialized) 2025-08-02 08:37:35,866 DEBUG [org.hibernate.engine.internal.Collections] (executor-thread-26) Collection found: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#f307d7a8-aded-4a79-922f-00d3e3071645], was: [org.keycloak.models.jpa.entities.IdentityProviderMapperEntity.config#f307d7a8-aded-4a79-922f-00d3e3071645] (initialized) 2025-08-02 08:37:35,866 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Flushed: 0 insertions, 0 updates, 0 deletions to 4 objects 2025-08-02 08:37:35,866 DEBUG [org.hibernate.event.internal.AbstractFlushingEventListener] (executor-thread-26) Flushed: 0 (re)creations, 0 updates, 0 removals to 4 collections 2025-08-02 08:37:35,866 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) Listing entities: 2025-08-02 08:37:35,867 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Name, id=ea9ebb89-cac2-4990-bcf6-f15fb0b2fbe9, config=[${brokerContext.profile.data.name}, INHERIT, firstName], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,867 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Username, id=1a448657-0987-4717-a523-68c5ea49e974, config=[${brokerContext.profile.data.username}, INHERIT, username], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,867 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map Email, id=f307d7a8-aded-4a79-922f-00d3e3071645, config=[${brokerContext.profile.data.confirmed_email}, INHERIT, email], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,867 DEBUG [org.hibernate.internal.util.EntityPrinter] (executor-thread-26) org.keycloak.models.jpa.entities.IdentityProviderMapperEntity{realmId=a67cc58c-bef1-4063-a4ca-1b8aab4880d4, identityProviderAlias=X, name=Map X ID, id=be7e59bd-23a4-462c-bc4b-660bb0d2356b, config=[${brokerContext.profile.data.id}, FORCE, broker.id], identityProviderMapper=hardcoded-attribute-idp-mapper} 2025-08-02 08:37:35,867 DEBUG [org.hibernate.engine.transaction.internal.TransactionImpl] (executor-thread-26) On TransactionImpl creation, JpaCompliance#isJpaTransactionComplianceEnabled == false 2025-08-02 08:37:35,867 DEBUG [org.hibernate.resource.jdbc.internal.LogicalConnectionManagedImpl] (executor-thread-26) Initiating JDBC connection release from beforeTransactionCompletion 2025-08-02 08:37:35,869 DEBUG [org.hibernate.resource.jdbc.internal.LogicalConnectionManagedImpl] (executor-thread-26) Initiating JDBC connection release from afterTransaction 2025-08-02 08:37:35,869 DEBUG [org.keycloak.transaction.JtaTransactionWrapper] (executor-thread-26) JtaTransactionWrapper end. Request Context: HTTP GET \/realms\/{realm}\/broker\/X\/endpoint 2025-08-02 08:37:36,110 DEBUG [io.quarkus.vertx.http.runtime.ForwardedParser] (vert.x-eventloop-thread-3) Using X-Forwarded-Proto to set scheme to https 2025-08-02 08:37:36,110 DEBUG [io.quarkus.vertx.http.runtime.ForwardedParser] (vert.x-eventloop-thread-3) Using {demain} to set host to{demain} and port to -1 2025-08-02 08:37:36,110 DEBUG [io.quarkus.vertx.http.runtime.ForwardedParser] (vert.x-eventloop-thread-3) Using X-Forwarded-Port to set port to 443 2025-08-02 08:37:36,110 DEBUG [io.quarkus.vertx.http.runtime.ForwardedParser] (vert.x-eventloop-thread-3) Using X-Forwarded-For to set for host to 139.227.252.149 and for port to 39402 2025-08-02 08:37:36,110 DEBUG [io.quarkus.vertx.http.runtime.ForwardedParser] (vert.x-eventloop-thread-3) Recalculated absoluteURI to {demain}\/keycloak\/realms\/{realm}\/login-actions\/first-broker-login?client_id={client_id}&tab_id=W8X8K3ajZcc&client_data=eyJydSI6Imh0dHBzOi8vd3d3LnZvaWNlc25hcC5tZS8iLCJydCI6ImNvZGUiLCJybSI6ImZyYWdtZW50Iiwic3QiOiIzZmVkMjllOS0wMDliLTRiOTQtYjcyZC04YTZiNGJlODJiNWIifQ 2025-08-02 08:37:36,110 DEBUG [org.keycloak.transaction.JtaTransactionWrapper] (executor-thread-26) new JtaTransactionWrapper. Was existing transaction suspended: false Request Context: HTTP GET \/realms\/{REALM}\/login-actions\/first-broker-login 2025-08-02 08:37:36,110 DEBUG [org.hibernate.resource.jdbc.internal.LogicalConnectionManagedImpl] (executor-thread-26) hibernate.connection.provider_disables_autocommit was enabled. This setting should only be enabled when you are certain that the Connections given to Hibernate by the ConnectionProvider have auto-commit disabled. Enabling this setting when the Connections do not have auto-commit disabled will lead to Hibernate executing SQL operations outside of any JDBC\/SQL transaction. 2025-08-02 08:37:36,110 DEBUG [org.hibernate.resource.transaction.backend.jta.internal.JtaTransactionCoordinatorImpl] (executor-thread-26) Hibernate RegisteredSynchronization successfully registered with JTA platform 2025-08-02 08:37:36,111 DEBUG [org.keycloak.services.resources.SessionCodeChecks] (executor-thread-26) Will use client '{CLIENT}' in back-to-application link 2025-08-02 08:37:36,111 DEBUG [org.keycloak.services.managers.AuthenticationSessionManager] (executor-thread-26) Found AUTH_SESSION_ID cookie with value MmIwZDJjZjktNjVjMy00MzY5LTg5NjgtY2QwODQ3YzQ5Y2YzLmEtblRaT0tFTW8tMk42X0xwTnJoUzVMbGFVYTJzd0JKRDNsT0ptR3dxOGx6dy1Ud1hMRFpSQUVDbXk2ZjBLdTE1bmx2cnhyUUltUFg3UDZaekJSUTZB.021f5a64ad27-57767 2025-08-02 08:37:36,111 ERROR [org.keycloak.services.error.KeycloakErrorHandler] (executor-thread-26) Uncaught server error: java.lang.RuntimeException: No identifier provider for identity. at org.keycloak.broker.provider.BrokeredIdentityContext.(BrokeredIdentityContext.java:58) at org.keycloak.authentication.authenticators.broker.util.SerializedBrokeredIdentityContext.deserialize(SerializedBrokeredIdentityContext.java:274) at org.keycloak.services.resources.LoginActionsService.brokerLoginFlow(LoginActionsService.java:888) at org.keycloak.services.resources.LoginActionsService.firstBrokerLoginGet(LoginActionsService.java:827) at org.keycloak.services.resources.LoginActionsService$quarkusrestinvoker$firstBrokerLoginGet_8353b960487f493f608861c3c713f082b792516d.invoke(Unknown Source) at org.jboss.resteasy.reactive.server.handlers.InvocationHandler.handle(InvocationHandler.java:29) at io.quarkus.resteasy.reactive.server.runtime.QuarkusResteasyReactiveRequestContext.invokeHandler(QuarkusResteasyReactiveRequestContext.java:141) at org.jboss.resteasy.reactive.common.core.AbstractResteasyReactiveContext.run(AbstractResteasyReactiveContext.java:147) at io.quarkus.vertx.core.runtime.VertxCoreRecorder$15.runWith(VertxCoreRecorder.java:638) at org.jboss.threads.EnhancedQueueExecutor$Task.doRunWith(EnhancedQueueExecutor.java:2675) at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2654) at org.jboss.threads.EnhancedQueueExecutor.runThreadBody(EnhancedQueueExecutor.java:1627) at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1594) at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:11) at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:11) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base\/java.lang.Thread.run(Thread.java:1583) 2025-08-02 08:37:36,112 FINE [freemarker.cache] (executor-thread-26) TemplateLoader.findTemplateSource(\"template_en_US.ftl\"): Not found 2025-08-02 08:37:36,112 FINE [freemarker.cache] (executor-thread-26) TemplateLoader.findTemplateSource(\"template_en.ftl\"): Not found 2025-08-02 08:37:36,112 FINE [freemarker.cache] (executor-thread-26) TemplateLoader.findTemplateSource(\"template.ftl\"): Found 2025-08-02 08:37:36,112 FINE [freemarker.cache] (executor-thread-26) \"template.ftl\"(\"en_US\", UTF-8, parsed): using cached since jar:file:\/opt\/keycloak\/lib\/lib\/main\/org.keycloak.keycloak-themes-26.3.1.jar!\/theme\/keycloak.v2\/login\/template.ftl hasn't changed. ``` I tried using Keycloak's built-in authentication for Twitter, but it failed. I tried creating a new flow for Twitter to replace the first broker login, but it failed.",
    "author_id":6015,
    "publication_date":1754131875000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ji Tiantian",
    "author_reputation":23.0,
    "tags":"keycloak, twitter",
    "text_length":26473,
    "title_length":92,
    "num_tags":2
  },
  {
    "id":6562,
    "title":"Angular child routes not reloading the component",
    "link":"https:\/\/stackoverflow.com\/questions\/79723227\/angular-child-routes-not-reloading-the-component",
    "text":"I'm trying to write a very basic calorie counting app, however I've run into an issue with child routing in Angular. I have a list of child routes selectable from a list that should route to a child component on the page for details about the ingredient selected. This works the first time you select a link, however it stops working if there is aleady a child component loaded, even when the route changes. app.routes.ts ``` export const routes: Routes = [ \/\/ ...existing code... { path: 'ingredients', component: Ingredients, title: 'Ingredients', children: [ { path: ':id', component: IngredientComponent, } ] }, \/\/ ...existing code... { path: '**', redirectTo: 'dashboard' } ]; ``` ingredients.html ``` ... <div class=\"col\"> <button type=\"button\" class=\"btn btn-primary position-relative\">Add Ingredient<\/button> <\/div> <\/div> <div class=\"list-group\"> @for (ingredient of ingredients; track ingredient.id) { <a class=\"list-group-item\" [routerLink]=\"[ingredient.id]\">{{ingredient.name}}<\/a> } @empty { <div class=\"list-group-item\">There are no ingredients currently stored<\/div> } <\/div> <div class=\"col-8\"> <router-outlet \/> <\/div> ``` IngredientComponent ``` export class IngredientComponent { id: number; ingredient: Ingredient | undefined; constructor( private IngredientService: IngredientService, private route: ActivatedRoute ) { this.id = Number(this.route.snapshot.params['id']); this.ingredient = this.IngredientService.getIngredient(this.id); } ngOnInit() { console.log('init\\'d component'); } ngOnDestroy() { console.log('Destroying component'); } } ``` Working stackblitz example here",
    "author_id":6014,
    "publication_date":1754131992000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ziz",
    "author_reputation":137.0,
    "tags":"typescript, angular",
    "text_length":1600,
    "title_length":48,
    "num_tags":2
  },
  {
    "id":6561,
    "title":"Tailwind theme variables",
    "link":"https:\/\/stackoverflow.com\/questions\/79723229\/tailwind-theme-variables",
    "text":"I've read in docs part about utility classes generation in theme variables, as docs said: ``` @theme { --color-mint-500: oklch(0.72 0.11 178); } ``` I've made my variable ``` @theme { --color-custom-yellow: #FFDD00; } ``` Used it as bg-custom-yellow or hover:bg-custom-yellow but nothing rendered in yellow, am I missing something? Using tailwind 4.1.11 UPD. It works as text-custom-yellow but not as background color, does it generate not with bg-* prefix? UPD.2. I find a reason for that. I'll left this question up here so anyone who will also stuck with this may be able to find out a solution(as I didn't seen in any related topic anything pointing on this one reason). I've had custom, not generated and written by myself in sass file bg-grey class and it conflicted with generated classes, after removing it everything works as intended.",
    "author_id":6013,
    "publication_date":1754132051000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"J.Doe",
    "author_reputation":39.0,
    "tags":"next.js, tailwind-css, tailwind-css-4",
    "text_length":844,
    "title_length":24,
    "num_tags":3
  },
  {
    "id":6560,
    "title":"Express.json() works on local but not on AWS Lambda",
    "link":"https:\/\/stackoverflow.com\/questions\/79723232\/express-json-works-on-local-but-not-on-aws-lambda",
    "text":"I've been struggling to parse the request body from POST. I tested the code locally via curl and returns the json body. On Lambda, I tried both curl and Postman but it just returned the buffer data. Basically, I can't manipulate the data. index.js ``` require('dotenv').config(); const express = require('express'); const bodyParser = require('body-parser'); const serverless = require('serverless-http'); const app = express(); app.use(express.json()); \/\/ app.use(bodyParser.json()); \/\/ Routes app.use('\/db', require('.\/routes\/db')); module.exports.handler = serverless(app); ``` routes > db.js ``` const express = require('express'); const { Pool } = require('pg'); const router = express.Router(); \/\/ Test router.post('\/test', (req, res) => { console.log('Raw req.body:', req.body); res.json({ received: req.body }); }); module.exports = router; ``` Sample request: ``` { \"movetype\": 1 } ``` This is the response using Postman: ``` { \"received\": { \"type\": \"Buffer\", \"data\": [ 123, 13, 10, 32, 32, 34, 109, 111, 118, 101, 116, 121, 112, 101, 34, 58, 32, 49, 13, 10, 125, 13, 10 ] } } ```",
    "author_id":6012,
    "publication_date":1754132406000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Lemuel Dingal",
    "author_reputation":23.0,
    "tags":"node.js, aws-lambda, express, amazon-web-services",
    "text_length":1089,
    "title_length":51,
    "num_tags":4
  },
  {
    "id":6559,
    "title":"D365 Custom button in Open in Microsoft Office menu",
    "link":"https:\/\/stackoverflow.com\/questions\/79723234\/d365-custom-button-in-open-in-microsoft-office-menu",
    "text":"I need an Export to excel button, but accidentally created Open in excel button. Is there any way to put the new custom menuitem in Export to excel group (not outside it) or to customize the Lines menuitem logic? I used this code in my form: ``` public void customizeMenuOptions(OfficeMenuOptions _menuOptions) { List exportOptions = _menuOptions.customMenuItems(); var exportOption = OfficeGeneratedExportMenuItem::construct(tableStr(MyEntity), CustomExportLinesToExcelOptionId); exportOption.displayName(\"123\"); exportOptions.addEnd(exportOption); } public ExportToExcelDataEntityContext getDataEntityContext(OfficeGeneratedExportMenuItem _menuItem) { ExportToExcelDataEntityContext context = ExportToExcelDataEntityContext::construct(tableStr(MyEntity), tableFieldGroupStr(MyEntity, AutoReport)); context.addEntityDefault(tableStr(MyEntity)); ListEnumerator enumerator = context.entities().getEnumerator(); while (enumerator.moveNext()) { ExportToExcelDataEntityInfo entity = enumerator.current(); if (entity.entityName() == tableStr(MyEntity)) { ExportToExcelFilterTreeBuilder filterBuilder = new ExportToExcelFilterTreeBuilder(tableStr(MyEntity)); FilterNode filterExpression = filterBuilder.areEqual(fieldStr(MyEntity, dataAreaId), curExt()); entity.filter(filterExpression); } } return context; } ```",
    "author_id":6011,
    "publication_date":1754132693000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"julia",
    "author_reputation":23.0,
    "tags":"axapta, dynamics-365-operations",
    "text_length":1307,
    "title_length":51,
    "num_tags":2
  },
  {
    "id":6558,
    "title":"live commenting system with laravel and ajax",
    "link":"https:\/\/stackoverflow.com\/questions\/79723245\/live-commenting-system-with-laravel-and-ajax",
    "text":"I want to implement a live commenting system for each post of my project this is my view ``` @extends('layouts.main') @section('content') <p>{{ $post->title }}<\/p> \/\/ there is nothing wrong about showing post data <p>{{ $post->body }}<\/p> <div class = \"row\"> <div class=\"col-md-6\"> <form id=\"company_form\" method=\"POST\" action = \"{{ route('addComment') }}\" class= \"my-5\"> @csrf <input type=\"hidden\" name = \"postId\" id = \"postId\" value=\"{{ $post->id }}\"> <div class=\"form-group my-2\"> <input type=\"text\" class=\"form-control\" name = \"name\" id = \"name\" aria-describedby=\"emailHelp\" required placeholder=\"your name\"> <\/div> <div class=\"form-group my-2\" > <textarea class=\"form-control\" name = \"comment\" rows=\"3\" id = \"comment\" required><\/textarea> <\/div> <button class=\"btn btn-primary\" id = \"company_form_btn\" type=\"submit\">submit<\/button> <\/form> <\/div> <\/div> ``` this is my controller ``` public function addComment(Request $request){ $comment = new Comment(); $comment->postId = $request->postId; $comment->name = $request->name; $comment->comment = $request->comment; $comment->save(); return response()->json(['success' => 'thanks for your comment']); } ``` and this is my ajax code ``` $(document).ready(function(){ $(\"#company_form_btn\").click(function(e){ e.preventDefault(); var url = $(this).attr(\"action\"); let formData = new FormData(this); $.ajax({ type:'POST', url: url, data: formData, contentType: false, dataType:'json', cache: false, processData: false, success:function(response){ alert(response.success) }, error: function(response){ } }); }); }); ``` comment submitted and saved in database totally fine but i get this, and whole view goes away i want to achieve something like this thanks for your help",
    "author_id":6010,
    "publication_date":1754133865000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ramin Safari",
    "author_reputation":21.0,
    "tags":"ajax, laravel, jquery",
    "text_length":1722,
    "title_length":44,
    "num_tags":3
  },
  {
    "id":6557,
    "title":"Project structural directive using ng-content in Angular",
    "link":"https:\/\/stackoverflow.com\/questions\/79723255\/project-structural-directive-using-ng-content-in-angular",
    "text":"I am working on an Angular project and I've made my own table ``` <app-table> ``` component by wrapping an existing table component from the team, let's say ``` <inner-table> ``` , inside it. Please keep in mind that I cannot edit the wrapped table component . I've made changes accordingly, redifining inputs on my wrapper and injecting them back to the ``` <inner-table> ``` . The existing ``` <inner-table> ``` offers a ``` *customCellTemplate ``` directive that allows us to define templates for the cells of a column. Here's how it is used : ``` <!-- app.component.html --> <inner-table [columns]=\"columns\" ...> <ng-container *customCellTemplate=\"let item; name: 'firstnameTemplate'\"> {{ item.firstname + ' !' }} <\/ng-container> <\/inner-table> ``` ``` \/\/ app.component.ts protected columns = { 'firstname': { cell: 'firstnameTemplate', header: 'Firstname' }, 'lastname': { cell: (item) => item.lastname, header: 'Last name' } } ``` This works fine, the firstname column's cells are displaying the firstname followed by a ' !'. Now the problem I can't define a ``` <ng-container *customCellTemplate...> ``` tag inside my ``` <app-table> ``` component (the same way I would in my ``` <inner-table> ``` ) and project that tag using an ``` <ng-content> ``` inside my ``` <inner-table> ``` in my app-table.component.html . If I understand correctly, the directive will be computed and the inner <inner-table> will not see that directive definition. I've made this example : This works (column cells are formatted correctly, with a ' !' suffix) : ``` <!-- app-table.component.html --> <div id=\"app-table-container\"> <inner-table [columns]=\"columns\"> <ng-container *customCellTemplate=\"let item; name: 'firstnameTemplate'\"> {{ item.firstname + ' !' }} <\/ng-container> <\/inner-table> <\/div> ``` This does not work (empty column) : ``` <!-- app.component.html --> <app-table [redefinedColumns]=\"columns\"> <ng-container *customCellTemplate=\"let item; name: 'firstnameTemplate'\"> {{ item.firstname + ' !' }} <\/ng-container> <\/app-table> ``` ``` <!-- app-table.component.html --> <div id=\"app-table-container\"> <inner-table [columns]=\"redefinedColumns\" ...> <ng-content><\/ng-content> <\/inner-table> Other stuff... <\/div> ``` I can't use the computed ng-template either, this doesn't work : ``` <!-- app-table.component.html --> <div id=\"app-table-container\"> <inner-table [columns]=\"redefinedColumns\" ...> <ng-template customCellTemplate let-item name=\"firstnameTemplate\"> {{ item.firstname + ' !' }} <\/ng-template> <\/inner-table> <\/div> ``` What are my solutions ? Is there any way I could define a template for a column's cells without having to define them in the wrapped ``` <inner-table> ``` component, as I don't want to end up with an app-table.component.html file being thousands of lines long if I have to define dozens of cell templates. I have thought of making a directive instead of wrapping my ``` <inner-table> ``` , but that would require alot of changes and I don't even know if it can work for some features, it may introduce new problems. Please give me some solid solutions Thank you !",
    "author_id":6009,
    "publication_date":1754134625000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nautilus",
    "author_reputation":21.0,
    "tags":"typescript, angular, angular-directive",
    "text_length":3098,
    "title_length":56,
    "num_tags":3
  },
  {
    "id":6556,
    "title":"Cannot mutate the dependencies of configuration &#39;:earlib&#39;",
    "link":"https:\/\/stackoverflow.com\/questions\/79723257\/cannot-mutate-the-dependencies-of-configuration-earlib",
    "text":"Running Gradle successfully with 8.14.3 using: ``` gradle-wrapper.properties distributionUrl=https\\:\/\/services.gradle.org\/distributions\/gradle-8.14.3-bin.zip ``` Gives: ``` BUILD SUCCESSFUL in 1m 7s 14 actionable tasks: 12 executed, 2 up-to-date NOTiFYs-MacBook-Pro:ViViFYd NOTiFY$ .\/gradlew ViViFYdEAR >>>>> 6.1 ViViFYdEAR >>>>> 5.1 copyViViFYdJAR >>>>> 4.1 copyViViFYdWAR >>>>> 3.1 ViViFYdJAR >>>>> 2.1 ViViFYdWAR >>>>> 1.1 deleteFiles > Task :compileJava Note: \/Users\/NOTiFY\/IdeaProjects\/ViViFYd\/src\/main\/java\/com\/vivifyd\/gson\/typeadapter\/UsersTypeAdapterFactory.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. [Incubating] Problems report is available at: file:\/\/\/Users\/NOTiFY\/IdeaProjects\/ViViFYd\/build\/reports\/problems\/problems-report.html BUILD SUCCESSFUL in 4s 9 actionable tasks: 9 executed ``` Creates ear, jar & war & successly deployed EAR on WildFly: ``` INFO [org.jboss.weld.deployer] (MSC service thread 1-6) WFLYWELD0003: Processing weld deployment ViViFYd.ear INFO [org.hibernate.validator.internal.util.Version] (MSC service thread 1-6) HV000001: Hibernate Validator 8.0.2.Final INFO [org.jboss.weld.deployer] (MSC service thread 1-3) WFLYWELD0003: Processing weld deployment ViViFYdWAR.war INFO [org.jboss.weld.deployer] (MSC service thread 1-8) WFLYWELD0003: Processing weld deployment ViViFYdJAR.jar ``` Runs on WildFly: ``` INFO [org.jboss.as.server] (External Management Request Threads -- 1) WFLYSRV0010: Deployed \"ViViFYd.ear\" (runtime-name : \"ViViFYd.ear\") ``` Migrating to Gradle 9.0.0 using: ``` distributionUrl=https\\:\/\/services.gradle.org\/distributions\/gradle-9.0.0-bin.zip ``` Run: ``` .\/gradlew build clean --warning-mode all ``` Fails with: ``` >>>>> 2.1 ViViFYdWAR >>>>> 1.1 deleteFiles >>>>> 3.1 ViViFYdJAR >>>>> 6.1 ViViFYdEAR >>>>> 5.1 copyViViFYdJAR >>>>> 4.1 copyViViFYdWAR FAILURE: Build failed with an exception. * Where: Build file '\/Users\/NOTiFY\/IdeaProjects\/ViViFYd\/build.gradle' line: 416 * What went wrong: Could not determine the dependencies of task ':test'. > Could not create task ':ViViFYdEAR'. > Cannot mutate the dependencies of configuration ':earlib' after the configuration was resolved. After a configuration has been observed, it should not be modified. * Try: > Run with --stacktrace option to get the stack trace. > Run with --info or --debug option to get more log output. > Run with --scan to generate a Build Scan (Powered by Develocity). > Get more help at https:\/\/help.gradle.org. BUILD FAILED in 20s ``` dependencies run: ``` dependencies { \/\/ line: 416 earlib group: \"com.google.code.gson\", name: \"gson\", version: \"2.13.1\", ext: \"jar\" earlib group: \"org.apache.httpcomponents\", name: \"httpclient\", version: \"4.5.14\", ext: \"jar\" earlib group: \"org.apache.httpcomponents\", name: \"httpcore\", version: \"4.4.16\", ext: \"jar\" earlib group: \"org.apache.pdfbox\", name: \"pdfbox\", version: \"3.0.5\", ext: \"jar\" earlib group: \"org.apache.commons\", name: \"commons-lang3\", version: \"3.17.0\", ext: \"jar\" earlib group: \"net.bytebuddy\", name: \"byte-buddy\", version: \"1.17.6\", ext: \"jar\" earlib group: \"org.jsoup\", name: \"jsoup\", version: \"1.20.1\", ext: \"jar\" earlib group: \"org.jetbrains.kotlin\", name: \"kotlin-stdlib\", version: \"2.2.0\", ext: \"jar\" earlib group: \"dev.morphia.morphia\", name: \"morphia-core\", version: \"2.5.0\", ext: \"jar\" earlib group: \"dev.morphia.morphia\", name: \"morphia-kotlin\", version: \"2.5.0\", ext: \"jar\" earlib group: \"org.mongodb\", name: \"mongodb-driver-core\", version: \"5.5.1\", ext: \"jar\" earlib group: \"org.mongodb\", name: \"mongodb-driver-sync\", version: \"5.5.1\", ext: \"jar\" earlib group: \"org.mongodb\", name: \"bson\", version: \"5.5.1\", ext: \"jar\" spotbugsPlugins(\"com.h3xstream.findsecbugs:findsecbugs-plugin:1.14.0\") } ``` System: macOS 15.6 (aarch64) Oracle OpenJDK Runtime Environment 24.0.2+12-54 Run 'stacktrace': ``` .\/gradlew build clean --scan ``` Failures: ``` Failure 1 of 1 Could not determine the dependencies of task ':test'. Build file '\/Users\/NOTiFY\/IdeaProjects\/ViViFYd\/build.gradle' line: 416 Failed Could not create task ':ViViFYdEAR'. > Cannot mutate the dependencies of configuration ':earlib' after the configuration was resolved. After a configuration has been observed, it should not be modified. Stack trace org.gradle.api.internal.tasks.TaskDependencyResolveException: Could not determine the dependencies of task ':test'. at org.gradle.api.internal.tasks.CachingTaskDependencyResolveContext.getDependencies(CachingTaskDependencyResolveContext.java:70) ••• Caused by: org.gradle.api.internal.tasks.DefaultTaskContainer$TaskCreationException: Could not create task ':ViViFYdEAR'. at org.gradle.api.internal.tasks.DefaultTaskContainer.taskCreationException(DefaultTaskContainer.java:729) ••• Caused by: org.gradle.api.InvalidUserCodeException: Cannot mutate the dependencies of configuration ':earlib' after the configuration was resolved. After a configuration has been observed, it should not be modified. at org.gradle.api.internal.artifacts.configurations.DefaultConfiguration.validateMutation(DefaultConfiguration.java:1222) ••• at build$_run_closure21$_closure37.doCall$original(\/Users\/NOTiFY\/IdeaProjects\/ViViFYd\/build.gradle:416) ••• at build$_run_closure21.doCall$original(\/Users\/NOTiFY\/IdeaProjects\/ViViFYd\/build.gradle:415) ••• ``` (# 2) I need to look into: ``` Mutating a configuration after it has been resolved, consumed as a variant, or used for generating published metadata. This behavior has been deprecated. This will fail with an error in Gradle 9.0. The dependencies of configuration ':earlib' were mutated after the configuration was resolved. After a configuration has been observed, it should not be modified. Consult the upgrading guide for further information: https:\/\/docs.gradle.org\/8.14.3\/userguide\/upgrading_version_8.html#mutate_configuration_after_locking ```",
    "author_id":6008,
    "publication_date":1754134770000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"NOTiFY",
    "author_reputation":1409.0,
    "tags":"gradle, jakarta-ee, dsl",
    "text_length":5836,
    "title_length":65,
    "num_tags":3
  },
  {
    "id":6555,
    "title":"Can I subscribe to both focused and !focused locally?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723259\/can-i-subscribe-to-both-focused-and-focused-locally",
    "text":"Yesterday was my firstly ever working with Appkit. Objective: Can a NSTextField locally be aware of both of the following: 1.) That it is the no longer the focus of the user? 2.) That it is has become the focus of the user? Goal: print \"2 Focused\" & \"1 Unfocused\" when the user selects the second TextField. ( And Vice Versa.) I know I could have multiple Fields subscribe to some sort of EnvironmentObject ( or whatever. ) Just wondering if it can be done locally... I am not opposed to changing every aspect of this code to accomplish this. Any help is appreciated. ``` struct FocusDemo: View { var body: some View { VStack ( spacing: 20 ) { FocusableField ( id: 1 ) FocusableField ( id: 2 ) } .padding() } } class CustomNSTextField: NSTextField { var id: Int? override func becomeFirstResponder() -> Bool { let became = super.becomeFirstResponder() if became { print ( id! , \": Focused\" ) } return became } override func resignFirstResponder() -> Bool { let resigned = super.resignFirstResponder() if resigned { print ( id! , \": Unfocused\" ) } return resigned } } struct FocusableField: NSViewRepresentable { let id: Int func makeNSView ( context: Context ) -> CustomNSTextField { let textField = CustomNSTextField ( string: \"\" ) textField.id = id textField.isEditable = true textField.isBordered = true return textField } func updateNSView ( _ nsView: CustomNSTextField , context: Context ) {} } ``` I also tried using the following in a \"public class Coordinator: NSObject , NSTextFieldDelegate {...}\": ``` public func controlTextDidBeginEditing ( _ obj: Notification ) { print ( \"focused\" ) } public func controlTextDidEndEditing ( _ obj: Notification ) { print ( \"unfocused\" ) } ``` I only got prints for unfocused...",
    "author_id":4830,
    "publication_date":1754134881000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Tim",
    "author_reputation":33.0,
    "tags":"appkit, nstextfield",
    "text_length":1724,
    "title_length":53,
    "num_tags":2
  },
  {
    "id":6554,
    "title":"Canvas Sketch is not rendering the specified colour",
    "link":"https:\/\/stackoverflow.com\/questions\/79723260\/canvas-sketch-is-not-rendering-the-specified-colour",
    "text":"The code works perfectly, including the animation. But I'm unable to change the background colour to anything except black when using rgb, hex or hsl codes. Only way to change the background colour is to use the colour names. Please review the code to find why the background is defaulting to black inspite of specifying a different colour. Static Output Image ``` const canvasSketch = require('canvas-sketch'); const random = require('canvas-sketch-util\/random'); const colr =require('canvas-sketch-util\/color'); const settings = { dimensions: [ 1080,1080 ], animate: true \/\/ enables animation } const sketch = ({context, width, height}) => { const agents = []; const myColour = colr.parse('rgba(222, 121, 213)'); for (let i = 0; i < 50; i++) { const x=random.range(0,width); const y= random.range(0,height); agents.push(new Agent(x,y)); } return ({ context, width, height }) => { console.log('Color Values:', myColour); context.clearRect(0, 0, width, height); context.fillStyle = 'rgba(${myColour[0]}, ${myColour[1]}, ${myColour[2]}, ${myColour[3]})'; context.fillRect(0, 0, width, height); agents.forEach(Agent => { Agent.update(); Agent.bounce(width, height); Agent.colour(context); Agent.draw(context); } ); } } canvasSketch(sketch, settings); class Vector { constructor(x,y){ this.x = x; this.y = y; } class Agent{ constructor(x,y){ this.pos = new Vector(x,y); this.vel = new Vector(random.range(-1, 1), random.range(-1, 1)); this.radius = random.range(10, 20); } update(){ this.pos.x += this.vel.x; this.pos.y += this.vel.y; } bounce(width, height){ if (this.pos.x <= 0 || this.pos.x >= width) { this.vel.x *= -1; } if (this.pos.y <=0 || this.pos.y >=height) { this.vel.y *= -1; } } colour(context){ this.gradient = context.createLinearGradient(random.range(10,20), random.range(10,20), random.range(10,20), random.range(10,20)); this.gradient.addColorStop(\"0.2\", \"purple\"); this.gradient.addColorStop(\"0.6\", \"orange\"); this.gradient.addColorStop(\"0.8\", \"blue\"); context.fillStyle = this.gradient; context.fill(); } draw(context){ context.save(); context.translate(this.pos.x, this.pos.y); context.lineWidth = 4; context.beginPath(); context.arc(0, 0, this.radius, 0, Math.PI * 2); context.stroke(); context.restore(); } } ```",
    "author_id":6007,
    "publication_date":1754135182000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aiswarya Rose",
    "author_reputation":11.0,
    "tags":"node.js, npm, animation, colors, canvas-sketch",
    "text_length":2233,
    "title_length":51,
    "num_tags":5
  },
  {
    "id":6553,
    "title":"How can I configure Mosquitto on kubernetes to auto-restart when cert-manager obtains a new Let&#39;s encrypt certificate automatically?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723270\/how-can-i-configure-mosquitto-on-kubernetes-to-auto-restart-when-cert-manager-ob",
    "text":"Is there an example configuration of eclipse-mosquitto 2.x ( docker container ) on Kubernetes to restart automatically the pod when cert-manager obtains a new Let's encrypt certificate automatically (renew)? With ingress-nginx is super easy because I can use it's api to do that (there are many examples on the web), but with mosquitto I don't understand the right approach. At the moment I'm copying cert files to it's config folder, but in this way I'm not able to restart the pod when those files change: ``` certfile \/etc\/mosquitto\/certs\/cert.pem cafile \/etc\/mosquitto\/certs\/chain.pem keyfile \/etc\/mosquitto\/certs\/privkey.pem ```",
    "author_id":6006,
    "publication_date":1754136746000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Stefano Cappa",
    "author_reputation":605.0,
    "tags":"lets-encrypt, kubernetes, mosquitto, certbot, cert-manager",
    "text_length":633,
    "title_length":136,
    "num_tags":5
  },
  {
    "id":6552,
    "title":"cdk overlay inside another cdk overlay",
    "link":"https:\/\/stackoverflow.com\/questions\/79723271\/cdk-overlay-inside-another-cdk-overlay",
    "text":"I'm using Angular CDK Overlay in my project, and I've created two separate components: one for a dropdown and another for a modal. Both the modal and the dropdown are implemented using overlay.create() — not through directives. The dropdown is tied to an input element within its own component. When I click a button inside the modal, the dropdown opens as expected. However, the problem is that when I scroll the modal, the dropdown visually breaks out of the modal and appears outside its boundaries. Any ideas on how to fix the issue will be appreciated.",
    "author_id":6005,
    "publication_date":1754136768000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"neda F.",
    "author_reputation":9.0,
    "tags":"angular, angular-cdk",
    "text_length":557,
    "title_length":38,
    "num_tags":2
  },
  {
    "id":6551,
    "title":"Lombok @RequiredArgsConstructor not working in VSCode — “restTemplate not initialized in default constructor”",
    "link":"https:\/\/stackoverflow.com\/questions\/79723272\/lombok-requiredargsconstructor-not-working-in-vscode-resttemplate-not-initia",
    "text":"I’m working on a Spring Boot project in VSCode, and I’m using Lombok for boilerplate code generation. Here’s my class: ``` @Service @RequiredArgsConstructor public class CapLoginServiceImpl implements CapLoginService { private final RestTemplate restTemplate; \/\/ ... } ``` I’ve added Lombok in pom.xml: ``` <dependency> <groupId>org.projectlombok<\/groupId> <artifactId>lombok<\/artifactId> <\/dependency> ``` I’ve also installed the Lombok plugin in VSCode, and everything compiles fine using Maven. However, VSCode shows an error: The final field restTemplate may not have been initialized or restTemplate is not initialized in the default constructor It seems like VSCode doesn’t recognize the constructor that Lombok generates via ``` @RequiredArgsConstructor ``` . What I tried: Installed Lombok plugin in VSCode Enabled “Annotation Processing” in Java settings Verified Lombok is working by using ``` @Getter ``` \/ ``` @Setter ``` Cleaned and rebuilt the project Tried using the downloaded ``` lombok.jar ``` in ``` .vscode\/settings.json ``` via ``` java.jdt.ls.vmargs ``` Ran ``` mvn clean compile ``` — works fine Code runs without issue — this is only a VSCode error My questions: Is there any extra configuration needed in VSCode to fully support Lombok constructor generation? Is there a way to make VSCode recognize it?",
    "author_id":6004,
    "publication_date":1754136886000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Yuntao Chen",
    "author_reputation":9.0,
    "tags":"java, spring-boot, visual-studio-code, lombok",
    "text_length":1328,
    "title_length":109,
    "num_tags":4
  },
  {
    "id":6550,
    "title":"How to Get the Option Names in Discord.js",
    "link":"https:\/\/stackoverflow.com\/questions\/79723278\/how-to-get-the-option-names-in-discord-js",
    "text":"Inside the Register Command File I want to assign credits to each action using the ``` value ``` parameter and ensure that users don't select the same action multiple times. However, since ``` value ``` is not a unique identifier, I need to retrieve the ``` name ``` of the selected actions. ``` const commands = [ { name: 'responsibility', description: 'It helps to manage your responsibilities.', options: [ { name: 'first', description: 'Choose your first responsibility.', type: ApplicationCommandOptionType.Number, choices: [ { name: 'Action One', value: 1 }, { name: 'Action Two', value: 1 }, { name: 'Action Three', value: 2 }, { name: 'Action Four', value: 1 } ] }, { name: 'second', description: 'Choose your second responsibility.', type: ApplicationCommandOptionType.Number, choices: [ { name: 'Action One', value: 1 }, { name: 'Action Two', value: 1 }, { name: 'Action Three', value: 2 }, { name: 'Action Four', value: 1 } ] } ] } ]; ``` The same choices are repeated for both ``` first ``` and ``` second ``` so that users can choose from any of the four actions for either option. Inside the Index File I can get the value like this: ``` const { options } = interaction; const firstResponsibility = options.getNumber('first'); ``` But I can't find a way to get the ``` name ``` of the selected option. Question Is there a way to get the ``` name ``` of the selected choice in Discord.js without changing the entire approach?",
    "author_id":6003,
    "publication_date":1754137199000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"EmreRl",
    "author_reputation":11.0,
    "tags":"javascript, discord, discord.js",
    "text_length":1438,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":6549,
    "title":"WebLogic 12c Foreign Server fails to authenticate with WildFly 26 JMS: JMSSecurityException (Username: null)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723280\/weblogic-12c-foreign-server-fails-to-authenticate-with-wildfly-26-jms-jmssecuri",
    "text":"I have a Java application running on WebLogic 12c that sends JMS messages. We're trying to configure it to send messages to a remote JMS queue on a WildFly 26 server. When we attempt to send a message, we receive the following error in both WebLogic and WildFly logs: ``` javax.jms.JMSSecurityException: AMQ229031: Unable to validate user from \/host:port. Username: null; SSL certificate subject DN: unavailable ``` It seems the problem is related to authentication on the connection factory used by WebLogic. I can't figure out how or where to correctly set the username and password so that WebLogic passes them to WildFly. WildFly Configuration Below is the relevant part of my ``` standalone-full.xml ``` configuration (ActiveMQ subsystem): ``` <subsystem xmlns=\"urn:jboss:domain:messaging-activemq:13.1\"> <server name=\"default\"> <security elytron-domain=\"ApplicationDomain\"\/> <statistics enabled=\"${wildfly.messaging-activemq.statistics-enabled:${wildfly.statistics-enabled:false}}\"\/> <security-setting name=\"#\"> <role name=\"userApplication\" send=\"true\" consume=\"true\" create-non-durable-queue=\"true\" delete-non-durable-queue=\"true\"\/> <\/security-setting> <address-setting name=\"#\" dead-letter-address=\"jms.queue.DLQ\" expiry-address=\"jms.queue.ExpiryQueue\" max-size-bytes=\"10485760\" page-size-bytes=\"2097152\" message-counter-history-day-limit=\"10\"\/> <http-connector name=\"http-connector\" socket-binding=\"http\" endpoint=\"http-acceptor\"\/> <http-connector name=\"http-connector-throughput\" socket-binding=\"http\" endpoint=\"http-acceptor-throughput\"> <param name=\"batch-delay\" value=\"50\"\/> <\/http-connector> <in-vm-connector name=\"in-vm\" server-id=\"0\"> <param name=\"buffer-pooling\" value=\"false\"\/> <\/in-vm-connector> <remote-connector name=\"netty-connector\" socket-binding=\"messaging\"\/> <acceptor name=\"netty-acceptor\" factory-class=\"org.apache.activemq.artemis.core.remoting.impl.netty.NettyAcceptorFactory\" socket-binding=\"messaging\"\/> <http-acceptor name=\"http-acceptor\" http-listener=\"default\"\/> <http-acceptor name=\"http-acceptor-throughput\" http-listener=\"default\"> <param name=\"batch-delay\" value=\"50\"\/> <param name=\"direct-deliver\" value=\"false\"\/> <\/http-acceptor> <in-vm-acceptor name=\"in-vm\" server-id=\"0\"> <param name=\"buffer-pooling\" value=\"false\"\/> <\/in-vm-acceptor> <jms-queue name=\"ExpiryQueue\" entries=\"java:\/jms\/queue\/ExpiryQueue\"\/> <jms-queue name=\"DLQ\" entries=\"java:\/jms\/queue\/DLQ\"\/> <jms-queue name=\"testQueueWildflyToWls\" entries=\"java:jboss\/exported\/jms\/queue\/testQueue_wildfy_to_wls\"\/> <jms-queue name=\"testQueueWlsToWildfly\" entries=\"java:jboss\/exported\/jms\/queue\/testQueue_wls_to_wildfly\"\/> <connection-factory name=\"InVmConnectionFactory\" entries=\"java:\/ConnectionFactory\" connectors=\"in-vm\"\/> <connection-factory name=\"RemoteConnectionFactory\" entries=\"java:jboss\/exported\/jms\/RemoteConnectionFactory\" connectors=\"http-connector\"\/> <connection-factory name=\"RemoteTcpConnectionFactory\" entries=\"java:jboss\/exported\/jms\/RemoteTcpConnectionFactory\" connectors=\"netty-connector\"\/> <pooled-connection-factory name=\"activemq-ra\" entries=\"java:\/JmsXA java:jboss\/DefaultJMSConnectionFactory\" connectors=\"in-vm\" transaction=\"xa\"\/> <\/server> <\/subsystem> <!-- Other configurations --> <socket-binding-group name=\"standard-sockets\" default-interface=\"public\" port-offset=\"${jboss.socket.binding.port-offset:0}\"> <socket-binding name=\"ajp\" port=\"${jboss.ajp.port:8009}\"\/> <socket-binding name=\"messaging\" port=\"61616\"\/> <socket-binding name=\"http\" port=\"${jboss.http.port:8088}\"\/> <socket-binding name=\"https\" port=\"${jboss.https.port:8443}\"\/> <socket-binding name=\"iiop\" interface=\"unsecure\" port=\"3528\"\/> <socket-binding name=\"iiop-ssl\" interface=\"unsecure\" port=\"3529\"\/> <socket-binding name=\"management-http\" interface=\"management\" port=\"${jboss.management.http.port:9990}\"\/> <socket-binding name=\"management-https\" interface=\"management\" port=\"${jboss.management.https.port:9993}\"\/> <socket-binding name=\"txn-recovery-environment\" port=\"4712\"\/> <socket-binding name=\"txn-status-manager\" port=\"4713\"\/> <outbound-socket-binding name=\"mail-smtp\"> <remote-destination host=\"${jboss.mail.server.host:localhost}\" port=\"${jboss.mail.server.port:25}\"\/> <\/outbound-socket-binding> <\/socket-binding-group> ``` WebLogic Configuration Foreign Server configuration: ``` <foreign-server name=\"ForeignServer\"> <default-targeting-enabled>true<\/default-targeting-enabled> <foreign-destination name=\"remoteQueueWlsToWildfly\"> <local-jndi-name>jms\/app\/remoteQueueWlsToWildfly<\/local-jndi-name> <remote-jndi-name>jms\/queue\/testQueue_wls_to_wildfly<\/remote-jndi-name> <\/foreign-destination> <foreign-destination name=\"remoteQueueWildflyToWls\"> <local-jndi-name>jms\/app\/remoteQueueWildflyToWls<\/local-jndi-name> <remote-jndi-name>jms\/queue\/testQueue_wildfy_to_wls<\/remote-jndi-name> <\/foreign-destination> <foreign-connection-factory name=\"WildflyConnectionFactory\"> <local-jndi-name>jms\/app\/remoteFactory<\/local-jndi-name> <remote-jndi-name>jms\/RemoteConnectionFactory<\/remote-jndi-name> <username>userApplication<\/username> <password-encrypted>{AES}******************<\/password-encrypted> <connection-health-checking>enabled<\/connection-health-checking> <\/foreign-connection-factory> <initial-context-factory>org.wildfly.naming.client.WildFlyInitialContextFactory<\/initial-context-factory> <connection-url>http-remoting:\/\/localhost:8088<\/connection-url> <jndi-properties-credential-encrypted>{AES}*********************<\/jndi-properties-credential-encrypted> <jndi-property> <key>jboss.naming.client.ejb.context<\/key> <value>true<\/value> <\/jndi-property> <jndi-property> <key>java.naming.security.principal<\/key> <value>userApplication<\/value> <\/jndi-property> <\/foreign-server> ``` How can I configure WebLogic 12c to send the username and password when connecting to a remote JMS queue on WildFly 26? Ideally, I want to do this through Foreign Server configuration or another method that does not require changes to application code. What I've Tried: Created a TCP port for messaging on WildFly and changed WebLogic’s connection URL from ``` http-remoting ``` to ``` tcp ``` , but the same error occurred. Configured a WebLogic JMS Bridge instead of a Foreign Server, but the error persisted. ✅ Tested with a simple standalone Java application (on Tomcat) where I passed the username and password manually using a constructor — and it worked , proving WildFly’s authentication is correctly configured. ✅ Disabled Elytron authentication on WildFly using ``` <security elytron-domain=\"ApplicationDomain\" enabled=\"false\"\/> ``` — and the connection worked, confirming the connection factory and JNDI setup are correct.",
    "author_id":6002,
    "publication_date":1754137404000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"vz1654056",
    "author_reputation":11.0,
    "tags":"java, authentication, jms, weblogic12c, wildfly-26",
    "text_length":6615,
    "title_length":108,
    "num_tags":5
  },
  {
    "id":6548,
    "title":"How to make scrollable list of buttons",
    "link":"https:\/\/stackoverflow.com\/questions\/79723291\/how-to-make-scrollable-list-of-buttons",
    "text":"Needed to make scrollable list of buttons in the left part of the screen. But the scrollbar in child div, even forced by overflow: scroll is not working. Read about removing overflow from parent objects, but without success. Left only body parent object for simplifying the code fragments below base.html ``` <\/header> <body> {% block content %} {% endblock %} <\/body> <html> ``` cities.html ``` {% extends \"base.html\" %} {% block content %} <div class=\"scrollbox\"> {% for city in cities %} <form action=\"{% url 'city_detail' %}\" method=\"post\"> {% csrf_token %} <input type=\"hidden\" name=\"city_id\" value={{ city.id }}> <button type=\"submit\">{{ city.name }} <\/button> <\/form> {% endfor %} <p> Censored...<\/p> <!-- added for testing ~100+ - enough for make scrollbar workable by text only - no success --> <p> Censored...<\/p> <\/div> {% endblock %} ``` CSS ``` @import url(\/\/fonts.googleapis.com\/css?family=Muli); body { margin:0; padding:0; \/* with overflow hidden main scrollbar not shown, but forced scrollbar in child div not working and the data is cut with value auto scroll shown for whole content, remainin not working for child div *\/ overflow: auto; font-family:helvetica, sans-serif; } .scrollbox { min-height: 90vh; border: 2px solid; \/* for being sure, that no syntax errors in names, etc - border shown - OK *\/ float: left; overflow-y: scroll; } ``` Any ideas? The code above is easy to repeat with any model with enough data, if interested, IMHO. Was trying to make it as simple, as possible - I hope, that it was done successfuly. Bottom of div with parent overflow: auto is below.",
    "author_id":6001,
    "publication_date":1754138370000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"dAlexis",
    "author_reputation":33.0,
    "tags":"django, html, css",
    "text_length":1594,
    "title_length":38,
    "num_tags":3
  },
  {
    "id":6547,
    "title":"Is there a way to synchronize love.system.setClipboardText() with browser&#39;s clipboard",
    "link":"https:\/\/stackoverflow.com\/questions\/79723292\/is-there-a-way-to-synchronize-love-system-setclipboardtext-with-browsers-clip",
    "text":"So I just hosted my first game in web using love.js but the text area where one can paste or copy from only works if the in game text is copied, meaning I cannot paste text copied text from another website. Is there a way to synchronize the love2d's set\/get clipboard functions with the web? Here is my love.keypressed function for detecting ctrl+v and ctrl+c actions ``` function love.keypressed(key) if FEN_input.active then if (key == \"v\" or key == \"V\") and (love.keyboard.isDown(\"lctrl\") or love.keyboard.isDown(\"rctrl\")) then local clip = love.system.getClipboardText() if clip then FEN_input.text = FEN_input.text .. clip end elseif (key == \"c\" or key == \"C\") and (love.keyboard.isDown(\"lctrl\") or love.keyboard.isDown(\"rctrl\")) then love.system.setClipboardText(FEN_input.text) elseif (key == \"a\" or key == \"A\") and (love.keyboard.isDown(\"lctrl\") or love.keyboard.isDown(\"rctrl\")) then FEN_input.selectAll = true else FEN_input.selectAll = false end end end ``` AI's suggested to use require(\"js\") and translate the love code to js but I couldn't find any resources to learn from.",
    "author_id":6000,
    "publication_date":1754138384000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aye Veska",
    "author_reputation":1.0,
    "tags":"lua, love2d",
    "text_length":1087,
    "title_length":89,
    "num_tags":2
  },
  {
    "id":6546,
    "title":"Show where in a class implementation an interface is used?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723295\/show-where-in-a-class-implementation-an-interface-is-used",
    "text":"Given ``` public MyClass<T> where T: MyInterface ``` I want to know all places where in MyClass properties or methods from MyInterface are called. Can I somehow view this in Rider?",
    "author_id":5999,
    "publication_date":1754138590000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"me.at.coding",
    "author_reputation":18150.0,
    "tags":"c#, rider",
    "text_length":180,
    "title_length":58,
    "num_tags":2
  },
  {
    "id":6545,
    "title":"nltk.FreqDist.plot fails to display plot",
    "link":"https:\/\/stackoverflow.com\/questions\/79723300\/nltk-freqdist-plot-fails-to-display-plot",
    "text":"I'm trying in vain to make NLTK show a Frequency Distribution Plot using ``` FreqDist.plot() ``` . I've followed the steps from the first chapter of the NLTK book (scroll down to \"3.1 Frequency Distributions\"), but ``` fdist.plot() ``` doesn't show anything. If there's already a matplotlib window open, plot tries to overwrite that existing plot. Minimal example with Python 3.12, matplotlib 3.10.5 and nltk 3.9.1 (all current): ``` import nltk import matplotlib nltk.download() from nltk.book import * fdist1 = FreqDist(text1) fdist1.plot(50, cumulative=True) # <-- no matplotlib window! ``` Extending this example by two more commands: ``` text4.dispersion_plot([\"citizens\", \"democracy\"]) # <-- works, dispersion plot shown fdist1.plot(50, cumulative=True) # will draw freqdist plot over dispersion plot ``` will show that the ``` fdist.plot() ``` in fact does something, but it will modify the dispersion plot instead of creating a new FreqDist plot (as desired). How to get the Frequency Distribution Plot shown correctly?",
    "author_id":5998,
    "publication_date":1754139013000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"tohuwawohu",
    "author_reputation":13696.0,
    "tags":"matplotlib, nltk, python-3.12",
    "text_length":1027,
    "title_length":40,
    "num_tags":3
  },
  {
    "id":6544,
    "title":"Will moving ssl::context cause UB?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723302\/will-moving-sslcontext-cause-ub",
    "text":"I have some code that creates an ``` ssl::stream ``` . The ``` ssl::stream ``` object takes an ``` ssl::context ``` in its constructor by reference. Creating an ``` ssl::stream ``` looks like this ``` ssl::context ssl_context(boost::asio::ssl::context::tls_client); ssl_context.set_options(boost::asio::ssl::context::default_workarounds); ssl::stream<stream_t> ssl_stream(std::move(socket), ssl_context); ``` Later I want to transfer ownership of the ssl_context and ssl_stream objects to another object like this: ``` session_t sess(std::move(ssl_stream), std::move(ssl_context)); ``` Will further use of ``` ssl::stream ``` lead to UB? I ask because ``` ssl::stream ``` takes ``` ssl::context ``` by reference and we move the ``` ssl::context ``` object, the reference to which was passed to ``` ssl::stream ``` . I think that after ``` ssl::context ``` was moved, ``` ssl::stream ``` has a dangling reference to it and uses this dangling reference to access SSL context. But I'm not sure.",
    "author_id":4345,
    "publication_date":1754139042000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Joe J",
    "author_reputation":1075.0,
    "tags":"c++, boost, boost-asio",
    "text_length":991,
    "title_length":34,
    "num_tags":3
  },
  {
    "id":6543,
    "title":"Using blocking network IO for thread synchronization and low latency task handling. Pros and cons?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723308\/using-blocking-network-io-for-thread-synchronization-and-low-latency-task-handli",
    "text":"A few weeks ago I took up learning some network programming. So far it has been quite basic low-level stuff: mostly TCP and UDP over IP in the C language. Some broadcasting for service announcement and discovery and then just rather simple TCP\/IP for sending and receiving messages. The ultimate goal to parallelize and distribute work loads in a mathematical library I have created. Now as I realized the default behavior when listening for traffic seemed to be blocking reads. In other words the function call to read will halt the calling thread until something exists to be read at the socket. What I came up with the other night is what if I create a number of threads, listening at one port each waiting for commands or jobs to be done. Or listening for other threads signalling that they are done with a task and possibly transmit the result. I suppose this also could be done locally listening and sending to loop-back? How would this compare performance-wise to doing \"normal\" multi-threading as in with for example pthreads and OS signalling between threads? How is network IO treated as compared to for example usleep timing and other OS-timing tools. Is it like a hardware or privileged level interrupt of some kind, almost ensuring the thread will get priority with a low latency? The main gain as I see it is that it seems very scalable and nice in a network sense. You can't (well as far as I know, anyways) OS-signal between threads running on different machines if you are running a network distributed computing of some kind.",
    "author_id":5997,
    "publication_date":1754139570000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mathreadler",
    "author_reputation":501.0,
    "tags":"c, multithreading, network-programming, distributed-computing",
    "text_length":1543,
    "title_length":98,
    "num_tags":4
  },
  {
    "id":6542,
    "title":"Complex Type Assignment",
    "link":"https:\/\/stackoverflow.com\/questions\/79723311\/complex-type-assignment",
    "text":"I want to type objects structured as an array of objects, each containing a field ``` value ``` , and additional context properties which are not defined in advance but satisfy ``` {[key: string]: string} ``` : ``` [ { \"unit\": \"MTR\", \"value\": 0.3 } ] ``` How can I properly type hint this as ``` TaggedValue<number> ``` ? I have tried the following: ``` type TaggedValue<V> = Array< { [key: Exclude<string, \"value\">]: string } & { value: V; } >; ``` or a \"degraded\" version ``` type TaggedValue<V> = Array<{ [K in string]: K extends 'value' ? V : string; } & { value: V }>; ``` or even ``` type TaggedValue< V, T extends Exclude<{ [key: string]: string }, \"value\"> = Omit< { [key: string]: string; }, \"value\" >, > = Array< { [K in keyof T]: string; } & { value: V } >; ```",
    "author_id":5996,
    "publication_date":1754139640000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Etienne Favre",
    "author_reputation":11.0,
    "tags":"typescript",
    "text_length":772,
    "title_length":23,
    "num_tags":1
  },
  {
    "id":6541,
    "title":"gdal2tiles.py hangs when run in Conda Docker environment",
    "link":"https:\/\/stackoverflow.com\/questions\/79723313\/gdal2tiles-py-hangs-when-run-in-conda-docker-environment",
    "text":"I'm running a Python script inside a Conda-based Docker container that processes geospatial data. The script runs a two step GDAL workflow, it uses gdaldem colorrelief to create a colorized GeoTIFF, and ``` gdal2tiles.py ``` to generate map tiles from that result. Gdaldem completes successfully every time. However, the script hangs indefinitely as soon as it calls ``` gdal2tiles.py ``` ... It produces no error output, and surprisingly, even the timeout argument in ``` subprocess.run ``` does not trigger an exception, the whole process just freezes with these log: ``` 2025-08-02 09:53:22,602 - INFO - Successfully created GeoTIFF: \/app\/geotiffs\/skjav\/reflectivity\/reflectivity_20250802T094500Z.tif 2025-08-02 09:53:22,636 - INFO - Step 1: Colorizing \/app\/geotiffs\/skjav\/reflectivity\/reflectivity_20250802T094500Z.tif with gdaldem. 2025-08-02 09:53:22,672 - INFO - Successfully colorized GeoTIFF to \/app\/static\/tiles\/skjav\/reflectivity\/20250802T094500Z\/colorized.tif 2025-08-02 09:53:22,672 - INFO - Step 2: Generating tiles from \/app\/static\/tiles\/skjav\/reflectivity\/20250802T094500Z\/colorized.tif with gdal2tiles.py. <-- HANGS HERE --> ``` The code snippet in question: ``` try: logging.info(f\"coloring with with gdaldem.\") color_map_content = create_color_map_file(product_config['cmap'], product_config['vmin'], product_config['vmax']) with open(color_file_path, 'w') as f: f.write(color_map_content) cmd_colorize = ['gdaldem', 'color-relief', geotiff_path, color_file_path, colorized_tiff_path, '-alpha'] subprocess.run(cmd_colorize, check=True, capture_output=True, text=True, timeout=60) logging.info(f\"colored geotiff to {colorized_tiff_path}\") logging.info(f\"generating tiles {colorized_tiff_path} with gdal2tiles.py.\") cmd_gdal2tiles = [ 'gdal2tiles.py', '--profile=raster', '--zoom=5-12', '--webp-quality=90', colorized_tiff_path, output_tile_dir ] subprocess.run(cmd_gdal2tiles, check=True, capture_output=True, text=True, timeout=180) logging.info(f\"success - {output_tile_dir}\") return output_tile_dir ``` What could cause ``` gdal2tiles.py ``` to hang so completely that it ignores the timeout from Python's subprocess module? Is there a known issue with running ``` gdal2tiles.py ``` non interactively from a Python script inside a Docker container that could lead to this kind of deadlock? Ruled out Environment Path Issues: I added a diagnostic log ( ``` shutil.which('gdal2tiles.py') ``` ) which confirmed the script is correctly finding the modern version of ``` gdal2tiles.py ``` inside the conda environment ( ``` \/opt\/conda\/envs\/radar-env\/bin\/gdal2tiles.py ``` ). Ruled out Multiprocessing: The hang occurs even with the ``` --processes ``` flag removed from the command. Ruled out output format: The hang persists whether I use ``` --webp-quality=90 ``` or remove it to default to png tiles. I also tried to replaced subprocess.run with the lower-level ``` subprocess.Popen ``` and ``` proc.communicate(timeout=) ``` this also hung and failed to trigger the ``` TimeoutExpired ``` exception.",
    "author_id":5995,
    "publication_date":1754139982000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mrotskcud",
    "author_reputation":23.0,
    "tags":"python, docker, subprocess, gdal, gdal2tiles.py",
    "text_length":3019,
    "title_length":56,
    "num_tags":5
  },
  {
    "id":6540,
    "title":"Why is `JArray.ToObject&lt;List&lt;T&gt;&gt;` faster than `JArray.ToObject&lt;T[]&gt;`",
    "link":"https:\/\/stackoverflow.com\/questions\/79723317\/why-is-jarray-toobjectlistt-faster-than-jarray-toobjectt",
    "text":"We are trying to micro-optimise some parts of our production code and I was expecting that using ``` Array ``` s would generally be better than ``` List ``` s in .NET, and most of the times, my benchmarks indicate that. However, it seems that ``` JArray.ToObject<List<T>> ``` is faster than ``` JArray.ToObject<T[]> ``` †. I tried to navigate the ``` Newtonsoft.Json ``` source code but I'm not sure I understand. ``` JArray ``` s are ``` IList<JToken> ``` and have a private field ``` private readonly List<JToken> _values ``` . Is this why ``` JArray.ToObject<List<T>> ``` is faster than ``` JArray.ToObject<T[]> ``` ? The former doesn't actually cast anything to anything? I'm looking to understand which part of the source code ``` JArray.ToObject<List<T>> ``` actually calls to verify that. It seems to me that I should look into ``` JsonSerializer.DeserializeInternal ``` because ``` JToken.ToObject ``` eventually calls it but I can't seem to understand the logic of the source code beyond that. Can you help me find the reason? †: MRE: ``` using BenchmarkDotNet.Attributes; using BenchmarkDotNet.Running; using Newtonsoft.Json.Linq; [MemoryDiagnoser] public class JArrayDeserializationBenchmark { private JArray myJArray; [GlobalSetup] public void Setup() { string json = \"\"\" [ {\"id\": 1, \"name\": \"aaads\", \"value\": 10}, {\"id\": 2, \"name\": \"aaas\", \"value\": 20}, {\"id\": 3, \"name\": \"ds\", \"value\": 30} ] \"\"\"; myJArray = JArray.Parse(json); } [Benchmark] public Dictionary<string, object>[] DeserializeToArray() { return myJArray.ToObject<Dictionary<string, object>[]>()!; } [Benchmark] public List<Dictionary<string, object>> DeserializeToList() { return myJArray.ToObject<List<Dictionary<string, object>>>()!; } public static void Main(string[] args) { BenchmarkRunner.Run<JArrayDeserializationBenchmark>(); \/\/ \/\/ Print the results to make sure the benchmarks are executed correctly \/\/ var benchmark = new JArrayDeserializationBenchmark(); \/\/ benchmark.Setup(); \/\/ var arrayResult = benchmark.DeserializeToArray(); \/\/ var listResult = benchmark.DeserializeToList(); \/\/ Console.WriteLine($\"Array Result Count: {arrayResult.Length}\"); \/\/ Console.WriteLine($\"List Result Count: {listResult.Count}\"); } } ```",
    "author_id":5994,
    "publication_date":1754140235000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"FluidMechanics Potential Flows",
    "author_reputation":666.0,
    "tags":"c#, .net, json.net, micro-optimization",
    "text_length":2207,
    "title_length":86,
    "num_tags":4
  },
  {
    "id":6539,
    "title":"Display child of clicked parent, hide other children in list",
    "link":"https:\/\/stackoverflow.com\/questions\/79723319\/display-child-of-clicked-parent-hide-other-children-in-list",
    "text":"Right now this is my code: ``` var elements = document.getElementsByClassName(\"parent\"); var myFunction = function() { var childNode = this.querySelector('.child'); \/\/ Find the child element within the clicked parent childNode.classList.toggle(\"display\"); \/\/ Show the child element }; \/\/ Attach click event to each parent element for (var i = 0; i < elements.length; i++) { elements[i].addEventListener('click', myFunction, false); } ``` ``` <div class=\"parent\"> <h1>Parent<\/h1> <ul class=\"child\"> <li> Some content <\/li> <\/ul> <\/div> ``` It works, but for my project only one child should be visible at a time. Is there a way to hide all children of elements before showing clicked child? Or should i do this a different way, like using radio buttons or something? Thank you",
    "author_id":5993,
    "publication_date":1754140391000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Einar Zotterman",
    "author_reputation":11.0,
    "tags":"javascript, html",
    "text_length":775,
    "title_length":60,
    "num_tags":2
  },
  {
    "id":6538,
    "title":"How to hide App bar (ActionBar) in .NET9 &quot;Android Application&quot;?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723324\/how-to-hide-app-bar-actionbar-in-net9-android-application",
    "text":"I have create \"Android Application\" project, .NET 9 (Visual Studio 2022 - see screen). How to hide App bar (ActionBar) in .NET9 \"Android Application\"? In previous my Xamarin Native project I use this: ``` <style name=\"AppTheme\" parent=\"Theme.AppCompat.Light\"> .. <style name=\"AppTheme.NoActionBar\"> <item name=\"windowActionBar\">false<\/item> <item name=\"windowNoTitle\">true<\/item> <\/style> ``` How to do this in .NET9 \"Android Application\" project? When I set Theme attribute here: MainActivity.cs: ``` [Activity(Label = \"@string\/ApplicationName\", MainLauncher = true, Theme = \"@style\/AppTheme.NoActionBar\")] public class MainActivity : Activity { ``` AndroidManifest.xml: ``` android:theme=\"@style\/AppTheme\" ``` I have this error: C:\\Program Files\\dotnet\\packs\\Microsoft.Android.Sdk.Windows\\35.0.78\\tools\\Xamarin.Android.Aapt2.targets(156,3): error APT2260: resource style\/Theme.AppCompat.Light (aka ...:style\/Theme.AppCompat.Light) not found. I don't understand what Theme should I use to customize it. Upd. I have added this code in main activity: using AndroidX.AppCompat.App; and add package xamarin.androidx.appcompat\\1.7.1\\ but this package has .net 8 dependencies and when the app run with error System.TypeLoadException: 'Could not resolve type with token 0100003f from typeref (expected class 'AndroidX.SavedState.ISavedStateRegistryOwner' in assembly 'Xamarin.AndroidX.SavedState, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null') assembly:Xamarin.AndroidX.SavedState, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null ...",
    "author_id":5992,
    "publication_date":1754140868000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"eworker2003",
    "author_reputation":1.0,
    "tags":"c#, android, .net-9.0, .net",
    "text_length":1542,
    "title_length":73,
    "num_tags":4
  },
  {
    "id":6537,
    "title":"Flutter Appcheck fails after following all steps",
    "link":"https:\/\/stackoverflow.com\/questions\/79723325\/flutter-appcheck-fails-after-following-all-steps",
    "text":"I have a Flutter app where everything works fine with unrestricted rules in firestore DB but I start getting this error after enabling appcheck: (25.1.4) [Firestore]: Listen for Query(target=Query(articles where category==Crypto and language==hi order by -date, - name );limitType=LIMIT_TO_FIRST) failed: Status{code=PERMISSION_DENIED, description=Missing or insufficient permissions., cause=null} I\/flutter (21167): Firestore fetch error: [cloud_firestore\/permission-denied] The caller does not have permission to execute the specified operation Things I tried: For Debug: captured the debug token and uploaded to Manage Debug Tokens in firebase for release: custom signed my app with jks and uploaded to play console and then uploaded both upload key and app signing key to firebase",
    "author_id":5991,
    "publication_date":1754141012000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"vengo",
    "author_reputation":11.0,
    "tags":"flutter, firebase, firebase-app-check, google-cloud-firestore",
    "text_length":784,
    "title_length":48,
    "num_tags":4
  },
  {
    "id":6536,
    "title":"Too many files open when trying to save Orbax checkpoint",
    "link":"https:\/\/stackoverflow.com\/questions\/79723328\/too-many-files-open-when-trying-to-save-orbax-checkpoint",
    "text":"I am testing Orbax checkpointing from TPUv4-64 on Google Cloud Storage Bucket. For simplicity, I use the toy example from Orbax docs: ``` import jax import orbax.checkpoint as ocp import numpy as np import os import functools from jax.sharding import Mesh, PartitionSpec, NamedSharding # 1. Initialize JAX for multi-host jax.distributed.initialize() HOST_ID = jax.process_index() path = f'path\/to\/checkpoint-test' # 2. Let ONLY the main host create the directory. if HOST_ID == 0: os.makedirs(path, exist_ok=True) # 3. Define a global mesh and sharding mesh = jax.sharding.Mesh(jax.devices(), ('model',)) sharding = jax.sharding.NamedSharding(mesh, PartitionSpec('model')) # 4. Create the sharded state correctly create_sharded_array = lambda x: jax.device_put(x, sharding) train_state = { 'a': np.arange(32, dtype=np.float32), 'b': np.ones(32, dtype=np.float32), } train_state = jax.tree.map(create_sharded_array, train_state) # 5. Define the sharding structure for the state PyTree train_state_sharding = jax.tree.map(lambda _: sharding, train_state) # 6. ✅ CRITICAL: Add sharding annotations to the JIT-ed function @functools.partial(jax.jit, in_shardings=(sharding,), out_shardings=sharding) def train_fn(state): return jax.tree_util.tree_map(lambda x: x + 1, state) # 7. Create the checkpointer on all hosts options = ocp.CheckpointManagerOptions(max_to_keep=3, save_interval_steps=1) mngr = ocp.CheckpointManager(path, options=options) num_steps = 10 print(f\"Host {HOST_ID}: Starting loop...\") for step in range(num_steps): train_state = train_fn(train_state) global_state = jax.experimental.multihost_utils.process_allgather(train_state) mngr.save(step, args=ocp.args.StandardSave(global_state)) mngr.wait_until_finished() print(f\"Host {HOST_ID}: Script finished successfully.\") ``` However, I get the following error: ``` OSError: [Errno 24] Too many open files: '\/path\/to\/checkpoint-test\/5.orbax-checkpoint-tmp\/default.orbax-checkpoint-tmp' -> 'path\/to\/checkpoint-test\/5.orbax-checkpoint-tmp\/default' ``` I tried to increase the open files limit by setting ``` ulimit -n 65536 ``` , but it didn't help.",
    "author_id":5990,
    "publication_date":1754141324000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"innerproduct",
    "author_reputation":3.0,
    "tags":"jax, tpu, checkpointing",
    "text_length":2111,
    "title_length":56,
    "num_tags":3
  },
  {
    "id":6535,
    "title":"There is an error inside the Branch SDK which freezes the game",
    "link":"https:\/\/stackoverflow.com\/questions\/79723329\/there-is-an-error-inside-the-branch-sdk-which-freezes-the-game",
    "text":"I am making a Unity game for android with the Branch SDK, and after building the game and running it on my Android device, I found this error log: ``` AndroidJavaException: java.lang.ClassNotFoundException: io.branch.unity.BranchUnityWrapper java.lang.ClassNotFoundException: io.branch.unity.BranchUnityWrapper ``` This error was caused after initializing the Branch SDK inside of Unity with the line: ``` Branch.initSession(CallbackWithBranchUniversalObject); ``` Apparently, Branch SDK is looking for a file called BranchUnityWrapper, but when I go to Assets\/Plugins\/Branch\/Android, I am able to find the BranchUnityWrapper: ``` Assets\/Plugins\/Branch\/Android\/BranchUnityWrapper.java ``` The weird part about all of this is that the Branch SDK has been working perfectly until a week ago I stumbled upon this error. Currently I am using the Branch SDK v5.12.0, as stated in the mainTemplate.gradle file: ``` io.branch.sdk.android:library:5.12.0 ``` I don't have any custom proguard rules file. But inside the Publisher Settings ---> Minify section, I have checked the \"Release\" and \"Debug\" checkboxes. Does anyone know what could be the reason for it? Or does the BranchUnityWrapper refers to some other file, not the BranchUnityWrapper.java?",
    "author_id":5832,
    "publication_date":1754141424000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Matias Scribe",
    "author_reputation":23.0,
    "tags":"android, unity-game-engine, branch.io",
    "text_length":1243,
    "title_length":62,
    "num_tags":3
  },
  {
    "id":6534,
    "title":"How to Give badge when dialogue is finished",
    "link":"https:\/\/stackoverflow.com\/questions\/79723340\/how-to-give-badge-when-dialogue-is-finished",
    "text":"In my game i want it so that when you talk to npc it gives a badge but I want it to only give the badge at the end of the conversation Npc name: Npc so theres a dialogue in his head In the Dialogue comes the DialogueChoice Named: Who In that DialogueChoice Another one named: Want In that there is 2 choices: Yes or No both of them have a choice name name of the dialogue for yes:Yes name of the dialogue for no:No after the player chooses Yes they should get the badge Badge ID: 4200998312163803 This is the code I put up: ``` local BadgeService = game:GetService(\"BadgeService\") local Players = game:GetService(\"Players\") -- Replace YOUR_BADGE_ID with the actual ID of your badge local badgeId = 4200998312163803 -- Function to award the badge local function awardBadge(player) local success, errorMessage = pcall(function() BadgeService:AwardBadge(player.UserId, badgeId) end) if success then print(\"Badge awarded to:\", player.Name) else warn(\"Failed to award badge:\", errorMessage) end end local dialog = script.Parent.Head.Dialog --Adjust this path to your dialog dialog.DialogChoiceSelected:Connect(function(player, choice) if choice.Name == \"Yes\" then --Replace \"SpecificChoice\" with your option name awardBadge(player) end end) ``` the thing is the badge is not getting awarded",
    "author_id":5989,
    "publication_date":1754142982000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"aadidev g",
    "author_reputation":43.0,
    "tags":"lua, roblox, luau, roblox-studio",
    "text_length":1285,
    "title_length":43,
    "num_tags":4
  },
  {
    "id":6533,
    "title":"Quill color and Cursor issue",
    "link":"https:\/\/stackoverflow.com\/questions\/79723341\/quill-color-and-cursor-issue",
    "text":"I'm implementing a custom color picker for Quill.js to allow users to select colors beyond the default palette. My solution involves using a native browser element, triggered by a custom 'color-picker' option in the toolbar. I have two main goals: A custom color picker: The user can open the browser's native color picker. Live preview: As the user changes the color in the picker (e.g., by dragging the cursor), the selected text in the Quill editor should update in real-time. My implementation works for applying the final color, but I'm facing a significant issue with the live preview. When listening to the input event on the color picker element, applying the format to the Quill editor causes the cursor to jump to the beginning of the editor with every color change. This makes the live preview unusable. : ``` import { Injectable } from '@angular\/core'; import { Quill } from 'quill'; export interface QuillColorPickerConfig { defaultColors?: string[]; colorPickerOption?: string; onColorChange?: (content: string, quillInstance: Quill) => void; } @Injectable({ providedIn: 'root' }) export class QuillCustomColorPickerService { private readonly QUILL_DEFAULT_COLORS = [ \"#000000\", \"#e60000\", \"#ff9900\", \"#ffff00\", \"#008a00\", \"#0066cc\", \"#9933ff\", \"#ffffff\", \"#facccc\", \"#ffebcc\", \"#ffffcc\", \"#cce8cc\", \"#cce0f5\", \"#ebd6ff\", \"#bbbbbb\", \"#f06666\", \"#ffc266\", \"#ffff66\", \"#66b966\", \"#66a3e0\", \"#c285ff\", \"#888888\", \"#a10000\", \"#b26b00\", \"#b2b200\", \"#006100\", \"#0047b2\", \"#6b24b2\", \"#444444\", \"#5c0000\", \"#663d00\", \"#666600\", \"#003700\", \"#002966\", \"#3d1466\" ]; private currentQuillInstance: Quill | null = null; private currentSelection: any = null; \/** * Get default toolbar options with custom color picker *\/ getToolbarOptions(config?: QuillColorPickerConfig): any[] { const colors = config?.defaultColors || this.QUILL_DEFAULT_COLORS; const colorPickerOption = config?.colorPickerOption || 'color-picker'; const allCustomColors = [...colors, colorPickerOption]; return [ ['bold', 'italic', 'underline', 'strike'], ['blockquote', 'code-block'], [{ 'header': 1 }, { 'header': 2 }], [{ 'list': 'ordered' }, { 'list': 'bullet' }], [{ 'script': 'sub' }, { 'script': 'super' }], [{ 'indent': '-1' }, { 'indent': '+1' }], [{ 'direction': 'rtl' }], [{ 'size': ['small', false, 'large', 'huge'] }], [{ 'header': [1, 2, 3, 4, 5, 6, false] }], [{ 'color': allCustomColors }, { 'background': allCustomColors }], [{ 'font': [] }], [{ 'align': [] }], ['clean'], ['link'] ]; } \/** * Setup custom color picker for a Quill editor instance *\/ setupCustomColorPicker(quillEditor: Quill, config?: QuillColorPickerConfig): void { if (!quillEditor) { console.warn('QuillCustomColorPickerService: Quill editor instance is required'); return; } this.currentQuillInstance = quillEditor; const toolbar = quillEditor.getModule('toolbar'); const colorPickerOption = config?.colorPickerOption || 'color-picker'; \/\/ Setup color handler toolbar.addHandler('color', (value: string) => { this.handleColorChange(quillEditor, value, 'color', colorPickerOption, config); }); \/\/ Setup background color handler toolbar.addHandler('background', (value: string) => { this.handleColorChange(quillEditor, value, 'background', colorPickerOption, config); }); } private handleColorChange( quillEditor: Quill, value: string, type: 'color' | 'background', colorPickerOption: string, config?: QuillColorPickerConfig ): void { \/\/ Store current selection this.currentSelection = quillEditor.getSelection(true); if (value === colorPickerOption) { \/\/ Show custom color picker this.showColorPicker(quillEditor, type, config); } else { \/\/ Apply predefined color this.applyColor(quillEditor, value, type, config); } } private showColorPicker( quillEditor: Quill, type: 'color' | 'background', config?: QuillColorPickerConfig ): void { \/\/ Remove existing color picker if any this.removeExistingColorPicker(); \/\/ Create color input element const colorInput = document.createElement('input'); colorInput.type = 'color'; colorInput.id = `quill-${type}-picker`; colorInput.style.position = 'absolute'; colorInput.style.visibility = 'hidden'; colorInput.style.width = '0'; colorInput.style.height = '0'; \/\/ Get current color value if text is selected const currentColor = this.getCurrentColor(quillEditor, type); if (currentColor) { colorInput.value = currentColor; } \/\/ Add event listeners colorInput.addEventListener('input', (event) => { const target = event.target as HTMLInputElement; this.applyColorWithPreview(quillEditor, target.value, type, config); }); colorInput.addEventListener('change', (event) => { const target = event.target as HTMLInputElement; this.applyColor(quillEditor, target.value, type, config); this.removeExistingColorPicker(); }); colorInput.addEventListener('blur', () => { setTimeout(() => this.removeExistingColorPicker(), 100); }); \/\/ Add to DOM and trigger click document.body.appendChild(colorInput); colorInput.click(); } private applyColorWithPreview( quillEditor: Quill, color: string, type: 'color' | 'background', config?: QuillColorPickerConfig ): void { if (!this.currentSelection) return; \/\/ Apply color to current selection if (this.currentSelection.length > 0) { quillEditor.setSelection(this.currentSelection.index, this.currentSelection.length); quillEditor.format(type, color); } else { \/\/ Set formatting for next input quillEditor.format(type, color); } \/\/ Notify color change this.notifyColorChange(quillEditor, config); } private applyColor( quillEditor: Quill, color: string, type: 'color' | 'background', config?: QuillColorPickerConfig ): void { if (!this.currentSelection) return; \/\/ Restore selection quillEditor.setSelection(this.currentSelection.index, this.currentSelection.length); if (this.currentSelection.length > 0) { \/\/ Apply to selected text quillEditor.formatText(this.currentSelection.index, this.currentSelection.length, type, color); } else { \/\/ Set formatting for next input quillEditor.format(type, color); } \/\/ Focus back to editor quillEditor.focus(); \/\/ Notify color change this.notifyColorChange(quillEditor, config); } private getCurrentColor(quillEditor: Quill, type: 'color' | 'background'): string | null { if (!this.currentSelection || this.currentSelection.length === 0) { return null; } const format = quillEditor.getFormat(this.currentSelection.index, this.currentSelection.length); return format[type] || null; } private removeExistingColorPicker(): void { const existingPickers = document.querySelectorAll('[id^=\"quill-\"][id$=\"-picker\"]'); existingPickers.forEach(picker => picker.remove()); } private notifyColorChange(quillEditor: Quill, config?: QuillColorPickerConfig): void { if (config?.onColorChange) { const htmlContent = quillEditor.root.innerHTML; config.onColorChange(htmlContent, quillEditor); } } \/** * Clean up method to remove any remaining color pickers *\/ cleanup(): void { this.removeExistingColorPicker(); this.currentQuillInstance = null; this.currentSelection = null; } } ```",
    "author_id":5988,
    "publication_date":1754143299000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Gulab_786",
    "author_reputation":29.0,
    "tags":"javascript, angular, quill, ngx-quill",
    "text_length":6933,
    "title_length":28,
    "num_tags":4
  },
  {
    "id":6532,
    "title":"Did Android Gradle Plugin 8.12.0 deprecated compileSdk?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723342\/did-android-gradle-plugin-8-12-0-deprecated-compilesdk",
    "text":"Recently I updated my Android Gradle Plugin to version 8.12.0 the upgrade was smooth and effortless all the project got upgraded to new Agp version no errors all good. My project level Build.gradle file is looking a little bit awkward as I am seeing a strikethrough text on \"compileSdk\" and the summary stating that compileSdk is deprecated. It was strange because as I remember Google has shifted to \"compileSdk\" from \"compileSdkVersion\" which was deprecated some years ago. There was no clear documentation on the Internet. Each time I found only is that \"compileSdkVersion\" deprecated and \"compileSdk\" is the new keyword to use. ``` android { compileSdk 36 } ``` compileSdk showing a warning of deprecated in Android Studio Any suggestions please? I was expecting \"compileSdk\" is the accepted keyword but it is showing as deprecated in the Agp version 8.12.0. Although the project is compiling and working",
    "author_id":5987,
    "publication_date":1754143397000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Basit Ali",
    "author_reputation":71.0,
    "tags":"android, android-gradle-plugin, build.gradle",
    "text_length":908,
    "title_length":55,
    "num_tags":3
  },
  {
    "id":6531,
    "title":"How to remove a test suite from a test plan without deleting the suite",
    "link":"https:\/\/stackoverflow.com\/questions\/79723343\/how-to-remove-a-test-suite-from-a-test-plan-without-deleting-the-suite",
    "text":"In Azure DevOps, I have a test plan with multiple test suites. It was determined that one of the features of the release will no longer be implemented for this run, but would be implemented in the next. So the test suite is valid, but is not valid for the current plan. However, I do not see a way to remove the suite from the plan, I only see the option to delete the suite, and all associated data with the suite. How do I disconnect the two? There isn't a relationship in the test plan work item itself that touches the two. So far: I have successfully added the suite to the new test plan. I have looked up things on the internet and they all refer to this remove functionality that will delete all of the associated data. I did not find anything on the Microsoft site that was helpful.",
    "author_id":5986,
    "publication_date":1754143430000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Stephen",
    "author_reputation":1.0,
    "tags":"azure-devops",
    "text_length":790,
    "title_length":70,
    "num_tags":1
  },
  {
    "id":6530,
    "title":"Provide manual tracking context when performing reads",
    "link":"https:\/\/stackoverflow.com\/questions\/79723351\/provide-manual-tracking-context-when-performing-reads",
    "text":"Solid.js determines the tracking context (i.e. the effect that should rerun) when reading a reactive primitive (e.g. signal, memo) based on the library's internal global stack. I need to slowly populate the dependencies of a ``` createEffect ``` over time, not just in the synchronous execution of the callback inside ``` createEffect ``` . Is there some way I can obtain a tracking context within ``` createEffect ``` , and then later use this tracking context when accessing a reactive primitive? E.g. something like this: ``` const [data, setData] = createSignal(); createEffect(async () => { const trackingContext = hypothetical_getTrackingContext(); await new Promise((res) => setTimeout(res, 100)); const trackedData = hypothetical_runWithTrackingContext(trackingContext, () => { return data(); }); }); ``` I am familiar with ``` getOwner ``` and ``` runWithOwner ``` and tried experimenting with this, however these appear to relate to the lifetimes of effects etc. and not the tracking context within a particular effect. What options do I have? If Solid.js does not have a builtin mechanism for this, is there any way to build a custom userland ``` createEffect ``` function that still leverages the vast majority of Solid.js internals and that achieves my desired outcome? Alternatively, is there a way to pile up effects \/ create brand new effects every time i need to \"run with tracking context\", somehow linking it back to the original effect? Or perhaps even some custom interface that behaves like this: ``` class Effect { constructor(...) {...} track(callback) {...} } const [data, setData] = createSignal(); const effect = new Effect(...); await new Promise((res) => setTimeout(res, 100)); effect.track(() => { const trackedData = data(); }); ```",
    "author_id":5497,
    "publication_date":1754144135000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"David Callanan",
    "author_reputation":5858.0,
    "tags":"javascript, solid-js",
    "text_length":1763,
    "title_length":53,
    "num_tags":2
  },
  {
    "id":6529,
    "title":"Close terminal window opened by .command Python script",
    "link":"https:\/\/stackoverflow.com\/questions\/79723352\/close-terminal-window-opened-by-command-python-script",
    "text":"I have a Python script. I'd like to double click a file on my desktop (macOS) to run it. I'd like the terminal window opened by running the script to close. To start, I changed the file extension of my script from ``` .py ``` to ``` .command ``` and gave it the following permissions: ``` chmod a+x file.py ``` . Here is the sample script: ``` #!\/usr\/bin\/env python # this worked! # https:\/\/stackoverflow.com\/questions\/5125907\/how-to-run-a-shell-script-in-os-x-by-double-clicking # chmod a+x filename # .command # https:\/\/nordvpn.com\/blog\/macos-cannot-verify-this-app-is-free-from-malware\/ # xattr -d com.apple.quarantine filepath from time import sleep from selenium import webdriver from selenium.webdriver.common.by import By import os service = webdriver.ChromeService(executable_path = '\/usr\/local\/bin\/chromedriver') driver = webdriver.Chrome(service=service) driver.get(\"https:\/\/google.com\/\") sleep(3) ``` This is working nicely, however, when the terminal window opens to run, it does not close itself. I'd like the terminal window to close after completion. Right now it just says '[Process Completed]' but doesn't close the actual terminal window. I've tried ``` quit() ``` and ``` exit() ``` .",
    "author_id":5985,
    "publication_date":1754144294000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"chrisallick",
    "author_reputation":1474.0,
    "tags":"python, macos, terminal",
    "text_length":1203,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6528,
    "title":"Makefile &quot;profiles&quot; to define different flags for the build target",
    "link":"https:\/\/stackoverflow.com\/questions\/79723354\/makefile-profiles-to-define-different-flags-for-the-build-target",
    "text":"I'm trying to find a way to easily compile a project with debugging flags or with optimized flags. I.e. ``` make debug ``` would build the project with ``` FLAGS = -static -W -Wall -Wextra -g -ggdb ``` and ``` make release ``` would use ``` FLAGS = -static -W -Wall -Wextra -O2 -flto ``` . What I have so far works with GNU Make: ``` FLAGS := -static -W -Wall -Wextra release: | opt_flags build opt_flags: $(eval FLAGS := $(FLAGS) -O2 -flto) debug: | dbg_flags build dbg_flags: $(eval FLAGS := $(FLAGS) -g -ggdb) build: # A lot of targets and recipes that use $(FLAGS) ``` However, the ``` | ``` that sets the prerequisites as order-only is available only in GNU Make. This Makefile doesn't work with BSD Make and removing the ``` | ``` will cause problems when running in parallel ( ``` make -j... ``` ). Is there a way to achieve what I want in a Makefile that is compatible with both GNU Make and BSD Make?",
    "author_id":5984,
    "publication_date":1754144610000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"optical",
    "author_reputation":237.0,
    "tags":"makefile",
    "text_length":909,
    "title_length":76,
    "num_tags":1
  },
  {
    "id":6527,
    "title":"Remove corner radius during push navigation transition in iOS 26",
    "link":"https:\/\/stackoverflow.com\/questions\/79723358\/remove-corner-radius-during-push-navigation-transition-in-ios-26",
    "text":"In iOS 26, when using NavigationStack or regular UINavigationController push transitions, there’s a new default animation that adds a corner radius to the incoming view. This corner radius breaks visual consistency in my app. I would like to completely remove it during the push transition. I was trying to play with UINavigationController via SwiftUIIntrospect but no luck. My first screen is similar to this: ``` struct ContentView: View { @State private var isPresented = false var body: some View { VStack { Text(\"Placeholder\") .padding() NavigationStack { VStack { Button { isPresented.toggle() } label: { SourceView() } .navigationDestination(isPresented: $isPresented) { DetailView() } } } } } } ```",
    "author_id":5983,
    "publication_date":1754144996000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dim Roe",
    "author_reputation":53.0,
    "tags":"ios, swift, swiftui, ios26, uinavigationcontroller",
    "text_length":706,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6526,
    "title":"Correct way to obtain address from a BIP39 seed phrase using ethersjs v6",
    "link":"https:\/\/stackoverflow.com\/questions\/79723360\/correct-way-to-obtain-address-from-a-bip39-seed-phrase-using-ethersjs-v6",
    "text":"What is the correct way to get an address from a seed phrase, using ethersjs v6? This is what I'm using: ``` import { HDNodeWallet } from 'ethers'; const hdWallet = HDNodeWallet.fromPhrase(seedPhrase, `m\/44'\/60'\/0'\/0\/0`); const address = hdWallet.address; ``` ...but for this seed phrase (DW, no actual funds here): ``` hidden stool require arrive erode theory gospel snow supply minor open speed ``` actual value (incorrect): ``` 0xEa94DcA98E36cC1d382447E4758C35ecC61c23df ``` expected value (correct): ``` 0x3A36C2DB9e79456a64f095fac21312a2303DdaeF ``` Why is there a difference? Also, what is the correct way to invoke the ethersjs v6 APIs to get the expected value?",
    "author_id":5730,
    "publication_date":1754145141000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"bguiz",
    "author_reputation":28429.0,
    "tags":"javascript, ethers.js",
    "text_length":669,
    "title_length":72,
    "num_tags":2
  },
  {
    "id":6525,
    "title":"Dynatrace Integration with Azure Linux App Service Issue",
    "link":"https:\/\/stackoverflow.com\/questions\/79723363\/dynatrace-integration-with-azure-linux-app-service-issue",
    "text":"I am trying to integrate Dynatrace with a Linux App Service in Azure, to enable us monitor an Azure Function with Java 17 as its application stack. Unlike the Windows App Service, where the Dynatrace extension can be added manually through the Portal or even through Terraform, it appears that extensions are not a feature supported by the Azure Linux App Service and is therefore not available. How can I overcome this challenge?",
    "author_id":5982,
    "publication_date":1754145310000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"hitman126",
    "author_reputation":971.0,
    "tags":"linux, dynatrace, azure-functions, azure-appservice",
    "text_length":430,
    "title_length":56,
    "num_tags":4
  },
  {
    "id":6524,
    "title":"How to implement Facebook Pixel events using Vue + Ionic + Capacitor?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723368\/how-to-implement-facebook-pixel-events-using-vue-ionic-capacitor",
    "text":"I’m building a mobile app using Vue 3, Ionic Framework, and Capacitor (v5). I want to track events (such as page views, button clicks, and purchases) using Facebook Pixel. However, Facebook Pixel is primarily designed for web applications, and I’m unsure how to implement it in a hybrid mobile app. My setup: Vue 3 (Vite) Ionic Framework Capacitor 5 Target platforms: iOS and Android What I’ve tried: Adding the Facebook Pixel JavaScript snippet directly to ``` index.html ``` . It seems to work in a web browser, but I’m unsure if it’s reliable or appropriate for mobile builds. Tried using the Facebook Business SDK for Android\/iOS, but integrating it via Capacitor plugins feels complex and undocumented.",
    "author_id":5981,
    "publication_date":1754146325000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Tele oceans",
    "author_reputation":9.0,
    "tags":"vue.js, capacitor, ionic-framework, ionic3",
    "text_length":707,
    "title_length":69,
    "num_tags":4
  },
  {
    "id":6523,
    "title":"I have a problem with the my the accuracy of my roulette game",
    "link":"https:\/\/stackoverflow.com\/questions\/79723373\/i-have-a-problem-with-the-my-the-accuracy-of-my-roulette-game",
    "text":"I’m building a small roulette wheel game in JavaScript. The wheel spins correctly, but sometimes the pointer shows the number next to the one it actually lands on. Here’s the relevant part of the code: ``` function getPointerNumber(rotation) { const segmentAngle = 360 \/ wheelNumbers.length; \/\/ Normalize rotation to 0–360 const normalizedRotation = ((rotation % 360) + 360) % 360; \/\/ Find which segment the pointer is in const rawSegmentIndex = Math.floor(normalizedRotation \/ segmentAngle); \/\/ Adjust for clockwise layout const adjustedIndex = (wheelNumbers.length - rawSegmentIndex) % wheelNumbers.length; return wheelNumbers[adjustedIndex]; } ``` The wheel has 37 numbers (0–36 in European layout). Numbers are drawn centered in each slice (using angle + (segmentAngle \/ 2) when positioning them). The pointer is fixed at the top (0°). Problem: When the wheel stops, some spins report the correct number, but others report the number next to it. I think it’s something about how I’m calculating the segment index in getPointerNumber(), maybe a rounding or alignment issue. Question: How can I fix the math in getPointerNumber() so the pointer always returns the correct number the arrow is pointing at?",
    "author_id":5980,
    "publication_date":1754147047000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Georgi E",
    "author_reputation":1.0,
    "tags":"javascript, html, css",
    "text_length":1206,
    "title_length":61,
    "num_tags":3
  },
  {
    "id":6522,
    "title":"Setting up ContentType for model binding errors in ASP.NET Core",
    "link":"https:\/\/stackoverflow.com\/questions\/79723381\/setting-up-contenttype-for-model-binding-errors-in-asp-net-core",
    "text":"When model binding error (example: ``` \/clients\/abc ``` instead of ``` \/clients\/123 ``` ) on ``` [ApiController] ``` occurs we get an automatic 400 Bad Request with ``` content-type: application\/json; charset=utf-8 ``` . Is there a way to set it up so that ``` content-type: application\/problem+json ``` is returned instead?",
    "author_id":5979,
    "publication_date":1754147619000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"darko99",
    "author_reputation":716.0,
    "tags":"c#, asp.net-core",
    "text_length":324,
    "title_length":63,
    "num_tags":2
  },
  {
    "id":6521,
    "title":"How to downcast any List or any Option in rust",
    "link":"https:\/\/stackoverflow.com\/questions\/79723390\/how-to-downcast-any-list-or-any-option-in-rust",
    "text":"I want to add properties window with auto properties based on struct fields. I'am downcasting every field, but for ``` Vec<> ``` and ``` Option<> ``` I have to duplicate all code, how can I automatically check is that any ``` Option ``` or ``` Vec ``` . Downcaster: ``` pub fn downcast_property(ui: &mut Ui, title: &str, value_mut: &mut (dyn Any + 'static), interactable: bool) { if let Some(string_v) = value_mut.downcast_mut::<String>() { string_property(ui, title, string_v, interactable); } else if let Some(int_v) = value_mut.downcast_mut::<i32>() { i_property(ui, title, int_v, interactable); } else if let Some(float_v) = value_mut.downcast_mut::<f32>() { f_property(ui, title, float_v, interactable); } else if let Some(usize_v) = value_mut.downcast_mut::<usize>() { i_property(ui, title, usize_v, interactable); } else if let Some(bool_v) = value_mut.downcast_mut::<bool>() { bool_property(ui, title, bool_v, interactable); } else if downcast_all_enum_properties(value_mut, ui, title, interactable, true) { \/\/ Automaticall enum downcaster } else if let Some(named_type_value) = value_mut.downcast_mut::<NamedTypeValue>() { namedvalue_property(ui, title, named_type_value, interactable); } else if let Some(argument_named_value) = value_mut.downcast_mut::<ArgumentNamedValue>() { argnamedvalue_property(ui, title, argument_named_value, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<String>>() { list_property(ui, title, v, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<i32>>() { list_property(ui, title, v, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<f32>>() { list_property(ui, title, v, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<bool>>() { list_property(ui, title, v, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<DataType>>() { list_property(ui, title, v, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<ValueType>>() { list_property(ui, title, v, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<NamedTypeValue>>() { list_property(ui, title, v, interactable); } else if let Some(v) = value_mut.downcast_mut::<Vec<ArgumentNamedValue>>() { list_property(ui, title, v, interactable); } else if let Some(o) = value_mut.downcast_mut::<Option<DataType>>() { option_propery(ui, title, o, interactable); } else if let Some(o) = value_mut.downcast_mut::<Option<String>>() { option_propery(ui, title, o, interactable); } else if let Some(o) = value_mut.downcast_mut::<Option<usize>>() { option_propery(ui, title, o, interactable); } else { ui.label(RichText::new(format!(\"Field {}: unknown type\", title)).color(Color32::LIGHT_RED)); } } ``` ``` list_property ``` and ``` option_property ``` functions: ``` pub fn list_property<T>(ui: &mut Ui, title: &str, value_mut: &mut Vec<T>, interactable: bool) where T: Default + 'static { egui::CollapsingHeader::new(title) .default_open(true) .enabled(interactable) .show(ui, |ui| { ui.horizontal_top(|ui| { if ui.button(\"➕ Add\").clicked() { value_mut.push(T::default()); } }); let mut i = 0; while i < value_mut.len() { ui.horizontal(|ui| { downcast_property(ui, \"\", &mut value_mut[i], interactable); if ui.button(\"❌\").on_hover_text(\"Remove\").clicked() { value_mut.remove(i); } else { i += 1; } }); } }); } pub fn option_propery<T>(ui: &mut Ui, title: &str, value_mut: &mut Option<T>, interactable: bool) where T: Default + 'static { let mut is_option_none = value_mut.is_none(); ui.horizontal(|ui| { ui.label(title); ui.add_space(PROPERTY_LABEL_PADDING); bool_property(ui, \"None\", &mut is_option_none, interactable); ui.add_space(2.0); if is_option_none { *value_mut = None; } else { if let Some(inner) = value_mut.as_mut() { downcast_property(ui, \"\", inner, interactable); } else { *value_mut = Some(T::default()); } } }); } ```",
    "author_id":5978,
    "publication_date":1754148508000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Kravtsov Hryhorii",
    "author_reputation":31.0,
    "tags":"vector, rust, option-type, downcast, vec",
    "text_length":3829,
    "title_length":46,
    "num_tags":5
  },
  {
    "id":6520,
    "title":"Flutter ListView prevent auto-scroll with `reverse: true`, and fix inconsistent scroll behavior with `maxScrollExtent`",
    "link":"https:\/\/stackoverflow.com\/questions\/79723392\/flutter-listview-prevent-auto-scroll-with-reverse-true-and-fix-inconsistent",
    "text":"Question: I'm building a chat app in Flutter where the latest messages should appear at the bottom . Messages can be sent or received, and AI replies are streamed (typed character-by-character). I’ve tried both using ``` reverse: true ``` and managing scroll manually, but both approaches have issues. ✅ What I want: Chat screen should start at the bottom . Streamed messages should grow downward without auto-scrolling . User should scroll manually or tap a button to return to bottom. No overscroll or jumping. 🧪 What I tried: Using ``` reverse: true ``` in ``` ListView ``` : Pros: Starts at the bottom instantly. Cons: When the AI streams a message (e.g., character by character), the ListView automatically scrolls as the message grows even if the user scrolled up. Without reverse: Rebuilt layout from top to bottom. After loading messages (on open), I use: ``` _scrollController.animateTo( _scrollController.position.maxScrollExtent, duration: Duration(milliseconds: 300), curve: Curves.easeOut, ); ``` Added a scroll-to-bottom icon when user scrolls up. Also tried ``` jumpTo ``` , but result is similar. Used ``` ClampingScrollPhysics ``` to avoid overscroll — still experiencing strange behaviors: Sometimes it doesn’t scroll to the bottom. Sometimes it overscrolls and shows empty space . Occasionally it shakes or jumps , then settles. ❓ Questions: 1. How can I stop auto-scroll when using ``` reverse: true ``` and streaming message content? I want the text to keep growing downward, but not force scroll , unless the user taps a button or scrolls themselves. 2. How can I make the scroll behavior consistent when: The ListView is not reversed , Messages are streamed and growing in size , I want the UI to avoid auto-scrolling during typing , but allow manual scroll or tap-to-bottom? Any best practices or clean solutions would be appreciated!",
    "author_id":5977,
    "publication_date":1754148654000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mohammad Alamoudi",
    "author_reputation":485.0,
    "tags":"flutter, undefined-behavior, listview, chat, scrollcontroller",
    "text_length":1858,
    "title_length":118,
    "num_tags":5
  },
  {
    "id":6519,
    "title":"Having EBUSY: resource busy or locked, rmdir &#39;blablabla...\\dist&#39; when trying to run Nest project locally via VSCode",
    "link":"https:\/\/stackoverflow.com\/questions\/79723401\/having-ebusy-resource-busy-or-locked-rmdir-blablabla-dist-when-trying-to",
    "text":"I was facing this exact error every time I tried to run a Nest project locally via VSCode and I didn't find anything related to it on the internet, so I'm gonna post a question and the answer I found to my specific context. Context: I'm using Nest.js I have some Vitest tests and Vitest extension installed The error disappears if I run it directly via console or manually delete the folder",
    "author_id":5976,
    "publication_date":1754149837000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"King Witcher",
    "author_reputation":41.0,
    "tags":"nestjs, vitest",
    "text_length":390,
    "title_length":123,
    "num_tags":2
  },
  {
    "id":6518,
    "title":"I entered the wrong command in Ubuntu",
    "link":"https:\/\/stackoverflow.com\/questions\/79723403\/i-entered-the-wrong-command-in-ubuntu",
    "text":"I entered the wrong command in Ubuntu Server 24.04. I wrote ``` sudo chown -R opendkim:opendkim \/ etc\/opendkim ``` instead of ``` sudo chown -R opendkim:opendkim \/etc\/opendkim ``` and now I can't even use sudo anymore. I thought that the command didn't worked because I got a lot of permission denied errors but it looks like it actually did something. What can I do now? Thanks",
    "author_id":5975,
    "publication_date":1754150314000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Cristian Mazzoni",
    "author_reputation":1.0,
    "tags":"linux, ubuntu, permissions, terminal",
    "text_length":378,
    "title_length":37,
    "num_tags":4
  },
  {
    "id":6517,
    "title":"How to Remove Shipping and Payment Steps from Checkout and Directly Enable “place the order” Button in BAGISTO?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723405\/how-to-remove-shipping-and-payment-steps-from-checkout-and-directly-enable-plac",
    "text":"I’m customizing the checkout process in my Bagisto-based store. Objective: I want to remove the shipping and payment steps from the checkout process entirely. That means: On the checkout (buy) page, once the billing address form is filled, I want to directly show and enable the \"Passer la commande\" (Place Order) button. I want to remove the \"Continue\" button and the intermediate steps for shipping and payment. Essentially, the customer fills in their details, and can immediately place the order without additional steps. This is intended for a simple store setup where no shipping method or payment choice is required (I have one shipping and one payment steps). Could anyone guide me on the cleanest way to achieve this in Bagisto 2.3.6? Thanks in advance!",
    "author_id":4840,
    "publication_date":1754150383000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"James",
    "author_reputation":13.0,
    "tags":"php, vue.js, laravel",
    "text_length":762,
    "title_length":111,
    "num_tags":3
  },
  {
    "id":6516,
    "title":"How can a custom remote launch another remote or replace the current layout with another one?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723406\/how-can-a-custom-remote-launch-another-remote-or-replace-the-current-layout-with",
    "text":"This question is intended to be answered by those who have a knowledge of programming Unified Remote (using lua and xml) and its framework, as detailed here https:\/\/github.com\/unifiedremote\/Docs\/tree\/master\/concepts . In this question, terms like \"remote\", \"layout\", \"loading\", \"tabs\" etc will be very familiar to developers who code unified remote, as they are very basic concepts, described in detail at the link above. This question asks what library and function can open another remote for use, or as an alternative, what library and function can change the user interface by loading a different layout xml file. I have now answered how to open another remote. However loading a different layout xml file would be preferable, so I am still looking for an answer to that (if it is possible). A description of my requirement follows: I have a custom remote that controls my IDE (IntelliJ IDEA). I would like to have different screen layouts (and specifically different tabs) when working in different languages. For example, the button on my phone that tells IDEA to surround the current statement with an \"unless\" statement makes no sense if I am not coding in perl. I can create and code different remotes for each language and add them to Unified remote's quick launch menu. However, I would like just one master remote for these listed in Unified Remote's list of remotes on its main menu page. This remote would switch to the last of these custom remotes that I used. The name of the remote can be saved as a setting in the events.focus callback. I can't work out how to display a different remote from within a remote. -- This has now been answered below. As an alternative, I could have one remote that manages all languages. It could replace the whole of the UI by loading a different layout file. I cannot find anyway of doing this. Preloading can be used to dynamically create a layout, but my understanding is that this is done once as the remote is created, so cannot be used to change the layout of a running remote (I might be wrong about this). The layout changes are not trivial enough to manage with visibility changes. For example, you cannot add and remove tabs this way. Can I load a different layout.xml file, as this would be preferable to having a number of separate remotes.",
    "author_id":5974,
    "publication_date":1754150613000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Steve Waring",
    "author_reputation":3267.0,
    "tags":"lua, unified-remote",
    "text_length":2301,
    "title_length":93,
    "num_tags":2
  },
  {
    "id":6515,
    "title":"NotImplementedError using chess.engine.SimpleEngine.popen_uci in Jupyter Notebook",
    "link":"https:\/\/stackoverflow.com\/questions\/79723408\/notimplementederror-using-chess-engine-simpleengine-popen-uci-in-jupyter-noteboo",
    "text":"All versions are up to date: Windows 11, Python 3.13.5, Jupyter: ``` IPython : 9.4.0 ipykernel : 6.30.0 ipywidgets : not installed jupyter_client : 8.6.3 jupyter_core : 5.8.1 jupyter_server : 2.16.0 jupyterlab : 4.4.5 nbclient : 0.10.2 nbconvert : 7.16.6 nbformat : 5.10.4 notebook : 7.4.4 qtconsole : not installed traitlets : 5.14.3 ``` stockfish binary 17.1, stockfish python lib 3.28.0, and chess lib 1.11.2 When I run the following Python program, it works also ok: ``` from stockfish import Stockfish import chess import chess.engine import chess.pgn engine_path = \"c:\/portable\/stockfish\/stockfish-windows-x86-64-avx2.exe\" stockfish = Stockfish(path=engine_path, depth=18, parameters={\"Threads\": 4}); board = chess.Board() print(board) engine = chess.engine.SimpleEngine.popen_uci(engine_path) board = chess.Board() while not board.is_game_over(): result = engine.play(board, chess.engine.Limit(time=0.01)) board.push(result.move) print(board) engine.quit() ``` It properly outputs: ``` c:\\projects\\source\\pgn_analyzer>python tst.py r n b q k b n r p p p p p p p p . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P P P P P P P P R N B Q K B N R r n b q k b n r p p p p p p p p . . . . . . . . . . . . . . . . . . . . P . . . . . . . . . . . P P P P . P P P R N B Q K B N R .... . . K . . . . . . . . . . . . . . . . . . k . . . . . . . . . . . . . . . . . . . . . . . . . . . . p . . . . . . . K . . . . . . . . . . . . . . . . . . . . . . . . . . k . . . . . . . . . . . . . . . . . . . . . . . . . . . . p . . . . . . . K . . . . . . . . . . . . . . . . . k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K . . . . . . . . . . . . . . . . . . . . . . . . . k . . . . . . . . . . . . . . . . . . . ``` If I run the same code in a Jupyter Notebook however, I get a NotImplementedError Here's the complete error stack - I'm out of ideas at the moment 8(( ``` --------------------------------------------------------------------------- NotImplementedError Traceback (most recent call last) Cell In[1], line 12 9 board = chess.Board() 10 print(board) ---> 12 engine = chess.engine.SimpleEngine.popen_uci(engine_path) 15 board = chess.Board() 16 while not board.is_game_over(): File ~\\AppData\\Roaming\\Python\\Python313\\site-packages\\chess\\engine.py:3052, in SimpleEngine.popen_uci(cls, command, timeout, debug, setpgrp, **popen_args) 3046 @classmethod 3047 def popen_uci(cls, command: Union[str, List[str]], *, timeout: Optional[float] = 10.0, debug: Optional[bool] = None, setpgrp: bool = False, **popen_args: Any) -> SimpleEngine: 3048 \"\"\" 3049 Spawns and initializes a UCI engine. 3050 Returns a :class:`~chess.engine.SimpleEngine` instance. 3051 \"\"\" -> 3052 return cls.popen(UciProtocol, command, timeout=timeout, debug=debug, setpgrp=setpgrp, **popen_args) File ~\\AppData\\Roaming\\Python\\Python313\\site-packages\\chess\\engine.py:3044, in SimpleEngine.popen(cls, Protocol, command, timeout, debug, setpgrp, **popen_args) 3041 simple_engine.close() 3042 await simple_engine.shutdown_event.wait() -> 3044 return run_in_background(background, name=f\"{cls.__name__} (command={command!r})\", debug=debug) File ~\\AppData\\Roaming\\Python\\Python313\\site-packages\\chess\\engine.py:77, in run_in_background(coroutine, name, debug) 74 future.set_exception(exc) 76 threading.Thread(target=background, name=name).start() ---> 77 return future.result() File C:\\Program Files\\Python313\\Lib\\concurrent\\futures\\_base.py:456, in Future.result(self, timeout) 454 raise CancelledError() 455 elif self._state == FINISHED: --> 456 return self.__get_result() 457 else: 458 raise TimeoutError() File C:\\Program Files\\Python313\\Lib\\concurrent\\futures\\_base.py:401, in Future.__get_result(self) 399 if self._exception is not None: 400 try: --> 401 raise self._exception 402 finally: 403 # Break a reference cycle with the exception in self._exception 404 self = None File ~\\AppData\\Roaming\\Python\\Python313\\site-packages\\chess\\engine.py:71, in run_in_background.<locals>.background() 69 def background() -> None: 70 try: ---> 71 asyncio.run(coroutine(future), debug=debug) 72 future.cancel() 73 except Exception as exc: File C:\\Program Files\\Python313\\Lib\\asyncio\\runners.py:195, in run(main, debug, loop_factory) 191 raise RuntimeError( 192 \"asyncio.run() cannot be called from a running event loop\") 194 with Runner(debug=debug, loop_factory=loop_factory) as runner: --> 195 return runner.run(main) File C:\\Program Files\\Python313\\Lib\\asyncio\\runners.py:118, in Runner.run(self, coro, context) 116 self._interrupt_count = 0 117 try: --> 118 return self._loop.run_until_complete(task) 119 except exceptions.CancelledError: 120 if self._interrupt_count > 0: File C:\\Program Files\\Python313\\Lib\\asyncio\\base_events.py:725, in BaseEventLoop.run_until_complete(self, future) 722 if not future.done(): 723 raise RuntimeError('Event loop stopped before Future completed.') --> 725 return future.result() File ~\\AppData\\Roaming\\Python\\Python313\\site-packages\\chess\\engine.py:3032, in SimpleEngine.popen.<locals>.background(future) 3031 async def background(future: concurrent.futures.Future[SimpleEngine]) -> None: -> 3032 transport, protocol = await Protocol.popen(command, setpgrp=setpgrp, **popen_args) 3033 threading.current_thread().name = f\"{cls.__name__} (pid={transport.get_pid()})\" 3034 simple_engine = cls(transport, protocol, timeout=timeout) File ~\\AppData\\Roaming\\Python\\Python313\\site-packages\\chess\\engine.py:1212, in Protocol.popen(cls, command, setpgrp, **popen_args) 1208 else: 1209 # Before Python 3.11 1210 popen_args[\"start_new_session\"] = True -> 1212 return await asyncio.get_running_loop().subprocess_exec(cls, *command, **popen_args) File C:\\Program Files\\Python313\\Lib\\asyncio\\base_events.py:1794, in BaseEventLoop.subprocess_exec(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs) 1792 debug_log = f'execute program {program!r}' 1793 self._log_subprocess(debug_log, stdin, stdout, stderr) -> 1794 transport = await self._make_subprocess_transport( 1795 protocol, popen_args, False, stdin, stdout, stderr, 1796 bufsize, **kwargs) 1797 if self._debug and debug_log is not None: 1798 logger.info('%s: %r', debug_log, transport) File C:\\Program Files\\Python313\\Lib\\asyncio\\base_events.py:539, in BaseEventLoop._make_subprocess_transport(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs) 535 async def _make_subprocess_transport(self, protocol, args, shell, 536 stdin, stdout, stderr, bufsize, 537 extra=None, **kwargs): 538 \"\"\"Create subprocess transport.\"\"\" --> 539 raise NotImplementedError NotImplementedError: ``` Update - in Ubuntu it works :\/",
    "author_id":5973,
    "publication_date":1754150650000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Google Gauner",
    "author_reputation":41.0,
    "tags":"python-3.x, jupyter-notebook, python-asyncio, python-chess",
    "text_length":6692,
    "title_length":81,
    "num_tags":4
  },
  {
    "id":6514,
    "title":"Could not resolve androidx.activity:activity:1.2.3",
    "link":"https:\/\/stackoverflow.com\/questions\/79723409\/could-not-resolve-androidx-activityactivity1-2-3",
    "text":"I am upgrading my java android project to match the new minimum APK 35 and billing 7.0.0, I am almost done, there is just the billing I can't get to work, my build works fine with ``` 'com.android.billingclient:billing:6.0.0' ``` but crashes with version 7.0.0 or 8.0.0, can anyone help me out? The new requirement is minimum 7.0.0. here is the error I get : ``` Could not resolve all artifacts for configuration 'classpath'. Could not resolve androidx.activity:activity:1.2.3. Required by: root project : com.android.billingclient:billing:7.0.0 No matching variant of androidx.activity:activity:1.2.3 was found. The consumer was configured to find a library for use during runtime, compatible with Java 21, packaged as a jar, and its dependencies declared externally, as well as attribute 'org.gradle.plugin.api-version' with value '8.13' but: - Variant 'releaseApiPublication' declares a library, and its dependencies declared externally: - Incompatible because this component declares a component for use during compile-time, with the library elements 'aar' and the consumer needed a component for use during runtime, packaged as a jar - Other compatible attributes: - Doesn't say anything about its target Java version (required compatibility with Java 21) - Doesn't say anything about org.gradle.plugin.api-version (required '8.13') - Variant 'releaseRuntimePublication' declares a library for use during runtime, and its dependencies declared externally: - Incompatible because this component declares a component, with the library elements 'aar' and the consumer needed a component, packaged as a jar - Other compatible attributes: - Doesn't say anything about its target Java version (required compatibility with Java 21) - Doesn't say anything about org.gradle.plugin.api-version (required '8.13') - Variant 'sourcesElements' declares a component for use during runtime, and its dependencies declared externally: - Incompatible because this component declares documentation and the consumer needed a library - Other compatible attributes: - Doesn't say anything about its elements (required them packaged as a jar) - Doesn't say anything about its target Java version (required compatibility with Java 21) - Doesn't say anything about org.gradle.plugin.api-version (required '8.13') > Could not resolve androidx.activity:activity:1.0.0. Required by: root project : > com.android.billingclient:billing:7.0.0 > com.google.android.gms:play-services-base:18.3.0 > androidx.fragment:fragment:1.1.0 > No matching variant of androidx.activity:activity:1.2.3 was found. The consumer was configured to find a library for use during runtime, compatible with Java 21, packaged as a jar, and its dependencies declared externally, as well as attribute 'org.gradle.plugin.api-version' with value '8.13' but: - Variant 'releaseApiPublication' declares a library, and its dependencies declared externally: - Incompatible because this component declares a component for use during compile-time, with the library elements 'aar' and the consumer needed a component for use during runtime, packaged as a jar - Other compatible attributes: - Doesn't say anything about its target Java version (required compatibility with Java 21) - Doesn't say anything about org.gradle.plugin.api-version (required '8.13') - Variant 'releaseRuntimePublication' declares a library for use during runtime, and its dependencies declared externally: - Incompatible because this component declares a component, with the library elements 'aar' and the consumer needed a component, packaged as a jar - Other compatible attributes: - Doesn't say anything about its target Java version (required compatibility with Java 21) - Doesn't say anything about org.gradle.plugin.api-version (required '8.13') - Variant 'sourcesElements' declares a component for use during runtime, and its dependencies declared externally: - Incompatible because this component declares documentation and the consumer needed a library - Other compatible attributes: - Doesn't say anything about its elements (required them packaged as a jar) - Doesn't say anything about its target Java version (required compatibility with Java 21) - Doesn't say anything about org.gradle.plugin.api-version (required '8.13') * Try: > No matching variant errors are explained in more detail at https:\/\/docs.gradle.org\/8.13\/userguide\/variant_model.html#sub:variant-no-match. > Review the variant matching algorithm at https:\/\/docs.gradle.org\/8.13\/userguide\/variant_attributes.html#sec:abm_algorithm. > Run with --stacktrace option to get the stack trace. > Run with --info or --debug option to get more log output. > Run with --scan to get full insights. > Get more help at https:\/\/help.gradle.org. Deprecated Gradle features were used in this build, making it incompatible with Gradle 9.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. For more on this, please refer to https:\/\/docs.gradle.org\/8.13\/userguide\/command_line_interface.html#sec:command_line_warnings in the Gradle documentation. BUILD FAILED in 684ms ``` here is my build.gradle: ``` buildscript { repositories { jcenter() google() mavenCentral() } dependencies { classpath 'com.android.tools.build:gradle:8.11.1' classpath 'com.android.billingclient:billing:7.0.0' classpath 'com.google.gms:google-services:4.3.14' } } allprojects { repositories { jcenter() google() mavenCentral() maven { url 'https:\/\/maven.google.com\/' } maven { url 'https:\/\/jitpack.io' } } } ``` and my app\/build.gradle: ``` apply plugin: 'com.android.application' android { compileSdk 35 defaultConfig { applicationId \"*******\" minSdkVersion 21 targetSdk 35 multiDexEnabled true compileOptions { sourceCompatibility JavaVersion.VERSION_17 targetCompatibility JavaVersion.VERSION_17 } lintOptions { checkReleaseBuilds false \/\/ Or, if you prefer, you can continue to check for errors in release builds, \/\/ but continue the build even when errors are found: abortOnError false } } buildTypes { release { minifyEnabled true proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-project.txt' } } namespace '******' } dependencies { implementation 'androidx.core:core:1.12.0' implementation 'com.android.billingclient:billing:7.0.0' implementation 'androidx.appcompat:appcompat:1.1.0' implementation 'com.android.support:multidex:1.0.3' implementation 'androidx.legacy:legacy-support-v4:1.0.0' implementation 'androidx.gridlayout:gridlayout:1.0.0' implementation 'pl.droidsonroids.gif:android-gif-drawable:1.2.18' implementation files('lib\/App42MultiPlayerGamingSDK.jar') implementation 'com.anjlab.android.iab.v3:library:2.0.3' implementation 'androidx.constraintlayout:constraintlayout:1.1.3' implementation 'com.google.android.material:material:1.0.0' implementation 'com.github.bumptech.glide:glide:4.7.1' implementation 'de.hdodenhof:circleimageview:1.3.0' implementation 'com.android.support:recyclerview-v7:27.0.0' implementation 'androidx.media:media:1.0.0' implementation 'com.github.Abhi347:NoobCameraFlash:0.2.0' \/\/ Google implementation 'com.google.android.gms:play-services-auth:16.0.1' \/\/Worker implementation 'androidx.startup:startup-runtime:1.1.1' implementation \"androidx.lifecycle:lifecycle-process:2.5.1\" implementation \"androidx.annotation:annotation:1.4.0\" implementation \"androidx.work:work-runtime:2.8.1\" implementation \"androidx.work:work-runtime-ktx:2.8.1\" implementation \"io.coil-kt:coil-compose:2.4.0\" } apply plugin: 'com.google.gms.google-services' ```",
    "author_id":5972,
    "publication_date":1754150693000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Phil",
    "author_reputation":359.0,
    "tags":"java, android",
    "text_length":7546,
    "title_length":50,
    "num_tags":2
  },
  {
    "id":6513,
    "title":"Using sizeof(pointer) with strncpy",
    "link":"https:\/\/stackoverflow.com\/questions\/79723418\/using-sizeofpointer-with-strncpy",
    "text":"I have this parser, I want to use the size of the pointer (that is 8 bytes) in the function ``` strncpy ``` , I was able do it with the \"method\" part, why does it crash the \"path\" part ? It tells me ``` Segmentation fault (core dumped) ``` in the path part, so it does not have the authorization? Am I missing something ? ``` int http_parser(char *request) { char tmp_request[BUFFER_SIZE]; char *method; char *path; strncpy(tmp_request, request, sizeof(tmp_request)); char *token = strtok(tmp_request, \" \"); if (token == NULL) { printf(\"\\nInvalid Request Format!\"); return 0; } \/\/METODO strncpy(method, token, sizeof(method)); token = strtok(NULL, \" \"); \/\/printf(\"METODO!\\n\"); if (token == NULL) { printf(\"Invalid Request Method!\\n\"); return 0; } else { printf(\"Metodo : %s\\n\",method); } \/\/path strncpy(path, token, sizeof(path)); token = strtok(NULL, \" \"); printf(\"PATH\\n\"); if (token == NULL) { printf(\"Invalid Request Path\\n\"); return 0; } else { printf(\"Path : %s\\n\",path); } } ``` I tried using ``` sizeof(8) ``` and it works, but I want to know why it doesn't work when I do it this way.",
    "author_id":4827,
    "publication_date":1754151410000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Solrojo",
    "author_reputation":61.0,
    "tags":"c, http, httpserver, strncpy",
    "text_length":1093,
    "title_length":34,
    "num_tags":4
  },
  {
    "id":6512,
    "title":"Can a npm package provide AI guidance to its host application?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723419\/can-a-npm-package-provide-ai-guidance-to-its-host-application",
    "text":"I have an NPM design system package (call it OpenDesign) whose components are well documented, and when it is included in an application, and the application developer prompts their IDE to develop screens, it should use those components from the design system. I know the user can include in the prompt \"Use components from OpenDesign\" but how can the emitted code best conform to the style within? Should the documentation \/ test suite be shipped in the NPM package so it can be included into context? Then how does the app developer tell their coding agent to use it when node_modules are typically ignored as a source of context? Don't assume the package is popular or already known by the coding agent - the context should be usable from local files alone. I'm specifically interested in Copilot or Windsurf instructions, but I think it's partly a packaging question too.. How to package a library so it provides hints to the application including it along the lines of a CLAUDE.md file or COPILOT_INSTRUCTIONS.md file, but just for its own preferences and conventions.",
    "author_id":5971,
    "publication_date":1754151416000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dean Radcliffe",
    "author_reputation":2202.0,
    "tags":"npm, claude, artificial-intelligence, github-copilot, agent",
    "text_length":1073,
    "title_length":62,
    "num_tags":5
  },
  {
    "id":6511,
    "title":"Mount a drive&#39;s folder (not the drive&#39;s root) for access",
    "link":"https:\/\/stackoverflow.com\/questions\/79723420\/mount-a-drives-folder-not-the-drives-root-for-access",
    "text":"Ok, background: I have created a LiveUSB using Rufus of a Linux distro (<4GB, FAT32) with persistence (16GB, EXT4) on a Sandisk Gen3 128GB flash drive (actual size is 114GB). The FAT32 partition has 94GB free but it is not normally accessible while in LiveUSB mode but full access in Windows. In Terminal, I can issue: ``` (Note that I already have the flash drive partition data) sudo -i mkdir \/mnt\/Transfers mount \/dev\/<flash drive partition 1> \/mnt\/Transfers ``` \"Transfers\" folder shows up on the Desktop and I have full access to the FAT32 partition of the LiveUSB drive. I would like to create a folder, let's say \"exo-OS Transfer\", on the drive to be used for transfers between OSes and have only it, and not the full drive, accessible in Linux (just so I don't corrupt the Linux OS with an accidental root level command\/action). If I screw up in Windows, I can copy the transfer folder and recreate the LiveUSB (losing the persistence, but it is mainly for updates and installs, so also recreate-able). Something like: ``` sudo -i mkdir \/mnt\/Transfers mount \"\/dev\/<flash drive partition 1>\/exo-OS Transfer\" \/mnt\/Transfers ``` So far, all searchs to see if you can mount a folder on a drive instead of the full drive have failed (points to drive\/partition only). I don't know if this is because what i want isn't possible or I am not phrasing it properly. Also, if possible, I would like to see the fstab entry format of this in case I decide to make it permanent.",
    "author_id":5970,
    "publication_date":1754151633000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"James V. Fields",
    "author_reputation":27.0,
    "tags":"linux, directory, mount, drive, automount",
    "text_length":1471,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6510,
    "title":"ImGui Image() function doesnt work, opengl",
    "link":"https:\/\/stackoverflow.com\/questions\/79723422\/imgui-image-function-doesnt-work-opengl",
    "text":"I'm rendering to a 2D texture using a compute shader, and then trying to display that texture inside an ImGui window using ``` ImGui::Image() ``` . The GUI window shows up, and I added ``` ImGui::Text(\"test\") ``` just below the image widget to confirm layout behavior. When I run the code, the image space seems to be allocated (as the text appears pushed down), but the texture content doesn't render — it's just blank or not visible. Here's the relevant code: ``` void MyGui::createGui(GLuint textureID, int texWidth, int texHeight) { dock(); \/\/ scene viewer ImGui::Begin(\"Scene View\"); ImVec2 windowSize = ImGui::GetContentRegionAvail(); ImGui::Image( (ImTextureID)textureID, windowSize ); ImGui::Text(\"test\"); ImGui::End(); } ``` Here is my main render loop: ``` void App::run() { printDebugInfo(); while (!glfwWindowShouldClose(m_window)) { \/\/ tell ImGui that a new frame is about to begin m_my_gui.newFrameImGui(); \/\/ increment frame count m_frame_count++; \/\/ clear the buffer glClearColor(0.1f, 0.1f, 0.2f, 0.0f); glClear(GL_COLOR_BUFFER_BIT); \/\/ compute shader pass m_compute_program.use(); setPerFrameUniforms(); \/\/ set !static uniforms every frame if (m_frame_count == 1) \/\/ set static uniforms once setStaticUniforms(); glBindImageTexture(0, m_compute_texture.getID(), 0, GL_FALSE, 0, GL_WRITE_ONLY, GL_RGBA32F); \/\/ If reading from same texture (for accumulation), bind as sampler glActiveTexture(GL_TEXTURE1); glBindTexture(GL_TEXTURE_2D, m_compute_texture.getID()); glUniform1i(glGetUniformLocation(m_compute_program.getID(), \"inputTexture\"), 1); glDispatchCompute(m_WINDOW_WIDTH \/ 8, m_WINDOW_HEIGHT \/ 4, 1); glMemoryBarrier(GL_ALL_BARRIER_BITS); ImGui::Begin(\"Save\"); if (ImGui::Button(\"Export\")) { exportImage(m_compute_texture.getID(), m_WINDOW_WIDTH, m_WINDOW_HEIGHT); } ImGui::End(); \/\/ draw all the GUI elements m_my_gui.createGui(m_compute_texture.getID(), m_WINDOW_WIDTH, m_WINDOW_HEIGHT); \/\/ redner ImGui, update events, swap buffers m_my_gui.render(); glfwSwapBuffers(m_window); glfwPollEvents(); } } ``` MyGui class: ``` #include \"MyGui.hpp\" MyGui::MyGui() { } MyGui::~MyGui() { } void MyGui::createGui(GLuint textureID, int texWidth, int texHeight) { dock(); \/\/ scene viewer ImGui::Begin(\"Scene View\"); ImVec2 windowSize = ImGui::GetContentRegionAvail(); ImGui::Image( (ImTextureID)textureID, windowSize ); ImGui::Text(\"test\"); ImGui::End(); } void MyGui::dock() { static bool opt_fullscreen = true; static bool opt_padding = false; static ImGuiDockNodeFlags dockspace_flags = ImGuiDockNodeFlags_None; ImGuiWindowFlags window_flags = ImGuiWindowFlags_MenuBar | ImGuiWindowFlags_NoDocking; if (opt_fullscreen) { ImGuiViewport* viewport = ImGui::GetMainViewport(); ImGui::SetNextWindowPos(viewport->Pos); ImGui::SetNextWindowSize(viewport->Size); ImGui::SetNextWindowViewport(viewport->ID); window_flags |= ImGuiWindowFlags_NoTitleBar | ImGuiWindowFlags_NoCollapse | ImGuiWindowFlags_NoResize | ImGuiWindowFlags_NoMove; window_flags |= ImGuiWindowFlags_NoBringToFrontOnFocus | ImGuiWindowFlags_NoNavFocus; } if (!opt_padding) ImGui::PushStyleVar(ImGuiStyleVar_WindowPadding, ImVec2(0.0f, 0.0f)); ImGui::Begin(\"DockSpace Demo\", nullptr, window_flags); if (!opt_padding) ImGui::PopStyleVar(); \/\/ DockSpace ImGuiID dockspace_id = ImGui::GetID(\"MyDockSpace\"); ImGui::DockSpace(dockspace_id, ImVec2(0.0f, 0.0f), dockspace_flags); ImGui::End(); } void MyGui::initImGui(GLFWwindow* window) { IMGUI_CHECKVERSION(); ImGui::CreateContext(); ImGuiIO& io = ImGui::GetIO(); (void)io; ImGui::StyleColorsDark(); ImGui_ImplGlfw_InitForOpenGL(window, true); ImGui_ImplOpenGL3_Init(\"#version 460\"); \/\/ flags io.ConfigFlags |= ImGuiConfigFlags_DockingEnable; io.ConfigFlags |= ImGuiConfigFlags_ViewportsEnable; } void MyGui::newFrameImGui() { ImGui_ImplOpenGL3_NewFrame(); ImGui_ImplGlfw_NewFrame(); ImGui::NewFrame(); } void MyGui::render() const { ImGui::Render(); ImGui_ImplOpenGL3_RenderDrawData(ImGui::GetDrawData()); \/\/ viewports ImGuiIO& io = ImGui::GetIO(); if (io.ConfigFlags & ImGuiConfigFlags_ViewportsEnable) { GLFWwindow* backup_current_context = glfwGetCurrentContext(); ImGui::UpdatePlatformWindows(); ImGui::RenderPlatformWindowsDefault(); glfwMakeContextCurrent(backup_current_context); } } ``` And texture class: ``` #include \"Texture.hpp\" Texture::Texture(int width, int height) : m_width(width), m_height(height), m_id(0) { } Texture::~Texture() { glDeleteTextures(1, &m_id); } void Texture::create() { glCreateTextures(GL_TEXTURE_2D, 1, &m_id); glTextureParameteri(m_id, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTextureParameteri(m_id, GL_TEXTURE_MAG_FILTER, GL_NEAREST); glTextureParameteri(m_id, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTextureParameteri(m_id, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glTextureStorage2D(m_id, 1, GL_RGBA32F, m_width, m_height); } void Texture::bind(GLenum texture_unit) const { glBindTextureUnit(texture_unit, m_id); } ``` Here is what it looks like I tried exporting the texture using ``` stb_iamge ``` to see if it isnt blank, and it wasnt. The texture does exists in the memmory. Tried switching from ImGui docking to ImGui master and that didnt help",
    "author_id":4748,
    "publication_date":1754152224000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"eduard",
    "author_reputation":1.0,
    "tags":"c++, opengl, glfw, imgui",
    "text_length":5108,
    "title_length":42,
    "num_tags":4
  },
  {
    "id":6509,
    "title":"Flutter app crash using camera with ImagePicker",
    "link":"https:\/\/stackoverflow.com\/questions\/79723428\/flutter-app-crash-using-camera-with-imagepicker",
    "text":"I have a main_screen with action button to open a camera. The code is below doing this ``` Future<void> _openCamera() async { final picker = ImagePicker(); try { final XFile? photo = await picker.pickImage(source: ImageSource.camera); if (photo != null) { setState(() { _mediaFiles.insert(0, photo.path); }); } } catch (e) { debugPrint('Error opening camera: $e'); if (mounted) { ScaffoldMessenger.of( context, ).showSnackBar(SnackBar(content: Text('Error opening camera: $e'))); } } } ``` After the camera got opened, the app is crashing with message \"Lost connection to device\". Using image_picker example , implemented this ``` @override void didChangeAppLifecycleState(AppLifecycleState state) { \/\/ When the app returns from the background (e.g., after using the camera), \/\/ check if the OS has killed our app to reclaim memory. debugPrint('App lifecycle changed: $state'); if (state == AppLifecycleState.resumed) { try { _retrieveLostData(); } catch (e) { debugPrint('Error retrieving lost data: $e'); } } } Future<void> _retrieveLostData() async { final LostDataResponse response = await ImagePicker().retrieveLostData(); if (response.isEmpty) { return; } final XFile? file = response.file; if (file != null) { final String messageType = response.type == RetrieveType.video ? 'video' : 'photo'; debugPrint('Retrieved lost $messageType: ${file.path}'); if (mounted) { setState(() { \/\/ Ensure we don't add a duplicate if the file was already handled. if (!_mediaFiles.contains(file.path)) { _mediaFiles.insert(0, file.path); } }); } } else if (response.exception != null) { debugPrint('Error retrieving lost data: ${response.exception}'); } } ``` but it did not help. This is with the device moto-g 2023 with latest image_picker 1.1.2",
    "author_id":5969,
    "publication_date":1754152568000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Senthil",
    "author_reputation":391.0,
    "tags":"flutter, dart, flutter-image-picker",
    "text_length":1738,
    "title_length":47,
    "num_tags":3
  },
  {
    "id":6508,
    "title":"Visual Studio C++, WinForms, Add external class to project",
    "link":"https:\/\/stackoverflow.com\/questions\/79723430\/visual-studio-c-winforms-add-external-class-to-project",
    "text":"I have started a project with Visual Studio. I am creating a GUI with WinForms. My goal is to click the button and start an external class\/function in C++\/CLI. Currently the GUI is locking like this: The Code in Forms.h is: ``` private: System::Void button1_Click(System::Object^ sender, System::EventArgs^ e) { this->button1->Text = L\"Start Function\"; MyFunction Func; Func.start(); } ``` I have added myfunction.h to the project. It is currently locking like this: ``` class MyFunction { public: bool start() { any code... MessageBox(0, L\"Button clicked!\", L\"Button clicked\", MB_OK | MB_ICONEXCLAMATION); } }; ``` How can I get the MyFunction running after I clicked the button ?",
    "author_id":5307,
    "publication_date":1754152804000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Alex",
    "author_reputation":1.0,
    "tags":"visual-studio, winforms, visual-c++, c++-cli",
    "text_length":681,
    "title_length":58,
    "num_tags":4
  },
  {
    "id":6507,
    "title":"React Native (Expo) KeyboardAvoidingView creates permanent space at the bottom after keyboard hides (Android)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723431\/react-native-expo-keyboardavoidingview-creates-permanent-space-at-the-bottom-a",
    "text":"I’m building a login screen in Expo (React Native) that includes a few input fields and a button at the bottom. Everything looks perfect before interacting with the keyboard, but after: Tapping into a TextInput (keyboard opens) Dismissing the keyboard (using back button or tapping outside) The layout stays shifted, and a blank white space remains at the bottom, pushing my button higher than before. It doesn’t reset. Screenshots: Video demo of the issue: https:\/\/jumpshare.com\/share\/TtEDn6ucbFqSE6nsQfPf When I switch to KeyboardAwareScrollView (from react-native-keyboard-aware-scroll-view), this issue is completely gone and layout works as expected before and after keyboard usage. But Expo doctor shows: Untested on New Architecture: react-native-keyboard-aware-scroll-view Unmaintained: react-native-keyboard-aware-scroll-view So I don’t want to rely on it long-term. My Environment: Expo SDK: ~53.0.20 React Native: 0.79.5 Platform: Android this is the code: ``` return ( <KeyboardAvoidingView behavior={Platform.OS === \"ios\" ? \"padding\" : \"height\"} style={{ flex: 1 }} > <ScrollView contentContainerStyle={{ flexGrow: 1 }} keyboardShouldPersistTaps=\"handled\" > <View className=\"w-full relative\" style={{ height: Dimensions.get(\"screen\").height \/ 2.5 }} > <ImageBackground source={images.loginGraphic} resizeMode=\"cover\" className=\"size-full overflow-hidden\" imageStyle={{ borderBottomLeftRadius: 24, borderBottomRightRadius: 24, }} \/> <\/View> <SafeAreaView className=\"flex-1 bg-white px-5\"> <Slot \/> <\/SafeAreaView> <\/ScrollView> <\/KeyboardAvoidingView> ); ``` How can I prevent the layout from staying shifted with a blank space after dismissing the keyboard on Android? Thanks 🙏",
    "author_id":5968,
    "publication_date":1754152854000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Michael ilkanayev",
    "author_reputation":116.0,
    "tags":"android, react-native, expo, layout, keyboardavoidingview",
    "text_length":1690,
    "title_length":109,
    "num_tags":5
  },
  {
    "id":6506,
    "title":"How to generate an access token for Cognito user without having the user&#39;s credentials",
    "link":"https:\/\/stackoverflow.com\/questions\/79723444\/how-to-generate-an-access-token-for-cognito-user-without-having-the-users-crede",
    "text":"I want to write a report against a list of AWS Cognito users. Generating the report requires calling non-AWS APIs that require a Cognito access token for the particular user. How can I generate such access tokens without having the user's credentials?",
    "author_id":5967,
    "publication_date":1754154102000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Johnny Petro",
    "author_reputation":149.0,
    "tags":"amazon-cognito, amazon-web-services",
    "text_length":251,
    "title_length":90,
    "num_tags":2
  },
  {
    "id":6505,
    "title":"How to add automatic attributes to the Python Markdown library",
    "link":"https:\/\/stackoverflow.com\/questions\/79723447\/how-to-add-automatic-attributes-to-the-python-markdown-library",
    "text":"I'm trying to import CSS styles into my HTML file exported by the Markdown library, but for some reason the attributes are basic, or so to speak, to generate a clean layout. Is it possible to add these attributes to the entire layout? ``` from markdown import markdown text = ''' #Example ## Emphasis **This is bold text** __This is bold text__ ... html = markdown(text,extensions=['fenced_code','codehilite']) css = '\"css\/github-markdown.css\"' doc = f'<html><head><link rel=\"stylesheet\" type=\"text\/css\" href={css}><\/head><body>{html}<\/body><\/html>' print(doc) ``` ``` <html><head><link rel=\"stylesheet\" type=\"text\/css\" href=\"css\/github-markdown.css\"><\/head> <body><h1>Example<\/h1> <h2>Emphasis<\/h2> <p><strong>This is bold text<\/strong><\/p> <p><strong>This is bold text<\/strong><\/p> <p><em>This is italic text<\/em><\/p> <p><em>This is italic text<\/em><\/p> <p>~~Strikethrough~~<\/p> <h2>Blockquotes<\/h2> <blockquote> <p>Blockquotes can also be nested...<\/p> <blockquote> <p>...by using additional greater-than signs right next to each other...<\/p> <blockquote> <p>...or with spaces between arrows.<\/p> <\/blockquote> <\/blockquote> <\/blockquote> <h2>Lists<\/h2> <p>Unordered<\/p> <ul> <li>Create a list by starting a line with <code>+<\/code>, <code>-<\/code>, or <code>*<\/code><\/li> <li>Sub-lists are made by indenting 2 spaces:<\/li> <li>Marker character change forces new list start:<ul> <li>Ac tristique libero volutpat at<\/li> <li>Facilisis in pretium nisl aliquet<\/li> <li>Nulla volutpat aliquam velit<\/li> <\/ul> <\/li> <li>Very easy!<\/li> <\/ul> <p>Ordered<\/p> <ol> <li>Lorem ipsum dolor sit amet<\/li> <li>Consectetur adipiscing elit<\/li> <li>Integer molestie lorem at massa<\/li> <\/ol><\/body><\/html> ``` By default, styles have attributes like ``` .markdown-body ``` and others. I expect a result like the one shown below: ``` <body class=\"markdown-body\"><h1>Example<\/h1> ```",
    "author_id":5966,
    "publication_date":1754154440000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"royer",
    "author_reputation":645.0,
    "tags":"python, markdown",
    "text_length":1871,
    "title_length":62,
    "num_tags":2
  },
  {
    "id":6504,
    "title":"Power Automate endlessly running",
    "link":"https:\/\/stackoverflow.com\/questions\/79723450\/power-automate-endlessly-running",
    "text":"I have created a flow which is the following: ``` Trigger when a form is submitted Scope - Copy Excel and reset Get file content of excel document that user fills out Compose New file name Create new folder Create new excel file of file that user fills out within new folder Delay 5 seconds Delete original file that user fills out Create copy of template excel document and puts it in original place for user Scope Excel List rows present in table - this is looking at new copied file Filter array with the filter of @and(not(empty(trim(item()?['Code']))), not(empty(trim(item()?['REF Number'])))) Select action of body of filter array, map trim(item()?['Code']) Compose UniqueExcelCodes with inputs of union(body('Select_Excel_Code_Strings'), body('Select_Excel_Code_Strings')) Compose CountExcelCodes with inputs of length(body('Filter_array_Excel')) Scope Sharepoint Get Items - gets items in a sharepoint list - in this Sharepoint list there is a code (title column), Key, Start Date, End Date Filter array from the value of get items, map of @equals(item()?['Key'], 'CD') Select from value of Get items, map is trim(item()?['Title']) Compose SPCodeCount with inputs length(outputs('Get_items')?['body\/value']) Initialise Variable Name = RowIDs Type = Array Value [1,2,3,4, ... , 100] - there are 100 rows in the excel document Compose MatchExists with the inputs of greater(length(intersection(body('Select_SP_Code_Strings'), union(body('Select_Excel_Code_Strings'), body('Select_Excel_Code_Strings')))), 0) Apply to each with the outputs of Compose UniqueExcelCodes Condition - output of Select from value of Get items contains Apply to each No = send email as there is no match of codes Yes = Apply to each 2 of Filter array from the value of get items Condition - items('Apply_to_each_2')?['Key'] equals CD, AND, formatDateTime(utcNow(), 'yyyy-MM-dd') > formatDateTime(items('Apply_to_each_2')?['EndDate'], 'yyyy-MM-dd') Yes = send email as the date in the sharepoint list is in the past No = Apply to each 3 with the outputs from List rows present in table (from scope Excel) Scope - nothing inside. Underneath this scope is 2 actions running parallel Filter array DisneyYES from value of List rows present in table, toLower(trim(items('Apply_to_each_3')?['Disney?'])) = yes Condition - length(body('Filter_array_DisneyYES')) > 0 Yes = Apply to each 4 of body of filter array DisneyYes Scope Task Creation - DisneyYES Create new folder 1 Create sharing link for folder 1 Create new folder 2 Create sharing link for folder 2 Create new folder 3 Create sharing link for folder 3 Get file content (finds blank powerpoint file on sharepoint) Create file (based on this powerpoint file) Create sharing link for new powerpoint file Get items CD Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'CD' Get items DSO Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'DSO' Get items AW Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'AW' Get items PR1 Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'PR1' Get items REP Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'REP' Get items PR2 Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'PR2' Get items FCR Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'FCR' Get items VR Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'VR' Compose CD Date with inputs of first(body('Get_items_-_CD_Date')?['value'])?['EndDate'] Compose DSO Date with inputs of first(body('Get_items_-_DSO_Date')?['value'])?['EndDate'] Compose AW Date with inputs of first(body('Get_items_-_AW_Date')?['value'])?['EndDate'] Compose PR1 Date with inputs of first(body('Get_items_-_PR1_Date')?['value'])?['EndDate'] Compose REP Date with inputs of first(body('Get_items_-_REP_Date')?['value'])?['EndDate'] Compose PR2 Date with inputs of first(body('Get_items_-_PR2_Date')?['value'])?['EndDate'] Compose FCR Date with inputs of first(body('Get_items_-_FCR_Date')?['value'])?['EndDate'] Compose VR Date with inputs of first(body('Get_items_-_VR_Date')?['value'])?['EndDate'] Compose CD Date with inputs of first(body('Get_items_-_CD_Date')?['value'])?['EndDate'] Compose CD Date for Taskcard with the inputs of formatDateTime(first(body('Get_items_-_CD_Date')?['value'])?['EndDate'], 'yyyy-MM-dd') Create planner task Create item in sharepoint list Filter array DisneyNO from value of List rows present in table, toLower(trim(items('Apply_to_each_3')?['Disney?'])) = no Condition - length(body('Filter_array_DisneyNO')) > 0 Yes = Apply to each 4 of body of filter array DisneyNO Scope Task Creation - DisneyNo Create new folder 1 Create sharing link for folder 1 Create new folder 2 Create sharing link for folder 2 Get file content (finds blank powerpoint file on sharepoint) Create file (based on this powerpoint file) Create sharing link for new powerpoint file Get items CD Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'CD' Get items DSO Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'DSO' Get items AW Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'AW' Get items PR1 Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'PR1' Get items REP Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'REP' Get items PR2 Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'PR2' Get items FCR Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'FCR' Get items VR Date - gets the same items referenced earlier but with a filter query of Title eq '@{items('Apply_to_each_4')?['Code']}' and Key eq 'VR' Compose CD Date with inputs of first(body('Get_items_-_CD_Date')?['value'])?['EndDate'] Compose DSO Date with inputs of first(body('Get_items_-_DSO_Date')?['value'])?['EndDate'] Compose AW Date with inputs of first(body('Get_items_-_AW_Date')?['value'])?['EndDate'] Compose PR1 Date with inputs of first(body('Get_items_-_PR1_Date')?['value'])?['EndDate'] Compose REP Date with inputs of first(body('Get_items_-_REP_Date')?['value'])?['EndDate'] Compose PR2 Date with inputs of first(body('Get_items_-_PR2_Date')?['value'])?['EndDate'] Compose FCR Date with inputs of first(body('Get_items_-_FCR_Date')?['value'])?['EndDate'] Compose VR Date with inputs of first(body('Get_items_-_VR_Date')?['value'])?['EndDate'] Compose CD Date with inputs of first(body('Get_items_-_CD_Date')?['value'])?['EndDate'] Compose CD Date for Taskcard with the inputs of formatDateTime(first(body('Get_items_-_CD_Date')?['value'])?['EndDate'], 'yyyy-MM-dd') Create planner task Create item in sharepoint list ``` The flow does everything I want it to do - it runs quickly, it creates task cards correctly and creates all the folders and entries in the Sharepoint Lists. However it hangs and endlessly runs - can someone help me so it doesn't? I've tried making new flows based on this, however when I do, they do not work as quickly or they fail.",
    "author_id":5965,
    "publication_date":1754154450000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mark Summerton",
    "author_reputation":1.0,
    "tags":"excel, sharepoint, power-automate, power-automate-desktop, microsoft-graph-plannertasks",
    "text_length":7985,
    "title_length":32,
    "num_tags":5
  },
  {
    "id":6503,
    "title":"Issue with code-first Entity Framework Core and Supabase",
    "link":"https:\/\/stackoverflow.com\/questions\/79723449\/issue-with-code-first-entity-framework-core-and-supabase",
    "text":"I am trying to create a code-first database context layer using Entity Framework Core for my Supabase database. I am getting an output regarding an issue with resolving the database migration I try to execute. I tried to add the proper connection string given by Supabase for a .NET application which gave me a ``` DbContext ``` output like this: ``` $ dotnet ef dbcontext info Build started… Build succeeded. Type: MyApp.DataAccess.ApplicationDbContext Provider name: Npgsql.EntityFrameworkCore.PostgreSQL Database name: postgres Data source: tcp:\/\/db.[DBHOSTNAME].supabase.co:5432 Options: None [user]@[COMPUTERNAME] MINGW64 ~\/MyApp\/MyApp.DataAccess (feature\/DataLayerSetup) ``` I tried to run the migration I added and got the following output: ``` $ dotnet ef database update Build started… Build succeeded. An error occurred using the connection to database ‘postgres’ on server ‘tcp:\/\/db.[DBHOSTNAME].supabase.co:5432’. System.Net.Sockets.SocketException (0x00002AF9): No such host is known. at System.Net.Dns.GetHostEntryOrAddressesCore(String hostName, Boolean justAddresses, AddressFamily addressFamily, Nullable`1 activityOrDefault) at System.Net.Dns.GetHostAddresses(String hostNameOrAddress, AddressFamily family) at Npgsql.Internal.NpgsqlConnector.Connect(NpgsqlTimeout timeout) at Npgsql.Internal.NpgsqlConnector.RawOpen(SslMode sslMode, NpgsqlTimeout timeout, Boolean async, CancellationToken cancellationToken) at Npgsql.Internal.NpgsqlConnector.<Open>g__OpenCore|214_1(NpgsqlConnector conn, SslMode sslMode, NpgsqlTimeout timeout, Boolean async, CancellationToken cancellationToken) at Npgsql.Internal.NpgsqlConnector.Open(NpgsqlTimeout timeout, Boolean async, CancellationToken cancellationToken) at Npgsql.PoolingDataSource.OpenNewConnector(NpgsqlConnection conn, NpgsqlTimeout timeout, Boolean async, CancellationToken cancellationToken) at Npgsql.PoolingDataSource.<Get>g__RentAsync|33_0(NpgsqlConnection conn, NpgsqlTimeout timeout, Boolean async, CancellationToken cancellationToken) at Npgsql.NpgsqlConnection.<Open>g__OpenAsync|42_0(Boolean async, CancellationToken cancellationToken) at Npgsql.NpgsqlConnection.Open() at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenDbConnection(Boolean errorsExpected) at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.OpenInternal(Boolean errorsExpected) at Microsoft.EntityFrameworkCore.Storage.RelationalConnection.Open(Boolean errorsExpected) at Microsoft.EntityFrameworkCore.Storage.RelationalCommand.ExecuteReader(RelationalCommandParameterObject parameterObject) at Microsoft.EntityFrameworkCore.Migrations.HistoryRepository.GetAppliedMigrations() at Npgsql.EntityFrameworkCore.PostgreSQL.Migrations.Internal.NpgsqlHistoryRepository.GetAppliedMigrations() at Npgsql.EntityFrameworkCore.PostgreSQL.Migrations.Internal.NpgsqlMigrator.Migrate(String targetMigration) at Microsoft.EntityFrameworkCore.Design.Internal.MigrationsOperations.UpdateDatabase(String targetMigration, String connectionString, String contextType) at Microsoft.EntityFrameworkCore.Design.OperationExecutor.UpdateDatabaseImpl(String targetMigration, String connectionString, String contextType) at Microsoft.EntityFrameworkCore.Design.OperationExecutor.UpdateDatabase.<>c__DisplayClass0_0.<.ctor>b__0() at Microsoft.EntityFrameworkCore.Design.OperationExecutor.OperationBase.Execute(Action action) No such host is known. ``` I expected the code-first migration to properly create tables with the entities I created in my Entity Framework Core class library.",
    "author_id":5964,
    "publication_date":1754154450000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Zachary Handel",
    "author_reputation":1.0,
    "tags":"c#, entity-framework-core, supabase, ef-code-first",
    "text_length":3523,
    "title_length":56,
    "num_tags":4
  },
  {
    "id":6502,
    "title":"Is there a way to make the reactjs block&#39;s innerblocks.content available in the wp gutenberg editor?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723448\/is-there-a-way-to-make-the-reactjs-blocks-innerblocks-content-available-in-the",
    "text":"Our current issue is that the content generated on the save (the content we use on the front end) isn't available in the editor. Since the structure and HTML are completely different, we can't apply custom scripting (in this case, swiper initialization) to it. Especially not with nested inner blocks, which are fully manageable via Gutenberg, as is the case here. I have a content slider block and it contains in the edit.js ``` <InnerBlocks template={[['skirt\/content-slide', {}]]} allowedBlocks={['skirt\/content-slide']} \/> ``` so that is also its own block and it contains this in the edit ``` const innerBlockTemplate = [ [ 'core\/heading', { placeholder: 'Title text' } ], [ 'core\/paragraph', { placeholder: 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.' } ], ]; <InnerBlocks template={innerBlockTemplate} \/> ``` The frontend of the content slider block uses a swiper. But I was wondering if you can also display the appearance shown on the frontend within the editor (when the block isn't selected). We tried some things with the isSelected prop but couldn't get it to work how to make the Content available in the edit",
    "author_id":5963,
    "publication_date":1754154450000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"C.Bighouse",
    "author_reputation":1.0,
    "tags":"reactjs, wordpress, wordpress-gutenberg, gutenberg-blocks",
    "text_length":1137,
    "title_length":104,
    "num_tags":4
  },
  {
    "id":6501,
    "title":"How to implement permission prerequisites in a role-based access control system?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723451\/how-to-implement-permission-prerequisites-in-a-role-based-access-control-system",
    "text":"I'm implementing a role-based access control (RBAC) system and want to model permission prerequisites ( certain permissions require other permissions to function correctly ) in my database. For example: If a role has the ``` DELETE_USER ``` permission, it must also have ``` READ_USER ``` , because deleting a user implies the need to read user data first. This should be enforced: ✅ When assigning permissions to a role (backend and UI) ✅ When checking permissions at runtime (e.g. in API authorization logic) Here's my schema so far: ``` -- Permissions table CREATE TABLE permissions ( id SERIAL PRIMARY KEY, name VARCHAR(100) UNIQUE NOT NULL, created_at TIMESTAMPTZ DEFAULT NOW() ); -- Roles table CREATE TABLE roles ( id SERIAL PRIMARY KEY, name VARCHAR(100) UNIQUE NOT NULL, created_at TIMESTAMPTZ DEFAULT NOW() ); -- Role-Permission mapping table CREATE TABLE role_permissions ( role_id INT NOT NULL REFERENCES roles(id) ON DELETE CASCADE, permission_id INT NOT NULL REFERENCES permissions(id) ON DELETE CASCADE, PRIMARY KEY (role_id, permission_id) ); -- Permission Prerequisites table -- e.g., DELETE_USER requires READ_USER CREATE TABLE permission_prerequisites ( permission_id INT NOT NULL REFERENCES permissions(id) ON DELETE CASCADE, required_permission_id INT NOT NULL REFERENCES permissions(id) ON DELETE CASCADE, PRIMARY KEY (permission_id, required_permission_id), CHECK (permission_id <> required_permission_id) -- Avoid self-dependency ); ``` Is this a good way to model permission prerequisites, and what’s the best approach to enforce them—auto-adding or erroring—using SQL recursion or backend logic, with validation also in the frontend? Any best practices, feedback, or implementation advice would be greatly appreciated!",
    "author_id":5962,
    "publication_date":1754154483000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"bcExpt1123",
    "author_reputation":644.0,
    "tags":"postgresql, user-permissions, role-based-access-control",
    "text_length":1744,
    "title_length":80,
    "num_tags":3
  },
  {
    "id":6500,
    "title":"Is this Sumtracker API request correct?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723457\/is-this-sumtracker-api-request-correct",
    "text":"I wish to use the Sumtracker API, and I want to connect and do a basic get for this endpoint . Let's say that my API key is ``` 1234567890abc ``` . The code is here: ``` import requests url = \"https:\/\/inventory-api.sumtracker.com\/api\/version\/2025-03\/products\/\" headers = { \"accept\": \"application\/json\", \"Authorization\": \"1234567890abc\" } response = requests.get(url, headers=headers) ``` The response gives a 403 and the text is: ``` {\"type\":\"client_error\",\"errors\":[{\"code\":\"permission_denied\",\"detail\":\"You do not have permission to perform this action.\",\"attr\":null}]} ``` This suggests to me that the API key is not working correctly. This is the exact example that they give on their website so I have to presume it is correct. That said, on their authentication section , it says this: ``` Authorization: Api-Key <api-key-value> ``` Lets say the API key is dv7dm.asm1hga2seks4uybay22hyuar Then, the value of the header will be ``` Api-Key dv7dm.asm1hga2seks4uybay22hyuar ``` Does this mean that the API key should also include that text? I have tried using this line: ``` \"Authorization\": \"Api-Key 1234567890abc\" ``` but it didn't seem to make a difference. Is it typical that the authorization will contain more than just the key itself?",
    "author_id":5961,
    "publication_date":1754155052000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"TheFaithfulLearner",
    "author_reputation":731.0,
    "tags":"python, rest, python-requests",
    "text_length":1244,
    "title_length":39,
    "num_tags":3
  },
  {
    "id":6499,
    "title":"MathJax \\color declaration spills over and colors subsequent text",
    "link":"https:\/\/stackoverflow.com\/questions\/79723459\/mathjax-color-declaration-spills-over-and-colors-subsequent-text",
    "text":"I tried ``` \\color ``` and ``` \\textcolor ``` in a markdown file and then in an html file. Only the latter scopes the color to its argument. With ``` \\color ``` , the rest of the expression stays red. ``` <script src=\"https:\/\/cdn.jsdelivr.net\/npm\/mathjax@3\/es5\/tex-mml-chtml.min.js\"><\/script> $$\\color{red}{red} black$$ $$\\textcolor{red}{red} black$$ ``` What's wrong?",
    "author_id":5960,
    "publication_date":1754155203000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Joy",
    "author_reputation":93.0,
    "tags":"markdown, mathjax",
    "text_length":368,
    "title_length":65,
    "num_tags":2
  },
  {
    "id":6498,
    "title":"find file and copy\/rsync them with there folder, make problem",
    "link":"https:\/\/stackoverflow.com\/questions\/79723462\/find-file-and-copy-rsync-them-with-there-folder-make-problem",
    "text":"with find \/mnt\/NAS\/FS-14-share-musik\/ -iname \"*.flac\" -mtime -14 -print i will geht a output like \/mnt\/NAS\/FS-14-share-musik\/Musik\/J \/mnt\/NAS\/FS-14-share-musik\/Musik\/J\/Jack_Johnson \/mnt\/NAS\/FS-14-share-musik\/Musik\/J\/Jack_Johnson\/In_Between_Dreams \/mnt\/NAS\/FS-14-share-musik\/Musik\/J\/Jack_Johnson\/In_Between_Dreams\/12.Belle.flac \/mnt\/NAS\/FS-14-share-musik\/Musik\/J\/Jack_Johnson\/In_Between_Dreams\/14.Constellations.flac \/mnt\/NAS\/FS-14-share-musik\/Musik\/J\/Jack_Johnson\/In_Between_Dreams\/09.Crying_Shame.flac \/mnt\/NAS\/FS-14-share-musik\/Musik\/J\/Jack_Johnson\/In_Between_Dreams\/01.Better_Together.flac the second part of the command would be to rsync this to a new destination. the Problem with the output like this, it will just start copy\/rsync all of the J, also if it is older. wish would be to just copy\/rsync the first folder and the flac file like this In_Between_Dreams In_Between_Dreams\/12.Belle.flac In_Between_Dreams\/14.Constellations.flac In_Between_Dreams\/09.Crying_Shame.flac In_Between_Dreams\/01.Better_Together.flac",
    "author_id":5959,
    "publication_date":1754155456000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"syswombat",
    "author_reputation":31.0,
    "tags":"linux, find",
    "text_length":1022,
    "title_length":61,
    "num_tags":2
  },
  {
    "id":6497,
    "title":"Click on Element not working in the Chrome browser on mobile phone after zoom in and zoom out the window",
    "link":"https:\/\/stackoverflow.com\/questions\/79723465\/click-on-element-not-working-in-the-chrome-browser-on-mobile-phone-after-zoom-in",
    "text":"After zoom in & zoom out the window, i try to click the option and button element not work any more: When i test on chrome browser with android mobile, before zoom in & zoom out the window click the element it's work, after zoom in and zoom out the window, i click the option and button element can't work, but it work well on others browser. I don't know what happened? I find similar problem solution but it not i want, i find similar topics, solution just set \"user-scalable=no\",but it not user-frindly. I need a good soulotion,both keep zoom function and click on the element work well .So, I need to set \"user-scalable=yes\". The code below: html ``` <!DOCTYPE html> <html lang=\"zh-Hant\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=yes\"> <title>匿名填報表單<\/title> <script> if (window.top !== window.self) { window.top.location = window.location.href; } <\/script> <style> html, body { margin: 0; font-family: \"Microsoft JhengHei\", sans-serif; background-color: #f0f0f0; padding: 20px; box-sizing: border-box; max-width: 100%; overflow-x: hidden; touch-action: auto; } *, *::before, *::after { box-sizing: inherit; } h2 { text-align: center; color: #444; } .form-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 10px; margin-bottom: 20px; } .form-item { background: #fff; padding: 10px; border: 1px solid #ccc; border-radius: 5px; min-width: 0; } .label { font-weight: bold; margin-bottom: 5px; display: block; } select, input[type=\"text\"], input[type=\"date\"], input[type=\"file\"] { width: 100%; padding: 8px; border: 1px solid #ccc; border-radius: 5px; } .submit-fixed { display: flex; justify-content: center; margin-top: 20px; gap: 20px; flex-wrap: wrap; } .submit-fixed button { padding: 16px 40px; font-size: 20px; background-color: #673ab7; color: white; border: none; border-radius: 10px; transition: background-color 0.3s ease; cursor: pointer; } .submit-fixed button:hover { background-color: #512da8; } <\/style> <\/head> <body> <h2>匿名填報表單<\/h2> <form id=\"form\"> <div id=\"form-rows\"><\/div> <div class=\"submit-fixed\"> <button type=\"button\" onclick=\"addRow()\">新增一列<\/button> <button type=\"submit\">送出<\/button> <\/div> <\/form> <script> const categories = [\"拆裝T\", \"拆裝M\", \"巡檢\", \"保養\", \"異常排除\"]; let rowCount = 0; function createSelect(name, options, selectedValue = '') { const select = document.createElement('select'); select.name = name; const defaultOption = document.createElement('option'); defaultOption.disabled = true; defaultOption.selected = !selectedValue; defaultOption.textContent = '請選擇'; select.appendChild(defaultOption); options.forEach(option => { const opt = document.createElement('option'); opt.value = option; opt.textContent = option; if (option === selectedValue) opt.selected = true; select.appendChild(opt); }); return select; } function addRow(data = {}, uploadedFileName = '') { const container = document.createElement('div'); container.className = 'form-grid'; container.innerHTML = ` <div class=\"form-item\"> <span class=\"label\">作業類別<\/span> <\/div> <div class=\"form-item\"> <span class=\"label\">日期<\/span> <input name=\"date_${rowCount}\" type=\"date\" value=\"${data.date || ''}\" \/> <\/div> <div class=\"form-item\"> <span class=\"label\">區域<\/span> <input name=\"area_${rowCount}\" type=\"text\" inputmode=\"none\" value=\"${data.area || ''}\" \/> <\/div> <div class=\"form-item\"> <span class=\"label\">廠別<\/span> <input name=\"site_${rowCount}\" type=\"text\" inputmode=\"none\" value=\"${data.site || ''}\" \/> <\/div> <div class=\"form-item\"> <span class=\"label\">設備名稱<\/span> <input name=\"equipment_${rowCount}\" type=\"text\" inputmode=\"none\" value=\"${data.equipment || ''}\" \/> <\/div> <div class=\"form-item\"> <span class=\"label\">備註<\/span> <input name=\"note_${rowCount}\" type=\"text\" inputmode=\"none\" value=\"${data.note || ''}\" \/> <\/div> <div class=\"form-item\"> <span class=\"label\">擋當<\/span> <input name=\"hold_${rowCount}\" type=\"text\" inputmode=\"none\" value=\"${data.hold || ''}\" \/> <\/div> <div class=\"form-item\"> <span class=\"label\">上傳檔案<\/span> <input name=\"file_${rowCount}\" type=\"file\" onchange=\"handleFileChange(this, ${rowCount})\" \/> <div id=\"file-label-${rowCount}\" style=\"font-size: 12px; color: green;\">${uploadedFileName ? `檔案已上傳：${uploadedFileName}` : ''}<\/div> <\/div> `; const selectContainer = container.querySelector('.form-item'); selectContainer.appendChild(createSelect(`category_${rowCount}`, categories, data.category || '')); document.getElementById('form-rows').appendChild(container); rowCount++; } function handleFileChange(input, rowIdx) { if (input.files.length > 0) { uploadFile(input.files[0], rowIdx); } } \/\/ 初始化一列 addRow(); <\/script> <\/body> <\/html> ``` gs ``` function doGet() { return HtmlService.createHtmlOutputFromFile('index') .setTitle(\"123\") .setXFrameOptionsMode(HtmlService.XFrameOptionsMode.ALLOWALL).addMetaTag('viewport', 'width=device-width, initial-scale=1'); } ```",
    "author_id":5958,
    "publication_date":1754155558000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"You-Xiang Liao",
    "author_reputation":1.0,
    "tags":"google-chrome, mobile, google-apps-script, web-applications",
    "text_length":4900,
    "title_length":104,
    "num_tags":4
  },
  {
    "id":6496,
    "title":"Detecting Widget additon on Home screen in UIKit app",
    "link":"https:\/\/stackoverflow.com\/questions\/79723467\/detecting-widget-additon-on-home-screen-in-uikit-app",
    "text":"Is there a way to check from the UIKit app if the user has added the widget of my app to their home screen",
    "author_id":5957,
    "publication_date":1754155909000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Raaj Raxa",
    "author_reputation":67.0,
    "tags":"ios, uikit, widget",
    "text_length":106,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":6495,
    "title":"Selenium Headless vrs Non Headless. I can&#39;t get Selenium to set a field while using headless mode",
    "link":"https:\/\/stackoverflow.com\/questions\/79723468\/selenium-headless-vrs-non-headless-i-cant-get-selenium-to-set-a-field-while-us",
    "text":"I saw I needed to add a window size however that did not fix my issue here is the working Non headless code ``` # _-_-_- Non Headless -_-_-_ from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import webbrowser def GetFAAapdURL(APD): driver = webdriver.Chrome() driver.get(\"https:\/\/www.faa.gov\/airports\/runway_safety\/diagrams\/\") input_field = driver.find_element(By.ID, \"ident\") # Replace with actual ID input_field.send_keys(APD) input_field.send_keys(Keys.RETURN) table = driver.find_element(By.XPATH, \"\/\/table\") # Adjust XPath as needed links = table.find_elements(By.TAG_NAME, \"a\") for link in links: href = link.get_attribute(\"href\") if href: # Ensure the href is not None if \"aeronav.faa\" in href: print(href) return href webbrowser.open(GetFAAapdURL(\"ATL\")) ``` Abd now the Headless version ``` # _-_-_- Headless -_-_-_ from selenium.webdriver.chrome.service import Service from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.chrome.options import Options from selenium import webdriver import webbrowser def GetFAAapdURL_Headless(APD): chrome_options = Options() chrome_options.add_argument(\"--headless\") # Run Chrome in headless mode chrome_options.add_argument('--window-size=1920x1080') driver = webdriver.Chrome(options=chrome_options) driver.get(\"https:\/\/www.faa.gov\/airports\/runway_safety\/diagrams\/\") input_field = driver.find_element(By.ID, \"ident\") input_field.send_keys(Keys.RETURN) table = driver.find_element(By.XPATH, \"\/\/table\") # Adjust XPath as needed links = table.find_elements(By.TAG_NAME, \"a\") for link in links: href = link.get_attribute(\"href\") if href: # Ensure the href is not None if \"aeronav.faa\" in href: print(href) return href webbrowser.open(GetFAAapdURL_Headless(\"ATL\")) ``` here is the line that errors ``` input_field = driver.find_element(By.ID, \"ident\") ``` I tried to minimize the window and that has a flashing of the window at start and still errors. So not a great solution.",
    "author_id":5956,
    "publication_date":1754155918000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"RJTTAZ",
    "author_reputation":75.0,
    "tags":"selenium-webdriver, python, headless-browser",
    "text_length":2054,
    "title_length":101,
    "num_tags":3
  },
  {
    "id":6494,
    "title":"Rselenium won&#39;t switch to another tab",
    "link":"https:\/\/stackoverflow.com\/questions\/79723469\/rselenium-wont-switch-to-another-tab",
    "text":"Using RSelenium, I'm trying to open a new tab and scroll down. Unfortunately, I can't make this work because my browser stays at the first tab. Does anyone know how to solve this? This is a simplied version of the code I am using: ``` library(RSelenium) library(netstat) # setup connection rs_driver_object <- rsDriver(browser = \"chrome\", chromever = \"138.0.7204.157\", verbose = T, port = free_port(), check = T, phantomver = NULL, extraCapabilities= list(\"-sessionTimeout 7200\")) remDr <- rs_driver_object$client # create two tabs remDr$navigate('https:\/\/stackoverflow.com\/') remDr$executeScript(\"window.open('https:\/\/stackoverflow.com\/questions', '_blank');\") # get the handles handles <- remDr$getWindowHandles() handles # switch to second tab and scroll down remDr$switchToWindow(handles[[2]]) remDr$executeScript(\"window.scrollTo(0, document.body.scrollHeight);\") # the above doesn't work because current tab is still the first remDr$getCurrentWindowHandle() ```",
    "author_id":5955,
    "publication_date":1754156008000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"dr_E",
    "author_reputation":35.0,
    "tags":"r, javascript, rselenium",
    "text_length":967,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":6493,
    "title":"How to print multiple ggplots per page with pdf device",
    "link":"https:\/\/stackoverflow.com\/questions\/79723471\/how-to-print-multiple-ggplots-per-page-with-pdf-device",
    "text":"I would like to print multiple ggplot graphs per page to a PDF device using base R. The following code works fine for plot() graphing but with ggplot() it ignores the par() function and only prints one graph per page. How can I use the base PDF device to print multiple ggplots per page with a 1\" margin on all sides? ``` x <- c(5,7,2,4,9) y <- c(2,10,15,8,14) df <- data.frame(x,y) pdf(\"test pdf.pdf\", width = 6.5, height = 9, paper=\"letter\") par(mfrow=c(3,1)) for(i in 1:10){ p<-ggplot(df, aes(x=x,y=y*i)) + geom_point() print(p) } dev.off() ``` Edit: I like the patchwork package suggestion. So far I have come up with this solution to handle a variable number of graphs, but I can't figure out how to make it handle numbers not divisible by 3. Any suggestions? Can we make it simpler? ``` x <- c(5,7,2,4,9) y <- c(2,10,15,8,14) df <- data.frame(x,y) myfunct <- function(i){ p<-ggplot(df, aes(x=x,y=y*i)) + geom_point() return(p) } myGrobs <- lapply(1:10,myfunct) library(patchwork) pdf(\"test pdf.pdf\", width = 6.5, height = 9, paper=\"letter\") for(i in c(1,4,7)){ print(myGrobs[[i]] \/ myGrobs[[i+1]] \/ myGrobs[[i+2]]) } dev.off() ```",
    "author_id":5954,
    "publication_date":1754156023000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"BenW",
    "author_reputation":123.0,
    "tags":"r, ggplot2, pdf, par",
    "text_length":1136,
    "title_length":54,
    "num_tags":4
  },
  {
    "id":6492,
    "title":"Animate camera to set positions",
    "link":"https:\/\/stackoverflow.com\/questions\/79723474\/animate-camera-to-set-positions",
    "text":"I’m trying to rotate\/position a camera to arbitrary views of a model using three.js. https:\/\/jsfiddle.net\/17gf8q6o\/ Problem: some of the animations go through the model (e.g. from ``` top ``` to ``` bottom ``` ), some are not smooth (e.g. from ``` top ``` to ``` start ``` ). It may be possible to attach the ``` PerspectiveCamera ``` to a parent ``` Object3D ``` \/ ``` Spherical ``` , but I'm not too sure how to implement it. Attempts: ``` Object3D ``` : https:\/\/jsfiddle.net\/upnyqcfx\/ (breaks the OrbitControls ) ``` Spherical ``` : https:\/\/jsfiddle.net\/upnyqcfx\/1\/ (how to add the camera to it?) Another way might be to define a curve for the camera's path ( example ). However, I'm not sure how to apply the example to my case, nor keep the camera looking at the model and not affect the orbit controls. Where am I going wrong? Thanks!",
    "author_id":5953,
    "publication_date":1754156352000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"sbgib",
    "author_reputation":5880.0,
    "tags":"javascript, three.js, animation, 3d",
    "text_length":840,
    "title_length":31,
    "num_tags":4
  },
  {
    "id":6491,
    "title":"Wakenet model is not initializing using esp-sr create function–it Returns Guru Meditation Error",
    "link":"https:\/\/stackoverflow.com\/questions\/79723475\/wakenet-model-is-not-initializing-using-esp-sr-create-function-it-returns-guru-m",
    "text":"I am using esp-idf v5.5 with esp-sr v2.1.4. below is my updated github repo with complete build available (complete!). I thank you greatly for your help. github_repo : https:\/\/github.com\/abdulbaseer-1\/ESP32_chatbot_hijason.git Below is the minimum code required to reproduce the error. The backtrace points the error out in ``` wn_handle = wakenet->create(wn_name, WAKE_MODE); ``` from ``` wakeword.c ``` main.c: ``` **#include <stdio.h> #include <inttypes.h> #include \"sdkconfig.h\" #include \"freertos\/FreeRTOS.h\" #include \"freertos\/task.h\" #include \"esp_chip_info.h\" #include \"esp_flash.h\" #include \"esp_system.h\" #include \"mic_i2s.h\" #include \"wakeword.h\" #include \"esp_log.h\" #define TAG \"MAIN\" static void wakeword_detected_callback() { ESP_LOGI(TAG, \"Wake word callback triggered\"); } void app_main(void) { ESP_LOGI(TAG, \"Starting application\"); \/\/ Initialize I2S microphone ESP_LOGI(TAG, \"Starting audio recording...\"); i2s_mic_init(); \/\/ Initialize wake-word detection wakeword_init_multinet(wakeword_detected_callback); \/\/, NULL \/\/ Init WakeNet }** ``` wakeword.c: ``` **#include \"esp_wn_models.h\" #include \"wakeword.h\" #include \"mic_i2s.h\" #include \"esp_log.h\" #include \"model_path.h\" #include \"esp_wn_iface.h\" #include \"freertos\/FreeRTOS.h\" #include \"freertos\/task.h\" static const char *TAG = \"WakeMultinet\"; #define SAMPLE_RATE 16000 #define BUFFER_SIZE 512 #define WAKE_MODE DET_MODE_95 #define CMD_TIMEOUT 40 \/\/ ~0.5s @ 16kHz static wakeword_callback_t ww_callback = NULL; static srmodel_list_t *sr_models = NULL; static const esp_wn_iface_t *wakenet = NULL; static model_iface_data_t *wn_handle = NULL; void wakeword_init_multinet(wakeword_callback_t wake_cb) { ww_callback = wake_cb; \/\/ Initialize model list from partition sr_models = esp_srmodel_init(\"model\"); if (!sr_models) { ESP_LOGE(TAG, \"Failed to init model list\"); return; } \/\/ Initialize WakeNet char *wn_name = esp_srmodel_filter(sr_models, ESP_WN_PREFIX, NULL); if (!wn_name) { ESP_LOGE(TAG, \"No WakeNet model found\"); esp_srmodel_deinit(sr_models); return; } int wn_index = esp_srmodel_exists(sr_models, wn_name); if (wn_index < 0 || wn_index >= sr_models->num) { ESP_LOGE(TAG, \"WakeNet model %s not found in list\", wn_name); esp_srmodel_deinit(sr_models); return; } \/\/srmodel_data_t *wn_data = sr_models->model_data[wn_index];\/\/dev says dont need this ,just add name wakenet = esp_wn_handle_from_name(wn_name); if (!wakenet) { ESP_LOGE(TAG, \"Failed to get WakeNet handle for %s\", wn_name); esp_srmodel_deinit(sr_models); return; } \/\/for debugging ESP_LOGI(TAG, \"Using WakeNet: %s\", wn_name); wn_handle = wakenet->create(wn_name, WAKE_MODE); if (!wn_handle) { ESP_LOGE(TAG, \"WakeNet creation failed for %s\", wn_name); esp_srmodel_deinit(sr_models); return; } ESP_LOGI(TAG, \"WakeNet (%s) and MultiNet (%s) initialized\", wn_name, \"placeholder\");\/\/mn_name } void wakeword_task(void *arg) { int16_t *buffer = calloc(BUFFER_SIZE, sizeof(int16_t)); if (!buffer) { ESP_LOGE(TAG, \"Buffer allocation failed\"); vTaskDelete(NULL); return; } while (1) { int bytes_read = i2s_mic_read((char *)buffer, BUFFER_SIZE * sizeof(int16_t)); if (bytes_read <= 0) continue; if (wakenet->detect(wn_handle, buffer) == WAKENET_DETECTED) { ESP_LOGI(TAG, \"Wake word detected\"); if (ww_callback) ww_callback(); } } \/\/ Cleanup (not usually reached) free(buffer); wakenet->destroy(wn_handle); esp_srmodel_deinit(sr_models); vTaskDelete(NULL); }** ``` Error Log: I (303) MODEL_LOADER: The storage free size is 32448 KB I (303) MODEL_LOADER: The partition size is 8192 KB I (313) MODEL_LOADER: Successfully load srmodels I (313) WakeMultinet: Using WakeNet: wn9_hijason_tts2 Guru Meditation Error: Core 0 panic'ed (StoreProhibited). Exception was unhandled. Core 0 register dump: PC : 0x40055835 PS : 0x00060b30 A0 : 0x8201f218 A1 : 0x3fc9bea0 A2 : 0x00000000 A3 : 0x3fceb888 A4 : 0x0000002a A5 : 0x0000ff00 A6 : 0x00ff0000 A7 : 0xff000000 A8 : 0x656b6177 A9 : 0x3fc9be70 A10 : 0x00000000 A11 : 0x000000ff A12 : 0x3c049f00 A13 : 0x3fc9be30 A14 : 0x3fc9bd14 A15 : 0x0000002a SAR : 0x00000016 EXCCAUSE: 0x0000001d EXCVADDR: 0x00000000 LBEG : 0x40055825 LEND : 0x4005583f LCOUNT : 0xffffffff Backtrace: 0x40055832:0x3fc9bea0 0x4201f215:0x3fc9beb0 0x4200d4a6:0x3fc9bef0 0x4200dba4:0x3fc9c0b0 0x4200a4f0:0x3fc9c0d0 0x4200a315:0x3fc9c100 0x4203d8fc:0x3fc9c120 0x4037b4ed:0x3fc9c150 I tried to initialize wakenet model using esp-sr lirary but got no success as it always results in the same error. I have tried changing the partitions, the way I load the models and even the models themselves but nothing seems to be working.",
    "author_id":5952,
    "publication_date":1754156443000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Abdulbaseer Baseer",
    "author_reputation":11.0,
    "tags":"embedded, esp32, esp-idf, voice-recognition",
    "text_length":4564,
    "title_length":95,
    "num_tags":4
  },
  {
    "id":6490,
    "title":"useNativewindColorScheme chaning theme not switching themes",
    "link":"https:\/\/stackoverflow.com\/questions\/79723476\/usenativewindcolorscheme-chaning-theme-not-switching-themes",
    "text":"Having an issue where I cannot switch theme in my app.... Forcing dark in classname works perfectly but switching the theme using nativewind does not quite work, console logs always output 'light' no matter what i try Tailwind.config.js contains darkMode: 'class' root _layout.tsx ``` import { ThemeProvider } from '@react-navigation\/native'; import 'react-native-css-interop'; import '..\/global.css'; import '..\/translation'; import { useColorScheme } from '~\/core\/useColorScheme'; import { DarkNavigationTheme, LightNavigationTheme } from '~\/core\/theme'; import AuthContextProvider from '~\/features\/auth\/context\/AuthContextProvider'; import RootQueryClient from '~\/root\/RootQueryClient'; import { Platform, StatusBar, View } from 'react-native'; import { cn } from '~\/shared\/utils\/utils'; import { GestureHandlerRootView } from 'react-native-gesture-handler'; import { BottomSheetModalProvider } from '@gorhom\/bottom-sheet'; import RootNavigator from '~\/root\/RootNavigator'; import UserOrderContextProvider from '~\/features\/user-order\/UserOrderContextProvider'; import CheckoutContextProvider from '~\/features\/checkout\/CheckoutContext'; import { useEffect, useLayoutEffect, useRef, useState } from 'react'; import { setAndroidNavigationBar } from '~\/shared\/lib\/theme'; export const unstable_settings = { initialRouteName: '(auth)\/index', }; export default function Layout() { const hasMounted = useRef(false); const [isColorSchemeLoaded, setIsColorSchemeLoaded] = useState(false); const { colorScheme } = useColorScheme(); \/\/ <KeyboardProvider> const navTheme = colorScheme === 'dark' ? DarkNavigationTheme : LightNavigationTheme; useIsomorphicLayoutEffect(() => { if (hasMounted.current) { return; } if (Platform.OS === 'web') { \/\/ Adds the background color to the html element to prevent white background on overscroll. document.documentElement.classList.add('bg-background'); } setAndroidNavigationBar(colorScheme); setIsColorSchemeLoaded(true); hasMounted.current = true; }, []); if (!isColorSchemeLoaded || !hasMounted.current) { return null; } return ( <GestureHandlerRootView className=\"flex-1\"> <BottomSheetModalProvider> <View className={cn(colorScheme === 'dark' ? 'dark' : '', 'flex-1')}> <ThemeProvider value={navTheme}> <AuthContextProvider> <RootQueryClient> <CheckoutContextProvider> <UserOrderContextProvider> <RootNavigator \/> <StatusBar barStyle={Platform.OS === 'ios' ? 'light-content' : 'default'} \/> <\/UserOrderContextProvider> <\/CheckoutContextProvider> <\/RootQueryClient> <\/AuthContextProvider> <\/ThemeProvider> <\/View> <\/BottomSheetModalProvider> <\/GestureHandlerRootView> ); \/* <\/KeyboardProvider> *\/ } const useIsomorphicLayoutEffect = Platform.OS === 'web' && typeof window === 'undefined' ? useEffect : useLayoutEffect; ``` useColorScheme.ts ``` import { useColorScheme as useNativewindColorScheme } from 'nativewind'; export function useColorScheme() { const { colorScheme, setColorScheme, toggleColorScheme } = useNativewindColorScheme(); return { colorScheme: colorScheme ?? 'dark', isDarkColorScheme: colorScheme === 'dark', setColorScheme, toggleColorScheme, }; } ``` ThemePicker.tsx ``` import { View } from 'react-native'; import Text from '.\/Text'; import ToggleGroup from '.\/ToggleGroup'; import { useColorScheme } from '~\/core\/useColorScheme'; import { useTranslation } from 'react-i18next'; import { setAndroidNavigationBar } from '~\/shared\/lib\/theme'; const ThemePicker = () => { const { t } = useTranslation(); const { colorScheme, setColorScheme } = useColorScheme(); const options = [ { label: t('theme.dark'), value: 'dark' }, { label: t('theme.light'), value: 'light' }, ]; function toggleColorScheme(scheme: 'light' | 'dark') { setColorScheme(scheme); setAndroidNavigationBar(scheme); } return ( <View> <Text className=\"text-xl font-medium\">{t('theme.theme')}<\/Text> <ToggleGroup type=\"single\" onValueChange={toggleColorScheme} options={options} maxRows={1} value={colorScheme} \/> <\/View> ); }; export default ThemePicker; ``` Cannot figure out what might be the problem",
    "author_id":5951,
    "publication_date":1754156553000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Janjac",
    "author_reputation":128.0,
    "tags":"react-native, nativewind",
    "text_length":4018,
    "title_length":59,
    "num_tags":2
  },
  {
    "id":6489,
    "title":"How to fix the gestureRecognizer over PDFKit breaking the paging in the new ios26 version?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723482\/how-to-fix-the-gesturerecognizer-over-pdfkit-breaking-the-paging-in-the-new-ios2",
    "text":"I had project going great, where i needed to do stuff with pdfs, drawing on top them etc. Since apple is all closed sourced i needed to become a bit hacky. Anyways, i have a problem since the new ios 26 update which breaks the behaviour. I simplified the code very much into a demo project, where you can quickly see what's wrong. When swiping left to go to the next page, it does change the page etc in the pdf Document, but visually nothing happens. I am stuck on the first page. I dont know what to do, tried a lot of things, but nothing works. Anyone skilled enough to help me out? ``` import UIKit import PDFKit import SwiftUI class PDFViewController: UIViewController { var pdfView: PDFView! var gestureHandler: GestureHandler! override func viewDidLoad() { super.viewDidLoad() setupPDFView() setupGestureHandler() loadPDF() } private func setupPDFView() { pdfView = PDFView(frame: view.bounds) \/\/ Your exact configuration pdfView.autoScales = true pdfView.pageShadowsEnabled = false pdfView.backgroundColor = .white pdfView.displayMode = .singlePage view.addSubview(pdfView) \/\/ Setup constraints pdfView.translatesAutoresizingMaskIntoConstraints = false NSLayoutConstraint.activate([ pdfView.topAnchor.constraint(equalTo: view.topAnchor), pdfView.leadingAnchor.constraint(equalTo: view.leadingAnchor), pdfView.trailingAnchor.constraint(equalTo: view.trailingAnchor), pdfView.bottomAnchor.constraint(equalTo: view.bottomAnchor) ]) } private func setupGestureHandler() { gestureHandler = GestureHandler(pdfView: pdfView) gestureHandler.setupSwipeGestures(on: view) } private func loadPDF() { if let path = Bundle.main.path(forResource: \"sonate12\", ofType: \"pdf\"), let document = PDFDocument(url: URL(fileURLWithPath: path)) { pdfView.document = document } else { print(\"Could not find sonate12.pdf in bundle\") } } } class GestureHandler { private weak var pdfView: PDFView? init(pdfView: PDFView) { self.pdfView = pdfView } func setupSwipeGestures(on view: UIView) { \/\/ Left swipe - go to next page let leftSwipe = UISwipeGestureRecognizer(target: self, action: #selector(handleSwipe(_:))) leftSwipe.direction = .left view.addGestureRecognizer(leftSwipe) \/\/ Right swipe - go to previous page let rightSwipe = UISwipeGestureRecognizer(target: self, action: #selector(handleSwipe(_:))) rightSwipe.direction = .right view.addGestureRecognizer(rightSwipe) } @objc private func handleSwipe(_ gesture: UISwipeGestureRecognizer) { guard let pdfView = pdfView, let document = pdfView.document, let currentPage = pdfView.currentPage else { print(\"🚫 No PDF view, document, or current page available\") return } let currentIndex = document.index(for: currentPage) let totalPages = document.pageCount print(\"📄 Current state: Page \\(currentIndex + 1) of \\(totalPages)\") print(\"👆 Swipe direction: \\(gesture.direction == .left ? \"LEFT (next)\" : \"RIGHT (previous)\")\") switch gesture.direction { case .left: \/\/ Next page guard currentIndex < document.pageCount - 1 else { print(\"🚫 Already on last page (\\(currentIndex + 1)), cannot go forward\") return } let nextPage = document.page(at: currentIndex + 1) if let page = nextPage { print(\"➡️ Going to page \\(currentIndex + 2)\") pdfView.go(to: page) pdfView.setNeedsDisplay() pdfView.layoutIfNeeded() \/\/ Check if navigation actually worked DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { if let newCurrentPage = pdfView.currentPage { let newIndex = document.index(for: newCurrentPage) print(\"✅ Navigation result: Now on page \\(newIndex + 1)\") if newIndex == currentIndex { print(\"⚠️ WARNING: Page didn't change visually!\") } } } } else { print(\"🚫 Could not get next page object\") } case .right: \/\/ Previous page guard currentIndex > 0 else { print(\"🚫 Already on first page (1), cannot go back\") return } let previousPage = document.page(at: currentIndex - 1) if let page = previousPage { print(\"⬅️ Going to page \\(currentIndex)\") pdfView.go(to: page) pdfView.setNeedsDisplay() pdfView.layoutIfNeeded() let bounds = pdfView.bounds pdfView.bounds = CGRect(x: bounds.origin.x, y: bounds.origin.y, width: bounds.width + 0.01, height: bounds.height) pdfView.bounds = bounds \/\/ Check if navigation actually worked DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { if let newCurrentPage = pdfView.currentPage { let newIndex = document.index(for: newCurrentPage) print(\"✅ Navigation result: Now on page \\(newIndex + 1)\") if newIndex == currentIndex { print(\"⚠️ WARNING: Page didn't change visually!\") } } } } else { print(\"🚫 Could not get previous page object\") } default: print(\"🤷‍♂️ Unknown swipe direction\") break } } } struct PDFViewerRepresentable: UIViewControllerRepresentable { func makeUIViewController(context: Context) -> PDFViewController { return PDFViewController() } func updateUIViewController(_ uiViewController: PDFViewController, context: Context) { \/\/ No updates needed } } ``` You can look at the code here as well: https:\/\/github.com\/vallezw\/swift-bug-ios26",
    "author_id":5950,
    "publication_date":1754157255000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Valentin Zwerschke",
    "author_reputation":51.0,
    "tags":"ios, swift, swiftui, ios26, pdfkit",
    "text_length":4919,
    "title_length":90,
    "num_tags":5
  },
  {
    "id":6488,
    "title":"Is vector reallocation bad? (Arena tree implementation)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723483\/is-vector-reallocation-bad-arena-tree-implementation",
    "text":"Let's say I have a tree, implemented with a Vec-backed arena and ``` type NodeID = Option<u32> ``` as references to locations in the arena. This tree can grow to an arbitrary large size. The naive implementation would be to use ``` vec.push ``` every time I add a node, which would cause reallocation whenever the vector exceeds capacity. During an interview where I got asked to implement a tree I used the above-mentioned approach and I got a code review saying that relying on vector reallocation is bad, but the interviewer refused to elaborate further. So my questions are: Is relying on reallocation bad? If yes, what could be the alternatives? The only alternative I could come up with would be to use a jagged array, like ``` Vec<Vec<Node>> ``` , where each ``` Vec<Node> ``` has a fixed maximum size, let's say ``` RowLength ``` . Whenever I would reach capacity I would allocate a ``` Vec<Node> ``` of size ``` RowLength ``` , and append it to the jagged array. The jagged array could experience reallocation, but it would be cheap because we are dealing with pointers of vectors, and not the full vector. To access ``` NodeID ``` node, I would access ``` arena[row][column] ``` , where row is ``` (NodeID as u64) % RowLength ``` and column is ``` (NodeID as u64) \/ RowLength ``` In this case I would reduce the cost of reallocation, in exchange for slightly slower element access, albeit still ``` O(1) ``` , due to pointer indirection. Is this approach better?",
    "author_id":5949,
    "publication_date":1754157418000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mascarpone",
    "author_reputation":2576.0,
    "tags":"rust, allocation, memory-reallocation",
    "text_length":1472,
    "title_length":55,
    "num_tags":3
  },
  {
    "id":6487,
    "title":"Cannot see &quot;Add App&quot; button in App Store Connect – Role is Developer",
    "link":"https:\/\/stackoverflow.com\/questions\/79723488\/cannot-see-add-app-button-in-app-store-connect-role-is-developer",
    "text":"I'm trying to add a new app in App Store Connect, but when I go to the Apps tab, it shows \"No apps have been added yet\" and I don't see any \"Add App\" button. I have been added to the team with the Developer role. Why can't I see the Add App button, and how can I fix this? Do I need a different role or permission?",
    "author_id":5948,
    "publication_date":1754157543000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Junaid Nasir",
    "author_reputation":87.0,
    "tags":"ios, app-store-connect, apple-developer, apple-id",
    "text_length":314,
    "title_length":78,
    "num_tags":4
  },
  {
    "id":6486,
    "title":"I have this error at Jenkins when building a job on it",
    "link":"https:\/\/stackoverflow.com\/questions\/79723493\/i-have-this-error-at-jenkins-when-building-a-job-on-it",
    "text":"I am a newbie at Jenkins, and when building a job on Jenkins I got this error. Could anyone tell me the solution or a track to solve? I would thank a lot the help. In the Output Console, the important part is this: ``` 16:55:56 [0;32m \"failed\": true,[0m 16:55:56 [0;32m \"msg\": \"Failed to build image quay.cloudmgmt.mutua.es\/soportefuncional-develop\/front-fundmm:3.0.0-SNAPSHOT: time=\\\\\"2025-08-01T16:55:13+02:00\\\\\" level=warning msg=\\\\\"missing \\\\\\\\\\\\\"LICENSE_JAR_URL\\\\\\\\\\\\\" build argument. Try adding \\\\\\\\\\\\\"--build-arg LICENSE_JAR_URL=<VALUE>\\\\\\\\\\\\\" to the command line\\\\\"\\\\ndebconf: delaying package configuration, since apt-utils is not installed\\\\n % Total % Received % Xferd Average Speed Time Time Time Current\\\\n Dload Upload Total Spent Left Speed\\\\n\\\\r 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0\\\\r100 54.7M 100 54.7M 0 0 111M 0 --:--:-- --:--:-- --:--:-- 111M\\\\n % Total % Received % Xferd Average Speed Time Time Time Current\\\\n Dload Upload Total Spent Left Speed\\\\n\\\\r 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0\\\\r100 115M 100 115M 0 0 193M 0 --:--:-- --:--:-- --:--:-- 193M\\\\ncp: cannot stat '.\/server\/config\/datos\/': No such file or directory\\\\nError: building at STEP \\\\\"RUN mkdir -p \/config\/apps && mkdir -p \/sharedlibs && cp .\/server\/config\/*.xml \/config\/ && cp .\/app.${ARTIFACT_TYPE} \/config\/apps\/ && cp -r .\/server\/config\/extra-lib\/ \/extra-lib\/ && cp -r .\/server\/config\/extra-cfg\/ \/extra-cfg\/ && cp -r .\/server\/config\/datos\/ \/datos\/ && cp -r .\/server\/config\/security\/ \/security\/ && if [ ! -z \\\\\"$(ls .\/src\/main\/liberty\/lib 2>\/dev\/null)\\\\\" ]; then cp -r .\/src\/main\/liberty\/lib\/* \/sharedlibs; fi\\\\\": while running runtime: exit status 1\\\\n\"[0m ```",
    "author_id":5947,
    "publication_date":1754158249000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"algar",
    "author_reputation":3.0,
    "tags":"jenkins, jenkins-pipeline, jenkins-job-builder, jenkins-build-flow",
    "text_length":1669,
    "title_length":54,
    "num_tags":4
  },
  {
    "id":6485,
    "title":"How can I detect and correct picture orientation?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723496\/how-can-i-detect-and-correct-picture-orientation",
    "text":"I have photos in .jpg format of people, I want the head to be at the top and the feet at the bottom and with landscapes I want the sky to be at the top and the ground at the bottom. I tried: ``` exiftool -orientation= -n -if ‘$Orientation ne “1”’ -auto-orient -overwrite_original *.jpg ``` with less than satisfactory results. I changed for a Python script: ``` import cv2 import os # Load OpenCV face detector (Haar cascade) face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Rotation map for OpenCV rotation_methods = { 0: lambda img: img, 90: lambda img: cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE), 180: lambda img: cv2.rotate(img, cv2.ROTATE_180), 270: lambda img: cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE) } def detect_faces(img): gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5) return len(faces) def auto_rotate_image(file_path): original_img = cv2.imread(file_path) if original_img is None: print(f\"Could not read {file_path}\") return best_angle = 0 max_faces = 0 for angle in [0, 90, 180, 270]: rotated = rotation_methods[angle](original_img) face_count = detect_faces(rotated) if face_count > max_faces: best_angle = angle max_faces = face_count if best_angle != 0: rotated_img = rotation_methods[best_angle](original_img) cv2.imwrite(file_path, rotated_img) print(f\"✓ Rotated {file_path} by {best_angle}° to align face(s)\") else: print(f\"× No face detected in {file_path}, skipped\") # Process all JPGs in current folder for filename in os.listdir('.'): if filename.lower().endswith('.jpg'): auto_rotate_image(filename) ``` This is working but results are poor. Is it possible to use MediaPipe face detection? Are you aware of a better solution that can be implemented in Linux?",
    "author_id":5946,
    "publication_date":1754158546000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Upax",
    "author_reputation":139.0,
    "tags":"python-3.x, linux, image",
    "text_length":1816,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":6484,
    "title":"Tailwind CSS v4 @apply not working properly with theme variables",
    "link":"https:\/\/stackoverflow.com\/questions\/79723504\/tailwind-css-v4-apply-not-working-properly-with-theme-variables",
    "text":"I just started using tailwind CSS v4 and trying to make reusable UI component with React JS. but I'm getting errors like: ``` Cannot apply unknown utility class `text-brand-secondary-800 ``` which i defined in ``` app.css ``` file. Here is my ``` app.css ``` ``` @import url(\"https:\/\/fonts.googleapis.com\/css2?family=Poppins&display=swap\"); * { scroll-behavior: smooth; font-family: \"Poppins\", sans-serif; } @import \"tailwindcss\"; @theme { --color-brand-secondary-800: #121212; \/* rest of the variables *\/ } ``` Here is my ``` component.css ``` : ``` @reference \"tailwindcss\"; .component-header-text { @apply font-semibold text-base text-brand-secondary-800; } ``` All the other default tailwind classes such as ``` bg-gray-200 ``` are working fine but theme variables. I'm following this method to make component class reusable and overridable. if you have any alternatives they’re appreciated. Any help would be greatly appreciated. Thank you in advance!",
    "author_id":5945,
    "publication_date":1754159523000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Muhammed Sahad",
    "author_reputation":126.0,
    "tags":"reactjs, html, css, tailwind-css, tailwind-css-4",
    "text_length":956,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6483,
    "title":"RuntimeWarning: coroutine &lt;aiortc&gt; was never awaited",
    "link":"https:\/\/stackoverflow.com\/questions\/79723508\/runtimewarning-coroutine-aiortc-was-never-awaited",
    "text":"I am trying to use two Python libraries together: windows-capture and aiortc Below is the code in my server.py: ``` import asyncio from windows_capture import Frame, InternalCaptureControl from aiohttp import web from aiortc import RTCPeerConnection, RTCSessionDescription my_channel = None is_my_channel_open = False async def offer(request): params = await request.json() offer = RTCSessionDescription(sdp=params[\"sdp\"], type=params[\"type\"]) pc = RTCPeerConnection() global my_channel my_channel = pc.createDataChannel(\"my_channel\") @my_channel.on(\"open\") def on_open(): # REFERENCE BELOW my_channel.send(\"Hello from Python aiortc server!\") # REFERENCE ABOVE global is_my_channel_open is_my_channel_open = True # handle offer await pc.setRemoteDescription(offer) # send answer answer = await pc.createAnswer() await pc.setLocalDescription(answer) def setup_aiohttp_server(): app = web.Application() app.router.add_post(\"\/offer\", offer) web.run_app( app, host=\"0.0.0.0\", port=8080 ) print(\"This print will never happen\") def setup_windows_capture(): # Called Every Time A New Frame Is Available @capture.event def on_frame_arrived(frame: Frame, capture_control: InternalCaptureControl): if is_my_channel_open: # PROBLEM AREA BELOW my_channel.send(\"data sent from on_frame_arrived\") # PROBLEM AREA ABOVE print_capture_fps() # This prints the capture FPS once a second to console # Starts the capture session capture.start() print(\"This print will never happen\") async def main(): task1 = asyncio.to_thread(setup_windows_capture) task2 = asyncio.to_thread(setup_aiohttp_server) await asyncio.gather(task1, task2) asyncio.run(main()) ``` My problem is that I'm trying to call ``` my_channel.send(\"data sent from on_frame_arrived\") ``` in the PROBLEM AREA but it seems to completely break the ``` on_frame_arrived ``` function. Every second, the FPS is printed to console, but after calling this method, the FPS stops and I get this error: ``` C:\\Users\\Dzenis\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\windows_capture\\__init__.py:224: RuntimeWarning: coroutine 'RTCSctpTransport._data_channel_flush' was never awaited self.capture.start() RuntimeWarning: Enable tracemalloc to get the object allocation traceback ``` On the browser, I get \"Hello from Python aiortc server!\" printed, but I only get \"data sent from on_frame_arrived\" once, instead of an expected ~60 times per second",
    "author_id":5944,
    "publication_date":1754160255000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dzenis Zigo",
    "author_reputation":343.0,
    "tags":"python, aiortc",
    "text_length":2394,
    "title_length":58,
    "num_tags":2
  },
  {
    "id":6482,
    "title":"How to set up a simple hello-world example where a C function calls a Cython function calling a Python function?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723518\/how-to-set-up-a-simple-hello-world-example-where-a-c-function-calls-a-cython-fun",
    "text":"I am having trouble making a simple \"Hello World\" python function which I can call from a C program. Here is the contents of my ``` helloworld.py ``` file: ``` def hw(): print(\"Hello World\") ``` Here is the contents of the ``` caller.pyx ``` file: ``` from helloworld import hw cdef public void call_hw(): hw() ``` And here is the contents of my ``` main.c ``` file: ``` #include <Python.h> #include \"caller.h\" int main() { Py_Initialize(); call_hw(); Py_Finalize(); } ``` Here are the commands I do: ``` $ cython caller.pyx $ gcc -g -Wall -I\/usr\/include\/python3.12 -c caller.c $ gcc -g -Wall -I\/usr\/include\/python3.12 -c main.c $ gcc -g -Wall -I\/usr\/include\/python3.12 -o main *.o -lpython3.12 $ .\/main Segmentation fault (core dumped) ``` Here is a backtrace: ``` Program received signal SIGSEGV, Segmentation fault. 0x0000555555558898 in __Pyx__GetModuleGlobalName (name=0x0) at caller.c:2739 2739 result = _PyDict_GetItem_KnownHash(__pyx_d, name, ((PyASCIIObject *) name)->hash); (gdb) bt #0 0x0000555555558898 in __Pyx__GetModuleGlobalName (name=0x0) at caller.c:2739 #1 0x0000555555556d9a in call_hw () at caller.c:2002 #2 0x000055555555cef6 in main () at main.c:8 ``` Can anyone tell me what I'm doing wrong?",
    "author_id":5943,
    "publication_date":1754161195000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"vibe",
    "author_reputation":585.0,
    "tags":"python, c, python-3.x, cython",
    "text_length":1215,
    "title_length":112,
    "num_tags":4
  },
  {
    "id":6481,
    "title":"Can I create a configurations in Visual Studio that differs in command line options only?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723523\/can-i-create-a-configurations-in-visual-studio-that-differs-in-command-line-opti",
    "text":"I need to create several configurations with different command line options for my program in C\/C++ (VS 2019). I have created 2 new configurations - C1, C2 with \"Create new solution configurations\" = \"no\", \"Copy settings from\" = Debug\/Win32. 2 configurations are created successfully, but if I change command line options in \"Debug\" tab of project properties window, then the options are changed in C1 and C2, but I'd like to have one options for C1 and another for C2. How can I do that? May be there is another way to switch fast between different command line options for one project ?",
    "author_id":5942,
    "publication_date":1754161618000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"LUN",
    "author_reputation":349.0,
    "tags":"visual-studio, configuration",
    "text_length":588,
    "title_length":89,
    "num_tags":2
  },
  {
    "id":6480,
    "title":"Gradle 9: Unresolved reference &#39;exec&#39;",
    "link":"https:\/\/stackoverflow.com\/questions\/79723532\/gradle-9-unresolved-reference-exec",
    "text":"I've got Gradle code like this: ``` tasks.register(\"myTask\") { exec { commandLine(\"sh\", \"-c\", \"myscript.sh\" } } ``` It worked in Gradle 8, but it fails in Gradle 9 with this error: Unresolved reference 'exec'. Why?",
    "author_id":5941,
    "publication_date":1754162748000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dan Fabulich",
    "author_reputation":39871.0,
    "tags":"gradle",
    "text_length":214,
    "title_length":45,
    "num_tags":1
  },
  {
    "id":6479,
    "title":"How to use index to find position of JSON record",
    "link":"https:\/\/stackoverflow.com\/questions\/79723552\/how-to-use-index-to-find-position-of-json-record",
    "text":"Is there a better way than iteration using a for loop to find the index of the record? My problem is that to use index I seem to need the index of the record I'm seeking. ``` import json from bs4 import BeautifulSoup as BS html = '''<html> <body> <div id=\"__next\"> <\/div> <script id=\"__NEXT_DATA__\" type=\"application\/json\"> {\"topic\": \"Chemo tablets make me feel sick\", \"id\": 152210029, \"body\": \"Venetoclax with or without food make me feel sick for 3 hours.\\n\\n\", \"dateCreated\": \"2025-07-31T15:20:15.000Z\", \"responses\": [ { \"id\": 152211099, \"body\": \"I was able to survive the nausea with room temperature ginger beer. Don't know why, but it worked!\\n\\n\", \"dateCreated\": \"2025-07-31T17:24:35.000Z\" }, { \"id\": 152211304, \"body\": \"Sucking on root ginger is good for sea sickness.\\n\\n\", \"dateCreated\": \"2025-07-31T19:00:09.000Z\" }, { \"id\": 152211658, \"body\": \"Ginger apparently contains a bioactive compound called gingerol. I might give it a try. Healthier than chips!\\n\\n\", \"dateCreated\": \"2025-07-31T21:48:15.000Z\" }] } <\/script> <\/body> <\/html>''' soup_page = BS(html, 'lxml') #print(soup_page.prettify()) json_script = soup_page.find('script', type='application\/json').text #print(res) json_data = json.loads(json_script, strict=False) #print(json_data) seek = 152211304 rec_no = 0 for record in json_data['responses']: # print(record['id']) if (record['id'] == seek): break rec_no = rec_no + 1 extract = json_data['responses'][rec_no]['body'] print(extract) ```",
    "author_id":5940,
    "publication_date":1754164235000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Peter Hill",
    "author_reputation":53.0,
    "tags":"python, json, beautifulsoup",
    "text_length":1463,
    "title_length":48,
    "num_tags":3
  },
  {
    "id":6478,
    "title":"getSubscriptionId() Cannot resolve method error",
    "link":"https:\/\/stackoverflow.com\/questions\/79723554\/getsubscriptionid-cannot-resolve-method-error",
    "text":"I want to ask why i get Cannot resolve method getSubscriptionId in CellInfo for the below code in Android Studio (java) while my minSdk is 31 and compileSdk,targetSdk are 33 ``` List<CellInfo> cellInfos = telephonyManager.getAllCellInfo(); if (cellInfos == null || cellInfos.isEmpty()) return; \/\/ Filter by selected SIM if (selectedSubId != SubscriptionManager.INVALID_SUBSCRIPTION_ID) { cellInfos.removeIf(ci -> ci.getSubscriptionId() != selectedSubId); } if (cellInfos.isEmpty()) return; ```",
    "author_id":5939,
    "publication_date":1754164445000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Moustafa",
    "author_reputation":3.0,
    "tags":"java, android",
    "text_length":493,
    "title_length":47,
    "num_tags":2
  },
  {
    "id":6477,
    "title":"Shadowing functions\/properties using generics works outside the object but not inside it?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723560\/shadowing-functions-properties-using-generics-works-outside-the-object-but-not-i",
    "text":"I have a generic type ( ``` Bar ``` ) that shadows the implementation of the function ``` foo ``` depending on ``` T ``` : ``` struct Bar<T> { func foo() { print(\"foo generic\") } func foo() where T == String { print(\"foo string\") } } ``` When I call it, it works like I expect it to: ``` B<Double>().foo() \/\/ foo generic B<String>().foo() \/\/ foo string ``` Now I create an another function that calls ``` foo ``` inside the object: ``` func boo() { foo() } ``` But when I call ``` boo ``` , it doesn't use the string \"shadow\" altogether. ``` B<String>().boo() \/\/ foo generic ``` Things I tried: I tried to mark the generic function ``` @_disfavoredOverload ``` but it didn't do anything. I tried exporting the ``` foo ``` function to another object. And I was surprised when this didn't work. I'd like to know why this solution didn't work. ``` struct Bar<T> { let inner: Far<T> func boo() { inner.foo() } } struct Far<T> { func foo() { print(\"foo generic\") } func foo() where T == String { print(\"foo string\") } } ``` Notice: I ran into this issue while creating a SwiftUI body. So I'm unable to use some solutions. ``` \/\/ Very simplified. struct MyView<APIClient: SomeClientProtocol>: View { let client: APIClient var body: some View { Button(\"Fetch\") { Task(operation: fetchData) \/\/ Always calls generic } } func fetchData() async { await client.fetch() } func fetchData() async where APIClient: FetchSpecially { await client.specialFetch() } } ```",
    "author_id":5938,
    "publication_date":1754165035000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Radioactive",
    "author_reputation":841.0,
    "tags":"ios, swift, swiftui, generics",
    "text_length":1451,
    "title_length":89,
    "num_tags":4
  },
  {
    "id":6476,
    "title":"How to enable DevServices only for tests",
    "link":"https:\/\/stackoverflow.com\/questions\/79723567\/how-to-enable-devservices-only-for-tests",
    "text":"I want to run devservices only in tests. In ``` src\/main\/resources\/application.properties ``` I configure datasource and disable dev services ``` quarkus.datasource.db-kind = postgresql quarkus.datasource.username = ${USERNAME} quarkus.datasource.password=${PASSWORD} quarkus.datasource.jdbc.url = jdbc:postgresql:\/\/... quarkus.devservices.enabled=false ``` In ``` src\/test\/resources\/application.properties ``` I overwrite datasource values and enable dev services. ``` %test.quarkus.datasource.devservices.enabled=true %test.quarkus.devservices.enabled=true %test.quarkus.datasource.db-kind=postgresql %test.quarkus.datasource.username = %test.quarkus.datasource.password= %test.quarkus.datasource.jdbc.url = ``` When I start tests i get error ``` Caused by: io.quarkus.runtime.configuration.ConfigurationException: Model classes are defined for the default persistence unit <default> but configured datasource <default> not found: the default EntityManagerFactory will not be created. To solve this, configure the default datasource. Refer to https:\/\/quarkus.io\/guides\/datasource for guidance. at io.quarkus.hibernate.orm.runtime.FastBootHibernatePersistenceProvider.injectDataSource(FastBootHibernatePersistenceProvider.java:372) at io.quarkus.hibernate.orm.runtime.FastBootHibernatePersistenceProvider.buildRuntimeSettings(FastBootHibernatePersistenceProvider.java:212) at io.quarkus.hibernate.orm.runtime.FastBootHibernatePersistenceProvider.getEntityManagerFactoryBuilderOrNull(FastBootHibernatePersistenceProvider.java:183) at io.quarkus.hibernate.orm.runtime.FastBootHibernatePersistenceProvider.createEntityManagerFactory(FastBootHibernatePersistenceProvider.java:66) at javax.persistence.Persistence.createEntityManagerFactory(Persistence.java:80) at javax.persistence.Persistence.createEntityManagerFactory(Persistence.java:55) at io.quarkus.hibernate.orm.runtime.JPAConfig$LazyPersistenceUnit.get(JPAConfig.java:165) at io.quarkus.hibernate.orm.runtime.JPAConfig$1.run(JPAConfig.java:66) at java.base\/java.lang.Thread.run(Thread.java:829) ``` But when I delete the datasource configuration in ``` src\/main\/resources\/application.properties ``` , Quarkus correctly configures the test container for testing. Quarkus version: 2.13.3.Final Output of ``` java -version ``` : openjdk version \"11.0.27\" 2025-04-15 OpenJDK Runtime Environment (build 11.0.27+6-post-Ubuntu-0ubuntu124.04) OpenJDK 64-Bit Server VM (build 11.0.27+6-post-Ubuntu-0ubuntu124.04, mixed mode, sharing)",
    "author_id":5937,
    "publication_date":1754165829000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Egor",
    "author_reputation":43.0,
    "tags":"quarkus",
    "text_length":2480,
    "title_length":40,
    "num_tags":1
  },
  {
    "id":6475,
    "title":"React WebApp is unreachable on the browser",
    "link":"https:\/\/stackoverflow.com\/questions\/79723580\/react-webapp-is-unreachable-on-the-browser",
    "text":"I'm trying to build a React app using vanilla CSS for the frontend. I'm using Linux VM as my test environment, and I've added all the codes in the project structure. it was compiled successfully but I couldn't view the output from the local: ``` http\/\/localhost:3000 ``` ``` Compiled successfully! You can now view feedback-frontend in the browser. Local: http:\/\/localhost:3000 On Your Network: http:\/\/10.0.0.4:3000 Note that the development build is not optimized. To create a production build, use npm run build. webpack compiled successfully ``` But the browser can't reach the page. what can I do to get the correct output?",
    "author_id":5936,
    "publication_date":1754167106000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Adeniyi Osofuye",
    "author_reputation":1.0,
    "tags":"reactjs, linux, web-applications",
    "text_length":627,
    "title_length":42,
    "num_tags":3
  },
  {
    "id":6474,
    "title":"Error: (&#39;Connection aborted.&#39;, ConnectionResetError(10054, &#39;An existing connection was forcibly closed by the remote host&#39;, None, 10054, None))",
    "link":"https:\/\/stackoverflow.com\/questions\/79723583\/error-connection-aborted-connectionreseterror10054-an-existing-connecti",
    "text":"When calling the API- ``` import requests url = \"https:\/\/api.themoviedb.org\/3\/movie\/98?language=en-US\" headers = { \"accept\": \"application\/json\", \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJjOTA5NTllZDlhODNmZjMwNDUxNmFjZWU4MjZkMDgxOSIsIm5iZiI6MTc1NDE1NjU1MC4wMiwic3ViIjoiNjg4ZTRlMDZiMzQxOWNhN2RhN2NmMThjIiwic2NvcGVzIjpbImFwaV9yZWFkIl0sInZlcnNpb24iOjF9.gtqj0_-PbxWWVXpLwB1ZfS_RFrUr3IWV4uPcGV2SDXg\" } response = requests.get(url, headers=headers) print(response.text) ``` I get the error- ``` Error: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)) ``` When going through the API docs given at https:\/\/developer.themoviedb.org\/reference\/movie-details , I see that the API test given on the page works fine on the website, but when tried locally gives the error. I see that some websites say, that this is more of a network related issue, but am unable to resolve it. I am unable to understand the issue here and resolve it. Please help!!",
    "author_id":5935,
    "publication_date":1754167275000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"ayush",
    "author_reputation":694.0,
    "tags":"python",
    "text_length":1035,
    "title_length":159,
    "num_tags":1
  },
  {
    "id":6473,
    "title":"How can I set CloudFront Signed Cookies for restricted S3 access without a reverse-proxy?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723584\/how-can-i-set-cloudfront-signed-cookies-for-restricted-s3-access-without-a-rever",
    "text":"I’m building a system to serve private S3 assets via CloudFront Signed Cookies—so end users can browse entire folders without issuing individual presigned URLs. What I'm trying: I have a CloudFront distribution at ``` https:\/\/d1234abcdef.cloudfront.net ``` . On login, my SPA calls ``` \/api\/getAccessToken ``` , which: Checks the user’s role\/permissions Creates a single-use, short-TTL token ( ``` access_id ``` ) in Backend Returns a URL: ``` https:\/\/d1234abcdef.cloudfront.net\/set-cookies.html?access_id=<UUID>&Signature=... ``` My static ``` set-cookies.html ``` (deployed to that same distribution) has inline JS that: Reads ``` access_id ``` from the query string Calls ``` \/api\/redeem?access_id= ``` —the backend: Backend validates the ``` access_id ``` and expires it. Returns the three CloudFront signed-cookie values (Policy, Signature, Key-Pair-Id) Sets each as ``` Secure; HttpOnly; SameSite=None; Path=\/ ``` cookies on ``` d1234abcdef.cloudfront.net ``` Redirects back to the SPA Why I’m Not Using Other Approaches: Reverse-proxy (e.g. ``` app.example.com ``` → CloudFront): defeats CDN offload, adds latency. Custom CNAME + parent-domain cookies (e.g. ``` cdn.example.com ``` with cookies on ``` .example.com ``` ): browsers allow setting cookies only on the same hostname or parent domain; I can’t safely set ``` .example.com ``` because we have multiple public-facing subdomains and don’t want all apps to inherit the signed-cookie. Lambda@Edge or API-Gateway JWT auth: would work, but adds per-request compute cost and more complexity in deployment\/testing. Questions: Is this approach appropriate? Any pitfalls or flaws in this approach?",
    "author_id":5934,
    "publication_date":1754167286000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Vasu Gajjar",
    "author_reputation":3.0,
    "tags":"security, cookies, amazon-s3, amazon-cloudfront, pre-signed-url",
    "text_length":1654,
    "title_length":89,
    "num_tags":5
  },
  {
    "id":6472,
    "title":"C# GDI+ questionable font rendering on Win 7",
    "link":"https:\/\/stackoverflow.com\/questions\/79723586\/c-gdi-questionable-font-rendering-on-win-7",
    "text":"I am trying to render some kind of a picture using GDI+ with dynamically loaded fonts. Here is a minimal example (.NET 8.0): ``` private void Form1_Load(object sender, EventArgs e) { PrivateFontCollection collection = new(); collection.AddFontFile(\"Unbounded-Medium.ttf\"); Bitmap bitmap = new(1000, 1000); using (Graphics g = Graphics.FromImage(bitmap)) { g.TextRenderingHint = TextRenderingHint.AntiAliasGridFit; Font font = new(collection.Families[0], 100, FontStyle.Bold, GraphicsUnit.Point); g.DrawString(\"Bbbb\", font, Brushes.Red, new PointF(10, 10)); } pictureBox1.Image = new Bitmap(bitmap, 1000, 1000); } ``` Everything works fine on multiple Win 10 machines: But on a Win 7 machines the same code produces the following output: For some reason, some letters are being rendered... questionably. Other letters look normal though. Using trial-and-error approach I've found out that this kind of rendering happens: Only on Win 7 machines Only for font sizes above 70 Only for certain fonts (Unbounded-Medium in my case) Only while using GDI+ I tried: Changing ``` g.TextRenderingHint ``` Adding ``` StringFormat.GenericTypographic ``` parameter to the ``` DrawString ``` method Enabling ClearType on the OS level Why does this happen? Is this a Win 7 GDI+ bug? Is it possible to fix this issue without updating OS?",
    "author_id":5933,
    "publication_date":1754167522000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"AndrewR",
    "author_reputation":642.0,
    "tags":"c#, fonts, .net-8.0, windows-7, gdi+",
    "text_length":1319,
    "title_length":44,
    "num_tags":5
  },
  {
    "id":6471,
    "title":"Add commonly used files (not in the current workspace) to Ctrl+P indexing",
    "link":"https:\/\/stackoverflow.com\/questions\/79723588\/add-commonly-used-files-not-in-the-current-workspace-to-ctrlp-indexing",
    "text":"Is there an extension or functionality where I can type Ctrl+P bashrc and have my ``` ~\/.bashrc ``` file appear, from a workspace my home directory is outside of? Sufficient for my use is a manual list of files one adds to ``` settings.json ``` , or a manual list of additional directories to look through.",
    "author_id":5932,
    "publication_date":1754167689000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"big_noob",
    "author_reputation":123.0,
    "tags":"visual-studio-code",
    "text_length":306,
    "title_length":73,
    "num_tags":1
  },
  {
    "id":6470,
    "title":"Fail to get split dns to resolve on GCE",
    "link":"https:\/\/stackoverflow.com\/questions\/79723590\/fail-to-get-split-dns-to-resolve-on-gce",
    "text":"I am trying to set up split-dns on a bastion host on GCP\/GCE. Basically I am following the tutorial of: https:\/\/tailscale.com\/kb\/1147\/cloud-gce See the full pulumi details here: https:\/\/gist.github.com\/geoHeil\/2db582127ec7a07799216ce78ec64b1e start a vm for the bastion host and eventuall tailscale define a private dns zone configure split dns for tailscale boot tailscale with the following arguments: ``` tailscale up \\\\ --authkey=\"{args[3]}\" \\\\ --hostname=\"{bastion_hostname}\" \\\\ --advertise-routes=\"{args[0]},{args[1]},{args[2]}\" \\\\ --accept-routes \\\\ --accept-dns=false \\\\ --snat-subnet-routes=true \\\\ --ssh ``` Result: The machine shows up nicely, ping, and IP based connectivity and forwarding works. However, I cannot get DNS resolution to work. Please can you help me to debug this and eventually get the DNS resolution to work.",
    "author_id":5931,
    "publication_date":1754167952000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Georg Heiler",
    "author_reputation":17854.0,
    "tags":"google-cloud-platform, dns, google-compute-engine, bastion-host, tailscale",
    "text_length":838,
    "title_length":39,
    "num_tags":5
  },
  {
    "id":6469,
    "title":"Is Building .NET Framework 4.8 Projects on Mono on MacOS Possible?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723593\/is-building-net-framework-4-8-projects-on-mono-on-macos-possible",
    "text":"I was tasked this sprint with trying to find a way to build and unit test a variety of worker services that my company has that target .NET Framework 4.8 on MacOS, as we have developers transitioning to M3 MacBooks for the better stability, battery life, and performance over the Windows laptops we have used in the past. I don't need the services to be able to fully run on MacOS, just be able to build them and run unit tests. I got a decent ways in using Mono and then ran into this exception: ``` 15>MyClass.cs(33,49): Error CS1705 : Assembly 'MyAssembly' with identity MyAssembly, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' uses 'System.Net.Http, Version=4.2.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' which has a higher version than referenced assembly 'System.Net.Http' with identity 'System.Net.Http, Version=4.1.1.3, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' ``` The build toolchain is looking for System.Net.Http 4.2.0.0 but the version it is finding is 4.1.1.3. I loaded Rider and inspected the version of System.Net.Http that is included in Mono and it is 4.1.1.3 targeting .NET Framework 4.6. I looked up the Mono documentation and found this indicating support in Mono for up to .NET Framework 4.7. This looks like a dead end to me unless I start throwing a ton of binding redirects into my build tooling which defeats the purpose of a non-surgical way to have devs building things on MacBooks. Am I correct that this is a showstopper and that we really just will need developers working on these older things to either get set up with something like a DevBox or a dedicate Windows machine of some kind?",
    "author_id":5930,
    "publication_date":1754168122000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Thomas Parikka",
    "author_reputation":812.0,
    "tags":".net, macos, mono, apple-silicon",
    "text_length":1649,
    "title_length":66,
    "num_tags":4
  },
  {
    "id":6468,
    "title":"Why does the lock-free queue use triple counting instead of double counting used in lock-free stack?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723596\/why-does-the-lock-free-queue-use-triple-counting-instead-of-double-counting-used",
    "text":"In the book C++ Concurrency In Action, the lock-free queue is implemented with triple counting. The basic structure of its nodes is as follows: ``` template <typename> struct node; template <typename T> struct counted_node { node<T> *data; long external_count; }; template <typename T> struct node { struct compressed_counter { unsigned long internal_counter : 62; unsigned long head_tail_counter : 2; }; std::atomic<counted_node<T>> next; std::atomic<compressed_counter> internal_counter; std::atomic<T *> value; }; ``` The double counting structure is designed as ``` template <typename> struct node; template <typename T> struct counted_node { node<T> *data; long external_count; }; template <typename T> struct node { counted_node<T> next; std::atomic<long> internal_count; std::shared_ptr<T> value; }; ``` The queue is based on a singly-linked list. All nodes are dynamically allocated, and node.data->value are also dynamically allocated. The queue size is unbounded. There is always an empty node serving as the tail. When inserting, push first updates tail.data->value, then allocates a new node without a value as tail.data->next. When popping, it first checks whether head.data == tail.data; if they are equal, the queue is considered empty. Otherwise, it deletes the first node. To avoid the situation where push repeatedly encounters a tail whose tail.data->value is already non-empty, when the CAS operation fails, the thread will help another thread by allocating a new empty-value tail node, preventing busy-waiting. ``` template <typename T> class lock_free_queue { private: std::atomic<counted_node<T>> head; std::atomic<counted_node<T>> tail; public: lock_free_queue() : head {new node<T> {}}, tail {head} { \/\/ if we implement it with triple counting, then \/\/ this->head.data->internal_counter.head_tail_counter = 2; \/\/ or if we implement with double counting this->head.external_count = 2; } ~lock_free_queue() noexcept { while(this->pop()); } void push(const T &t) { auto value {std::make_unique<T>(t)}; \/\/ increase external count of this->tail (old_tail) \/\/ if (CAS) old_tail.data->value is nullptr, replace it by value.release() \/\/ allocate a new node named new_tail \/\/ if (CAS) old_tail.data->next is nullptr, replace it by new_tail \/\/ free external count for old_tail (-2, tail part and push part) } std::unique_ptr<T> pop() noexcept { \/\/ increase external count of this->head (old_head) \/\/ if old_head.data equals to this->tail.data, return nullptr \/\/ if (CAS) old_head equals to this->head \/\/ load the address of old_head.data->value \/\/ making a object typed std::unique_ptr<T> \/\/ free external count for old_head (-1, head part) \/\/ free external count for old_head (-1, pop part) \/\/ return the result } }; ``` You can find the source code in https:\/\/github.com\/anthonywilliams\/ccia_code_samples\/tree\/main\/listings , ``` listing_7.16.cpp ``` to ``` listing_7.22.cpp ``` . Why put the ``` head ``` \/ ``` tail ``` counter inside the node’s data instead of directly storing it in ``` external_count ``` ? I have considered some concurrency scenarios, but double counting seems to handle these cases just fine, since the counter decrements operate on the internal counter. The book directly states that ``` tail ``` may be updated by pop. However, in the code shown in the book, ``` tail ``` is only loaded and compared; I don’t see any actual modification to the data of the ``` tail ``` node (including reclamation).",
    "author_id":5929,
    "publication_date":1754168546000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jonny0201",
    "author_reputation":493.0,
    "tags":"c++, multithreading, atomic, stdatomic, lock-free",
    "text_length":3440,
    "title_length":100,
    "num_tags":5
  },
  {
    "id":6467,
    "title":"cmake cannot find jkqtplotter installed with vcpkg",
    "link":"https:\/\/stackoverflow.com\/questions\/79723599\/cmake-cannot-find-jkqtplotter-installed-with-vcpkg",
    "text":"I'm trying to use the library JKQtPlotter in a windows project. I have already have some libraries installed with vcpkg and everything works fine (openssl, boost, ...). So I ran ``` .\\vcpkg install jkqtplotter:x64-windows ``` in the vcpkg root folder and I can see the port installed in .\\installed\\x64-windows\\share\\jkqtplotter. If running the command ``` .\\vcpkg list ``` I can see the port listed as ``` jkqtplotter:x64-windows ``` , so it is available in the vcpkg root folder. I tried including the libraries in a test project. Here is the CMakeLists.txt ``` cmake_minimum_required(VERSION 3.6) project(testJKQtPlotter) find_package(JKQtPlotter REQUIRED) ``` Inside the project's folder, I create a build folder and run cmake in this folder: ``` mkdir build cd build cmake -DCMAKE_TOOLCHAIN_FILE=C:\/vcpkg\/scripts\/buildsystems\/vcpkg.cmake -DVCPKG_TARGET_TRIPLET=x64-windows ..\/ ``` Unfortunately cmake cannot find the package... which I do not understand why. The error message is: ``` [cmake] CMake Error at C:\/vcpkg\/scripts\/buildsystems\/vcpkg.cmake:896 (_find_package): [cmake] Could not find a package configuration file provided by \"JKQtPlotter\" with [cmake] any of the following names: [cmake] [cmake] JKQtPlotterConfig.cmake [cmake] jkqtplotter-config.cmake [cmake] [cmake] Add the installation prefix of \"JKQtPlotter\" to CMAKE_PREFIX_PATH or set [cmake] \"JKQtPlotter_DIR\" to a directory containing one of the above files. If [cmake] \"JKQtPlotter\" provides a separate development package or SDK, be sure it [cmake] has been installed. ``` In the folder .\\installed\\x64-windows\\share\\jkqtplotter there is a file named \"JKQTPlotterSharedLibConfig.cmake\". So I suspect that i do not have the right name. Thus, I tried instead ``` find_package(JKQtPlotterSharedLib CONFIG REQUIRED) ``` but still not found. I also tried different spelling: ``` jkqtplotter ``` , ``` jkqtplottersharedlib ``` , ``` JKQTPlotterSharedLib ``` , ``` JKQtPlotterLib ``` , ``` JKQTPlotterLib ``` , but I cannot have cmake find this library. I tried the same on macOS and fedora, and it yields the exact same problem. With other libraries (openssl or boost) I do not have this problem. What I'm doing wrong? How can I use vcpkg successfully? [EDIT] My dirty solution: I went into the folder .\\installed\\x64-windows\\share\\jkqtplotter And I copied all cmake files using the following PowerShell command: ``` Get-Children -Filter \"*SharedLib*\" | ForEach-Object { $newName = $_.Name -replace \"SharedLib\", '' $destination = Join-Path -Path $_.Directory - ChildPath $newName Copy-Item -Path $_.FullName -Destination $destination } ``` Then changed my CMakeLists.txt to: ``` find_package(JKQtPlotter CONFIG REQUIRED) ``` And now it works (tested on windows and macOS, not fedora). But obviously it's fundamentally wrong...",
    "author_id":5928,
    "publication_date":1754168755000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"ToreSupra",
    "author_reputation":17.0,
    "tags":"qt, cmake, vcpkg",
    "text_length":2796,
    "title_length":50,
    "num_tags":3
  },
  {
    "id":6466,
    "title":"How can I track Cloud SQL backup costs per instance in GCP if resource.id is missing in the billing export?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723602\/how-can-i-track-cloud-sql-backup-costs-per-instance-in-gcp-if-resource-id-is-mis",
    "text":"I'm trying to analyze and optimize the cost of Cloud SQL backups in Google Cloud. In the GCP billing export, I see multiple line items with the SKU: ``` \"Cloud SQL: Backups in [region]\" (e.g., Northern Virginia) ``` Each line has different costs and usage amounts throughout the day, but no resource.id is attached, so I can't tell which Cloud SQL instance the charges belong to. What I'm trying to understand: How can I attribute backup costs to specific Cloud SQL instances? Is there a way to correlate billing lines with instance names or labels? How can I identify if a particular backup is unnecessary or consuming excessive storage? I’ve already tried: Checking for labels in the billing export (none are present) Looking at Cloud SQL monitoring metrics (backup_storage_used) Searching Cloud Audit Logs for backup events But none of these give me a clear cost breakdown per SQL instance. Any tips, tools, or workflows to bridge the gap between backup costs and specific Cloud SQL instances?",
    "author_id":5927,
    "publication_date":1754169079000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Tomer S",
    "author_reputation":1082.0,
    "tags":"google-cloud-platform, google-cloud-sql, google-cloud-billing",
    "text_length":996,
    "title_length":107,
    "num_tags":3
  },
  {
    "id":6465,
    "title":"C# Modular Ability System and Tooltips (GameDev)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723603\/c-modular-ability-system-and-tooltips-gamedev",
    "text":"I am prototyping an ability system where each ability just applies a number of effects. I am doing this in Godot, but for now I am just concerned with pure C# and as such removed all the Godot stuff from my code examples. I want nicely writable\/readable tooltips for my abilities but struggle with how to do it. For readability I already opted into SmartFormat, but I wonder if I really need to use plain ``` object[] ``` arrays and similar. I have an abstract ``` BaseEffect ``` : ``` public abstract class BaseEffect { public abstract object ReturnParams(); } ``` And two concrete implementations inheriting from the ``` BaseEffect ``` : ``` public class AttackEffect : BaseEffect { private int _damageValue = 64; public override object ReturnParams() { return new {damageValue = _damageValue}; } } ``` ``` public class DebuffEffect : BaseEffect { private int _debuffNumber = 2; private string _debuffName = \"burn\"; public override object ReturnParams() { return new { debuffNumber = _debuffNumber, debuffName = _debuffName }; } } ``` In addition, I have an ``` Ability ``` class that contains an array of effects. The same kind of effect should probably be able to be included multiple times (deal 10 damage and then deal 7 damage or something). ``` using System.Linq; using SmartFormat; public class Ability { private BaseEffect[] _effects; private string _tooltip = \"Deal {0.damageValue} damage. Apply {1.debuffNumber} {1.debuffName}.\"; void UpdateTooltip() { var effectParams = GetEffectParams(); Console.WriteLine(Smart.Format(_tooltip, effectParams)); } private object[] GetEffectParams() { return _effects.Select(effect => effect.ReturnParams()).ToArray(); } } ``` This works in a way. However I don't like having to create an ``` object[] ``` from the anonymous objects returned by ``` GetEffectParams() ``` . I was considering getting just strings but I need numeric values for pluralization (during localization) Or should I try to move the part to be pluralized into the effect and have it then return a string (however would that work in every language? \"2 apples\" -> \"2 Äpfel\" would work, but do other languages perhaps change parts of verbs depending on the plural?) Or give each effect its own valid string and then try to puzzle the strings together depending on the number of effects. E.G. \"Deal 1 damage.\" (1 effect), \"Deal 1 damage and apply 1 burn\" (2 effects), \"Deal 1 damage, apply 1 burn and heal 1\" (3 effects) (benefit would be not to write the strings by hand, on the other hand formulation of the string would become quite inflexible. If there is anyone experienced that has a good idea on how to approach this kind of ability system.",
    "author_id":5926,
    "publication_date":1754169387000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Courier",
    "author_reputation":91.0,
    "tags":"c#, game-development, .net, modularity",
    "text_length":2663,
    "title_length":48,
    "num_tags":4
  },
  {
    "id":6464,
    "title":"How to format types with suffices in libfmt?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723604\/how-to-format-types-with-suffices-in-libfmt",
    "text":"I'm working as a side project on a strong physical type class template for C++. And now I'm having a problem in writing a formatter for {fmt} to display the value with the unit string properly. Here is my goal: ``` Frequency my_frequency{200.12345}; fmt::print(\"{}\", my_frequency); \/\/ Returns \"200.12345 Hz\" (or whatever the exact floating point representation is on that machine) \/\/ Here is what I'm trying to achieve but is not working fmt::print(\"{:.2f}\", my_frequency); \/\/ Should print \"200.12 Hz\" fmt::print(\"{:<15}\", my_frequency); \/\/ Should print \"200.12345 Hz \" fmt::print(\"{:^15.2f}\", my_frequency); \/\/ Should print \" 200.12 Hz \" ``` I tried building a formatter by subclassing the formatter for double and then append the unit. But this leads to results like this: ``` \/\/ with the double formatter as parent class: fmt::print(\"{:<15}\", my_frequency); \/\/ -> \"200.12345 Hz\" \/\/ ^~~~~~~~~~~~~~^ \/\/ 15 chars as per the format string plus the type's suffix ``` Another alternative I tried was deriving my formatter from ``` fmt::formatter<string_view> ``` . But then I don't have the format information for the floating point available: ``` template<> struct fmt::formatter<Frequency> : fmt::formatter<string_view> { \/\/ parsing is done by the parent class template<typename FormatContext> auto format(const Frequency& freq, FormatContext &ctx) const { auto buffer = fmt::format(\"{} Hz\", freq.value()); \/\/ formats a floating point number without any format specifiers return fmt::formatter<string_view>::format(buffer, ctx); \/\/ performs any alignment of the string } }; ``` I found a hint on the Internet to look at the chrono formatters. So, I took the formatter for ``` std::chrono::duration ``` as an example and came up with a formatter of almost 200 lines of undocumented copy, paste, and modify code, trying to closely resemble that duration formatter. However, my formatter can now print any physical type value. But as soon as I put any format specifier into the placeholder in the format string, {fmt} throws an \"unknown format specifier\" exception at me from deep in there. My main problem when using pre-existing formatters, either as base class or as members, is that they either eat too much of the format string or too less. For instance, the double formatter eats the whole format string and aligns and pads the floating point number. Or the string formatter cannot cope with the \".2f\" part of the format string. I really want to avoid writing a parser for decomposing the entire format string and then recomposing parts of it for the floating point part and the string_view formatter for handling the padding and alignment. I've tried working with ``` fmt::detail::parse_align ``` , ``` fmt::detail::parse_width ``` , and ``` fmt::detail::parse_precision ``` . But none of them catches anything from the format string. Maybe my spec handler is just wrong. None of the functions in the spec handler get invoked. 🤷 After googling, trying and testing for almost two days now, I'm stuck. Is there someone out there who can help? Maybe even some demo code for a formatter for prefix + floating point number + suffix ? Any help or pointer into the right direction will be greatly appreciated! Solution Somehow SO doesn't let me post a reply. Maybe because this question got closed in the meantime. So, I'm just appending it here. Thanks to @Alan Birtles (in the comments) the solution is using a ``` nested_formatter ``` . Here is my super simple formatter now which does everything I listed above: ``` template <util::common::physical_scalar T, typename Char> struct fmt::formatter<T, Char> : fmt::nested_formatter<typename T::value_type, Char> { using formatted_type = T; auto format(const formatted_type& aNumber, fmt::format_context& ctx) const { return this->write_padded(ctx, [=, *this](auto out) { return format_to(out, \"{} {}\", this->nested(aNumber.value()), formatted_type::dimension::unit_string()); }); } }; ```",
    "author_id":5925,
    "publication_date":1754169726000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"dodo",
    "author_reputation":29.0,
    "tags":"c++, fmt",
    "text_length":3935,
    "title_length":44,
    "num_tags":2
  },
  {
    "id":6463,
    "title":"On woocommerce search custom taxonomy doesn&#39;t work with &#39;relation&#39; =&gt; &#39;AND&#39; but it works with &#39;relation&#39; =&gt; &#39;OR&#39;",
    "link":"https:\/\/stackoverflow.com\/questions\/79723605\/on-woocommerce-search-custom-taxonomy-doesnt-work-with-relation-and-but",
    "text":"I am trying to build search for woocommerce. I am using woocommerce 9.7 and latest wordpress. And the search i want is with multiple taxonomies (categories) so I registered 2 more taxonomies on top of the default product_cat and product_brand so here is the taxonomy code: ``` function custom_product_filter_taxonomies() { register_taxonomy('vrsta', 'product', [ 'label' => 'Vrsta', 'rewrite' => ['slug' => 'vrsta'], 'hierarchical' => true, 'public' => true, 'publicly_queryable' => true, 'show_ui' => true, 'show_admin_column' => true, 'show_in_nav_menus' => true, 'show_in_rest' => true, 'query_var' => true, 'show_in_rest' => true, ]); register_taxonomy('karakteristike', 'product', [ 'label' => 'Karakteristike', 'rewrite' => ['slug' => 'karakteristike'], 'hierarchical' => true, 'public' => true, 'publicly_queryable' => true, 'show_ui' => true, 'show_admin_column' => true, 'show_in_nav_menus' => true, 'show_in_rest' => true, 'query_var' => true, 'show_in_rest' => true, ]); ``` } add_action('init', 'custom_product_filter_taxonomies'); And my search form contains: ``` input name=\"s\" input name=\"product_cat[]\" input name=\"product_brand[]\" input name=\"vrsta[]\" input name=\"kategorija[]\" input name=\"min_price\" input name=\"max_price\" ``` And here is the query i am trying to modify: ``` function woo_custom_filter_products_query($query) { if (is_admin() || !$query->is_main_query() || !is_post_type_archive('product')) return; $taxonomies = array('product_cat', 'product_brand', 'vrsta', 'karakteristike'); $filter_args = false; foreach ($taxonomies as $taxonomy) { if (isset($_GET[$taxonomy]) && !empty($_GET[$taxonomy])) { \/\/$terms = array_map('intval', array_filter((array) $_GET[$taxonomy])); $terms = array_filter($_GET[$taxonomy]); if (!empty($terms)) { $filter_args[] = array( 'taxonomy' => $taxonomy, 'field' => 'term_id', 'terms' => $terms, 'operator' => 'IN', ); } } } if(!empty($filter_args)) { $taxquery = array_merge(array('relation' => 'OR'), $filter_args); \/\/$taxquery = array( 'relation' => 'OR', $filter_args ); $query->set('tax_query', $taxquery); } \/\/ Price filtering if (isset($_GET['min_price']) || isset($_GET['max_price'])) { $meta_query = $query->get('meta_query', array()); if (isset($_GET['min_price']) && is_numeric($_GET['min_price'])) { $meta_query[] = array( 'key' => '_price', 'value' => floatval($_GET['min_price']), 'compare' => '>=', 'type' => 'NUMERIC' ); } if (isset($_GET['max_price']) && is_numeric($_GET['max_price'])) { $meta_query[] = array( 'key' => '_price', 'value' => floatval($_GET['max_price']), 'compare' => '<=', 'type' => 'NUMERIC' ); } $query->set('meta_query', $meta_query); } } add_action('pre_get_posts', 'woo_custom_filter_products_query'); ``` The problem I am running into is if I use ``` OR ``` in ``` tax_query ``` then it works if i select ``` vrsta ``` category and it shows every product from that category, for example ``` vrsta ``` ``` food ``` it shows all product selected with ``` food ``` BUT it also shows all products from other selected categories. So selecting ``` vrsta ``` ``` food ``` and ``` procuct_brand ``` ``` Good brand ``` it will show me all products from ``` good brand ``` and also all products from ``` food ``` which means it will show me products from other brands too. I was thinking that using ``` relation ``` ``` AND ``` will solve this and it will show only what was selected in ``` vrsta ``` ``` AND ``` ``` product_brand ``` however in this case it shows me There are no products at all moreover, if i select only ``` vrsta ``` ``` food ``` it is supposed only to show ``` food ``` products and not ``` drinks ``` products but it shows like taxonomy doesn't even exist so like it doesn't work at all.",
    "author_id":5924,
    "publication_date":1754169797000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"lonerunner",
    "author_reputation":1332.0,
    "tags":"php, wordpress, woocommerce, custom-taxonomy, meta-query",
    "text_length":3702,
    "title_length":154,
    "num_tags":5
  },
  {
    "id":6462,
    "title":"Using multi-char literals as enum values",
    "link":"https:\/\/stackoverflow.com\/questions\/79723607\/using-multi-char-literals-as-enum-values",
    "text":"To make debugging easier, one could set ``` enum ``` values to be multi-char literals, which could then be printed. I never saw this being done, and so I'm wondering if there are any reasons why this wouldn't be a good idea? Here's what I mean: ``` enum MyEnum { ME_FOO = 'FOO\\0', ME_BAR = 'BAR\\0', } ```",
    "author_id":5923,
    "publication_date":1754169960000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"fiji3300",
    "author_reputation":153.0,
    "tags":"c",
    "text_length":304,
    "title_length":40,
    "num_tags":1
  },
  {
    "id":6461,
    "title":"In Ruby, why does`it` get set to false after a rescue?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723612\/in-ruby-why-doesit-get-set-to-false-after-a-rescue",
    "text":"In Ruby 3.4 why does Ruby's ``` it ``` default block parameter get set to ``` false ``` after a ``` rescue ``` in a block? ``` 42.tap do puts \"it is #{it}\" raise \"uh oh\" rescue puts \"it is now #{it}\" end ``` prints: ``` it is 42 it is now false ``` If you use ``` _1 ``` instead of ``` it ``` it works as expected.",
    "author_id":5922,
    "publication_date":1754170507000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"nicholaides",
    "author_reputation":19570.0,
    "tags":"ruby",
    "text_length":314,
    "title_length":54,
    "num_tags":1
  },
  {
    "id":6460,
    "title":"Riscv 64 ALU Control Unit I-Type vs R-Type",
    "link":"https:\/\/stackoverflow.com\/questions\/79723613\/riscv-64-alu-control-unit-i-type-vs-r-type",
    "text":"I'm currently building a 64-bit single-cycle RISC-V core in systemverilog using the Computer Organization and Design RISC-V edition book. I'm having struggles decoding immediate type instructions so that they generate the right ALU instructions using the combinational logic below: ``` `include \"parameters.sv\" module alu_cont ( input logic [6:0] funct7, input logic [2:0] funct3, input logic [1:0] aluop, output logic [3:0] alucontrol ); always_comb begin case (aluop) `ALUOP_LWSW: begin alucontrol = `ALU_ADD; \/\/ Effective address: base + offset end `ALUOP_BRANCH: begin alucontrol = `ALU_SUB; \/\/ Comparator end `ALUOP_RTYPE: begin case ({funct7, funct3}) {7'b0000000, 3'b000}: alucontrol = `ALU_ADD; \/\/ ADD {7'b0100000, 3'b000}: alucontrol = `ALU_SUB; \/\/ SUB {7'b0000000, 3'b111}: alucontrol = `ALU_AND; \/\/ AND {7'b0000000, 3'b110}: alucontrol = `ALU_OR; \/\/ OR {7'b0000000, 3'b100}: alucontrol = `ALU_XOR; \/\/ XOR default: alucontrol = `ALU_INVALID; endcase end default: alucontrol = `ALU_INVALID; endcase end endmodule ``` Here is the control unit which passes the ``` aluop ``` to the ALU control unit: ``` `include \"parameters.sv\" module control_unit ( input logic [6:0] opcode, output logic branch, output logic reg_we, output logic dmu_we, output logic dmu_re, output logic mtreg, output logic alu_src, output logic pc_src, output logic [1:0] aluop ); always_comb begin branch = 0; reg_we = 0; dmu_we = 0; dmu_re = 0; mtreg = 0; alu_src = 0; pc_src = 0; aluop = `ALUOP_RTYPE; case (opcode) `C_R_TYPE: begin reg_we = 1; alu_src = 0; aluop = `ALUOP_RTYPE; end `C_IM_TYPE: begin reg_we = 1; alu_src = 1; aluop = `ALUOP_RTYPE; end `C_LW_TYPE: begin reg_we = 1; dmu_re = 1; mtreg = 1; alu_src = 1; aluop = `ALUOP_LWSW; end `C_SW_TYPE: begin dmu_we = 1; alu_src = 1; aluop = `ALUOP_LWSW; end `C_B_TYPE: begin branch = 1; alu_src = 0; aluop = `ALUOP_BRANCH; end `OP_LUI: begin reg_we = 1; alu_src = 1; mtreg = 0; end default: begin branch = 0; reg_we = 0; dmu_we = 0; dmu_re = 0; mtreg = 0; alu_src = 0; pc_src = 0; aluop = `ALUOP_RTYPE; end endcase end endmodule ``` As you can see when an immediate instruction is found I pass the R-type aluop signal and set the right control signals. From my research it mentions that this aluop signal can never be of value 2'b11 therefore, I have to use the R-type signal instead. Am I right in thinking this? How do I determine that an I-type instruction is being executed in the alu control unit so that it can execute the right ALU operation?",
    "author_id":5921,
    "publication_date":1754170629000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"C Flux",
    "author_reputation":7.0,
    "tags":"verilog, system-verilog, riscv, hdl",
    "text_length":2484,
    "title_length":42,
    "num_tags":4
  },
  {
    "id":6459,
    "title":"MongoDB Chatbot Framework Embedded Content Ingestion: Possible reason for no data in collection?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723622\/mongodb-chatbot-framework-embedded-content-ingestion-possible-reason-for-no-dat",
    "text":"I have a Node.js TypeScript project based upon MongoDB Chatbot Framework. For setting up the chatbot data, we feed parsed documents into an \"ingest\" process which is pre-provided by the framework. From this ingestion process, 2 Mongo collections are populated: ``` pages ``` and ``` embedded_content ``` . The framework takes as an input an array of Page objects, which has the definition: ``` export type Page = { url: string; title?: string; \/\/ A human-readable title. body: string; \/\/ The text of the page. format: PageFormat; \/\/ The file format of the page. This format determines how the page should be chunked and vector-embedded. sourceName: string; \/\/ Data source name. metadata?: PageMetadata; \/\/ Arbitrary metadata for page. }; ``` After ingestion, the collection ``` pages ``` contains this data in a more or less identical structure, with some added metadata and timestamps. The collection ``` embedded_content ``` however, is empty; and I can't work out why. It seems that the Mongo framework provides for different actions on the content, namely create update delete In the ``` pages ``` collection, I can see a data field ``` action: \"deleted\" ``` Yet this is the first time pushing the data in. And after all sample logs from an ingestion run read as follows: ``` {\"level\":\"info\",\"message\":\"Creating embedded content for M64:https:\/\/link\/to\/M64\"} {\"level\":\"info\",\"message\":\"Vectorizing chunk 1\/1 for M64: https:\/\/link\/to\/M64\"} {\"level\":\"info\",\"message\":\"Updating last successful run date\"} ``` A final comment to make is that this exact same framework and my use of the codebase has previously worked successfully for data taken from another source entirely. In that example, data did end up in the ``` embedded_content ``` collection. I can't figure out what is different about this data run, and I've run out of ideas of what to look for, what to inspect, what to double-check, to make sure I'm doing it correctly. Why is the data marked as \"deleted\", and not \"created\"? Why is the embedding data not being entered into the ``` embedded_content ``` collection? What is Mongo Chatbot Framework's logic for deciding the action to be done on the data?",
    "author_id":5920,
    "publication_date":1754171126000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Stewart",
    "author_reputation":18421.0,
    "tags":"mongodb, artificial-intelligence, chatbot, mongodb-atlas",
    "text_length":2166,
    "title_length":96,
    "num_tags":4
  },
  {
    "id":6458,
    "title":"Why am I getting &quot;nan&quot; when I try to overload the modulo operator in C++?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723625\/why-am-i-getting-nan-when-i-try-to-overload-the-modulo-operator-in-c",
    "text":"I am trying to overload the ``` % ``` operator in C++ to make it do division instead of modulo. For example, 5 % 4 should give me 1.25 after overloading. But I get \"nan\". Why? This is the code I tried (but I get nan as the answer): ``` #include <iostream> using namespace std; class Modulo { public: int x; Modulo(int a) { x = a; } float operator%(const Modulo& other) { float result = float(x \/ (float)other.x); } }; int main() { Modulo m1(5), m2(4); cout << m1%m2; } ```",
    "author_id":5919,
    "publication_date":1754171458000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"RAVINDRA DIVEKAR",
    "author_reputation":1.0,
    "tags":"c++, overloading, nan, operator-keyword, modulo",
    "text_length":472,
    "title_length":83,
    "num_tags":5
  },
  {
    "id":6457,
    "title":"No Data in menu with Ant Design",
    "link":"https:\/\/stackoverflow.com\/questions\/79723633\/no-data-in-menu-with-ant-design",
    "text":"I have to keep two type of dropdown in one component. And based on the props, I have to make that select to appear. But when I am using select, it's not showing data in the UI : https:\/\/i.sstatic.net\/v8XimFGo.png I use Ant Design version 5.26.7. ``` import React, { useState } from 'react'; import { Select, TreeSelect, Input, Divider } from 'antd'; import styles from '.\/styles.js'; \/\/ For styling const treeData = [ { title: 'Fruits', value: 'fruits', children: [ { title: 'Apple', value: 'apple' }, { title: 'Banana', value: 'banana' }, ], }, { title: 'Vegetables', value: 'vegetables', children: [ { title: 'Carrot', value: 'carrot' }, { title: 'Tomato', value: 'tomato' }, ], }, ]; const options = [ { value: 'apple', label: 'Apple' }, { value: 'banana', label: 'Banana' }, { value: 'carrot', label: 'Carrot' }, { value: 'tomato', label: 'Tomato' }, ]; function CapUnifiedSelect({ mode, placeholder, onChange }) { const [search, setSearch] = useState(''); const filteredOptions = options.filter(opt => opt.label.toLowerCase().includes(search.toLowerCase()) ); const filteredTreeData = treeData.map(parent => ({ ...parent, children: parent.children?.filter(child => child.title.toLowerCase().includes(search.toLowerCase()) ), })); const renderPopup = (originNode) => ( <div style={styles.popup}> <Input placeholder=\"Search...\" value={search} onChange={e => setSearch(e.target.value)} style={styles.search} \/> <Divider style={{ margin: '8px 0' }} \/> {originNode} <\/div> ); if (mode === 'select' || mode === 'multiselect') { return ( <Select mode={mode === 'multiselect' ? 'multiple' : undefined} style={{ width: '100%' }} options={filteredOptions} popupRender={renderPopup} placeholder={placeholder || 'Select an option'} onChange={onChange} \/> ); } return ( <TreeSelect treeData={filteredTreeData} multiple={mode === 'multitreeselect'} treeDefaultExpandAll style={{ width: '100%' }} dropdownRender={renderPopup} placeholder={placeholder || 'Select from tree'} onChange={onChange} \/> ); } export default CapUnifiedSelect; ```",
    "author_id":5918,
    "publication_date":1754172771000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Bhavik Devganiya",
    "author_reputation":1.0,
    "tags":"reactjs, javascript, css, antd",
    "text_length":2028,
    "title_length":31,
    "num_tags":4
  },
  {
    "id":6456,
    "title":"React Native KeyboardAvoidingView causes flickering and layout shift when keyboard opens\/closes",
    "link":"https:\/\/stackoverflow.com\/questions\/79723636\/react-native-keyboardavoidingview-causes-flickering-and-layout-shift-when-keyboa",
    "text":"I'm building a chat screen in React Native and using KeyboardAvoidingView to keep the TextInput above the keyboard. But I'm running into the following issues: When i open keyboard first time its hidden half behind the keyboard. After minimizing and re-opennig the app it fixes itself After closing the keyboard, the TextInput shifts upward slightly and stays there. ``` <SafeAreaView style={{ flex: 1, backgroundColor: colorScheme ? \"black\" : \"white\", elevation: 15, }} > <StatusBar barStyle={colorScheme ? \"light-content\" : \"dark-content\"} backgroundColor={colorScheme ? \"black\" : \"white\"} \/> <KeyboardAvoidingView behavior={Platform.OS === \"ios\" ? \"padding\" : \"height\"} keyboardVerticalOffset={Platform.OS === \"ios\" ? 60 : 0} style={{ flex: 1, }} > <View style={{ flex: 1 }}> <Text style={{ fontSize: 15, padding: 10, fontWeight: \"600\", textAlign: \"center\", color: colorScheme ? \"white\" : \"black\", }} > Chat With Bot <\/Text> <FlatList<Message> data={Messages} ref={flatListRef} style={{ flex: 1 }} keyExtractor={(item) => item.id} keyboardShouldPersistTaps=\"always\" contentContainerStyle={{ flexGrow: 1, justifyContent: \"flex-end\", padding: 5, }} renderItem={({ item }) => ( <TouchableWithoutFeedback onPress={Keyboard.dismiss}> <View style={{ borderWidth: 1, borderRadius: 20, padding: 15, maxWidth: \"80%\", margin: 5, backgroundColor: colorScheme ? \"#333333\" : \"lightgray\", borderColor: colorScheme ? \"#333333\" : \"lightgray\", alignSelf: item.sender === \"user\" ? \"flex-end\" : \"flex-start\", }} > <Text style={{ color: colorScheme ? \"white\" : \"black\", alignSelf: item.sender === \"user\" ? \"flex-end\" : \"flex-start\", }} > {item.text} <\/Text> <\/View> <\/TouchableWithoutFeedback> )} \/> <\/View> <View style={{ marginLeft: 5, flexDirection: \"row\", padding: 5, }} > <TextInput placeholder=\"Type your Message..\" placeholderTextColor={colorScheme ? \"white\" : \"black\"} style={{ borderWidth: 1, borderRadius: 30, padding: 10, paddingLeft: 20, width: \"85%\", height: 50, backgroundColor: colorScheme ? \"#333333\" : \"lightgray\", borderColor: colorScheme ? \"#333333\" : \"lightgray\", color: colorScheme ? \"white\" : \"black\", }} onChangeText={(text) => setInput(text)} value={input} \/> <TouchableOpacity style={{ alignItems: \"center\", justifyContent: \"center\", marginLeft: 10, }} onPress={sendButton} > <FontAwesome name=\"send\" size={28} color=\"gray\" \/> <\/TouchableOpacity> <\/View> <\/KeyboardAvoidingView> <\/SafeAreaView> ); } ``` What I’ve Tried: Setting behavior to \"padding\", \"height\", and \"position\" — same result. Wrapping the screen in SafeAreaView and adjusting keyboardVerticalOffset. Using keyboardShouldPersistTaps=\"handled\" on FlatList. Manually calling scrollToEnd() on the FlatList after new messages.",
    "author_id":5917,
    "publication_date":1754172863000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ahmed Ch",
    "author_reputation":1.0,
    "tags":"react-native, javascript, keyboardavoidingview",
    "text_length":2694,
    "title_length":95,
    "num_tags":3
  },
  {
    "id":6455,
    "title":"Delphi VCL DBGrid: don&#39;t save dataset\/row changes upon selecting another dataset\/row",
    "link":"https:\/\/stackoverflow.com\/questions\/79723642\/delphi-vcl-dbgrid-dont-save-dataset-row-changes-upon-selecting-another-dataset",
    "text":"I have a DBGrid component in my application. I want to modify values in several columns within a given row, but all changes should be cancelled when I click on another row. Is it possible to do something like this?",
    "author_id":5916,
    "publication_date":1754173556000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"andoryu",
    "author_reputation":11.0,
    "tags":"delphi, dbgrid",
    "text_length":214,
    "title_length":88,
    "num_tags":2
  },
  {
    "id":6454,
    "title":"Cordova apps in upgraded Android environment display the URL bar",
    "link":"https:\/\/stackoverflow.com\/questions\/79723648\/cordova-apps-in-upgraded-android-environment-display-the-url-bar",
    "text":"I am trying to prevent the URL (etc) bar being displayed in some upgraded Cordova apps that wasn’t there before the upgrade. The apps themselves are unchanged, but the environment needed updating: I upgraded Android Studio from Koala to Narwhal (2025.1.1), and thence the Cordova apps from Cordova 7.1.1 to Cordova 14.0.1. This then also necessitated an upgrade of SdkVersion 34 to 35; Gradle 7.5 to 8.14.3, AGP 7.4 to 8.11.1. The emulator is Android 15.0 (API 35) – a Pixel 8 Pro. Currently: In app\/src\/main\/assets\/www, under ``` <script type=\"text\/javascript\" src=\"js\/index.js\"><\/script> ``` the URL is invoked with ``` <script type=\"text\/javascript\">window.open(\"https:\/\/mydomainnamepage.html\"); <\/script> ``` The emulator displays the page correctly but with status bar and URL (etc) bar Possible custom-config solution: (Vide post: Add custom config into AndroidManifest.xml ) With the cordova-custom-config plugin installed: in app\/src\/main\/res\/xml\/config.xml ``` <preference name=\"android-manifest\/application\/activity\/@android:theme\" value=\"@android:style\/Theme.Light.NoTitleBar.Fullscreen\"\/> ``` Displays as above (with URL bar etc) Possible InAppBrowser solution (Vide post Cordova InAppBrowser - How to disable URL and Navigation Bar in Android ) With the InApp browser plugin installed: In app\/src\/main\/assets\/www\/index.js, under app.initialize() ``` ref = cordova.InAppBrowser.open('https:\/\/mydomainpage’, ‘hideurlbar=yes’); ``` plus ``` app\/src\/main\/res\/xml\/config.xml <feature name=\"InAppBrowser\" \/> <param name=\"android-package\" value=\"org.apache.cordova.inappbrowser.InAppBrowser\" \/> ``` Displays a grey screen Any suggestions please?",
    "author_id":5915,
    "publication_date":1754174362000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"decomplexity",
    "author_reputation":381.0,
    "tags":"android-studio, cordova, plugins, url, inappbrowser",
    "text_length":1651,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6453,
    "title":"Pygame image not facing the mouse",
    "link":"https:\/\/stackoverflow.com\/questions\/79723653\/pygame-image-not-facing-the-mouse",
    "text":"Here is my rotating code: ``` pos = pygame.mouse.get_pos() x_dist = pos[0] - self.rect.centerx y_dist = -(pos[1] - self.rect.centery) self.angle = math.degrees(math.atan2(y_dist, x_dist)) self.image = pygame.transform.rotate(self.original_image, self.angle) ``` Here is my image: The problem is, when I rotate it, I have to subtract the angle by 90 to get the right result. I have searched all over stack overflow but nothing works.",
    "author_id":5914,
    "publication_date":1754175193000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aadvik",
    "author_reputation":624.0,
    "tags":"python, pygame, math, degrees, radians",
    "text_length":432,
    "title_length":33,
    "num_tags":5
  },
  {
    "id":6452,
    "title":"Using ASW Javascript SDK for S3 in the browser without Node.js",
    "link":"https:\/\/stackoverflow.com\/questions\/79723658\/using-asw-javascript-sdk-for-s3-in-the-browser-without-node-js",
    "text":"TLDR: How do i get the AWS S3 SDK (with Cognito aswell) in the browser without using Node.js? Im trying to build a verry simple website and use Cognito with js for authenticating to AWS S3 buckets. I have 2 roles Unauth and Auth (for now) that I want to use to check whether a user acan upload data or not. So far I was using the v2 SDK to try and get this done, but The tutoarials and everything else were inconsistent and many didn't work. I was trying to rewrite the site usingthe v3 SDK, but can't find it on their website. All of teh links mention how it's webpack based now and that it needs to be built, but there aren't any actuial links to v3 sdk and where to build it. I found one github project that shows how to build it using Node, but im not using any framenworks for this site. Is there a way to get, build, and use it in the browser similar to the v2 SDK?",
    "author_id":5913,
    "publication_date":1754176273000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"TableTopRug",
    "author_reputation":41.0,
    "tags":"javascript, amazon-web-services, amazon-s3",
    "text_length":871,
    "title_length":62,
    "num_tags":3
  },
  {
    "id":6451,
    "title":"Why does SIMD[DType.bool, 4](True, False, True) give unexpected reduce_bit_count() results in Mojo?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723661\/why-does-simddtype-bool-4true-false-true-give-unexpected-reduce-bit-count",
    "text":"I'm investigating the method reduce_bit_count () from Mojo's SIMD API. According to the documentation: Returns the total number of bits set in the ``` SIMD ``` vector. I expected the following behavior: ``` 3 for [True, False, True, True] # 3 True values 2 for [True, False, True, False] 2 for [True, False, True] # assumed missing value is False ``` But in practice, only the first two cases behave as expected. Here's the code I'm using: ``` def main(): var all_bits_tftt = SIMD[DType.bool, 4](True, False, True, True) print('all_bits [tftt]', all_bits_tftt, all_bits_tftt.reduce_bit_count()) var all_bits_tftf = SIMD[DType.bool, 4](True, False, True, False) print('all_bits [tftf]', all_bits_tftf, all_bits_tftf.reduce_bit_count()) var all_bits_tft = SIMD[DType.bool, 4](True, False, True) print('all_bits [tft]', all_bits_tft, all_bits_tft.reduce_bit_count()) print(all_bits_tft.reduce_bit_count()) # behaves differently (!) ``` And there are results: ``` all_bits [tftt] [True, False, True, True] 3 all_bits [tftf] [True, False, True, False] 2 all_bits [tft] [True, False, True, False] 0 1 ``` Observations: The first two lines give the expected counts. The third case prints the SIMD vector as [True, False, True, False] — so I would expect the result to be 2. But I get 0 or 1, depending on how I print the result: If printed in a string with other text: 0 If printed directly: 1 Questions: Why does ``` SIMD[DType.bool, 4](True, False, True) ``` fill the last value with ``` False ``` , but then ``` reduce_bit_count() ``` behaves inconsistently? Why does the return value of ``` reduce_bit_count() ``` depend on whether there’s a string before it in ``` print() ``` ? For reference, I'm using Mojo version: ``` Mojo 25.5.0.dev2025072805 (64e41dbf) ```",
    "author_id":5706,
    "publication_date":1754176640000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Daniel",
    "author_reputation":15619.0,
    "tags":"undefined-behavior, mojolang",
    "text_length":1760,
    "title_length":99,
    "num_tags":2
  },
  {
    "id":6450,
    "title":"how to solve the error of &#39;Table&#39; object has no attribute &#39;userId&#39;",
    "link":"https:\/\/stackoverflow.com\/questions\/79723667\/how-to-solve-the-error-of-table-object-has-no-attribute-userid",
    "text":"Please how do I solve this error, it occure every time i tried to load the page. this is the full error i am experiencing: ``` [2025-08-02 23:32:25,966] ERROR in app: Exception on \/admin\/createadmin [GET] Traceback (most recent call last): File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\flask\\app.py\", line 1511, in wsgi_app response = self.full_dispatch_request() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\flask\\app.py\", line 919, in full_dispatch_request rv = self.handle_user_exception(e) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\flask\\app.py\", line 917, in full_dispatch_request rv = self.dispatch_request() ^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\flask\\app.py\", line 902, in dispatch_request return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args) # type: ignore[no-any-return] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\myproject\\admin.py\", line 89, in createadmin found_admin = db.session.scalars(db.select(Users).where(Users.role == \"Admin\")).first() ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\scoping.py\", line 1905, in scalars return self._proxied.scalars( ^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 2473, in scalars return self._execute_internal( ^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\session.py\", line 2251, in _execute_internal result: Result[Any] = compile_state_cls.orm_execute_statement( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\context.py\", line 306, in orm_execute_statement result = conn.execute( ^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1415, in execute return meth( ^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py\", line 523, in _execute_on_connection return connection._execute_clauseelement( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py\", line 1629, in _execute_clauseelement compiled_sql, extracted_params, cache_hit = elem._compile_w_cache( ^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py\", line 711, in _compile_w_cache compiled_sql = self._compiler( ^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py\", line 320, in _compiler return dialect.statement_compiler(dialect, self, **kw) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\compiler.py\", line 1446, in __init__ Compiled.__init__(self, dialect, statement, **kwargs) File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\compiler.py\", line 886, in __init__ self.string = self.process(self.statement, **compile_kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\compiler.py\", line 932, in process return obj._compiler_dispatch(self, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\visitors.py\", line 141, in _compiler_dispatch return meth(self, **kw) # type: ignore # noqa: E501 ^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\compiler.py\", line 4728, in visit_select compile_state = select_stmt._compile_state_factory( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\sql\\base.py\", line 687, in create_for_statement return klass.create_for_statement(statement, compiler, **kw) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\context.py\", line 447, in create_for_statement return cls._create_orm_context( ^^^^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\context.py\", line 1175, in _create_orm_context _QueryEntity.to_compile_state( File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\context.py\", line 2628, in to_compile_state _MapperEntity( File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\context.py\", line 2708, in __init__ entity._post_inspect File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py\", line 1338, in __get__ obj.__dict__[self.__name__] = result = self.fget(obj) ^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\mapper.py\", line 2724, in _post_inspect self._check_configure() File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\mapper.py\", line 2401, in _check_configure _configure_registries({self.registry}, cascade=True) File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\mapper.py\", line 4214, in _configure_registries _do_configure_registries(registries, cascade) File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\mapper.py\", line 4255, in _do_configure_registries mapper._post_configure_properties() File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\mapper.py\", line 2418, in _post_configure_properties prop.init() File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\interfaces.py\", line 589, in init self.do_init() File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\relationships.py\", line 1655, in do_init self._process_dependent_arguments() File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\relationships.py\", line 1696, in _process_dependent_arguments rel_arg._resolve_against_registry(self._clsregistry_resolvers[1]) File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\relationships.py\", line 270, in _resolve_against_registry self.resolved = clsregistry_resolver( ^^^^^^^^^^^^^^^^^^^^^ File \"G:\\Alkhaira_Media_env\\.venv\\Lib\\site-packages\\sqlalchemy\\orm\\clsregistry.py\", line 533, in __call__ x = eval(self.arg, globals(), self._dict) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"<string>\", line 1, in <module> AttributeError: 'Table' object has no attribute 'userId' ``` in My FLASK\/BLUEPRINT ``` admin.py ``` this is my query i am trying to verify if there is a user with admin role in the database or create one if not exist with this query but rather i got the above error ``` found_admin = db.session.scalars(db.select(Users).where(Users.role == \"Admin\")).first() ``` ``` Models.py ``` ``` class Users(db.Model): id = db.Column(db.Integer, primary_key=True) fullname = db.Column(db.String(100)) username = db.Column(db.String(100), unique=True) email = db.Column(db.String(100), unique=True) gender = db.Column(db.String(100)) phonenumber = db.Column(db.Integer) role = db.Column(db.String(100)) Password = db.Column(db.String(100)) datecreated = db.Column(db.DateTime, default=datetime.now) assessmentSubmit = db.relationship('AssessmentSubmit', backref='users') ProjectResult = db.relationship('ProjectResult', backref='users') def __init__(self, fullname, username, email, gender, phonenumber, role, Password): self.fullname = fullname self.username = username self.email = email self.gender = gender self.phonenumber = phonenumber self.role = role self.Password=Password ``` I have read different post saying whether i miss called a column name or I should use ``` User.c.role=\"admin\" ``` which none of them work. Please guide me on what to do.",
    "author_id":5912,
    "publication_date":1754177856000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Alkhaira Media",
    "author_reputation":21.0,
    "tags":"python, flask, flask-sqlalchemy",
    "text_length":7629,
    "title_length":82,
    "num_tags":3
  },
  {
    "id":6449,
    "title":"Flutter iOS app UI shown on startup before splash",
    "link":"https:\/\/stackoverflow.com\/questions\/79723670\/flutter-ios-app-ui-shown-on-startup-before-splash",
    "text":"I’m building an app for ios\/android using flutter. When the app starts, the flow is: main.dart native splash showing dev logo (during which app start up happens) authgate app widget (extracted this from main to reduce how much work main was doing) flutter splash showing app logo (during which permissions are called) landing screen The issue I am seeing is when the iOS app starts, it briefly flashes the last viewed UI screen BEFORE the native splash screen. e.g. If I was viewing the settings screen in the app then quit the app, next time I start the app, the first thing I see before the native splash screen is a flash of the settings screen. I just wanted to know if anyone has experienced that before, and how they resolved it. I’ve googled for hours and cannot find anything similar, nor can I find anything in flutter documentation. It seems I’ve managed to do something in start up that created this issue that no one else gets? It’s driving me insane. Given that this occurs prior to native splash, I assume it is a snapshot particular to iOS since it doesn’t happen in the android app, and while it’s not a breaking issue, it doesn’t look great from a user perspective. I’ve tried adding a white background to hide snapshot, instead I got native splash then white screen then native splash again so removed the white background. Added ``` UIApplication.shared.ignoreSnapshotOnNextApplicationLaunch() ``` to AppDelegate with no change in behaviour ``` 1. Main.dart Future<void> main() async { WidgetsFlutterBinding.ensureInitialized(); tz.initializeTimeZones(); await Future.delayed(const Duration(seconds: 2)); await Firebase.initializeApp(options: DefaultFirebaseOptions.currentPlatform); await SystemChrome.setPreferredOrientations([ DeviceOrientation.portraitUp, ]); await Hive.initFlutter(); Hive.registerAdapter(MedicationAdapter()); \/\/ TypeID 0 Hive.registerAdapter(DoseEntryAdapter()); \/\/ TypeID 1 Hive.registerAdapter(HealthProfessionalAdapter()); \/\/ TypeID 2 Hive.registerAdapter(DiscussionItemAdapter()); \/\/ TypeID 3 Hive.registerAdapter(AppointmentAdapter()); \/\/ TypeID 4 Hive.registerAdapter(UserProfileAdapter()); \/\/ TypeID 5 Hive.registerAdapter(TodoItemAdapter()); \/\/ TypeID 7 Hive.registerAdapter(DiaryEntryAdapter()); \/\/ TypeID 8 Hive.registerAdapter(SelectedTrackersAdapter()); \/\/ TypeID 11 Hive.registerAdapter(SelectedGoalsAdapter()); \/\/ TypeID 70 Hive.registerAdapter(GpsActivityEntryAdapter()); \/\/ TypeID 60 Hive.registerAdapter(WeightEntryAdapter()); \/\/ TypeID 50 Hive.registerAdapter(WeightGoalAdapter()); \/\/ TypeID 31 Hive.registerAdapter(ActivityGoalAdapter()); \/\/ TypeID 28 Hive.registerAdapter(DailyGoalProgressAdapter()); \/\/ TypeID 33 Hive.registerAdapter(WeeklyWeightEntryAdapter()); \/\/ TypeID 32 Hive.registerAdapter(MenstrualCycleEntryAdapter()); \/\/ TypeID 43 Hive.registerAdapter(WorkoutActivityAdapter()); \/\/ TypeID 41 Hive.registerAdapter(MeditationGoalAdapter()); \/\/ TypeID 98 Hive.registerAdapter(DailyMeditationProgressAdapter()); \/\/ TypeID 97 await Hive.openBox('settings'); await Hive.openBox('medicationAlertsSent'); await Hive.openBox<MenstrualCycleEntry>('menstrualCycleBox'); await Hive.openBox<GpsActivityEntry>('gps_activities'); await Hive.openBox<GpsActivityEntry>('gpsActivity'); await NotificationService.initialize(); await LocalisationSettingsService.init(); \/\/ Load saved locale final savedLocaleCode = LocalisationSettingsService.getSelectedLocale(); final initialLocale = savedLocaleCode != null ? Locale(savedLocaleCode) : null; runApp( MultiProvider( providers: [ ChangeNotifierProvider(create: (_) => ThemeNotifier()), ChangeNotifierProvider(create: (_) => LocaleNotifier()..setLocale(initialLocale)), ], child: const MyApp(), ), ); } class MyApp extends StatelessWidget { const MyApp({super.key}); @override Widget build(BuildContext context) { final themeNotifier = Provider.of<ThemeNotifier>(context); final localeNotifier = Provider.of<LocaleNotifier>(context); return MaterialApp( title: 'Pebbl', navigatorKey: navigatorKey, theme: AppTheme.lightTheme, darkTheme: AppTheme.darkTheme, themeMode: themeNotifier.themeMode, debugShowCheckedModeBanner: false, home: const NativeSplashWrapper(), locale: localeNotifier.locale, localizationsDelegates: const [ AppLocalizations.delegate, GlobalMaterialLocalizations.delegate, GlobalCupertinoLocalizations.delegate, GlobalWidgetsLocalizations.delegate, ], supportedLocales: AppLocalizations.supportedLocales, localeResolutionCallback: (locale, supportedLocales) { for (var supportedLocale in supportedLocales) { if (supportedLocale.languageCode == locale?.languageCode && supportedLocale.countryCode == locale?.countryCode) { return supportedLocale; } } return const Locale('en', 'GB'); } ); } } ``` ``` 2. Native splash wrapper import 'package:flutter\/material.dart'; import 'package:pebbl\/screens\/login\/auth_gate.dart'; \/\/ ✅ Now this works class NativeSplashWrapper extends StatefulWidget { const NativeSplashWrapper({super.key}); @override State<NativeSplashWrapper> createState() => _NativeSplashWrapperState(); } class _NativeSplashWrapperState extends State<NativeSplashWrapper> { bool _ready = false; @override void initState() { super.initState(); _init(); } Future<void> _init() async { await Future.delayed(const Duration(milliseconds: 100)); if (mounted) { setState(() { _ready = true; }); } } @override Widget build(BuildContext context) { if (!_ready) { return const Scaffold( backgroundColor: Color(0xFFF5F5F5), body: Center( child: Image( image: AssetImage('assets\/images\/pebbl_logo.png'), width: 200, height: 200, ), ), ); } return const AuthGate(); } } ``` ``` 3. Pubspec.yaml (snippet for flutter native splash) flutter_native_splash: color: \"#F5F5F5\" image: assets\/images\/dev_logo.png fullscreen: true android: true ios: true android_12: image: assets\/images\/android12_dev_logo.png ``` ``` 4. Auth gate import 'package:flutter\/material.dart'; import 'package:firebase_auth\/firebase_auth.dart'; import 'package:pebbl\/screens\/login\/splash_screen.dart'; import 'package:pebbl\/screens\/login\/login_screen.dart'; import 'package:pebbl\/widgets\/pebbl_app.dart'; \/\/ We’ll extract PebblApp too (see below) class AuthGate extends StatelessWidget { const AuthGate({super.key}); @override Widget build(BuildContext context) { return StreamBuilder<User?>( stream: FirebaseAuth.instance.authStateChanges(), builder: (context, authSnapshot) { final isLoggedIn = authSnapshot.hasData; final screen = isLoggedIn ? SplashScreen() : const LoginScreen(); return PebblApp(initialScreen: screen); }, ); } } ``` ``` 5. Pebbl app widget import 'package:flutter\/material.dart'; import 'package:pebbl\/screens\/landing\/landing_screen.dart'; import 'package:pebbl\/services\/notification_service.dart'; import 'package:pebbl\/utils\/logger.dart'; import 'package:pebbl\/main.dart'; \/\/ for navigatorKey class PebblApp extends StatefulWidget { final Widget initialScreen; const PebblApp({super.key, required this.initialScreen}); @override State<PebblApp> createState() => _PebblAppState(); } class _PebblAppState extends State<PebblApp> with WidgetsBindingObserver { @override void initState() { super.initState(); WidgetsBinding.instance.addObserver(this); } @override void dispose() { WidgetsBinding.instance.removeObserver(this); super.dispose(); } @override void didChangeAppLifecycleState(AppLifecycleState state) { if (state == AppLifecycleState.resumed) { final payload = NotificationService.getAndClearPendingPayload(); if (payload != null) { Logger.log('App resumed. Routing to payload: $payload'); navigatorKey.currentState?.pushAndRemoveUntil( MaterialPageRoute(builder: (_) => LandingScreen(payload: payload)), (route) => false, ); } } } @override Widget build(BuildContext context) { return GestureDetector( onTap: () => FocusScope.of(context).unfocus(), child: widget.initialScreen, ); } } ``` ``` 6. Splash screen import 'package:flutter\/foundation.dart'; import 'package:pebbl\/utils\/logger.dart'; import 'dart:io'; import 'package:device_info_plus\/device_info_plus.dart'; import 'package:flutter\/material.dart'; import 'package:firebase_auth\/firebase_auth.dart'; import 'package:package_info_plus\/package_info_plus.dart'; import 'package:pebbl\/screens\/landing\/landing_screen.dart'; import 'package:pebbl\/screens\/login\/login_screen.dart'; import 'package:permission_handler\/permission_handler.dart'; import 'package:pebbl\/services\/notification_service.dart'; import 'package:geolocator\/geolocator.dart'; class SplashScreen extends StatefulWidget { final String? payloadOverride; const SplashScreen({super.key, this.payloadOverride}); @override State<SplashScreen> createState() => _SplashScreenState(); } class _SplashScreenState extends State<SplashScreen> with SingleTickerProviderStateMixin { late AnimationController _controller; late Animation<double> _animation; String _version = ''; @override void initState() { super.initState(); _controller = AnimationController( duration: const Duration(seconds: 2), vsync: this, )..forward(); _animation = CurvedAnimation( parent: _controller, curve: Curves.easeInOut, ); _init(); } Future<void> _init() async { try { final info = await PackageInfo.fromPlatform(); setState(() { _version = 'v${info.version}+${info.buildNumber}'; }); if (kDebugMode) { Logger.log('✅ Package info loaded'); } } catch (e) { if (kDebugMode) { Logger.log('❌ Package info failed: $e'); } _version = 'v1.0.0'; } \/\/ 🔒 iOS location permission check try { if (Platform.isIOS) { final serviceEnabled = await Geolocator.isLocationServiceEnabled(); if (kDebugMode) { Logger.log('✅ [iOS] Location service enabled: $serviceEnabled'); } if (serviceEnabled) { var permission = await Geolocator.checkPermission(); if (kDebugMode) { Logger.log('✅ [iOS] Location permission status: $permission'); } if (permission == LocationPermission.denied) { permission = await Geolocator.requestPermission(); if (kDebugMode) { Logger.log('✅ [iOS] Location permission requested: $permission'); } } } } } catch (e) { if (kDebugMode) { Logger.log('iOS Location permission request failed: $e'); } } \/\/ ✅ Android 13+ notification permission try { if (Platform.isAndroid) { final androidInfo = await DeviceInfoPlugin().androidInfo; if (kDebugMode) { Logger.log('✅ Android SDK: ${androidInfo.version.sdkInt}'); } if (androidInfo.version.sdkInt >= 33) { final status = await Permission.notification.request(); if (kDebugMode) { Logger.log('✅ Notification permission: $status'); } if (!status.isGranted) { if (kDebugMode) { Logger.log('Notification permission denied'); } \/\/ Optionally show a dialog here } } } } catch (e) { if (kDebugMode) { Logger.log('Notification permission request failed: $e'); } } \/\/ ✅ Android location permissions: foreground and background try { if (Platform.isAndroid) { \/\/ ✅ 1. Check and request notification permission (Android 13+) final notifStatus = await Permission.notification.status; if (!notifStatus.isGranted) { if (kDebugMode) { Logger.log('🔔 Requesting notification permission...'); } final notifResult = await Permission.notification.request(); if (kDebugMode) { Logger.log('🔔 Notification permission result: $notifResult'); } } \/\/ ✅ 2. Check foreground location permission if (kDebugMode) { Logger.log('📍 Checking foreground location permission...'); } var fgStatus = await Permission.location.status; if (kDebugMode) { Logger.log('📍 Foreground location permission status: $fgStatus'); } if (!fgStatus.isGranted) { if (kDebugMode) { Logger.log('📍 Requesting foreground location permission...'); } fgStatus = await Permission.location.request(); if (kDebugMode) { Logger.log('📍 Foreground location permission result: $fgStatus'); } } \/\/ ✅ 3. Check background location permission (if foreground granted) if (fgStatus.isGranted) { if (kDebugMode) { Logger.log('📍 Checking background location permission...'); } var bgStatus = await Permission.locationAlways.status; if (kDebugMode) { Logger.log('📍 Background location permission status: $bgStatus'); } if (!bgStatus.isGranted) { if (kDebugMode) { Logger.log('📍 Requesting background location permission...'); } bgStatus = await Permission.locationAlways.request(); if (kDebugMode) { Logger.log('📍 Background location permission result: $bgStatus'); } } else { if (kDebugMode) { Logger.log('✅ Background location permission already granted'); } } } else { if (kDebugMode) { Logger.log('❌ Foreground location denied, skipping background request'); } } } } catch (e) { if (kDebugMode) { Logger.log('❗ Permission request failed: $e'); } } await Future.delayed(const Duration(seconds: 2)); if (!mounted) return; final user = FirebaseAuth.instance.currentUser; if (kDebugMode) { Logger.log('SplashScreen: currentUser = ${user?.uid}'); } final payload = widget.payloadOverride ?? NotificationService.getInitialPayload() ?? NotificationService.consumePendingPayload(); if (kDebugMode) { Logger.log('SplashScreen init: Firebase user: ${user?.uid}, payload: $payload'); } if (kDebugMode) { Logger.log('SplashScreen: payload resolved = $payload'); } if (user != null) { try { if (kDebugMode) { Logger.log('Calling rescheduleAllReminders...'); } await NotificationService.rescheduleAllReminders() .timeout(const Duration(seconds: 5)); if (kDebugMode) { Logger.log('rescheduleAllReminders completed'); } } catch (e) { if (kDebugMode) { Logger.log('rescheduleAllReminders failed or timed out: $e'); } } } if (!mounted) return; Navigator.of(context).pushReplacement( MaterialPageRoute( builder: (_) => user == null ? const LoginScreen() : LandingScreen(payload: payload), ), ); } @override void dispose() { _controller.dispose(); super.dispose(); } @override Widget build(BuildContext context) { return Scaffold( backgroundColor: const Color(0xFFF5F5F5), body: Column( mainAxisAlignment: MainAxisAlignment.center, children: [ const Spacer(), FadeTransition( opacity: _animation, child: Center( child: Image.asset( 'assets\/images\/pebbl_logo.png', width: 200, height: 200, ), ), ), const Spacer(), Padding( padding: const EdgeInsets.only(bottom: 20), child: Text( _version, style: const TextStyle(color: Colors.black54), ), ), ], ), ); } } ``` ``` AppDelegate import UIKit import Flutter import UserNotifications import AVFAudio import GoogleMaps \/\/ 👈 Add this import for Maps @main @objc class AppDelegate: FlutterAppDelegate { \/\/lazy var flutterEngine = FlutterEngine(name: \"my_engine\") override func application( _ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]? ) -> Bool { UIApplication.shared.ignoreSnapshotOnNextApplicationLaunch() \/\/ Start engine (plugins auto-register when the engine is attached to the view) \/\/flutterEngine.run() \/\/ Set up audio playback category do { try AVAudioSession.sharedInstance().setCategory( .playback, mode: .default, options: [.mixWithOthers] ) try AVAudioSession.sharedInstance().setActive(true) } catch { print(\"Failed to set audio session category.\") } \/\/ ✅ Register Google Maps API key GMSServices.provideAPIKey(\"key\") \/\/ 👈 Replace with your actual key \/\/ Ensure notification taps are delivered to Flutter UNUserNotificationCenter.current().delegate = self return super.application(application, didFinishLaunchingWithOptions: launchOptions) func applicationWillResignActive(_ application: UIApplication) { \/\/ Add a white view over the window before backgrounding let whiteView = UIView(frame: window?.bounds ?? .zero) whiteView.backgroundColor = UIColor.white whiteView.tag = 999 \/\/ So we can remove it later window?.addSubview(whiteView) } func applicationDidBecomeActive(_ application: UIApplication) { \/\/ Remove the white view when app becomes active if let whiteView = window?.viewWithTag(999) { whiteView.removeFromSuperview() } } } } ``` ``` SceneDelegate import UIKit import Flutter class SceneDelegate: UIResponder, UIWindowSceneDelegate { var window: UIWindow? func scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) { guard let windowScene = (scene as? UIWindowScene) else { return } let window = UIWindow(windowScene: windowScene) \/\/let appDelegate = UIApplication.shared.delegate as! AppDelegate \/\/let flutterEngine = appDelegate.flutterEngine let flutterViewController = FlutterViewController() GeneratedPluginRegistrant.register(with: flutterViewController.engine) window.rootViewController = flutterViewController self.window = window window.makeKeyAndVisible() } } ```",
    "author_id":5911,
    "publication_date":1754178201000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"KGNZ",
    "author_reputation":33.0,
    "tags":"ios, flutter, startup",
    "text_length":16333,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":6448,
    "title":"Zoom webhook validation in PHP failing but not details provided in their UI",
    "link":"https:\/\/stackoverflow.com\/questions\/79723671\/zoom-webhook-validation-in-php-failing-but-not-details-provided-in-their-ui",
    "text":"I wrote some basic PHP code using AI to validate the webhook now that it's mandatory in Zoom and based on docs \/ results I was going to look at testing it myself and when calling validate from the Zoom UI and use debug and dump to log instead using the file put contents commented out code. I assume the validation logic should be identical for everyone? The zoom UI validation fails without any details or response. There's no easy way to test the logic and I don't see a PHP zoom sdk. The below is my attempt to support web hook events and validate the web hook from the zoom management UI when clicking the validate button under the web hook url. Only a generic error and no details are available. I am also trying to log to file on the server to troubleshoot but it's a horizontally scaled and load balanced app so have to track the server and file and scale down to one instance. What is the correct validation logic if the docs and code aren't correct for today using PHP? ``` <?php ini_set('display_errors','Off'); error_reporting(0); require_once(\"..\/libraries\/inj\/classes\/jboot.php\"); inj\\jboot(__DIR__ . '\/..\/'); defined('_JEXEC') or die('No direct access.'); define('DEBUG', false); \/\/ Replace this with your actual Zoom Webhook Secret $webhookSecret = '<secret token>'; \/\/ not using the deprecated value of verification token \/\/ Get raw POST body and headers $requestBody = file_get_contents('php:\/\/input'); $headers = getallheaders(); $signature = $headers['x-zm-signature'] ?? ''; $timestamp = $headers['x-zm-request-timestamp'] ?? ''; \/\/ Validate Zoom signature $message = $timestamp . $requestBody; $expectedSignature = base64_encode(hash_hmac('sha256', $message, $webhookSecret, true)); \/\/ Output debugging info if enabled if (DEBUG) { echo \"<h2>Zoom Webhook Debug<\/h2>\"; echo \"<strong>Signature from Zoom:<\/strong> $signature<br>\"; echo \"<strong>Expected Signature:<\/strong> $expectedSignature<br>\"; echo \"<strong>Timestamp:<\/strong> $timestamp<br>\"; echo \"<strong>Raw Request Body:<\/strong><pre>$requestBody<\/pre>\"; } if (!hash_equals($expectedSignature, $signature)) { http_response_code(401); echo 'Invalid signature'; exit; } \/\/ Decode payload $payload = json_decode($requestBody, true); $event = $payload['event'] ?? ''; \/\/ Handle Zoom URL validation if ($event === 'endpoint.url_validation') { $plainToken = $payload['payload']['plainToken'] ?? ''; $encryptedToken = hash_hmac('sha256', $plainToken, $webhookSecret); if (DEBUG) { echo \"<h3>URL Validation<\/h3>\"; echo \"<strong>plainToken:<\/strong> $plainToken<br>\"; echo \"<strong>encryptedToken:<\/strong> $encryptedToken<br>\"; } header('Content-Type: application\/json'); echo json_encode([ 'plainToken' => $plainToken, 'encryptedToken' => $encryptedToken, ]); exit; } \/\/ ✅ Process other Zoom events (e.g., meeting.ended, recording.completed, etc.) $eventData = $payload['payload']['object'] ?? []; if (DEBUG) { echo \"<h3>Zoom Event Received<\/h3>\"; echo \"<strong>Event Type:<\/strong> $event<br>\"; echo \"<pre>\" . json_encode($eventData, JSON_PRETTY_PRINT) . \"<\/pre>\"; } \/\/ Example: Log to file (or use your own handler) \/\/file_put_contents(__DIR__ . '\/zoom-events.log', json_encode([ \/\/ 'event' => $event, \/\/ 'data' => $eventData, \/\/ 'time' => date('c') \/\/], JSON_PRETTY_PRINT) . PHP_EOL, FILE_APPEND); http_response_code(200); echo DEBUG ? \"<strong style='color: green;'>Event processed successfully.<\/strong>\" : 'OK'; ?> ```",
    "author_id":5910,
    "publication_date":1754178258000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"fr332lanc3",
    "author_reputation":187.0,
    "tags":"php, validation, zoom-sdk",
    "text_length":3397,
    "title_length":75,
    "num_tags":3
  },
  {
    "id":6447,
    "title":"Fail on generating the querydsl metamodel with java 21 and querydsl 5.1.0 with a @MappedSuperclass that have a validation annotation",
    "link":"https:\/\/stackoverflow.com\/questions\/79723672\/fail-on-generating-the-querydsl-metamodel-with-java-21-and-querydsl-5-1-0-with-a",
    "text":"You can download this sample here I use this stack: java 21 (OpenJDK Runtime Environment build 21.0.8+9) maven 3.9.10 jakarta-persistence 3.1.0 jakarta-annotation 2.1.1 querydsl.version 5.1.0 (querydsl-apt, querydsl-jpa) hibernate-validator 8.0.2.Final I have a @MappedSuperclass with a @NotNull annotation (or any other validation annotation): ``` import jakarta.persistence.Id; import jakarta.persistence.MappedSuperclass; import jakarta.validation.constraints.NotNull; @MappedSuperclass public abstract class AbstractMappedSuperclass<T> { @Id private Long id; @NotNull private T something; } ``` and an Entity that extends that superclass: ``` import jakarta.persistence.Entity; @Entity public class Ent extends AbstractMappedSuperclass<String> { } ``` My pom: ``` <project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/xsd\/maven-4.0.0.xsd\"> <modelVersion>4.0.0<\/modelVersion> <artifactId>qMetamodelGen<\/artifactId> <groupId>com.mgsCatDev<\/groupId> <version>0.0.1-SNAPSHOT<\/version> <properties> <java.version>21<\/java.version> <jakarta-persistence.version>3.1.0<\/jakarta-persistence.version> <jakarta-annotation.version>2.1.1<\/jakarta-annotation.version> <querydsl.version>5.1.0<\/querydsl.version> <hibernate-validator.version>8.0.2.Final<\/hibernate-validator.version> <\/properties> <dependencies> <dependency> <groupId>jakarta.persistence<\/groupId> <artifactId>jakarta.persistence-api<\/artifactId> <version>${jakarta-persistence.version}<\/version> <\/dependency> <dependency> <groupId>jakarta.annotation<\/groupId> <artifactId>jakarta.annotation-api<\/artifactId> <version>${jakarta-annotation.version}<\/version> <\/dependency> <dependency> <groupId>com.querydsl<\/groupId> <artifactId>querydsl-jpa<\/artifactId> <version>${querydsl.version}<\/version> <classifier>jakarta<\/classifier> <\/dependency> <dependency> <artifactId>hibernate-validator<\/artifactId> <groupId>org.hibernate.validator<\/groupId> <version>${hibernate-validator.version}<\/version> <\/dependency> <\/dependencies> <profiles> <profile> <id>METAMODEL<\/id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins<\/groupId> <artifactId>maven-compiler-plugin<\/artifactId> <configuration> <release>${java.version}<\/release> <annotationProcessorPaths> <path> <groupId>com.querydsl<\/groupId> <artifactId>querydsl-apt<\/artifactId> <classifier>jakarta<\/classifier> <version>${querydsl.version}<\/version> <\/path> <path> <groupId>com.querydsl<\/groupId> <artifactId>querydsl-jpa<\/artifactId> <classifier>jakarta<\/classifier> <version>${querydsl.version}<\/version> <\/path> <path> <groupId>jakarta.persistence<\/groupId> <artifactId>jakarta.persistence-api<\/artifactId> <version>${jakarta-persistence.version}<\/version> <\/path> <path> <groupId>jakarta.annotation<\/groupId> <artifactId>jakarta.annotation-api<\/artifactId> <version>${jakarta-annotation.version}<\/version> <\/path> <\/annotationProcessorPaths> <\/configuration> <\/plugin> <\/plugins> <\/build> <\/profile> <\/profiles> <build> <pluginManagement> <plugins> <plugin> <groupId>org.apache.maven.plugins<\/groupId> <artifactId>maven-compiler-plugin<\/artifactId> <configuration> <release>${java.version}<\/release> <\/configuration> <\/plugin> <\/plugins> <\/pluginManagement> <\/build> <\/project> ``` then, to generate the metamodel with maven I execute: ``` mvn -X -P METAMODEL clean compile ``` and get this error: ``` Caused by: java.lang.IllegalStateException: Did not find type T in AbstractMappedSuperclass<?> for Ent at com.querydsl.codegen.TypeResolver.resolveVar (TypeResolver.java:74) at com.querydsl.codegen.TypeResolver.resolve (TypeResolver.java:45) at com.querydsl.codegen.Property.createCopy (Property.java:92) at com.querydsl.codegen.EntityType.include (EntityType.java:262) at com.querydsl.apt.AbstractQuerydslProcessor.addSupertypeFields (AbstractQuerydslProcessor.java:416) ``` And the important thing here is just the space before T (\" T\"). I don't know why, but if you remove de validation annotation (@NotNull, or any other validation annotation) then there is no error and the metamodel is correctly generated. The problem is only with bean validation annotations. If the validation annotation is keep, on debugging you can see that the method com.querydsl.codegen.TypeResolver.resolveVar(Type resolved, String varName, Type declaringType, EntityType context) uses this: ``` if (Objects.equals(getVarName(param), varName)) { index = i; } ``` where varName = \"T\" and getVarName(param) = \" T\" (whith that white space at the start), so the equals returns false. Maybe something is wrong with Hibernate validation 8.0.2.Final, or OpenJDK 21.0.8+9 or simply, querydsl needs to trim the strings: ``` if (Objects.equals(getVarName(param).trim(), varName.trim())) { index = i; } ``` Am I doing something wrong or indeed there is a bug somewhere? same discussion on github",
    "author_id":5909,
    "publication_date":1754178303000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mgsCatDev",
    "author_reputation":146.0,
    "tags":"querydsl, hibernate-validator, java-21",
    "text_length":4913,
    "title_length":132,
    "num_tags":3
  },
  {
    "id":6446,
    "title":"Repetitive ownership checks for nested resources in Spring Boot with JWT authentication",
    "link":"https:\/\/stackoverflow.com\/questions\/79723677\/repetitive-ownership-checks-for-nested-resources-in-spring-boot-with-jwt-authent",
    "text":"I'm building a Spring Boot backend API with JWT authentication. The domain model is: ``` Project ``` owned by a ``` User ``` Each ``` Project ``` has many ``` Exams ``` Each ``` Exam ``` has many ``` Questions ``` Authentication is working, but authorization\/ownership checks are becoming repetitive and error-prone when I go down into nested endpoints. Examples: ``` GET \/projects ``` filters projects by the authenticated user. ``` GET \/projects\/{projectId}\/exams ``` checks that the project belongs to the user before returning exams. ``` DELETE \/projects\/{projectId}\/exams\/{examId} ``` could be abused if someone supplies a random ``` examId ``` unless I verify ownership again. Right now I end up manually passing the ``` User ``` everywhere and doing checks like ``` findAllByOwnerAndId(user, projectId) ``` for example. This repetition is spreading across controllers and services. Is there a cleaner, centralized pattern to handle ownership\/authorization for nested resources so I don't have to litter every handler with the same logic? ProjectController: ``` @RestController @RequestMapping(\"\/projects\") public class ProjectController { private final ProjectService projectService; public ProjectController(ProjectService projectService) { this.projectService = projectService; } @GetMapping public ResponseEntity<Iterable<ProjectDTO>> getAll(@AuthenticationPrincipal UserPrincipal principal) { return ResponseEntity.ok(projectService.getAllByUser(principal.getUser())); } @DeleteMapping(\"\/{projectId}\") public ResponseEntity<?> delete(@PathVariable Integer projectId, @AuthenticationPrincipal UserPrincipal principal) { projectService.delete(projectId, principal.getUser()); return ResponseEntity.ok().build(); } } ``` ExamController: ``` @RestController @RequestMapping(\"\/projects\/{projectId}\/exams\") public class ExamController { private final ExamService examService; public ExamController(ExamService examService) { this.examService = examService; } @GetMapping public ResponseEntity<Iterable<Exam>> getAll( @PathVariable Integer projectId, @AuthenticationPrincipal UserPrincipal principal ) { return ResponseEntity.ok(examService.getAllByProject(projectId, principal.getUser())); } @DeleteMapping(\"\/{examId}\") public ResponseEntity<?> delete(@PathVariable Integer examId) { examService.delete(examId); return ResponseEntity.ok().build(); } } ``` ExamService (ownership check inside): This ownership check feels very awkward. ``` public Iterable<Exam> getAllByProject(Integer projectId, User user) { Optional<Project> project = projectRepository.findAllByOwnerAndId(user, projectId); if (project.isEmpty()) { throw new ProjectNotFoundException(projectId); } return project.get().getExams(); } ``` Is there a better architectural or Spring idiomatic way to centralize or eliminate these repetitive ownership checks for nested resources (project → exam → question)?",
    "author_id":5908,
    "publication_date":1754179462000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jelles",
    "author_reputation":50.0,
    "tags":"java, spring, spring-boot",
    "text_length":2877,
    "title_length":87,
    "num_tags":3
  },
  {
    "id":6445,
    "title":"MidiPlayerJS doesn&#39;t actually play MIDI file",
    "link":"https:\/\/stackoverflow.com\/questions\/79723682\/midiplayerjs-doesnt-actually-play-midi-file",
    "text":"I wanted to make a MIDI player with vanilla JS, so I found a MIDI player module (in this case, MidiPlayerJS) and went to coding. It worked well, but after a tweak on a part of the code completely unrelated to the MIDI player part, it stopped actually playing the MIDIs. It still parsed them, but the actual MIDI events weren't being played. I checked the console logs, and I found out that it only logged the tracks that the MIDI file had. (Note that MidiPlayerJS logs every MIDI event to the console.) I also did a lot of Googling but I didn't get an answer. Any idea about what might be happening?",
    "author_id":5907,
    "publication_date":1754179985000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"ccjt",
    "author_reputation":25.0,
    "tags":"javascript, midi, midi-interface",
    "text_length":599,
    "title_length":48,
    "num_tags":3
  },
  {
    "id":6444,
    "title":"pgadmin4 postgres 16 on ubuntu 24 binary path won&#39;t validate",
    "link":"https:\/\/stackoverflow.com\/questions\/79723684\/pgadmin4-postgres-16-on-ubuntu-24-binary-path-wont-validate",
    "text":"I've tried the paths \/usr\/bin and \/usr\/lib\/postgresql\/16\/bin but they won't work. Even though psql, pg_dump utils are in there. Not sure if it's permission issues. Or what any help is appreciated.",
    "author_id":5906,
    "publication_date":1754180223000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"robbyce",
    "author_reputation":29.0,
    "tags":"pgadmin-4",
    "text_length":196,
    "title_length":64,
    "num_tags":1
  },
  {
    "id":6443,
    "title":"Supabase: Getting &quot;role &#39;user&#39; does not exist&quot; error only when authenticated user queries product\/category tables",
    "link":"https:\/\/stackoverflow.com\/questions\/79723686\/supabase-getting-role-user-does-not-exist-error-only-when-authenticated-use",
    "text":"Supabase: Getting \"role 'user' does not exist\" error only when authenticated user queries product\/category tables Problem Summary I'm experiencing a strange issue with Supabase where: When user is logged out (anonymous): GET requests to ``` \/rest\/v1\/product ``` and ``` \/rest\/v1\/category ``` return 200 OK When user is logged in (authenticated): Same requests return 400 Bad Request with error: ``` { \"code\": \"22023\", \"details\": null, \"hint\": null, \"message\": \"role \\\"user\\\" does not exist\" } ``` Database Schema I have the following tables in my Supabase project: ``` -- profiles table (contains user role info) CREATE TABLE public.profiles ( id UUID REFERENCES auth.users(id) PRIMARY KEY, name TEXT, email TEXT, role TEXT DEFAULT 'user', created_at TIMESTAMPTZ DEFAULT NOW(), updated_at TIMESTAMPTZ DEFAULT NOW() ); -- product table CREATE TABLE public.product ( id INT4 PRIMARY KEY, name VARCHAR, price NUMERIC, stock INT4, category INT4, image_url TEXT, created_at TIMESTAMPTZ DEFAULT NOW(), updated_at TIMESTAMPTZ DEFAULT NOW() ); -- category table CREATE TABLE public.category ( id INT4 PRIMARY KEY, name VARCHAR, created_at TIMESTAMPTZ DEFAULT NOW() ); ``` Custom Access Token Hook I have a custom access token hook that reads user role from profiles: ``` CREATE OR REPLACE FUNCTION public.custom_access_token_hook(event jsonb) RETURNS jsonb LANGUAGE plpgsql SECURITY DEFINER AS $$ DECLARE user_role text; user_name text; user_email text; BEGIN SELECT role, name, email INTO user_role, user_name, user_email FROM public.profiles WHERE id = (event->>'user_id')::uuid; IF user_role IS NULL THEN user_role := 'user'; END IF; RETURN jsonb_set( event, '{claims}', COALESCE(event->'claims', '{}') || jsonb_build_object( 'role', user_role, 'user_name', COALESCE(user_name, ''), 'user_email', COALESCE(user_email, '') ) ); END; $$; ``` Frontend Code ``` \/\/ This works when user is logged out const { data, error } = await supabase .from('product') .select('*'); \/\/ Same code returns error when user is logged in ``` What I've Tried Disabled RLS on product and category tables: ``` ALTER TABLE public.product DISABLE ROW LEVEL SECURITY; ALTER TABLE public.category DISABLE ROW LEVEL SECURITY; ``` Removed all RLS policies from these tables Granted permissions to anon and authenticated roles: ``` GRANT SELECT ON public.product TO anon; GRANT SELECT ON public.product TO authenticated; GRANT SELECT ON public.category TO anon; GRANT SELECT ON public.category TO authenticated; ``` Simplified the custom access token hook to just return the event without modifications Temporarily disabled the custom access token hook entirely None of these solutions worked. The error persists only when the user is authenticated. Additional Context Supabase project is relatively new The error started appearing after implementing the custom access token hook User login\/logout functionality works correctly The error specifically mentions PostgreSQL role \"user\" which suggests it's trying to use the application role as a database role Questions Why does the same query work for anonymous users but fail for authenticated users? How is the custom JWT claim ``` role: \"user\" ``` being interpreted as a PostgreSQL database role? What's the proper way to handle user roles in Supabase without causing this conflict? Environment Supabase: Latest version Frontend: Next.js with @supabase\/supabase-js Database: PostgreSQL (Supabase managed) Any help would be greatly appreciated! i want data products and category can be access anyone",
    "author_id":5905,
    "publication_date":1754180398000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Amos wijaya",
    "author_reputation":13.0,
    "tags":"postgresql, javascript, jwt, supabase, row-level-security",
    "text_length":3512,
    "title_length":131,
    "num_tags":5
  },
  {
    "id":6442,
    "title":"Message Not Received? (solved)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723687\/message-not-received-solved",
    "text":"So I've just started exploring messaging between content and background scripts in Chrome. The following works in so far as it raises the alert as required. Content ``` document.addEventListener(\"keydown\", function (event) { if (document.hasFocus()) { chrome.runtime.sendMessage({ key: event.key }, (response) => { alert(response); } ) } } ) ``` Background ``` chrome.runtime.onMessage.addListener((message, sender, response) => { const ans = { text: \"You have pressed \" + message.key }; response(ans); } ) ``` But the alert simply says \"undefined\" and does not contain the details as intended. I initially thought I maybe needed to go with response.text instead but then it ceases to work altogether. So where am I going wrong?",
    "author_id":5904,
    "publication_date":1754180548000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Baz The Bald",
    "author_reputation":1.0,
    "tags":"javascript, google-chrome-extension, messaging",
    "text_length":728,
    "title_length":30,
    "num_tags":3
  },
  {
    "id":6441,
    "title":"How do I use the files I uploaded to Google Drive via the API in an img HTML tag",
    "link":"https:\/\/stackoverflow.com\/questions\/79723692\/how-do-i-use-the-files-i-uploaded-to-google-drive-via-the-api-in-an-img-html-tag",
    "text":"I'm working on my first React full stack project, and I want to upload the media files (mainly images) on Google Drive, but also read and delete them. The uploading works fine, but when I try to display the file in an img tag, I get this error in my browser's network dev tools: Request URL: https:\/\/drive.google.com\/uc?export=view&id=1wqUbG0_NgOhnWmlRmJmjc4Fd9jOjXK7t Request Method: GET Status Code: 403 Forbidden Referrer Policy: strict-origin-when-cross-origin But it shows when I put the link in my browser. How do I display it in my frontend? Here's the upload function: ``` async function uploadFile(filePath, fileName, parentID) { try { const oauthToken = readToken() if (!oauthToken) throw new Error('No authentication token') setToken(oauthToken) const drive = google.drive({ version: 'v3', auth: oAuth2Client }) \/\/assemble file data let fileMetadata if(parentID) { fileMetadata = { name: fileName, parents: [parentID] } } else { fileMetadata = { name: fileName } } const mimeType = mime.lookup(fileName) || 'application\/octet-stream' const media = { mimeType, body: fs.createReadStream(filePath) } \/\/create the file const response = await drive.files.create({ resource: fileMetadata, media, fields: 'id, webViewLink, name, webContentLink', }) \/\/set permission await drive.permissions.create({ fileId: response.data.id, requestBody: { role: 'reader', type: 'anyone', } }) \/\/ log the permissions const permissions = await drive.permissions.list({ fileId: response.data.id }) console.log(JSON.stringify(permissions.data, null, 2)) fs.unlinkSync(filePath) \/\/ cleanup return { id: response.data.id, name: response.data.name, webViewLink: response.data.webViewLink, webContentLink: response.data.webContentLink, directLink: `https:\/\/drive.google.com\/uc?export=view&id=${response.data.id}` } } catch (error) { console.log(\"Error uploading file on Google Drive:\", error.message) } } ``` And here's the route the upload function is used in: ``` router.put('\/create', uploadPictures.single('picture'), async (req, res) => { try { \/\/get data const { name, position, featured, comment } = req.body if(!name || !position || !comment) return res.status(400).json({ error: \"All fields except picture are necessary\" }) \/\/upload picture let picture = req.file?.filename if(req.file) { const filePath = path.resolve(req.file.path) const fileName = req.file.originalname const uploaded = await uploadFile(filePath, fileName, folderID) if (!uploaded || !uploaded.webViewLink) { console.log(error) console.log(uploaded) return res.status(500).json({ error: \"Picture upload to Google Drive failed\" }) } picture = uploaded.directLink } \/\/create record const newTestimonial = new testimonialModel({ name, position, featured: featured == 'true', comment, picture }) await newTestimonial.save() \/\/respond res.status(200).json({ message: \"Testimonial created successfully\" }) } catch (error) { console.log(\"Error creating testimonial:\", error.message || error) res.status(500).json({ error: \"Backend Error: Couldn't create testimonial\" }) } }) ``` If you need any more info, pls ask.",
    "author_id":5903,
    "publication_date":1754181137000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Carlhensky Renauld Govain",
    "author_reputation":1.0,
    "tags":"reactjs, google-cloud-platform, node.js, google-api, google-drive-api",
    "text_length":3067,
    "title_length":80,
    "num_tags":5
  },
  {
    "id":6440,
    "title":"How to programmatically map Akeneo attributes to Magento 2 custom attributes from a webhook payload?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723699\/how-to-programmatically-map-akeneo-attributes-to-magento-2-custom-attributes-fro",
    "text":"I'm building a Magento 2 module to synchronize product data from Akeneo PIM using webhooks. My current approach for mapping attributes from the Akeneo payload to a Magento product is hardcoded, which is not scalable. For example, I'm currently using a static mapper class: ``` \/\/ app\/code\/Offineeds\/Webhook\/Model\/AkeneoToMagentoMapper.php public function map(array $akeneoData, ?ProductInterface $magentoProduct = null): ProductInterface { \/\/ ... $magentoProduct->setName($akeneoData['values']['name'][0]['data'] ?? null); $magentoProduct->setPrice($akeneoData['values']['price'][0]['data'][0]['amount'] ?? 0); \/\/ ... return $magentoProduct; } ``` This presents a challenge: I need to map a large number of attributes, some of which are custom EAV attributes that do not have a corresponding setter method in Magento's ``` ProductInterface ``` . The attributes in the Akeneo payload can vary per product type, so the mapping logic needs to be dynamic. I need to handle both simple string values and more complex structures like Akeneo's ``` price ``` attribute, which is an array of arrays. How can I refactor my ``` AkeneoToMagentoMapper ``` class to dynamically iterate through the Akeneo payload and programmatically set the corresponding Magento attributes, including both standard and custom EAV attributes, without hardcoding each attribute name? I am specifically looking for an approach that leverages Magento's attribute system (e.g., ``` AttributeRepositoryInterface ``` ) to find and set attributes by their code, and a clean way to handle the configurable mapping logic.",
    "author_id":5902,
    "publication_date":1754182044000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Pavan shetty",
    "author_reputation":1.0,
    "tags":"magento2, dynamic, e-commerce, inventory-management, akeneo",
    "text_length":1582,
    "title_length":100,
    "num_tags":5
  },
  {
    "id":6439,
    "title":"How to have AWS EKS pod communiate out to raspberry pies securely?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723701\/how-to-have-aws-eks-pod-communiate-out-to-raspberry-pies-securely",
    "text":"I have a scenario where I have AWS EKS hosted in one region in AWS (us-east-2). I have a pod that communicates out to raspberry pis outside via a VPN. The VPN is an OpenVPN server hosted on an EC2 instance located in us-east-1. I have successfully been able to ssh from the K8s pod to the raspberry pis but I am uncertain if this is the right approach in being secure. On the OpenVPN server, the security group has my CIDR block of AWS EKS (10.50.0.0\/16) which I would expect for it to work with SSH protocol but nothing. After running several tests in my pod I noticed the only way for me ssh to my raspberry pis is use the NAT Gateway's public IP that is assigned at creation. I grabbed the public ip (18.203.10.3\/32) and placed it in the OpenVPN server's security group and wala, it worked! However, this is where the crux of my question comes from. Is applying the NAT Gateway's public ip onto my OpenVPN server safe? I would appreciate any feedback.",
    "author_id":5901,
    "publication_date":1754182747000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dylan",
    "author_reputation":43.0,
    "tags":"ssh, amazon-web-services, aws-networking",
    "text_length":954,
    "title_length":66,
    "num_tags":3
  },
  {
    "id":6438,
    "title":"alttagcaption value assign in suitescript",
    "link":"https:\/\/stackoverflow.com\/questions\/79723703\/alttagcaption-value-assign-in-suitescript",
    "text":"The image file has attribute alttagcaption which is responsible for setting an alt attribute on the website. I can't find a way to set its value from suitescript directly on the file (or whatever this instance is). There is possibility to change it on the inventory item record in itemimages sublist, but in this case its length limited to 35 chars. Changing directly on the file pushes the field limit to 200 chars which fits my needs. Any suggestions on how to assign the value not via inventory item manipulation?",
    "author_id":5900,
    "publication_date":1754183356000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Андрей Дорошенко",
    "author_reputation":1.0,
    "tags":"netsuite, suitescript",
    "text_length":516,
    "title_length":41,
    "num_tags":2
  },
  {
    "id":6437,
    "title":"How to use Windowing TVF with Flink Table API?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723707\/how-to-use-windowing-tvf-with-flink-table-api",
    "text":"How can I use Windowing table-valued functions (TVFs) with Flink's Table API ? They seem to be available only in SQL. I am using Flink v1.20. This is important because Flink optimises Windowing TVFs with Mini-Batch and Local Aggregation optimizations . However, the regular Group Window Aggregation from Table API isn't optimised, even after setting the appropriate optimisation configuration properties. In fact, Group Window Aggregation is deprecated , but it is the only window aggregation available in Table API. In concrete, what is the equivalent of this Flink SQL snippet in Table API? ``` tableEnv.sqlQuery( \"\"\" SELECT sensor_id, window_start, window_end, COUNT(*) FROM TABLE( TUMBLE(TABLE Sensors, DESCRIPTOR(reading_timestamp), INTERVAL '1' MINUTES)) GROUP BY sensor_id, window_start, window_end \"\"\" ) ``` I tried ``` \/\/ Mini-batch settings tableConfig.setString(\"table.exec.mini-batch.enabled\", \"true\"); tableConfig.setString(\"table.exec.mini-batch.allow-latency\", \"1s\"); \/\/ Allow 1 second latency for batching tableConfig.setString(\"table.exec.mini-batch.size\", \"1000\"); \/\/ Batch size of 1000 records \/\/ Local-Global aggregation for data skew handling tableConfig.setString(\"table.optimizer.agg-phase-strategy\", \"TWO_PHASE\"); table .window(Tumble.over(lit(1).minutes()).on($(\"reading_timestamp\")).as(\"w\")) .groupBy($(\"sensor_id\"), $(\"w\")) .select( $(\"sensor_id\"), $(\"reading_timestamp\").max(), $(\"w\").rowtime(), $(\"reading_timestamp\").arrayAgg().as(\"AggregatedSensorIds\") ); ``` However the execution plan shows that it only does global aggregation without any mini batch nor local aggregation optimizations: ``` Calc(select=[sensor_id, EXPR$0, EXPR$1, EXPR$2 AS AggregatedSensorIds]) +- GroupWindowAggregate(groupBy=[sensor_id], window=[TumblingGroupWindow('w, reading_timestamp, 60000)], properties=[EXPR$1], select=[sensor_id, MAX(reading_timestamp) AS EXPR$0, ARRAY_AGG(reading_timestamp) AS EXPR$2, rowtime('w) AS EXPR$1]) +- Exchange(distribution=[hash[sensor_id]]) +- Calc(select=[sensor_id, location_code, CAST(reading_timestamp AS TIMESTAMP(3)) AS reading_timestamp, measurements]) +- WatermarkAssigner(rowtime=[reading_timestamp], watermark=[(reading_timestamp - 5000:INTERVAL DAY TO SECOND)]) +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[sensor_id, location_code, reading_timestamp, measurements]) ``` I expect either the following plan instead or some way to Window TVFs with Table API. See the MiniBatchAssigner and LocalWindowAggregate optimizations. ``` Calc(select=[sensor_id, EXPR$0, window_start, window_end, EXPR$1]) +- GlobalWindowAggregate(groupBy=[sensor_id], window=[TUMBLE(slice_end=[$slice_end], size=[1 min])], select=[sensor_id, MAX(max$0) AS EXPR$0, COUNT(count$1) AS EXPR$1, start('w$) AS window_start, end('w$) AS window_end]) +- Exchange(distribution=[hash[sensor_id]]) +- LocalWindowAggregate(groupBy=[sensor_id], window=[TUMBLE(time_col=[reading_timestamp_0], size=[1 min])], select=[sensor_id, MAX(reading_timestamp) AS max$0, COUNT(sensor_id) AS count$1, slice_end('w$) AS $slice_end]) +- Calc(select=[sensor_id, CAST(reading_timestamp AS TIMESTAMP(3)) AS reading_timestamp, reading_timestamp AS reading_timestamp_0]) +- MiniBatchAssigner(interval=[1000ms], mode=[RowTime]) +- WatermarkAssigner(rowtime=[reading_timestamp], watermark=[(reading_timestamp - 5000:INTERVAL DAY TO SECOND)]) +- TableSourceScan(table=[[default_catalog, default_database, Sensors]], fields=[sensor_id, location_code, reading_timestamp, measurements]) ```",
    "author_id":5899,
    "publication_date":1754184024000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Tom&#225;s Cerd&#225;",
    "author_reputation":1.0,
    "tags":"apache-flink, flink-streaming, flink-sql",
    "text_length":3503,
    "title_length":46,
    "num_tags":3
  },
  {
    "id":6436,
    "title":"Vertex AI `gcloud ai models list` returns &quot;Listed 0 items&quot; on a fully activated project",
    "link":"https:\/\/stackoverflow.com\/questions\/79723710\/vertex-ai-gcloud-ai-models-list-returns-listed-0-items-on-a-fully-activated",
    "text":"I am trying to access Gemini models via the Vertex AI API but am unable to see any publisher models in my project. Project ID: ``` virio-462215 ``` The Core Problem: When I run the command ``` gcloud ai models list --region=us-central1 ``` , the output is simply ``` Listed 0 items. ``` . This happens for every region I have tried. As a result, any direct API call to a model like ``` gemini-1.5-pro-latest ``` fails with a ``` 404 NOT_FOUND ``` error. Troubleshooting steps I've gone through: My Google Cloud free trial is over. I have successfully upgraded to a full, active pay-as-you-go billing account. The console confirms \"You've activated your full account\" and shows no billing health issues. I have enabled the Vertex AI API for my project in the Google Cloud Console. I have successfully authenticated my local gcloud CLI using ``` gcloud auth application-default login --scopes=openid,https:\/\/www.googleapis.com\/auth\/userinfo.email,https:\/\/www.googleapis.com\/auth\/cloud-platform ``` and have consented to all required permissions in the browser. I have tried revoking credentials ( ``` gcloud auth application-default revoke ``` ) and re-authenticating multiple times. I have waited several hours after activating the billing account for all changes to propagate. The ``` gcloud ``` command runs without any authentication or permission errors; it simply finds no models. This strongly suggests a backend provisioning issue with my project's access to the model catalog. Could anyone tell me why my project is unable to list any of the standard Vertex AI publisher models? Edit: ``` gcloud ai models list --region=global ``` gives a 404 error Thank you.",
    "author_id":5898,
    "publication_date":1754185240000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ed Landau",
    "author_reputation":1028.0,
    "tags":"google-cloud-platform, gcloud, google-cloud-vertex-ai",
    "text_length":1666,
    "title_length":97,
    "num_tags":3
  },
  {
    "id":6435,
    "title":"How to add a Bridging Header in Visual Studio Code for a Swift project",
    "link":"https:\/\/stackoverflow.com\/questions\/79723717\/how-to-add-a-bridging-header-in-visual-studio-code-for-a-swift-project",
    "text":"I have installed Swift 6.1.2 on my Windows PC. I have no problem compiling my Swift project but I want to add a Bridging Header to import some C code. I have seen a few posts and articles but I am completely lost. Let’s say I have 4 files: ``` main.swift ``` (only 1 line) ``` foo() ``` ``` projectName-Bridging-Header.h ``` (only 1 line) ``` #include \"bar.h\" ``` ``` bar.h ``` (only 1 line) ``` void foo(); ``` ``` bar.c ``` (4 lines) ``` #include <stdio.h> void foo() { printf(\"Hello World!\"); } ``` How do I proceed? As it is, I always get Error: cannot find foo in scope.",
    "author_id":5897,
    "publication_date":1754187458000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mazarin64",
    "author_reputation":267.0,
    "tags":"c, swift, windows, visual-studio-code, bridging-header",
    "text_length":575,
    "title_length":70,
    "num_tags":5
  },
  {
    "id":6434,
    "title":"Can&#39;t run C# programs - &quot;The term &#39;dotnet&#39; is not recognized&quot; after installing.NET SDK",
    "link":"https:\/\/stackoverflow.com\/questions\/79723721\/cant-run-c-programs-the-term-dotnet-is-not-recognized-after-installing-n",
    "text":"I'm trying to set up a C# development environment on my Windows 10 machine. I installed the.NET 8 SDK from the official Microsoft.NET site. The installation completed without errors, but when itry to run dotnet from the command line, I get the following error: 'dotnet' is not recognized as an internal or external command, operable program or batch file. already tried: • Restarting my computer • Reinstalling the SDK • Checking the environment variables manually I checked C:\\Program Files\\dotnet and the files are there. However, it seems like the path isn't being picked up in the terminal. What I'm trying to do: Just run dotnet new console to create a basic C# app. System info: • OS: Windows 10 (64-bit) • Terminal: PowerShell & CMD • .NET SDK version: 8.0.100 Any idea what I'm missing here?",
    "author_id":5896,
    "publication_date":1754188981000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"lucaRavi",
    "author_reputation":46.0,
    "tags":"c#",
    "text_length":799,
    "title_length":108,
    "num_tags":1
  },
  {
    "id":6433,
    "title":"Apache Flink sinkfunction python code throws exception",
    "link":"https:\/\/stackoverflow.com\/questions\/79723724\/apache-flink-sinkfunction-python-code-throws-exception",
    "text":"The Apache Flink 2.1 does not support mongodb python connectors. So I make the sample python codes by using SinkFunction. ``` from pyflink.datastream import StreamExecutionEnvironment from pyflink.datastream.functions import SinkFunction from pymongo import MongoClient import json class MongoSink(SinkFunction): def __init__(self, uri, database, collection): self._uri = uri self._db = database self._coll = collection self._client = None def open(self, runtime_context): self._client = MongoClient(self._uri) self.collection = self._client[self._db][self._coll] def invoke(self, value, context): doc = value if isinstance(value, str): doc = json.loads(value) self.collection.insert_one(doc) def close(self): if self._client: self._client.close() def main(): env = StreamExecutionEnvironment.get_execution_environment() env.add_jars('file:\/\/\/home\/joseph\/flink\/jars\/flink-connector-mongodb-2.0.0-1.20.jar' ,'file:\/\/\/home\/joseph\/flink\/jars\/flink-connector-mongodb-cdc-3.0.1.jar') # your data stream ds = env.from_collection([ '{\"_id\":1, \"name\":\"Alice\"}', '{\"_id\":2, \"name\":\"Bob\"}' ]) ds.add_sink(MongoSink( uri=\"mongodb:\/\/user:pass@127.0.0.1:27017\", database=\"my_db\", collection=\"my_coll\" )) env.execute(\"PyFlink MongoDB\") if __name__ == \"__main__\": main() ``` But the exceptions are thrown from Sink class. ``` Traceback (most recent call last): File \"\/home\/joseph\/VSCode_Workspace\/etl-stream-python\/com\/aaa\/etl\/etl_data_uploader_mysql.py\", line 78, in <module> main() File \"\/home\/joseph\/VSCode_Workspace\/etl-stream-python\/com\/aaa\/etl\/etl_data_uploader_mysql.py\", line 70, in main ds.add_sink(MongoSink( File \"\/home\/joseph\/VSCode_Workspace\/.venv-etl\/lib\/python3.11\/site-packages\/pyflink\/datastream\/data_stream.py\", line 819, in add_sink return DataStreamSink(self._j_data_stream.addSink(sink_func.get_java_function())) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/home\/joseph\/VSCode_Workspace\/.venv-etl\/lib\/python3.11\/site-packages\/pyflink\/datastream\/functions.py\", line 586, in get_java_function return self._j_function ^^^^^^^^^^^^^^^^ AttributeError: 'MongoSink' object has no attribute '_j_function' ``` I want to know if I can make sink class with pyflink 2.1 or not. Kindly inform me the python MongoDB sink class example codes.",
    "author_id":5895,
    "publication_date":1754190005000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Joseph Hwang",
    "author_reputation":1431.0,
    "tags":"python, apache-flink, pyflink",
    "text_length":2226,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6432,
    "title":"Why is my find command saving errors to a file while discarding output, despite using 2&gt;&amp;1?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723726\/why-is-my-find-command-saving-errors-to-a-file-while-discarding-output-despite",
    "text":"I expected this command in linux will save all the errors but actually it saved all the stdout can anyone explain why this happend ? ``` find \/ -name \"*.py\" 2>&1 > \/dev\/null > nx1.txt ``` this command I think should save the errors ``` find \/ -name \"*.py\" 2>&1 > \/dev\/null > nx1.txt ``` 2>&1 send error and output in the same way \/dev\/null send output to blackhole and > nx1.txt just save the erros but it saved just the errors",
    "author_id":5894,
    "publication_date":1754190270000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Sorenyt Mikelyt",
    "author_reputation":11.0,
    "tags":"bash, linux, ubuntu, find, command",
    "text_length":427,
    "title_length":98,
    "num_tags":5
  },
  {
    "id":6431,
    "title":"How can I make TypeScript narrow the value of an indexed type based on the key type?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723727\/how-can-i-make-typescript-narrow-the-value-of-an-indexed-type-based-on-the-key-t",
    "text":"Playground ``` declare class A { a: string } declare class B { b: string } declare class C { c: string } declare const obj: { a: (_: A) => void b: (_: B) => void c: (_: C) => void } type O = typeof obj type K = keyof O function f<const T extends K>(k: T, v: Parameters<O[T]>[0]) { k \/\/^? v \/\/^? obj[k](v) } ``` The above code will fail with a message like below. ``` Argument of type 'A | B | C' is not assignable to parameter of type 'A & B & C'. Type 'A' is not assignable to type 'A & B & C'. Property 'b' is missing in type 'A' but required in type 'B'.(2345) ``` And the message is quite obvious. I can't assign ``` A | B | C ``` to ``` A & B & C ``` . But I can't understand where the ``` & ``` came from. Also, typescript doesn't infer the narrowest indexed type when ``` T ``` is given. Any workaround for this? For the context: I'm working on something similar to an event emitter. I think this kind of key-to-type mapping thing is a rather common case.",
    "author_id":5893,
    "publication_date":1754190511000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Abiria",
    "author_reputation":23.0,
    "tags":"typescript, typescript-generics",
    "text_length":962,
    "title_length":84,
    "num_tags":2
  },
  {
    "id":6430,
    "title":"Failed Endpoint fetching",
    "link":"https:\/\/stackoverflow.com\/questions\/79723732\/failed-endpoint-fetching",
    "text":"I have correctly crosschecked the endpoint names, my code is running just fine but I kept a breakpoint on the get method in the user controller the code doesn't seem to enter. the html and css is just fine. https:\/\/i.sstatic.net\/nALin8PN.png ``` <script> const API_BASE_URL = 'http:\/\/localhost:7180\/api'; function getAuthToken() { return localStorage.getItem('authToken'); } function getProductIdFromUrl() { const params = new URLSearchParams(window.location.search); return params.get('id'); } async function fetchProductDetails(productId) { try { const response = await fetch(`${API_BASE_URL}\/Product\/${productId}`); if (!response.ok) throw new Error('Failed to fetch product details'); const product = await response.json(); console.log(product) displayProductDetails(product); } catch (error) { console.error(error); document.getElementById('product-details').innerHTML = '<p class=\"text-red-600\">Failed to load product details.<\/p>'; } } function displayProductDetails(product) { document.getElementById('product-image').src = product.imageUrl || 'https:\/\/via.placeholder.com\/300'; document.getElementById('product-image').alt = product.name || 'Product Image'; document.getElementById('product-name').textContent = product.name || 'No name available'; document.getElementById('product-description').textContent = product.description || 'No description available.'; document.getElementById('product-price').textContent = product.price ? `$${product.price.toFixed(2)}` : 'No price available'; } async function addToCart(productId) { const token = getAuthToken(); if (!token) { window.location.href = 'login.html'; return; } <\/script> <\/body> <\/html> ```",
    "author_id":5892,
    "publication_date":1754191957000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Obaloluwa Adenekan",
    "author_reputation":1.0,
    "tags":"javascript, rest, asp.net-core",
    "text_length":1657,
    "title_length":24,
    "num_tags":3
  },
  {
    "id":6429,
    "title":"Qt tray icon in Gnome",
    "link":"https:\/\/stackoverflow.com\/questions\/79723735\/qt-tray-icon-in-gnome",
    "text":"Why when I click on any item in Qt tray menu in Gnome the cursor becomes to busy shape even if it closes the app? How to fix this? ``` QMenu *menu = new QMenu(); QAction *closeAction = menu->addAction(\"&Close\"); QObject::connect(closeAction, &QAction::triggered, &app, &QApplication::exit); QSystemTrayIcon trayIcon; trayIcon.setIcon(icon); trayIcon.setContextMenu(menu); trayIcon.show(); ``` The busy cursor is visible when I put mouse cursor to gnome panel or any window borders. Qt6.8, Gnome 42.5, X The issue is being reproduced with an official Qt tray icon example https:\/\/doc.qt.io\/qt-6\/qtwidgets-desktop-systray-example.html",
    "author_id":5891,
    "publication_date":1754192960000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user12125591",
    "author_reputation":83.0,
    "tags":"qt, gnome, trayicon",
    "text_length":632,
    "title_length":21,
    "num_tags":3
  },
  {
    "id":6428,
    "title":"The packaging plugin for project javafx-base did not assign a file to the build artifact",
    "link":"https:\/\/stackoverflow.com\/questions\/79723737\/the-packaging-plugin-for-project-javafx-base-did-not-assign-a-file-to-the-build",
    "text":"I keep having this error: \"The packaging plugin for project javafx-base did not assign a file to the build artifact.\" Six month ago, I didn't had it; I havn't touched this project since. Is there any problem with my pom.xml? ``` <project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/xsd\/maven-4.0.0.xsd\"> <modelVersion>4.0.0<\/modelVersion> <groupId>org.example<\/groupId> <artifactId>javafx-base<\/artifactId> <version>1.0-SNAPSHOT<\/version> <properties> <maven.compiler.source>22<\/maven.compiler.source> <maven.compiler.target>22<\/maven.compiler.target> <project.build.sourceEncoding>UTF-8<\/project.build.sourceEncoding> <\/properties> <dependencies> <dependency> <groupId>org.openjfx<\/groupId> <artifactId>javafx-controls<\/artifactId> <version>22.0.1<\/version> <\/dependency> <dependency> <groupId>junit<\/groupId> <artifactId>junit<\/artifactId> <version>4.13.2<\/version> <scope>test<\/scope> <\/dependency> <dependency> <groupId>org.junit.jupiter<\/groupId> <artifactId>junit-jupiter<\/artifactId> <version>RELEASE<\/version> <scope>test<\/scope> <\/dependency> <\/dependencies> <build> <plugins> <plugin> <groupId>org.openjfx<\/groupId> <artifactId>javafx-maven-plugin<\/artifactId> <version>0.0.8<\/version> <configuration> <mainClass>g62202.dev.oxono.view.OxonoApp<\/mainClass> <\/configuration> <\/plugin> <\/plugins> <\/build> <\/project> ``` I did maven javafx install:install from intellij tool window.",
    "author_id":5890,
    "publication_date":1754193765000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aryan ARIF",
    "author_reputation":1.0,
    "tags":"javafx",
    "text_length":1519,
    "title_length":88,
    "num_tags":1
  },
  {
    "id":6427,
    "title":"R\/R studio crashing after update",
    "link":"https:\/\/stackoverflow.com\/questions\/79723740\/r-r-studio-crashing-after-update",
    "text":"I updated my to the macOS Monterrey version 12.7.6 version and decided to update both my R and RStudio but it has been crashing out ever since. I managed to get it to open but now my session gets aborted every 5 minutes. I ran that last line well before it crashed out. I was just editing my code when it happened. The error message and a screenshot of the IDE error: R Session Aborted R encountered a fatal error. The session was terminated. This is my R version: R version 4.4.1 (2024-06-14) -- \"Race for Your Life\" Copyright (C) 2024 The R Foundation for Statistical Computing Platform: x86_64-apple-darwin20 This is my RStudio version: RStudio 2024.04.0 Build 735 © 2009-2024 Posit Software, PBC \"Chocolate Cosmos\" Release (a00d0e77, 2024-04-24) for macOS I tried writing this on the terminal ``` defaults write org.R-project.R force.LANG en_US.UTF-8 ``` but it did not work. I also tried downloading different versions of RStudio.",
    "author_id":5889,
    "publication_date":1754194231000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"MARIA DI MAIO",
    "author_reputation":1.0,
    "tags":"r, macos, rstudio, fatal-error",
    "text_length":935,
    "title_length":32,
    "num_tags":4
  },
  {
    "id":6426,
    "title":"Error while trying to create JWT token in register API using TypeScript",
    "link":"https:\/\/stackoverflow.com\/questions\/79723741\/error-while-trying-to-create-jwt-token-in-register-api-using-typescript",
    "text":"``` Code: ``` ``` const newUser = new User({ username, email, password: passwordHash, organization: null, designation: null, role: \"user\", }); await newUser.save(); const token = jwt.sign( { id: newUser.id, username: newUser.username, email, organization: null, designation: null, role: \"user\", }, JWT_SECRET, { expiresIn: JWT_EXPIRES_IN, } ); }; ``` ``` Error: ``` ``` return new TSError(diagnosticText, diagnosticCodes, diagnostics); ^ TSError: ⨯ Unable to compile TypeScript: src\/controllers\/auth.controller.ts:33:21 - error TS2769: No overload matches this call. Overload 1 of 5, '(payload: string | object | Buffer<ArrayBufferLike>, secretOrPrivateKey: null, options?: (SignOptions & { algorithm: \"none\"; }) | undefined): string', gave the following error. Argument of type 'string' is not assignable to parameter of type 'null'. Overload 2 of 5, '(payload: string | object | Buffer<ArrayBufferLike>, secretOrPrivateKey: Secret | PrivateKeyInput | Buffer<ArrayBufferLike> | JsonWebKeyInput, options?: SignOptions | undefined): string', gave the following error. Type 'string' is not assignable to type 'number | StringValue | undefined'. Overload 3 of 5, '(payload: string | object | Buffer<ArrayBufferLike>, secretOrPrivateKey: Secret | PrivateKeyInput | Buffer<ArrayBufferLike> | JsonWebKeyInput, callback: SignCallback): void', gave the following error. Object literal may only specify known properties, and 'expiresIn' does not exist in type 'SignCallback'. 33 const token = jwt.sign( ~~~~ at createTSError (C:\\Users\\aman8\\Music\\OMS-Project\\OMS-backend\\node_modules\\ts-node\\src\\index.ts:859:12) at reportTSError (C:\\Users\\aman8\\Music\\OMS-Project\\OMS-backend\\node_modules\\ts-node\\src\\index.ts:863:19) at getOutput (C:\\Users\\aman8\\Music\\OMS-Project\\OMS-backend\\node_modules\\ts-node\\src\\index.ts:1077:36) at Object.compile (C:\\Users\\aman8\\Music\\OMS-Project\\OMS-backend\\node_modules\\ts-node\\src\\index.ts:1433:41) at Module.m._compile (C:\\Users\\aman8\\Music\\OMS-Project\\OMS-backend\\node_modules\\ts-node\\src\\index.ts:1617:30) at node:internal\/modules\/cjs\/loader:1895:10 at Object.require.extensions.<computed> [as .ts] (C:\\Users\\aman8\\Music\\OMS-Project\\OMS-backend\\node_modules\\ts-node\\src\\index.ts:1621:12) at Module.load (node:internal\/modules\/cjs\/loader:1465:32) at Function._load (node:internal\/modules\/cjs\/loader:1282:12) at TracingChannel.traceSync (node:diagnostics_channel:322:14) { diagnosticCodes: [ 2769 ] } ``` I was trying to create a jwt token using jsonwebtoken library using typescript but getting error.",
    "author_id":5888,
    "publication_date":1754194466000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aman Sharma",
    "author_reputation":1.0,
    "tags":"typescript, javascript, node.js, express, jwt",
    "text_length":2522,
    "title_length":71,
    "num_tags":5
  },
  {
    "id":6425,
    "title":"Re-opening a file from handle in a cross platform way",
    "link":"https:\/\/stackoverflow.com\/questions\/79723742\/re-opening-a-file-from-handle-in-a-cross-platform-way",
    "text":"Consider a situation where you have a file handle and would like to open a new file handle from it. You might want to do this because you need different access rights, or perhaps you want the two handles to have separate file offsets. You can't simply open the file again from a path because the file in question is unlinked; it's only accessible through the file handle you hold. What doesn't work (as these only create a new handle to existing file description, i.e. shared file offset): ``` DuplicateHandle ``` (Win32) ``` dup ``` ``` fcntl ``` + ``` F_DUPFD ``` I have found the following options available on various platforms and would like to know if I am missing any. Windows ``` ReOpenFile ``` (since Vista) Linux ``` open ``` + \/proc\/self\/fd\/ FreeBSD ``` open ``` + \/dev\/fd (only if fdescfs was mounted with the \"nodup\" option) ``` openat ``` + ``` O_EMPTY_PATH ``` OS X Nothing.",
    "author_id":5887,
    "publication_date":1754194934000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Chris_F",
    "author_reputation":5731.0,
    "tags":"cross-platform, file-handling, file-descriptor",
    "text_length":889,
    "title_length":53,
    "num_tags":3
  },
  {
    "id":6424,
    "title":"Combining pointer to implementation (pImpl) idiom and variadic template arguments",
    "link":"https:\/\/stackoverflow.com\/questions\/79723747\/combining-pointer-to-implementation-pimpl-idiom-and-variadic-template-argument",
    "text":"I am trying to combine the \"pointer to implementation\" (pImpl) idiom with variadic template arguments. However, I'm encountering a linker error, which I suspect might be due to a missing instantiation. Here is a simplified version of my code: foo.h ``` #pragma once class Foo { public: Foo(); template <class... Args> int Method(Args&&... args); private: class Impl; Impl* m_Impl; }; ``` foo.cpp ``` #include <utility> #include \"foo.h\" #include \"impl.h\" Foo::Foo() : m_Impl(new Foo::Impl()) { } template <class... Args> int Foo::Method(Args&&... args) { return m_Impl->Method(std::forward<Args>(args)...); } ``` impl.h ``` #pragma once #include \"foo.h\" class Foo::Impl { public: int Method(int argc, char* argv[]); }; ``` impl.cpp ``` #include <iostream> #include \"impl.h\" int Foo::Impl::Method(int argc, char* argv[]) { for (int i = 0; i < argc; ++i) std::cout << argv[i] << std::endl; return EXIT_SUCCESS; } ``` main.cpp ``` #include \"foo.h\" int main(int argc, char* argv[]) { Foo foo; return foo.Method(argc, argv); } ``` My goal is to maintain the separation between ``` Foo ``` and ``` Foo::Impl ``` . However, the linker reports that it cannot find the symbol ``` int Foo::Method<int&, char**&>(int&, char**&) ``` ( see example here ). I've tried multiple method signatures to instantiate the missing symbol, but haven't been able to resolve the issue. There seems to be a subtlety in the way this is handled that I cannot fully understand. If the issue is indeed due to a missing instantiation, hopefully it can be resolved within the ``` impl ``` files to preserve the separation and avoid contaminating the ``` foo ``` files. Thank you!",
    "author_id":5886,
    "publication_date":1754195967000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Erunehtar",
    "author_reputation":1722.0,
    "tags":"c++, variadic-templates, pimpl-idiom",
    "text_length":1645,
    "title_length":81,
    "num_tags":3
  },
  {
    "id":6423,
    "title":"How to run Mamba SSM on Kaggle?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723770\/how-to-run-mamba-ssm-on-kaggle",
    "text":"Guys trying to run Mamba SSM on Kaggle but I keep running into the same runtime error I run this command to install mamab ``` !pip install -q mamba-ssm[causal-conv1d] --no-build-isolation ``` and i run into this erro when importing mamba ``` --------------------------------------------------------------------------- ImportError Traceback (most recent call last) \/tmp\/ipykernel_35\/3732049677.py in <cell line: 0>() 1 import torch 2 import torch.nn as nn ----> 3 from mamba_ssm import Mamba 4 5 class HyperspectralMambaClassifier(nn.Module): \/usr\/local\/lib\/python3.11\/dist-packages\/mamba_ssm\/__init__.py in <module> 1 __version__ = \"2.2.5\" 2 ----> 3 from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn 4 from mamba_ssm.modules.mamba_simple import Mamba 5 from mamba_ssm.modules.mamba2 import Mamba2 \/usr\/local\/lib\/python3.11\/dist-packages\/mamba_ssm\/ops\/selective_scan_interface.py in <module> 18 from mamba_ssm.ops.triton.layer_norm import _layer_norm_fwd 19 ---> 20 import selective_scan_cuda 21 22 ImportError: \/usr\/local\/lib\/python3.11\/dist-packages\/selective_scan_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb ```",
    "author_id":5885,
    "publication_date":1754202146000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Rana Adeel Tahir",
    "author_reputation":9.0,
    "tags":"pytorch, kaggle, mamba-ssm",
    "text_length":1311,
    "title_length":31,
    "num_tags":3
  },
  {
    "id":6422,
    "title":"Why does grid-template-areas break when combining minmax() and auto-fit in CSS Grid?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723777\/why-does-grid-template-areas-break-when-combining-minmax-and-auto-fit-in-css-g",
    "text":"I’m trying to create a responsive layout using CSS Grid. The grid should have a header, main content, and a footer. I’m using ``` grid-template-areas ``` combined with ``` auto-fit ``` and ``` minmax() ``` to make the layout flexible. But when I resize the browser, the areas break, and sometimes main and footer overlap or don’t span the full width. I want each area (header, main, footer) to always be on its own row, and the grid columns should expand and shrink only within the row’s width, without breaking the layout. Is this a limitation when using ``` grid-template-areas ``` with ``` auto-fit ``` and ``` minmax() ``` ? Or is there a correct way to make the rows behave as expected while keeping it responsive? ``` .container { display: grid; grid-template-areas: \"header\" \"main\" \"footer\"; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 10px; height: 100vh; } header { grid-area: header; background: lightcoral; padding: 20px; } main { grid-area: main; background: lightblue; padding: 20px; } footer { grid-area: footer; background: lightgreen; padding: 20px; } * { box-sizing: border-box; margin: 0; padding: 0; } ``` ``` <body> <div class=\"container\"> <header>Header<\/header> <main>Main Content<\/main> <footer>Footer<\/footer> <\/div> <\/body> ```",
    "author_id":5884,
    "publication_date":1754203323000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mohammad",
    "author_reputation":29.0,
    "tags":"html, css, css-grid, responsive-design",
    "text_length":1274,
    "title_length":84,
    "num_tags":4
  },
  {
    "id":6421,
    "title":"How can I create a scheduled task under a Microsoft account without a local password on a personal PC?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723780\/how-can-i-create-a-scheduled-task-under-a-microsoft-account-without-a-local-pass",
    "text":"I'm trying to run a Python script via Task Scheduler on my personal Windows machine. My user account is a Microsoft account (not a local one), and I sign in with Windows Hello (PIN) — there’s no traditional password set. The task fails to run because Task Scheduler requires a password to authenticate the user, but I don’t have one set. Running the script manually from command prompt works fine. I have full admin access on this account and do not want to create a new local or secondary account just to schedule this task. Is there a way to get Task Scheduler to run under my Microsoft account without needing to convert to a local account or set a separate password?",
    "author_id":5883,
    "publication_date":1754203746000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Kaz-99-",
    "author_reputation":1.0,
    "tags":"windows-task-scheduler",
    "text_length":670,
    "title_length":102,
    "num_tags":1
  },
  {
    "id":6420,
    "title":"Expo React Native Firebase App iOS crashes when opening navigating to auth screens",
    "link":"https:\/\/stackoverflow.com\/questions\/79723781\/expo-react-native-firebase-app-ios-crashes-when-opening-navigating-to-auth-scree",
    "text":"I'm working on an Expo RN app with React-Native-Firebase. In the past, I've had no issues with building and releasing the app. However, it seems recently every release build I've put out (either on TestFlight or app store, development or production profile) has been crashing when I navigate my app to the login screen. I\"m thinking it's something related to Firebase auth because I can open the app without issue, but as soon as a user tries to login or sign up, the app crashes. I'm not facing this issue when I build the app locally with a development build, auth and everything else Firebase related works fine. This occurs only when I build the app for release and download from TestFlight or the App Store. RN-Firebase deps: ``` \"@react-native-firebase\/app\": \"^22.0.0\", \"@react-native-firebase\/auth\": \"^22.0.0\", \"@react-native-firebase\/crashlytics\": \"^22.0.0\", \"@react-native-firebase\/database\": \"^22.0.0\", \"@react-native-firebase\/firestore\": \"^22.0.0\", \"@react-native-firebase\/messaging\": \"^22.0.0\", \"@react-native-firebase\/storage\": \"^22.0.0\", ``` Expo\/RN reps: ``` \"expo\": \"~52.0.47\", \"expo-av\": \"~15.0.2\", \"expo-blur\": \"~14.0.3\", \"expo-build-properties\": \"~0.13.3\", \"expo-camera\": \"~16.0.18\", \"expo-constants\": \"~17.0.8\", \"expo-dev-client\": \"~5.0.20\", \"expo-document-picker\": \"~13.0.3\", \"expo-font\": \"~13.0.4\", \"expo-image-picker\": \"~16.0.6\", \"expo-linking\": \"~7.0.5\", \"expo-location\": \"~18.0.10\", \"expo-router\": \"~4.0.21\", \"expo-secure-store\": \"~14.0.1\", \"expo-splash-screen\": \"~0.29.24\", \"expo-status-bar\": \"~2.0.1\", \"expo-video-thumbnails\": \"~9.0.3\", \"lodash.clonedeep\": \"^4.5.0\", \"react\": \"18.3.1\", \"react-dom\": \"18.3.1\", \"react-native\": \"0.76.9\", ``` Thought I'd also provide this ``` AuthContext ``` that I use in the app in case it helps: ``` import { router } from 'expo-router'; import React, { useEffect, useState, createContext } from 'react'; import { Account, deleteAccount } from '..\/adapters\/Accounts'; import { onAuthStateChanged, signIn, signOut } from '..\/adapters\/Auth'; import { updateUserProperties } from '..\/utils\/analytics'; type TAuthContext = { account?: Account; deleteAccount: (accountId: string) => Promise<void>; signIn: (email: string, password: string) => void; signOut: () => void; email: string; }; const AuthContext = createContext<TAuthContext>({ account: undefined, deleteAccount, signIn, signOut, email: '', }); const AuthProvider = ({ children }: { children: React.ReactNode }) => { const [account, setAccount] = useState<Account>(); const [email, setEmail] = useState(''); useEffect(() => { const unsubscribe = onAuthStateChanged( (account: Account | undefined, email: string) => { setAccount(account); setEmail(email); if (account) { const properties = { role: account.role || '', name: `${account.firstName} ${account.lastName}`, }; updateUserProperties(properties); } router.replace('\/'); }, ); return unsubscribe; }, []); return ( <AuthContext.Provider value={{ account, deleteAccount, signIn, signOut, email, }} > {children} <\/AuthContext.Provider> ); }; export { AuthContext, AuthProvider }; ``` Happy to provide any other dependencies or anything else that may help in resolving! I've tried redownloading and reconfiguring my ``` Google-Service-Info.plist ``` files and rebuilding previous commits that worked, which all seem to crash now which is leading me to believe something related to Firebase might be the issue. This is what I have from my crashlytics logs: ``` Fatal Exception: NSInvalidArgumentException -[__NSDictionaryM doubleValue]: unrecognized selector sent to instance 0x110dd3d00 5 Ollo RCTComponentData.m - Line 323 __49-[RCTComponentData createPropBlock:isShadowView:]_block_invoke_13 + 323 6 Ollo RCTComponentData.m - Line 344 __49-[RCTComponentData createPropBlock:isShadowView:]_block_invoke_2.39 + 344 7 Ollo RCTComponentData.m - Line 392 __50-[RCTComponentData setProps:forView:isShadowView:]_block_invoke + 392 ```",
    "author_id":5882,
    "publication_date":1754203856000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"christen03",
    "author_reputation":1.0,
    "tags":"react-native, firebase, expo, react-native-firebase",
    "text_length":3896,
    "title_length":82,
    "num_tags":4
  },
  {
    "id":6419,
    "title":"Aws CdK + argocd",
    "link":"https:\/\/stackoverflow.com\/questions\/79723782\/aws-cdk-argocd",
    "text":"I am looking for some solution wich can continuously does cdk deploy, cdk drift to make sure if my git state matches actual cloud state. And remediate drifts. Like terraform cloud does. Some aws blog posts ahowa how to utilise codePieline and other ci\/cd stuff, but it is sounds a little bit like reinventing the wheel. I believe there is should be some k8s opeartor or other turn-key solution",
    "author_id":5881,
    "publication_date":1754204560000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ilia Belov",
    "author_reputation":13.0,
    "tags":"kubernetes, amazon-web-services, aws-cdk, argocd, gitops",
    "text_length":393,
    "title_length":16,
    "num_tags":5
  },
  {
    "id":6418,
    "title":"dependencies between assets in airflow",
    "link":"https:\/\/stackoverflow.com\/questions\/79723784\/dependencies-between-assets-in-airflow",
    "text":"I define two ``` assets ``` in airflow 3.0.3 that after running first asset ( ``` user ``` ) second asset ( ``` user_location ``` ) run too. the code is like this: ``` from airflow.sdk import asset, Asset, Context from airflow.providers.amazon.aws.hooks.s3 import S3Hook import json @asset( uri=\"s3:\/\/csv\/users.json\", schedule=\"@daily\", group='test' ) def user(self): s3_path = self.uri.replace(\"s3:\/\/\", \"\") bucket, key = s3_path.split(\"\/\", 1) hook = S3Hook(aws_conn_id='minio_conn') content = hook.read_key(key=key, bucket_name=bucket) data = json.loads(content) return data @asset( schedule=user, ) def user_location(self, context: Context, user: Asset) -> dict[str]: user_data = context['ti'].xcom_pull( dag_id=\"user\", task_ids=\"user\", include_prior_dates=True ) return user_data['results'][0]['location'] ``` but after running the code following error happened: ``` [2025-08-03, 08:36:15] INFO - DAG bundles loaded: dags-folder, example_dags: source=\"airflow.dag_processing.bundles.manager.DagBundlesManager\" [2025-08-03, 08:36:15] INFO - Filling up the DagBag from \/opt\/airflow\/dags\/user.py: source=\"airflow.models.dagbag.DagBag\" [2025-08-03, 08:36:24] INFO - Task instance is in running state: chan=\"stdout\": source=\"task\" [2025-08-03, 08:36:24] INFO - Previous state of the Task instance: TaskInstanceState.QUEUED: chan=\"stdout\": source=\"task\" [2025-08-03, 08:36:24] INFO - Current task name:user_location: chan=\"stdout\": source=\"task\" [2025-08-03, 08:36:24] INFO - Dag name:user_location: chan=\"stdout\": source=\"task\" [2025-08-03, 08:36:24] ERROR - Task failed with exception: source=\"task\" TypeError: 'NoneType' object is not subscriptable File \"\/home\/airflow\/.local\/lib\/python3.12\/site-packages\/airflow\/sdk\/execution_time\/task_runner.py\", line 877 in run File \"\/home\/airflow\/.local\/lib\/python3.12\/site-packages\/airflow\/sdk\/execution_time\/task_runner.py\", line 1164 in _execute_task File \"\/home\/airflow\/.local\/lib\/python3.12\/site-packages\/airflow\/sdk\/bases\/operator.py\", line 397 in wrapper File \"\/home\/airflow\/.local\/lib\/python3.12\/site-packages\/airflow\/providers\/standard\/operators\/python.py\", line 217 in execute File \"\/home\/airflow\/.local\/lib\/python3.12\/site-packages\/airflow\/providers\/standard\/operators\/python.py\", line 240 in execute_callable File \"\/home\/airflow\/.local\/lib\/python3.12\/site-packages\/airflow\/sdk\/execution_time\/callback_runner.py\", line 81 in run File \"\/opt\/airflow\/dags\/user.py\", line 35 in user_location [2025-08-03, 08:36:24] INFO - Task instance in failure state: chan=\"stdout\": source=\"task\" [2025-08-03, 08:36:24] INFO - Task start: chan=\"stdout\": source=\"task\" [2025-08-03, 08:36:24] INFO - Task:<Task(_AssetMainOperator): user_location>: chan=\"stdout\": source=\"task\" [2025-08-03, 08:36:24] INFO - Failure caused by 'NoneType' object is not subscriptable: chan=\"stdout\": source=\"task\" ``` How can solve the problem?",
    "author_id":5880,
    "publication_date":1754204927000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Tavakoli",
    "author_reputation":1423.0,
    "tags":"airflow, assets, airflow-xcom",
    "text_length":2853,
    "title_length":38,
    "num_tags":3
  },
  {
    "id":6417,
    "title":"Dependency &#39;androidx.core:core:1.16.0&#39; requires Android Gradle plugin 8.6.0 or higher",
    "link":"https:\/\/stackoverflow.com\/questions\/79723785\/dependency-androidx-corecore1-16-0-requires-android-gradle-plugin-8-6-0-or-h",
    "text":"Building ``` Android ``` in ``` android studio ``` , build fail message: ``` Dependency 'androidx.core:core:1.16.0' requires Android Gradle plugin 8.6.0 or higher. Dependency 'androidx.core:core-ktx:1.16.0' requires Android Gradle plugin 8.6.0 or higher. ```",
    "author_id":5879,
    "publication_date":1754205330000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"yu yang Jian",
    "author_reputation":7263.0,
    "tags":"android",
    "text_length":258,
    "title_length":93,
    "num_tags":1
  },
  {
    "id":6416,
    "title":"I can&#39;t see why Steel-Bank Common Lisp thinks a string is a function in my code",
    "link":"https:\/\/stackoverflow.com\/questions\/79723788\/i-cant-see-why-steel-bank-common-lisp-thinks-a-string-is-a-function-in-my-code",
    "text":"I am using a Windows 11 computer, running Steel Bank Common Lisp (version 2.0.0) in PowerShell by inputting functions directly into the SBCL.exe REPL to see if they work. In the process, I have hit upon a problem with the following code. The point of this function is to take the string passed into it and trim off excess spaces (for example, ``` \"This is a string\" ``` becomes ``` \"This is a string\" ``` ) ``` (defun trim-spaces (str) (let* ((len (length(str))) (i 1) (last-char (char str (- 1 i))) (curr-char (char str i)) ) (dotimes (i len) (cond ((eql last-char #\\Space) (cond ((eql curr-char #\\Space) (setq str (remove curr-char str))))))))) (trim-spaces \"This is a string\") ``` I have made similar functions before (such as one function to remove parentheses from a string) and they have worked fine. In this case, however, if I run the above code, I get the following message. This message is the same if I input a string variable instead of a string literal: debugger invoked on a UNDEFINED-FUNCTION @1000000C2C in thread #<THREAD tid=19952 \"main thread\" RUNNING {1100BC8003}>: The function COMMON-LISP-USER::STR is undefined. I would like to know how this problem is coming about, and how to fix\/sidestep it. Thank you for your time.",
    "author_id":5878,
    "publication_date":1754205712000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ashley Ben Story",
    "author_reputation":79.0,
    "tags":"string, common-lisp, lisp, sbcl",
    "text_length":1242,
    "title_length":83,
    "num_tags":4
  },
  {
    "id":6415,
    "title":"QNetworkRequest is converting header names to lowercase for case sensitive service",
    "link":"https:\/\/stackoverflow.com\/questions\/79723789\/qnetworkrequest-is-converting-header-names-to-lowercase-for-case-sensitive-servi",
    "text":"I'm connecting Tank IP Camera (IPC) API to my windows application with Qt C++. IPC uses basic authorization. I'm using QtNetwork library for HTTP client connection, Qt 6.8.3 with C++17. I tested the API with Postman successfully. When I set the Authorization header with setRawHeader() method of the QNetworkRequest, it didn't work. Used every possible variants, but none of them worked. After 1 week discovery, I have found that the setRawHeader() method converting the header names into all lower-case letters: \"authorization\" instead of \"Authorization\". And I've found that the IPC API is case sensitive. Questions: Why QtNetwork is forcely converting header names into lower case? How to handle this situation and solve the problem? This is the header of the request from the app. An this is the header of the request from the Postman. And here is my code snippet with previously tested variants in comments: ``` void postrequest(QNetworkAccessManager *manager){ QNetworkRequest deviceInfoRequest; QString auth = \"Basic YWRtaW46MTIzNDU2\";\/\/ + QString(creds.toLocal8Bit().toBase64()); deviceInfoRequest.setUrl(QUrl(\"http:\/\/192.168.226.201:8080\/GetDeviceInfo\")); deviceInfoRequest.setRawHeader(\"Authorization\", auth.toUtf8()); \/\/ QNetworkReply *reply = manager->post(deviceInfoRequest, QByteArray(\"\")); QNetworkReply *reply = manager->sendCustomRequest(deviceInfoRequest, \"POST\"); manager->connect(manager, &QNetworkAccessManager::finished, [](QNetworkReply *reply) { reply->ignoreSslErrors(); qDebug() << QString(\"Finished. %1. %2\").arg(reply->errorString()).arg(reply->error()); deb << reply->headers(); qDebug() << \"READ: \" << reply->readAll(); }); } ```",
    "author_id":5877,
    "publication_date":1754205732000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Davronito",
    "author_reputation":41.0,
    "tags":"c++, qt, qt6, http-headers, qtnetwork",
    "text_length":1659,
    "title_length":82,
    "num_tags":5
  },
  {
    "id":6414,
    "title":"How to add profile picture (not sending an image or file) to emails in PHPMailer",
    "link":"https:\/\/stackoverflow.com\/questions\/79723791\/how-to-add-profile-picture-not-sending-an-image-or-file-to-emails-in-phpmailer",
    "text":"I am trying to send an email with a profile picture using PHPMailer ! Specifically I mean adding a profile picture to the email (not sending an image or file). Is it possible to add this feature to the code below: https:\/\/github.com\/PHPMailer\/PHPMailer#user-content-a-simple-example I searched the resources and didn't find anything, has anyone who has worked with PHP Miller been able to do this, in any way ? ``` <?php \/\/Import PHPMailer classes into the global namespace \/\/These must be at the top of your script, not inside a function use PHPMailer\\PHPMailer\\PHPMailer; use PHPMailer\\PHPMailer\\SMTP; use PHPMailer\\PHPMailer\\Exception; \/\/Load Composer's autoloader (created by composer, not included with PHPMailer) require 'vendor\/autoload.php'; \/\/Create an instance; passing `true` enables exceptions $mail = new PHPMailer(true); \/\/Server settings $mail->SMTPDebug = SMTP::DEBUG_SERVER; \/\/Enable verbose debug output $mail->isSMTP(); \/\/Send using SMTP $mail->Host = 'smtp.example.com'; \/\/Set the SMTP server to send through $mail->SMTPAuth = true; \/\/Enable SMTP authentication $mail->Username = 'user@example.com'; \/\/SMTP username $mail->Password = 'secret'; \/\/SMTP password $mail->SMTPSecure = PHPMailer::ENCRYPTION_SMTPS; \/\/Enable implicit TLS encryption $mail->Port = 465; \/\/TCP port to connect to; use 587 if you have set `SMTPSecure = PHPMailer::ENCRYPTION_STARTTLS` \/\/Recipients $mail->setFrom('from@example.com', 'Mailer'); $mail->addAddress('joe@example.net', 'Joe User'); \/\/Add a recipient $mail->addAddress('ellen@example.com'); \/\/Name is optional $mail->addReplyTo('info@example.com', 'Information'); $mail->addCC('cc@example.com'); $mail->addBCC('bcc@example.com'); \/\/Attachments $mail->addAttachment('\/var\/tmp\/file.tar.gz'); \/\/Add attachments $mail->addAttachment('\/tmp\/image.jpg', 'new.jpg'); \/\/Optional name \/\/Content $mail->isHTML(true); \/\/Set email format to HTML $mail->Subject = 'Here is the subject'; $mail->Body = 'This is the HTML message body <b>in bold!<\/b>'; $mail->AltBody = 'This is the body in plain text for non-HTML mail clients'; $mail->send(); echo 'Message has been sent'; ``` Has anyone been able to do this (is it possible) ?",
    "author_id":5876,
    "publication_date":1754206182000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"masoudiofficial",
    "author_reputation":1.0,
    "tags":"php, email, phpmailer",
    "text_length":2165,
    "title_length":80,
    "num_tags":3
  },
  {
    "id":6413,
    "title":"Rapier does not move RigidBody in expected direction",
    "link":"https:\/\/stackoverflow.com\/questions\/79723794\/rapier-does-not-move-rigidbody-in-expected-direction",
    "text":"I have a ``` RigidBody ``` with the components ``` (Velocity, Transform) ``` . When changing the rotation with ``` transform.rotation = … ``` , the “forward” direction of the body remains the same when adding velocity. How come this happens? I can’t use angular velocity, because I’m rotating alongside a camera. Thanks!",
    "author_id":5875,
    "publication_date":1754206567000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Clover Johnson",
    "author_reputation":57.0,
    "tags":"rust, bevy, rapier, rapier-3d",
    "text_length":320,
    "title_length":52,
    "num_tags":4
  },
  {
    "id":6412,
    "title":"Can&#39;t access WhatsApp Status folder using SAF or MediaStore without MANAGE_EXTERNAL_STORAGE (Flutter)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723799\/cant-access-whatsapp-status-folder-using-saf-or-mediastore-without-manage-exter",
    "text":"I'm developing an Android app that downloads WhatsApp and WhatsApp Business status files (images and videos). These status files are stored in a hidden folder under: ``` \/Android\/media\/com.whatsapp\/WhatsApp\/Media\/.Statuses\/ \/Android\/media\/com.whatsapp.w4b\/WhatsApp Business\/Media\/.Statuses\/ ``` Starting from Android 11 (API level 30), accessing these folders is restricted due to Scoped Storage. I’ve tried the following: ✅ What I've tried: 1- Storage Access Framework (SAF): I implemented SAF to request user access to the .Statuses folder. However, the folder is hidden and doesn't show in the system picker, or returns empty content even after access is granted. 2- MediaStore API: I tried querying videos and images from WhatsApp's media paths using MediaStore. Unfortunately, the .Statuses directory is not indexed, so these files aren't returned. 3- Direct file access: This works only when the app has MANAGE_EXTERNAL_STORAGE, but this permission is heavily restricted. ❌ Issue with MANAGE_EXTERNAL_STORAGE: I attempted to use the MANAGE_EXTERNAL_STORAGE permission to directly access the hidden status folder. However, Google Play Console rejected my app, stating: \"Your app is not eligible to use the All files access (MANAGE_EXTERNAL_STORAGE) permission. Only specific use cases are permitted, such as file manager apps, backup & restore apps, antivirus apps, etc.\" This means I cannot publish my app with this permission, even though it's the only method that allows access to the hidden WhatsApp status folder. ❓ My question: Is there any working, Play Store-compliant method to access the WhatsApp .Statuses folder on Android 11+ devices without using MANAGE_EXTERNAL_STORAGE? I’m looking for a solution that either: Works with MediaStore or SAF Uses user-granted permissions in a way that can access hidden folders Is accepted by Play Console policies. Whatsapp Business Controller code: ``` import 'dart:io'; import 'package:flutter\/services.dart'; import 'package:get\/get.dart'; import 'package:permission_handler\/permission_handler.dart'; import 'package:device_info_plus\/device_info_plus.dart'; import 'package:shared_preferences\/shared_preferences.dart'; class WBStatusController extends GetxController { final RxList<File> files = <File>[].obs; final RxBool isLoading = false.obs; final RxString errorMessage = ''.obs; final RxBool hasPermissions = false.obs; final RxString folderUri = ''.obs; static const MethodChannel _channel = MethodChannel('saf_channel'); final DeviceInfoPlugin deviceInfo = DeviceInfoPlugin(); @override void onInit() { super.onInit(); print('Controller initialized'); _initialize(); } Future<void> _initialize() async { print('Initializing...'); await _checkPermissions(); await _loadUri(); print('Permissions granted: \\${hasPermissions.value}'); if (hasPermissions.value) { await fetchStatuses(); } } Future<void> _checkPermissions() async { if (!GetPlatform.isAndroid) { hasPermissions.value = true; return; } final androidInfo = await deviceInfo.androidInfo; final sdkInt = androidInfo.version.sdkInt; print('Android SDK version: $sdkInt'); if (sdkInt >= 33) { \/\/ Android 13+ final photos = await Permission.photos.request(); final videos = await Permission.videos.request(); hasPermissions.value = photos.isGranted || videos.isGranted; } else { \/\/ Android 12 and below final storage = await Permission.storage.request(); hasPermissions.value = storage.isGranted; } print('Storage or media permissions granted: ${hasPermissions.value}'); } Future<void> openFolderPicker() async { try { print('Opening folder picker...'); final uri = await _channel.invokeMethod<String>('openDocumentTree'); print('URI selected: \\$uri'); if (uri != null && uri.isNotEmpty) { await _persistUri(uri); await fetchStatuses(); } } catch (e) { errorMessage.value = 'Folder access failed: \\$e'; print(errorMessage.value); Get.snackbar('Error', errorMessage.value); } } Future<void> _persistUri(String uri) async { final prefs = await SharedPreferences.getInstance(); await prefs.setString('wb_folder_uri', uri); folderUri.value = uri; print('URI persisted: \\$uri'); } Future<void> _loadUri() async { final prefs = await SharedPreferences.getInstance(); folderUri.value = prefs.getString('wb_folder_uri') ?? ''; print('Loaded URI: \\${folderUri.value}'); } Future<void> fetchStatuses() async { isLoading(true); files.clear(); errorMessage(''); print('Fetching WhatsApp Business statuses...'); try { final List<File> fetchedFiles = []; \/\/ SAF-based fetch if (folderUri.value.isNotEmpty) { print('Using SAF with URI: ${folderUri.value}'); final result = await _channel.invokeMethod<List<dynamic>>( 'listFiles', {'uri': folderUri.value}, ); if (result != null) { print('SAF returned ${result.length} items'); for (var item in result) { final localPath = item['copiedPath']; if (_isMedia(localPath)) { fetchedFiles.add(File(localPath)); } } } } \/\/ Fallback paths for WhatsApp Business if (fetchedFiles.isEmpty) { print('Trying fallback WhatsApp Business paths...'); final List<String> wbPaths = [ '\/storage\/emulated\/0\/Android\/media\/com.whatsapp.w4b\/WhatsApp Business\/Media\/.Statuses', '\/storage\/emulated\/0\/WhatsApp Business\/Media\/.Statuses', \/\/ Legacy ]; for (final path in wbPaths) { final dir = Directory(path); print('Checking: $path'); if (await dir.exists()) { final mediaFiles = dir .listSync() .whereType<File>() .where((file) => _isMedia(file.path)) .toList(); print('Found ${mediaFiles.length} media files at $path'); if (mediaFiles.isNotEmpty) { fetchedFiles.addAll(mediaFiles); break; } } else { print('Directory not found: $path'); } } } if (fetchedFiles.isNotEmpty) { files.assignAll(fetchedFiles); print('Statuses loaded: ${fetchedFiles.length}'); } else { errorMessage.value = 'No WhatsApp Business statuses found.'; Get.snackbar('No Statuses', errorMessage.value); } } catch (e) { errorMessage.value = 'Error loading statuses: $e'; print(errorMessage.value); Get.snackbar('Error', errorMessage.value); } finally { isLoading(false); print('Finished loading statuses'); } } bool _isMedia(String path) { final p = path.toLowerCase(); return p.endsWith('.jpg') || p.endsWith('.png') || p.endsWith('.mp4') || p.endsWith('.gif'); } Future<void> downloadFile(File file) async { try { print('Downloading file: \\${file.path}'); await _channel.invokeMethod('downloadFile', {'uri': file.path}); Get.snackbar('Saved', 'Saved to WBStatusSaver'); } catch (e) { print('Download failed: \\$e'); Get.snackbar('Error', 'Download failed: \\$e'); } } Future<void> deleteFile(File file) async { try { print('Deleting file: \\${file.path}'); final success = await _channel.invokeMethod<bool>('deleteFile', {'uri': file.path}); if (success == true) { files.remove(file); Get.snackbar('Deleted', 'Status deleted'); } else { throw Exception('Delete failed'); } } catch (e) { print('Delete failed: \\$e'); Get.snackbar('Error', 'Delete failed: \\$e'); } } } ``` Any help or workarounds would be greatly appreciated.",
    "author_id":5874,
    "publication_date":1754207600000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Murtaza",
    "author_reputation":557.0,
    "tags":"flutter, file-permissions, status",
    "text_length":6919,
    "title_length":105,
    "num_tags":3
  },
  {
    "id":6411,
    "title":"How to use C++ coroutine with Qt?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723800\/how-to-use-c-coroutine-with-qt",
    "text":"I am trying to use coroutines with Qt. Here's minimal(I guess) example to reproduce my problem Basically, bellow code is adopted from the example of cppreference here: https:\/\/en.cppreference.com\/w\/cpp\/coroutine\/noop_coroutine.html ``` #include <QtCore\/QCoreApplication> #include <QtCore\/QTimer> #include <coroutine> #include <utility> template<class T> struct task { struct promise_type { auto get_return_object() { return task(std::coroutine_handle<promise_type>::from_promise(*this)); } std::suspend_always initial_suspend() { return {}; } struct final_awaiter { bool await_ready() noexcept { return false; } void await_resume() noexcept {} std::coroutine_handle<> await_suspend(std::coroutine_handle<promise_type> h) noexcept { if (auto previous = h.promise().previous; previous) return previous; else return std::noop_coroutine(); } }; final_awaiter final_suspend() noexcept { return {}; } void unhandled_exception() { throw; } void return_value(T value) { result = std::move(value); } T result; std::coroutine_handle<> previous; }; task(std::coroutine_handle<promise_type> h) : coro(h) {} task(task&& t) = delete; ~task() { coro.destroy(); } struct awaiter { bool await_ready() { return false; } T await_resume() { return std::move(coro.promise().result); } auto await_suspend(std::coroutine_handle<> h) { coro.promise().previous = h; return coro; } std::coroutine_handle<promise_type> coro; }; awaiter operator co_await() { return awaiter{coro}; } T operator()() { coro.resume(); return std::move(coro.promise().result); } private: std::coroutine_handle<promise_type> coro; }; class qt_timeout { int m_delay = 0; public: qt_timeout(int delay) noexcept : m_delay{delay} {} auto await_ready() const noexcept { return false; } auto await_suspend(std::coroutine_handle<> h) noexcept { QTimer::singleShot(m_delay, qApp, [h] { h.resume(); }); } auto await_resume() noexcept {} }; auto timeout(int ms) -> task<int> { co_await qt_timeout(ms); co_return ms; } auto main(int argc, char **argv) -> int { QCoreApplication app{argc, argv}; QMetaObject::invokeMethod(&app, [] { auto t = timeout(1000); const auto r = t(); qDebug() << \"timeout\" << r; }, Qt::QueuedConnection); return app.exec(); } ``` This example tries resume coroutine in Qt's event loop. I have expected \"timeout 1000\" is printed after about 1000ms. However, it prints \"timeout 0\" and exits abnormally. How can I fix it? Edited: I have simply implemented qt_task from the code snippet from Ahmed AEK's answer. At first it looked working, but actually it doesn't: ``` #include <QtCore\/QCoreApplication> #include <QtCore\/QPointer> #include <QtCore\/QTimer> #include <coroutine> #include <utility> struct qt_task { struct promise_type { auto get_return_object() { return qt_task(std::coroutine_handle<promise_type>::from_promise(*this)); } std::suspend_always initial_suspend() { return {}; } struct final_awaiter { bool await_ready() noexcept { return false; } void await_resume() noexcept {} std::coroutine_handle<> await_suspend(std::coroutine_handle<promise_type> h) noexcept { if (auto previous = h.promise().previous; previous) { return previous; } else { h.promise().parent_object->deleteLater(); return std::noop_coroutine(); } } }; final_awaiter final_suspend() noexcept { return {}; } void unhandled_exception() { throw; } void return_void() { } std::coroutine_handle<> previous; QObject* parent_object = nullptr; }; qt_task(std::coroutine_handle<promise_type> h) : coro(h) {} qt_task(qt_task&& t) : coro{ std::exchange(t.coro, std::coroutine_handle<promise_type>{}) } { } qt_task& operator=(qt_task&& t) { std::ranges::swap(coro, t.coro); return *this; } ~qt_task() { if (coro) { coro.destroy(); } } struct awaiter { bool await_ready() { return false; } void await_resume() { } template <typename U> auto await_suspend(std::coroutine_handle<U> h) requires requires { h.promise().parent_object; } { coro.promise().previous = h; coro.promise().parent_object = h.promise().parent_object; return coro; } std::coroutine_handle<promise_type> coro; }; awaiter operator co_await() { return awaiter{ coro }; } friend void dispatch(QObject* parent, qt_task t); private: std::coroutine_handle<promise_type> coro; }; class QAsyncTask : public QObject { public: QAsyncTask(qt_task task, QObject* parent) :QObject{parent}, m_task{std::move(task)} { } qt_task m_task; }; class qt_timeout { int m_delay = 0; public: qt_timeout(int delay) noexcept : m_delay{ delay } {} auto await_ready() const noexcept { return false; } template <typename T> auto await_suspend(std::coroutine_handle<T> h) noexcept requires requires { h.promise().parent_object; } { QTimer::singleShot(m_delay, h.promise().parent_object, [h] { h.resume(); }); } auto await_resume() noexcept {} }; inline void dispatch(QObject* parent, qt_task t) { auto qtask = new QAsyncTask(std::move(t), parent); qtask->m_task.coro.promise().parent_object = qtask; qtask->m_task.coro.resume(); } auto main(int argc, char** argv) -> int { QCoreApplication app{ argc, argv }; QMetaObject::invokeMethod(&app, [&app] { dispatch(&app, [t=1] -> qt_task { co_await qt_timeout(1000); qDebug() << \"done\"; qDebug() << t; }()); }, Qt::QueuedConnection); return app.exec(); } ``` I have made qt_task only for void for simplicity. The result should be obviously \"done\\n1\" but it prints garbage value for ``` t ``` . I guess somehow the captured variable is destroyed or its memory is corrupted. How can I fix it, again?",
    "author_id":5873,
    "publication_date":1754207673000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"slyx",
    "author_reputation":2326.0,
    "tags":"c++, qt, coroutine, c++-coroutine",
    "text_length":5412,
    "title_length":33,
    "num_tags":4
  },
  {
    "id":6410,
    "title":"Programme works but says &quot;warning: integer constant is so large that it is unsigned&quot;, solution?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723801\/programme-works-but-says-warning-integer-constant-is-so-large-that-it-is-unsig",
    "text":"I am trying solve the problem posed in this question that asks << 1 operation be performed on a 64 bit number using NAND operation only and without using any arithmetic operation. My attempted solution went as follows : First I took an 8 bit number, scanned the digits by ANDing with powers of 2s, then shifted the digits leftward by ORring. ``` #include <stdio.h> #define BIT 8 int main () { unsigned char number = 121, shift = 0, digit; unsigned char power[] = {1, 2, 4, 8, 16, 32, 64, 128, 0}; for (int i = 0; i < BIT+1; i++) { digit = power[i] & number; if (number & digit != 0) {shift = shift | power[i+1];} } printf (\"8bit number = %u\\nnumber << 1 = %u\", number, shift); return 0; } ``` Once it worked I replaced the AND, OR, NOT logic with NAND logic by using the following identities derived by applying Demorgan. ``` NAND (x, y) = OR (NOT(x), NOT(y)) AND (x, y) = NAND (NAND (x, y), NAND (x, y)) OR (x, y) = NAND (NAND (x, x), NAND (y, y)) ``` Also enabled user input and made sure that it shows that the result of our custom shifter equals the result of the built in shifter. ``` #include <stdio.h> #define BIT 8 unsigned char NAND (unsigned char x, unsigned char y) { return ~x | ~y; } void bitwise_shift (unsigned char * input, unsigned char * output) { *output = *input << 1; } void nand_shift (unsigned char * input, unsigned char * output) { unsigned char digit, power[] = {1, 2, 4, 8, 16, 32, 64, 128, 0}; *output = 0; for (int i = 0; i < BIT+1; i++) { digit = NAND (NAND (*input, power[i]), NAND (*input, power[i])); if (NAND (NAND (*input, digit), NAND (*input, digit)) != 0) { *output = NAND (NAND (*output, *output), NAND (power[i+1], power[i+1])); } } } int main () { while (1) { unsigned char number, nand_shifted, bitwise_shifted; printf (\"Give me an 8-bit number x = \"); int temp = scanf (\"%u\", &number); if (temp != 1 | temp == EOF) {break;} nand_shift (&number, &nand_shifted); bitwise_shift (&number, &bitwise_shifted); printf (\"x << 1 = %u\\nx x << 1 (NAND) = %u\\n\\n\", bitwise_shifted, nand_shifted); } printf (\"!!ERROR!!\"); return 0; } ``` Once it worked I attempted to extend it to 64-bit. I changed all the ``` unsigned char ``` types to ``` unsigned long long ``` types and calculated the powers of 2's array (now containing 65 element) by running this side programme and copy pasting its output into the main program. ``` #include <stdio.h> #define BIT 64 void generate_power_array () { int base = 2, exponent = BIT-1; unsigned long long power = 1; printf (\"power[] = {1\"); for (int i = 1; i <= exponent; i++) { power *= base; printf (\", %llu\", power); } printf (\", 0};\"); } int main () { generate_power_array (); } ``` I ended up with the following which works but gives this- \"warning: integer constant is so large that it is unsigned\". ``` #include <stdio.h> #define BIT 64 unsigned long long NAND (unsigned long long x, unsigned long long y) { return ~x | ~y; } void bitwise_shift (unsigned long long * input, unsigned long long * output) { *output = *input << 1; } void nand_shift (unsigned long long * input, unsigned long long * output) { unsigned long long power[] = {1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304, 8388608, 16777216, 33554432, 67108864, 134217728, 268435456, 536870912, 1073741824, 2147483648, 4294967296, 8589934592, 17179869184, 34359738368, 68719476736, 137438953472, 274877906944, 549755813888, 1099511627776, 2199023255552, 4398046511104, 8796093022208, 17592186044416, 35184372088832, 70368744177664, 140737488355328, 281474976710656, 562949953421312, 1125899906842624, 2251799813685248, 4503599627370496, 9007199254740992, 18014398509481984, 36028797018963968, 72057594037927936, 144115188075855872, 288230376151711744, 576460752303423488, 1152921504606846976, 2305843009213693952, 4611686018427387904, 9223372036854775808, 0}; unsigned long long digit; *output = 0; for (int i = 0; i < BIT+1; i++) { digit = NAND (NAND (*input, power[i]), NAND (*input, power[i])); if (NAND (NAND (*input, digit), NAND (*input, digit)) != 0) { *output = NAND (NAND (*output, *output), NAND (power[i+1], power[i+1])); } } } int main () { while (1) { unsigned long long number, nand_shifted, bitwise_shifted; printf (\"Give me a 64-bit number x = \"); int temp = scanf (\"%llu\", &number); if (temp != 1 | temp == EOF) {break;} nand_shift (&number, &nand_shifted); bitwise_shift (&number, &bitwise_shifted); printf (\"x << 1 = %llu\\nx << 1 (NAND) = %llu\\n\\n\", bitwise_shifted, nand_shifted); } printf (\"!!ERROR!!\"); return 0; } ``` But if I do the same for 32-bit numbers, by changing all the ``` unsigned char ``` types to ``` unsigned int ``` types and by calculating the powers of 2s array using the aforementioned side programme, I get the following, which works fine and gives no warning. ``` #include <stdio.h> #define BIT 32 unsigned int NAND (unsigned int x, unsigned int y) { return ~x | ~y; } void bitwise_shift (unsigned int * input, unsigned int * output) { *output = *input << 1; } void nand_shift (unsigned int * input, unsigned int * output) { unsigned int power[] = {1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304, 8388608, 16777216, 33554432, 67108864, 134217728, 268435456, 536870912, 1073741824, 2147483648, 0}; unsigned int digit; *output = 0; for (int i = 0; i < BIT+1; i++) { digit = NAND (NAND (*input, power[i]), NAND (*input, power[i])); if (NAND (NAND (*input, digit), NAND (*input, digit)) != 0) { *output = NAND (NAND (*output, *output), NAND (power[i+1], power[i+1])); } } } int main () { while (1) { unsigned int number, nand_shifted, bitwise_shifted; printf (\"Give me an 8-bit number x = \"); int temp = scanf (\"%u\", &number); if (temp != 1 | temp == EOF) {break;} nand_shift (&number, &nand_shifted); bitwise_shift (&number, &bitwise_shifted); printf (\"x << 1 = %u\\nx x << 1 (NAND) = %u\\n\\n\", bitwise_shifted, nand_shifted); } printf (\"!!ERROR!!\"); return 0; } ``` So, I have one solution for 3 situations. For 8-bit it works, for 32-bit it works, for 64-bit it works but gives this- \"warning: integer constant is so large that it is unsigned\". Where is this warning originating from and how can it be gotten rid of?",
    "author_id":5218,
    "publication_date":1754207835000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"uran42",
    "author_reputation":105.0,
    "tags":"c, bit-manipulation, bitwise-operators, bit-shift, bitmask",
    "text_length":6263,
    "title_length":105,
    "num_tags":5
  },
  {
    "id":6409,
    "title":"How to use python to extract files from folder base on the first 7 digits file name",
    "link":"https:\/\/stackoverflow.com\/questions\/79723815\/how-to-use-python-to-extract-files-from-folder-base-on-the-first-7-digits-file-n",
    "text":"I am still a new learner in writing python script. I have a question here: I have a \"folder\" which contain multiple sub folders where I wish to extract PDF files base on the same first 7 digits file name in these sub folders (eg File name:AB12345_J,AB12345_K) and move these files to another new folder that I created as \"Duplicate Folder\". But I got a error message shown below: ``` Traceback (most recent call last) AttributeError Cell In[3], line 19 16 break 18 if not if_dupl: ---> 19 duplicateFiles.append([file_x]) 21 #state the output folder 22 Output_dir = Path('Duplicate') AttributeError: 'set' object has no attribute 'append' ``` This is my python script: ``` # Importing Libraries import os from pathlib import Path from filecmp import cmp # list of doc DATA_DIR = Path(\"folder\") files = sorted(os.listdir(DATA_DIR)) # list same content pdf_files = list(DATA_DIR.glob(\"\\*.pdf\")) duplicateFiles = set(\\[file.name\\[:6\\] for file in pdf_files\\]) # compare the documents for file_x in files: if_dupl = False for class_ in duplicateFiles: # Comparing files() class_[6] if_dupl = cmp( DATA_DIR \/ file_x, DATA_DIR \/ class_[6], shallow=False ) if if_dupl: class_.append(file_x) break if not if_dupl: duplicateFiles.append([file_x]) #state the output folder Output_dir = Path(\"Duplicate Folder\") #save to output with open(Output_dir,'wb') as outputfle: out_file.write(source_file.read()) ```",
    "author_id":5872,
    "publication_date":1754210337000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"userXM",
    "author_reputation":7.0,
    "tags":"python, jupyter-notebook",
    "text_length":1395,
    "title_length":83,
    "num_tags":2
  },
  {
    "id":6408,
    "title":"Python language features do not work with libraries installed in Poetry local virtual environment (.venv)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723817\/python-language-features-do-not-work-with-libraries-installed-in-poetry-local-vi",
    "text":"I have a Python Poetry project. Visual Studio Code features like auto-import and auto-complete do not work for the project dependency libraries. Pylance (the Python language server for Visual Studio Code) does not find dependencies, and they are highlighted with red underlining. Python interpreter is correctly configured and selected. How to fix? In the Visual Studio Code console Pylance reports: ``` 2025-08-03 08:25:50.202 [info] (6995) Auto-excluding **\/node_modules 2025-08-03 08:25:50.202 [info] (6995) Auto-excluding **\/__pycache__ 2025-08-03 08:25:50.202 [info] (6995) Auto-excluding **\/.* 2025-08-03 08:25:50.202 [info] (6995) Assuming Python version 3.11.10.final.0 2025-08-03 08:25:50.293 [info] (6995) Found 3 source files ```",
    "author_id":5860,
    "publication_date":1754210456000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mikko Ohtamaa",
    "author_reputation":84446.0,
    "tags":"visual-studio-code, pylance",
    "text_length":740,
    "title_length":105,
    "num_tags":2
  },
  {
    "id":6407,
    "title":"Using LinearGradient in Expo App causing white screen on iOS device",
    "link":"https:\/\/stackoverflow.com\/questions\/79723820\/using-lineargradient-in-expo-app-causing-white-screen-on-ios-device",
    "text":"I'm building a React Native app using Expo, and I’m using the ``` LinearGradient ``` component from ``` expo-linear-gradient ``` . It works perfectly on Android simulators, but when I run the app on a real iOS device, it shows a white screen or sometimes crashes without an error. Example ``` import { LinearGradient } from 'expo-linear-gradient'; <LinearGradient colors={['#000000', '#00FF00']} className=\"flex-1 justify-center items-center\" > <Text className=\"text-white\">Hello Gradient<\/Text> <\/LinearGradient> ``` What I've tried: Ensured the package is installed via expo install Cleared cache: npm start -clear Rebuilt the app using EAS Build Tried changing gradient colors Verified it's imported from expo-linear-gradient Behavior: Works on Android simulator White screen only on physical iOS device No error logs, just a blank white screen What could be causing ``` LinearGradient ``` to render a white screen on iOS devices only? Are there any known issues with ``` expo-linear-gradient ``` on physical iPhones, or something specific needed in the build config? Any suggestions or fixes appreciated. Or shud I not use it with NativeWind and switch to custom CSS ?",
    "author_id":5871,
    "publication_date":1754210470000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Bhavsagar",
    "author_reputation":1.0,
    "tags":"ios, react-native, expo, nativewind",
    "text_length":1172,
    "title_length":67,
    "num_tags":4
  },
  {
    "id":6406,
    "title":"OLA Map Web SDK With HTML, Javascript Not Showing Map",
    "link":"https:\/\/stackoverflow.com\/questions\/79723821\/ola-map-web-sdk-with-html-javascript-not-showing-map",
    "text":"I'm trying to integrate OLA Map Web SDK In my project. I've gone through the documentation and tried everything I can but found nothing to be useful. Every time I got the following error: Error: Source layer \"3d_model\" does not exist on source \"vectordata\" as specified by style layer \"3d_model_data\". HTML: Scripts: Error: I've tried different tiles setting like 2d and 3d with with different styles but the outcome is same.",
    "author_id":5870,
    "publication_date":1754210533000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"full_stack_developer",
    "author_reputation":83.0,
    "tags":"javascript, html, sdk, jquery, webapi",
    "text_length":425,
    "title_length":53,
    "num_tags":5
  },
  {
    "id":6405,
    "title":"`subClass extends baseClass` and not with `new` keyword in JavaScript",
    "link":"https:\/\/stackoverflow.com\/questions\/79723828\/subclass-extends-baseclass-and-not-with-new-keyword-in-javascript",
    "text":"I'm new to JavaScript and I'm trying to achieve the logic I have commented in the constructor method of the base class ``` export default class Building { constructor(sqft) { this._sqft = sqft; \/\/ when a subClass extends from this class \/\/ and does not implement `certainMethod()`, throw an error. \/\/ this should happen only when `subClass extends baseClass` and not with `new` keyword \/\/ object creation. } } ```",
    "author_id":5869,
    "publication_date":1754211409000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"noel-reeds",
    "author_reputation":9.0,
    "tags":"javascript, ecmascript-6, es6-class",
    "text_length":413,
    "title_length":69,
    "num_tags":3
  },
  {
    "id":6404,
    "title":"How to prevent duplicate transaction calculations in a ClickHouse materialized view",
    "link":"https:\/\/stackoverflow.com\/questions\/79723832\/how-to-prevent-duplicate-transaction-calculations-in-a-clickhouse-materialized-v",
    "text":"I’m planning to use ClickHouse to calculate wallet balances based on transactions in my base table. However, there’s an issue: if something goes wrong and I end up inserting the same transactions into my base table again, my materialized view (MV) will recalculate those transactions. Is there any way to prevent the MV from processing duplicate transactions? I would appreciate any suggestions on how to ensure the MV doesn’t double-count or recalculate transactions that have already been processed. this is my base table for transaction ``` CREATE TABLE eth_trx_detail ( 'hash' String, 'blockHash' String, 'blockNumber' Int32, 'from' String, 'to' String, 'value' Decimal128(S), 'time' Int32, 'gasPrice' int16, 'fee' Decimal32(S)) ENGINE = ReplacingMergeTree() ORDER BY (hash, blockNumber) ``` this is my balances table ``` CREATE TABLE eth_address_balances ( `address` String, `balance` AggregateFunction(sum, Decimal(38, 18)), `last_updated` DateTime ) ENGINE = AggregatingMergeTree() ORDER BY (address); ``` and this is my mv table ``` CREATE MATERIALIZED VIEW eth_balance_mv TO eth_address_balances AS SELECT address, sumState(value) as balance, max(time) as last_updated FROM ( -- Outgoing transactions (negative balance) SELECT `from` as address, -value as value, time FROM eth_trx_detail UNION ALL -- Incoming transactions (positive balance) SELECT `to` as address, value as value, time FROM eth_trx_detail WHERE `to` != '' -- Exclude contract creation transactions ) GROUP BY address; ``` and by inserting this sample values for two times ``` INSERT INTO blockbin_test.eth_trx_detail (*) VALUES (0xa5a9292698e6b2c3415d7bed272a5c86a2af398bf5b3f5c98cfa9e61b8f37c17,0x3fd256da659f74174bcc0c90f2bc0e7b5013914c94e819f24f0fe7c315bd606f,19840831,0x7591d15ca9c726faac98ef757f25009ce0efb1e9,0x8800d494d79b79955ec3575cc8bd400528853849,0.006,1715358527,75,0.0016134); ``` i found out that the calculated balance would be ``` ┌─address──────────────┬─balance─┬────────last_updated─┐ │ 7.764412632850413e47 │ 0.012 │ 2024-05-10 16:28:47 │ │ 6.712037662395873e47 │ -0.012 │ 2024-05-10 16:28:47 │ └──────────────────────┴─────────┴─────────────────────┘ ``` instead of ``` ┌─address──────────────┬─balance─┬────────last_updated─┐ │ 7.764412632850413e47 │ 0.006 │ 2024-05-10 16:28:47 │ │ 6.712037662395873e47 │ -0.006 │ 2024-05-10 16:28:47 │ └──────────────────────┴─────────┴─────────────────────┘ ```",
    "author_id":5868,
    "publication_date":1754211921000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Amirhossein Masihi",
    "author_reputation":11.0,
    "tags":"database, aggregation, duplicates, clickhouse, materialized",
    "text_length":2396,
    "title_length":83,
    "num_tags":5
  },
  {
    "id":6403,
    "title":"Regex match positive and negative numbers except for certain number formatting",
    "link":"https:\/\/stackoverflow.com\/questions\/79723833\/regex-match-positive-and-negative-numbers-except-for-certain-number-formatting",
    "text":"We're trying to write a regular expression to match floats. Inputs always have 9 decimal places, never begin with a 0 (unless it's actually 0), and can either be positive or negative as long as the number isn't ``` 0.000000000 ``` -- in that case it must always be positive. Examples of match success (decimal and negative aligned -- in reality they have no whitespace): ``` 15.472668143 13.000000000 0.000000000 0.123456789 - 0.123456789 - 0.123456780 - 0.010000000 0.010000000 - 0.000000010 0.000000001 - 0.000000001 0.100000000 ``` Examples of match failures (decimal and negative aligned): ``` 53 - 0.000000000 07.500038112 4.125881650081762241 6.5 .950017824 ``` What we've tried so far: ``` ^(-?(0|[1-9][0-9]*)\\.[0-9]{9}|-?0\\.[1-9][0-9]{8})$ ``` -- matches on ``` -0.000000000 ``` = failure ``` ^((-?[1-9][0-9]*|0)\\.[0-9]{9}|-0\\.[0-9]{8}[1-9]|-0\\.[1-9]{8}0)$ ``` -- doesn't match on ``` -0.010000000 ``` and ``` -0.000000010 ``` = failure ``` ^(?!-0\\.0{9}$)(-?(0|[1-9][0-9]*)\\.[0-9]{9})$ ``` -- THIS DOES WORK but our regex processor does not allow lookarounds of any kind! To clarify what I mean about starting 0s, I mean that the whole number cannot be 0 padded. It's okay for the decimal place numbers (all 9 after the point) to have 0s anywhere.",
    "author_id":5867,
    "publication_date":1754212315000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"leetbacoon",
    "author_reputation":1347.0,
    "tags":"regex, posix-ere",
    "text_length":1255,
    "title_length":78,
    "num_tags":2
  },
  {
    "id":6402,
    "title":"Share limited Outlook calendar details and access through VBA",
    "link":"https:\/\/stackoverflow.com\/questions\/79723834\/share-limited-outlook-calendar-details-and-access-through-vba",
    "text":"Is there a way to share limited calendar details in outlook 2019 with other users on the same exchange server, so that they can use vba to see start, end and title without granting reviewer permissions? It seems whenever the permission level is different from Reviewer vba can open the user's calendar folder but says there's 0 items in the calendar. Seeing the items' start, end, title and location through the normal Outlook UI works though.",
    "author_id":5866,
    "publication_date":1754212321000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ladde",
    "author_reputation":1.0,
    "tags":"vba, outlook, outlook-2019",
    "text_length":443,
    "title_length":61,
    "num_tags":3
  },
  {
    "id":6401,
    "title":"How do I properly use scanf and what are its limits?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723838\/how-do-i-properly-use-scanf-and-what-are-its-limits",
    "text":"I am currently trying to make a simple little program (BMI index calculator), since I have just started learning C. Currently using Eclipse. Here is my code: ``` #include <stdio.h> int weight, height; int main(void) { printf(\"This is your weight (kg): \"); scanf(\"Weight: %d\", &weight); printf(\"\\nThis is your height (m): \"); scanf(\"Height: %d\", &height); return 0; } ``` I was expecting my terminal to prompt me for weight, which it does, followed by height. I've realized that after the first scanf my program basically terminates and spits out the rest of the code? It doesn't prompt me a second time. Additionally the value stored in weight doesn't match what I entered. Can anyone explain whether I'm using the proper functions or if maybe I'm missing something?",
    "author_id":5865,
    "publication_date":1754212420000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"John.D",
    "author_reputation":59.0,
    "tags":"c, scanf",
    "text_length":766,
    "title_length":52,
    "num_tags":2
  },
  {
    "id":6400,
    "title":"TStream.Read fails if file is located on a cloud-mapped drive",
    "link":"https:\/\/stackoverflow.com\/questions\/79723850\/tstream-read-fails-if-file-is-located-on-a-cloud-mapped-drive",
    "text":"A simple ``` TFileStream.Read ``` or ``` TFileStream.Write ``` repeatedly fails if the file is located on a cloud-mapped drive. Do I need to repeat each read and write operation for a while to make sure it works?",
    "author_id":5864,
    "publication_date":1754213592000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"AWirthK2",
    "author_reputation":96.0,
    "tags":"delphi, cloud, tfilestream",
    "text_length":212,
    "title_length":61,
    "num_tags":3
  },
  {
    "id":6399,
    "title":"linux script to search for particular word in web search results",
    "link":"https:\/\/stackoverflow.com\/questions\/79723851\/linux-script-to-search-for-particular-word-in-web-search-results",
    "text":"I want to find out a particular word in web search results page.Just like ctrl+f from keyboard. interested in a linux command to do this. I have used https:\/\/www.google.com\/search?q=KEYWORD&start=10 and so on to navigate through pages but unable to find the command to look for the particular word in the results. Would like to make a linux script for doing the same. Any help will be appreciated.",
    "author_id":5863,
    "publication_date":1754213595000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Anirban ",
    "author_reputation":1.0,
    "tags":"linux, command-line, search, keyword, browser",
    "text_length":397,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6398,
    "title":"Type gymnastics of heterogeneously-typed lists in Rust",
    "link":"https:\/\/stackoverflow.com\/questions\/79723853\/type-gymnastics-of-heterogeneously-typed-lists-in-rust",
    "text":"I'm designing a pass system inspired by frunk . Background This is a little introduction of the ``` Selector ``` in frunk. If you are familiar with it, just skip this section. Roughly speaking, frunk uses a ``` Selector ``` to simulate specialization in a very graceful way: ``` pub struct HCons<H, T> { pub head: H, pub tail: T, } pub trait Selector<S, I> { fn get(&self) -> &S; fn get_mut(&mut self) -> &mut S; } impl<T, Tail> Selector<T, Here> for HCons<T, Tail> { fn get(&self) -> &T { &self.head } fn get_mut(&mut self) -> &mut T { &mut self.head } } impl<Head, Tail, FromTail, TailIndex> Selector<FromTail, There<TailIndex>> for HCons<Head, Tail> where Tail: Selector<FromTail, TailIndex>, { fn get(&self) -> &FromTail { self.tail.get() } fn get_mut(&mut self) -> &mut FromTail { self.tail.get_mut() } } ``` With the introduction of the second generic parameter, Rust compiler could infer type with some kind of specialization. Now we can use this like: ``` let h = hlist![1i32, 2u32, \"hello\", true, 42f32]; \/\/ Often, type inference can figure out the type you want. \/\/ You can help guide type inference when necessary by \/\/ using type annotations. let b: &bool = h.get(); if !b { panic!(\"no way!\") }; \/\/ If space is tight, you can also use turbofish syntax. \/\/ The Index is still left to type inference by using `_`. match *h.get::<u32, _>() { 2 => { } _ => panic!(\"it can't be!!\"), } ``` Question I design a pass trait as ``` trait Pass { fn do_a<I>(pass_chain: &mut PC, ...) where PC: Selector<Self, I>; fn do_b<I>(pass_chain: &mut PC, ...) where PC: Selector<Self, I>; \/\/ ... } ``` Where a pass chain would be something like a hlist containing different passes. In the impl of each ``` do_a ``` , ``` do_b ``` , I want each pass can pass the execution towards other funcs in the same pass, or pass to next pass, or pass to the initial pass to redo the whole thing, which means I want each pass to have abilities to invoke ``` do_a ``` , ``` do_b ``` of current pass, next pass, and initial pass of the pass chain. As a result, I modify the ``` Selector ``` trait like this: ``` pub trait Selector<S, I> { type Next; fn get_current(&self) -> &S; fn get_current_mut(&mut self) -> &mut S; fn get_next(&self) -> &Self::Next; fn get_next_mut(&mut self) -> &mut Self::Next; } impl<T, Tail> Selector<T, Here> for (T, Tail) { type Next = Tail; fn get_current(&self) -> &T { let (head, _) = self; head } fn get_current_mut(&mut self) -> &mut T { let (head, _) = self; head } fn get_next(&self) -> &Tail { let (_, next) = self; next } fn get_next_mut(&mut self) -> &mut Tail { let (_, next) = self; next } } impl<Head, Tail, FromTail, TailIndex> Selector<FromTail, There<TailIndex>> for (Head, Tail) where Tail: Selector<FromTail, TailIndex>, { type Next = Tail::Next; fn get_current(&self) -> &FromTail { let (_, tail) = self; tail.get_current() } fn get_current_mut(&mut self) -> &mut FromTail { let (_, tail) = self; tail.get_current_mut() } fn get_next(&self) -> &Tail::Next { let (_, tail) = self; tail.get_next() } fn get_next_mut(&mut self) -> &mut Tail::Next { let (_, tail) = self; tail.get_next_mut() } } ``` However, I could not find out how to write a generic bound for the ``` Next ``` element and ``` PC ``` , so that I could write codes like ``` impl Pass for PassA { fn do_a<I>(pass_chain: &mut PC, ...) where PC: Selector<Self, I> { PC::Next::do_a(pass_chain, ...); \/\/ Let next passes do other things PC::do_a(pass_chain, ...); \/\/ Redo the whole thing from the beginning of the chain } } ``` This problem has made me struggled for about two weeks. Any advice will be appreciated!",
    "author_id":5862,
    "publication_date":1754213720000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Evian",
    "author_reputation":1223.0,
    "tags":"rust, types, heterogeneous-list",
    "text_length":3596,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6397,
    "title":"Visual Studio Code Python Language Server crashes with JavaScript out of memory",
    "link":"https:\/\/stackoverflow.com\/questions\/79723854\/visual-studio-code-python-language-server-crashes-with-javascript-out-of-memory",
    "text":"I have a large Python project I'd like to edit in Visual Studio Code. However, the Python Language server keeps crashing with JavaScript out of memory errors, like one below. How can I increase the memory available for Pylance? This is a local macOS project.",
    "author_id":5860,
    "publication_date":1754213760000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mikko Ohtamaa",
    "author_reputation":84446.0,
    "tags":"visual-studio-code, pylance",
    "text_length":258,
    "title_length":79,
    "num_tags":2
  },
  {
    "id":6396,
    "title":"Is there a standard way to do bounds checking when using `take`?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723856\/is-there-a-standard-way-to-do-bounds-checking-when-using-take",
    "text":"What is the best way to implement a non-circular take ( ``` # ``` ) i.e. ``` take[5; 1 2 3] ``` returning ``` 1 2 3 ``` rather than ``` 1 2 3 1 2 ``` ? Would ``` {(x&count y)#y} ``` be the general best? Are there any types of ``` y ``` for which I would prefer NOT to use ``` count ``` due to inefficiency and use something else instead say ``` {{x where not null x}y til x} ``` This I'm thinking especially if it's a slice e.g. would Python's ``` x[a:b] ``` ever be more efficiently implemented as ``` {[x;a;b]{x where not null x}x a+til b-a} ``` if for whatever reason ``` x ``` makes ``` count x ``` slower than that?",
    "author_id":5861,
    "publication_date":1754213766000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Gabi",
    "author_reputation":455.0,
    "tags":"kdb+, k",
    "text_length":620,
    "title_length":64,
    "num_tags":2
  },
  {
    "id":6395,
    "title":"Keras loading the model: TypeError: too many positional arguments",
    "link":"https:\/\/stackoverflow.com\/questions\/79723869\/keras-loading-the-model-typeerror-too-many-positional-arguments",
    "text":"When loading the model I am getting: ``` TypeError: too many positional arguments ``` ``` model = load_model(model_path) ^^^^^^^^^^^^^^^^^^^^^^ File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/saving\/saving_api.py\", line 196, in load_model return legacy_h5_format.load_model_from_hdf5( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/legacy\/saving\/legacy_h5_format.py\", line 133, in load_model_from_hdf5 model = saving_utils.model_from_config( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/legacy\/saving\/saving_utils.py\", line 85, in model_from_config return serialization.deserialize_keras_object( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/legacy\/saving\/serialization.py\", line 495, in deserialize_keras_object deserialized_obj = cls.from_config( ^^^^^^^^^^^^^^^^ File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/models\/model.py\", line 582, in from_config return functional_from_config( ^^^^^^^^^^^^^^^^^^^^^^^ File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/models\/functional.py\", line 570, in functional_from_config process_node(layer, node_data) File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/models\/functional.py\", line 505, in process_node layer(*args, **kwargs) File \"\/Users\/moo\/code\/xxx\/.venv\/lib\/python3.11\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler raise e.with_traceback(filtered_tb) from None File \"\/Users\/moo\/.pyenv\/versions\/3.11.10\/lib\/python3.11\/inspect.py\", line 3195, in bind return self._bind(args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"\/Users\/moo\/.pyenv\/versions\/3.11.10\/lib\/python3.11\/inspect.py\", line 3116, in _bind raise TypeError('too many positional arguments') from None ``` What could be wrong?",
    "author_id":5860,
    "publication_date":1754215377000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mikko Ohtamaa",
    "author_reputation":84446.0,
    "tags":"keras",
    "text_length":1936,
    "title_length":65,
    "num_tags":1
  },
  {
    "id":6394,
    "title":"What is the PEP-8 guideline for a default parameter with a union type?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723871\/what-is-the-pep-8-guideline-for-a-default-parameter-with-a-union-type",
    "text":"AFAIK, the default parameters should not have space around the ``` = ``` sign. ``` def foo(bar: str=\"baz\") ``` What is the convention when there are union types? ``` def foo(bar: str | None=None) def foo(bar: str | None = None) def foo(bar: (str | None)=None) # Are parenthesis standard? ```",
    "author_id":5859,
    "publication_date":1754215503000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"GrandeKnight",
    "author_reputation":191.0,
    "tags":"python, python-typing, pep8, default-parameters",
    "text_length":291,
    "title_length":70,
    "num_tags":4
  },
  {
    "id":6393,
    "title":"Spring Boot Test",
    "link":"https:\/\/stackoverflow.com\/questions\/79723873\/spring-boot-test",
    "text":"Here is my service class, but why does it always return a Bad Request error when i test it instead of Unauthorized ? Please help, Thank you This is the class service to handle login ``` @Service public class AuthService { @Autowired private UserRepository userRepository; @Autowired private ValidationService validationService; @Autowired private Validator validator; @Transactional public TokenResponse login(LoginUserRequest request){ validationService.validate(request); User user = userRepository.findById(request.getUsername()) .orElseThrow(() -> new ResponseStatusException(HttpStatus.UNAUTHORIZED, \"Username or password wrong\")); if (BCrypt.checkpw(request.getPassword(), user.getPassword())){ user.setToken(UUID.randomUUID().toString()); user.setTokenExpiredAt(next30Days()); userRepository.save(user); return TokenResponse.builder() .token(user.getToken()) .expiredAt(user.getTokenExpiredAt()) .build(); }else { throw new ResponseStatusException(HttpStatus.UNAUTHORIZED, \"Username or password wrong\"); } } private Long next30Days(){ return System.currentTimeMillis() + (1000*16*24*30); } ``` This is Request Body Login ``` @Data @AllArgsConstructor @NoArgsConstructor @Builder public class LoginUserRequest { @NotBlank @Size(max = 100) private String username; @NotBlank @Size(max = 100) private String password; } ``` This is Controller ``` @RestController public class AuthController { @Autowired private AuthService authService; @PostMapping( path = \"\/api\/auth\/login\", consumes = MediaType.APPLICATION_JSON_VALUE, produces = MediaType.APPLICATION_JSON_VALUE ) public WebResponse<TokenResponse> login(@RequestBody LoginUserRequest request){ TokenResponse tokenResponse = authService.login(request); return WebResponse.<TokenResponse>builder().data(tokenResponse).build(); } } ``` This is Test Controller ``` @SpringBootTest @AutoConfigureMockMvc class AuthControllerTest { @Autowired private MockMvc mockMvc; @Autowired private UserRepository userRepository; @Autowired private ObjectMapper objectMapper; @BeforeEach void setUp(){ userRepository.deleteAll(); } @Test void loginFailedUserNotFound() throws Exception{ LoginUserRequest request = new LoginUserRequest(); request.setUsername(\"test\"); request.setPassword(\"test\"); mockMvc.perform( post(\"\/api\/auth\/login\") .accept(MediaType.APPLICATION_JSON) .contentType(MediaType.APPLICATION_JSON) .content(objectMapper.writeValueAsString(request)) ).andExpectAll( status().isUnauthorized() ).andDo(result -> { WebResponse<String> response = objectMapper.readValue(result.getResponse().getContentAsString(), new TypeReference<>() { }); assertNotNull(response.getErrors()); }); } } ``` ``` \"C:\\Program Files\\Java\\jdk-17\\bin\\java.exe\" -javaagent:C:\\Users\\user\\AppData\\Local\\JetBrains\\IntelliJIdea2025.1\\captureAgent\\debugger-agent.jar=file:\/\/\/C:\/Users\/user\/AppData\/Local\/Temp\/capture11055425344567066349.props -ea -Didea.test.cyclic.buffer.size=1048576 \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2025.1.2\\lib\\idea_rt.jar=51042\" -Dkotlinx.coroutines.debug.enable.creation.stack.trace=false -Ddebugger.agent.enable.coroutines=true -Dkotlinx.coroutines.debug.enable.flows.stack.trace=true -Dkotlinx.coroutines.debug.enable.mutable.state.flows.stack.trace=true -Dfile.encoding=UTF-8 -classpath \"C:\\Users\\user\\.m2\\repository\\org\\junit\\platform\\junit-platform-launcher\\1.12.2\\junit-platform-launcher-1.12.2.jar;C:\\Program Files\\JetBrains\\IntelliJ IDEA 2025.1.2\\lib\\idea_rt.jar;C:\\Program Files\\JetBrains\\IntelliJ IDEA 2025.1.2\\plugins\\junit\\lib\\junit5-rt.jar;C:\\Program Files\\JetBrains\\IntelliJ IDEA 2025.1.2\\plugins\\junit\\lib\\junit-rt.jar;E:\\belajar-spring\\belajar-spring-restful-api\\target\\test-classes;E:\\belajar-spring\\belajar-spring-restful-api\\target\\classes;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-data-jpa\\3.5.3\\spring-boot-starter-data-jpa-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter\\3.5.3\\spring-boot-starter-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot\\3.5.3\\spring-boot-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-autoconfigure\\3.5.3\\spring-boot-autoconfigure-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-logging\\3.5.3\\spring-boot-starter-logging-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\ch\\qos\\logback\\logback-classic\\1.5.18\\logback-classic-1.5.18.jar;C:\\Users\\user\\.m2\\repository\\ch\\qos\\logback\\logback-core\\1.5.18\\logback-core-1.5.18.jar;C:\\Users\\user\\.m2\\repository\\org\\apache\\logging\\log4j\\log4j-to-slf4j\\2.24.3\\log4j-to-slf4j-2.24.3.jar;C:\\Users\\user\\.m2\\repository\\org\\apache\\logging\\log4j\\log4j-api\\2.24.3\\log4j-api-2.24.3.jar;C:\\Users\\user\\.m2\\repository\\org\\slf4j\\jul-to-slf4j\\2.0.17\\jul-to-slf4j-2.0.17.jar;C:\\Users\\user\\.m2\\repository\\jakarta\\annotation\\jakarta.annotation-api\\2.1.1\\jakarta.annotation-api-2.1.1.jar;C:\\Users\\user\\.m2\\repository\\org\\yaml\\snakeyaml\\2.4\\snakeyaml-2.4.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-jdbc\\3.5.3\\spring-boot-starter-jdbc-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\com\\zaxxer\\HikariCP\\6.3.0\\HikariCP-6.3.0.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-jdbc\\6.2.8\\spring-jdbc-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\hibernate\\orm\\hibernate-core\\6.6.18.Final\\hibernate-core-6.6.18.Final.jar;C:\\Users\\user\\.m2\\repository\\jakarta\\persistence\\jakarta.persistence-api\\3.1.0\\jakarta.persistence-api-3.1.0.jar;C:\\Users\\user\\.m2\\repository\\jakarta\\transaction\\jakarta.transaction-api\\2.0.1\\jakarta.transaction-api-2.0.1.jar;C:\\Users\\user\\.m2\\repository\\org\\jboss\\logging\\jboss-logging\\3.6.1.Final\\jboss-logging-3.6.1.Final.jar;C:\\Users\\user\\.m2\\repository\\org\\hibernate\\common\\hibernate-commons-annotations\\7.0.3.Final\\hibernate-commons-annotations-7.0.3.Final.jar;C:\\Users\\user\\.m2\\repository\\io\\smallrye\\jandex\\3.2.0\\jandex-3.2.0.jar;C:\\Users\\user\\.m2\\repository\\com\\fasterxml\\classmate\\1.7.0\\classmate-1.7.0.jar;C:\\Users\\user\\.m2\\repository\\net\\bytebuddy\\byte-buddy\\1.17.6\\byte-buddy-1.17.6.jar;C:\\Users\\user\\.m2\\repository\\org\\glassfish\\jaxb\\jaxb-runtime\\4.0.5\\jaxb-runtime-4.0.5.jar;C:\\Users\\user\\.m2\\repository\\org\\glassfish\\jaxb\\jaxb-core\\4.0.5\\jaxb-core-4.0.5.jar;C:\\Users\\user\\.m2\\repository\\org\\eclipse\\angus\\angus-activation\\2.0.2\\angus-activation-2.0.2.jar;C:\\Users\\user\\.m2\\repository\\org\\glassfish\\jaxb\\txw2\\4.0.5\\txw2-4.0.5.jar;C:\\Users\\user\\.m2\\repository\\com\\sun\\istack\\istack-commons-runtime\\4.1.2\\istack-commons-runtime-4.1.2.jar;C:\\Users\\user\\.m2\\repository\\jakarta\\inject\\jakarta.inject-api\\2.0.1\\jakarta.inject-api-2.0.1.jar;C:\\Users\\user\\.m2\\repository\\org\\antlr\\antlr4-runtime\\4.13.0\\antlr4-runtime-4.13.0.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\data\\spring-data-jpa\\3.5.1\\spring-data-jpa-3.5.1.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\data\\spring-data-commons\\3.5.1\\spring-data-commons-3.5.1.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-orm\\6.2.8\\spring-orm-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-context\\6.2.8\\spring-context-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-aop\\6.2.8\\spring-aop-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-tx\\6.2.8\\spring-tx-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-beans\\6.2.8\\spring-beans-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\slf4j\\slf4j-api\\2.0.17\\slf4j-api-2.0.17.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-aspects\\6.2.8\\spring-aspects-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\aspectj\\aspectjweaver\\1.9.24\\aspectjweaver-1.9.24.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-validation\\3.5.3\\spring-boot-starter-validation-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\apache\\tomcat\\embed\\tomcat-embed-el\\10.1.42\\tomcat-embed-el-10.1.42.jar;C:\\Users\\user\\.m2\\repository\\org\\hibernate\\validator\\hibernate-validator\\8.0.2.Final\\hibernate-validator-8.0.2.Final.jar;C:\\Users\\user\\.m2\\repository\\jakarta\\validation\\jakarta.validation-api\\3.0.2\\jakarta.validation-api-3.0.2.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-web\\3.5.3\\spring-boot-starter-web-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-json\\3.5.3\\spring-boot-starter-json-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\com\\fasterxml\\jackson\\core\\jackson-databind\\2.19.1\\jackson-databind-2.19.1.jar;C:\\Users\\user\\.m2\\repository\\com\\fasterxml\\jackson\\core\\jackson-annotations\\2.19.1\\jackson-annotations-2.19.1.jar;C:\\Users\\user\\.m2\\repository\\com\\fasterxml\\jackson\\core\\jackson-core\\2.19.1\\jackson-core-2.19.1.jar;C:\\Users\\user\\.m2\\repository\\com\\fasterxml\\jackson\\datatype\\jackson-datatype-jdk8\\2.19.1\\jackson-datatype-jdk8-2.19.1.jar;C:\\Users\\user\\.m2\\repository\\com\\fasterxml\\jackson\\datatype\\jackson-datatype-jsr310\\2.19.1\\jackson-datatype-jsr310-2.19.1.jar;C:\\Users\\user\\.m2\\repository\\com\\fasterxml\\jackson\\module\\jackson-module-parameter-names\\2.19.1\\jackson-module-parameter-names-2.19.1.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-tomcat\\3.5.3\\spring-boot-starter-tomcat-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\apache\\tomcat\\embed\\tomcat-embed-core\\10.1.42\\tomcat-embed-core-10.1.42.jar;C:\\Users\\user\\.m2\\repository\\org\\apache\\tomcat\\embed\\tomcat-embed-websocket\\10.1.42\\tomcat-embed-websocket-10.1.42.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-web\\6.2.8\\spring-web-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\io\\micrometer\\micrometer-observation\\1.15.1\\micrometer-observation-1.15.1.jar;C:\\Users\\user\\.m2\\repository\\io\\micrometer\\micrometer-commons\\1.15.1\\micrometer-commons-1.15.1.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-webmvc\\6.2.8\\spring-webmvc-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-expression\\6.2.8\\spring-expression-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\com\\mysql\\mysql-connector-j\\9.2.0\\mysql-connector-j-9.2.0.jar;C:\\Users\\user\\.m2\\repository\\org\\projectlombok\\lombok\\1.18.38\\lombok-1.18.38.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-test\\3.5.3\\spring-boot-starter-test-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-test\\3.5.3\\spring-boot-test-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\boot\\spring-boot-test-autoconfigure\\3.5.3\\spring-boot-test-autoconfigure-3.5.3.jar;C:\\Users\\user\\.m2\\repository\\com\\jayway\\jsonpath\\json-path\\2.9.0\\json-path-2.9.0.jar;C:\\Users\\user\\.m2\\repository\\jakarta\\xml\\bind\\jakarta.xml.bind-api\\4.0.2\\jakarta.xml.bind-api-4.0.2.jar;C:\\Users\\user\\.m2\\repository\\jakarta\\activation\\jakarta.activation-api\\2.1.3\\jakarta.activation-api-2.1.3.jar;C:\\Users\\user\\.m2\\repository\\net\\minidev\\json-smart\\2.5.2\\json-smart-2.5.2.jar;C:\\Users\\user\\.m2\\repository\\net\\minidev\\accessors-smart\\2.5.2\\accessors-smart-2.5.2.jar;C:\\Users\\user\\.m2\\repository\\org\\ow2\\asm\\asm\\9.7.1\\asm-9.7.1.jar;C:\\Users\\user\\.m2\\repository\\org\\assertj\\assertj-core\\3.27.3\\assertj-core-3.27.3.jar;C:\\Users\\user\\.m2\\repository\\org\\awaitility\\awaitility\\4.2.2\\awaitility-4.2.2.jar;C:\\Users\\user\\.m2\\repository\\org\\hamcrest\\hamcrest\\3.0\\hamcrest-3.0.jar;C:\\Users\\user\\.m2\\repository\\org\\junit\\jupiter\\junit-jupiter\\5.12.2\\junit-jupiter-5.12.2.jar;C:\\Users\\user\\.m2\\repository\\org\\junit\\jupiter\\junit-jupiter-api\\5.12.2\\junit-jupiter-api-5.12.2.jar;C:\\Users\\user\\.m2\\repository\\org\\opentest4j\\opentest4j\\1.3.0\\opentest4j-1.3.0.jar;C:\\Users\\user\\.m2\\repository\\org\\junit\\platform\\junit-platform-commons\\1.12.2\\junit-platform-commons-1.12.2.jar;C:\\Users\\user\\.m2\\repository\\org\\apiguardian\\apiguardian-api\\1.1.2\\apiguardian-api-1.1.2.jar;C:\\Users\\user\\.m2\\repository\\org\\junit\\jupiter\\junit-jupiter-params\\5.12.2\\junit-jupiter-params-5.12.2.jar;C:\\Users\\user\\.m2\\repository\\org\\junit\\jupiter\\junit-jupiter-engine\\5.12.2\\junit-jupiter-engine-5.12.2.jar;C:\\Users\\user\\.m2\\repository\\org\\junit\\platform\\junit-platform-engine\\1.12.2\\junit-platform-engine-1.12.2.jar;C:\\Users\\user\\.m2\\repository\\org\\mockito\\mockito-core\\5.17.0\\mockito-core-5.17.0.jar;C:\\Users\\user\\.m2\\repository\\net\\bytebuddy\\byte-buddy-agent\\1.17.6\\byte-buddy-agent-1.17.6.jar;C:\\Users\\user\\.m2\\repository\\org\\objenesis\\objenesis\\3.3\\objenesis-3.3.jar;C:\\Users\\user\\.m2\\repository\\org\\mockito\\mockito-junit-jupiter\\5.17.0\\mockito-junit-jupiter-5.17.0.jar;C:\\Users\\user\\.m2\\repository\\org\\skyscreamer\\jsonassert\\1.5.3\\jsonassert-1.5.3.jar;C:\\Users\\user\\.m2\\repository\\com\\vaadin\\external\\google\\android-json\\0.0.20131108.vaadin1\\android-json-0.0.20131108.vaadin1.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-core\\6.2.8\\spring-core-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-jcl\\6.2.8\\spring-jcl-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\springframework\\spring-test\\6.2.8\\spring-test-6.2.8.jar;C:\\Users\\user\\.m2\\repository\\org\\xmlunit\\xmlunit-core\\2.10.2\\xmlunit-core-2.10.2.jar;C:\\Users\\user\\.m2\\repository\\org\\postgresql\\postgresql\\42.7.3\\postgresql-42.7.3.jar;C:\\Users\\user\\.m2\\repository\\org\\checkerframework\\checker-qual\\3.42.0\\checker-qual-3.42.0.jar\" com.intellij.rt.junit.JUnitStarter -ideVersion5 -junit5 belajar_spring_restful_api.restful.controller.AuthControllerTest,loginFailedUserNotFound 21:21:24.204 [main] INFO org.springframework.test.context.support.AnnotationConfigContextLoaderUtils -- Could not detect default configuration classes for test class [belajar_spring_restful_api.restful.controller.AuthControllerTest]: AuthControllerTest does not declare any static, non-private, non-final, nested classes annotated with @Configuration. 21:21:24.324 [main] INFO org.springframework.boot.test.context.SpringBootTestContextBootstrapper -- Found @SpringBootConfiguration belajar_spring_restful_api.restful.BelajarSpringResTfulApiApplication for test class belajar_spring_restful_api.restful.controller.AuthControllerTest . ____ _ __ _ _ \/\\\\ \/ ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\\/ _` | \\ \\ \\ \\ \\\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | \/ \/ \/ \/ =========|_|==============|___\/=\/_\/_\/_\/ :: Spring Boot :: (v3.5.3) 2025-08-03T21:21:24.747+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] b.restful.controller.AuthControllerTest : Starting AuthControllerTest using Java 17.0.12 with PID 32496 (started by user in E:\\belajar-spring\\belajar-spring-restful-api) 2025-08-03T21:21:24.748+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] b.restful.controller.AuthControllerTest : No active profile set, falling back to 1 default profile: \"default\" 2025-08-03T21:21:25.357+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode. 2025-08-03T21:21:25.411+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 45 ms. Found 1 JPA repository interface. 2025-08-03T21:21:25.802+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] o.hibernate.jpa.internal.util.LogHelper : HHH000204: Processing PersistenceUnitInfo [name: default] 2025-08-03T21:21:25.863+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] org.hibernate.Version : HHH000412: Hibernate ORM core version 6.6.18.Final 2025-08-03T21:21:25.897+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] o.h.c.internal.RegionFactoryInitiator : HHH000026: Second-level cache disabled 2025-08-03T21:21:26.231+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] o.s.o.j.p.SpringPersistenceUnitInfo : No LoadTimeWeaver setup: ignoring JPA class transformer 2025-08-03T21:21:26.261+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting... 2025-08-03T21:21:26.418+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] com.zaxxer.hikari.pool.HikariPool : HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@54463380 2025-08-03T21:21:26.420+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed. 2025-08-03T21:21:26.458+07:00 WARN 32496 --- [Belajar Spring RESTful API] [ main] org.hibernate.orm.deprecation : HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default) 2025-08-03T21:21:26.480+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] org.hibernate.orm.connections.pooling : HHH10001005: Database info: Database JDBC URL [Connecting through datasource 'HikariDataSource (HikariPool-1)'] Database driver: undefined\/unknown Database version: 17.5 Autocommit mode: undefined\/unknown Isolation level: undefined\/unknown Minimum pool size: undefined\/unknown Maximum pool size: undefined\/unknown 2025-08-03T21:21:27.183+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] o.h.e.t.j.p.i.JtaPlatformInitiator : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) 2025-08-03T21:21:27.185+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default' 2025-08-03T21:21:27.645+07:00 WARN 32496 --- [Belajar Spring RESTful API] [ main] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning 2025-08-03T21:21:28.107+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] o.s.b.t.m.w.SpringBootMockServletContext : Initializing Spring TestDispatcherServlet '' 2025-08-03T21:21:28.108+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] o.s.t.web.servlet.TestDispatcherServlet : Initializing Servlet '' 2025-08-03T21:21:28.109+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] o.s.t.web.servlet.TestDispatcherServlet : Completed initialization in 1 ms 2025-08-03T21:21:28.139+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ main] b.restful.controller.AuthControllerTest : Started AuthControllerTest in 3.643 seconds (process running for 4.807) Java HotSpot(TM) 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended Hibernate: select u1_0.username, u1_0.name, u1_0.password, u1_0.token, u1_0.token_expired_at from users u1_0 Hibernate: delete from users where username=? Hibernate: select u1_0.username, u1_0.name, u1_0.password, u1_0.token, u1_0.token_expired_at from users u1_0 where u1_0.username=? MockHttpServletRequest: HTTP Method = POST Request URI = \/api\/auth\/login Parameters = {} Headers = [Content-Type:\"application\/json;charset=UTF-8\", Accept:\"application\/json\", Content-Length:\"37\"] Body = {\"username\":\"test\",\"password\":\"test\"} Session Attrs = {} Handler: Type = belajar_spring_restful_api.restful.controller.AuthController Method = belajar_spring_restful_api.restful.controller.AuthController#login(LoginUserRequest) Async: Async started = false Async result = null Resolved Exception: Type = org.springframework.web.server.ResponseStatusException ModelAndView: View name = null View = null Model = null FlashMap: Attributes = null MockHttpServletResponse: Status = 400 Error message = null Headers = [Content-Type:\"application\/json\"] Content type = application\/json Body = {\"data\":null,\"errors\":\"Username not found\"} Forwarded URL = null Redirected URL = null Cookies = [] java.lang.AssertionError: Status expected:<401> but was:<400> Expected :401 Actual :400 <Click to see difference> at org.springframework.test.util.AssertionErrors.fail(AssertionErrors.java:61) at org.springframework.test.util.AssertionErrors.assertEquals(AssertionErrors.java:128) at org.springframework.test.web.servlet.result.StatusResultMatchers.lambda$matcher$9(StatusResultMatchers.java:640) at org.springframework.test.web.servlet.MockMvc$1.andExpect(MockMvc.java:214) at org.springframework.test.web.servlet.ResultActions.lambda$andExpectAll$0(ResultActions.java:86) at org.springframework.test.util.ExceptionCollector.execute(ExceptionCollector.java:49) at org.springframework.test.web.servlet.ResultActions.andExpectAll(ResultActions.java:86) at belajar_spring_restful_api.restful.controller.AuthControllerTest.loginFailedUserNotFound(AuthControllerTest.java:49) at java.base\/java.lang.reflect.Method.invoke(Method.java:568) at java.base\/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base\/java.util.ArrayList.forEach(ArrayList.java:1511) 2025-08-03T21:21:28.770+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ionShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default' 2025-08-03T21:21:28.770+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ionShutdownHook] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated... 2025-08-03T21:21:28.772+07:00 INFO 32496 --- [Belajar Spring RESTful API] [ionShutdownHook] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. Process finished with exit code -1 ``` When i tested in Postman, it returned error code 400 broo",
    "author_id":5858,
    "publication_date":1754215621000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Timothy",
    "author_reputation":29.0,
    "tags":"spring-boot, backend, junit5",
    "text_length":21179,
    "title_length":16,
    "num_tags":3
  },
  {
    "id":6392,
    "title":"Getting around X-Frame-Options DENY in a Chrome extension? (2)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723874\/getting-around-x-frame-options-deny-in-a-chrome-extension-2",
    "text":"I have an iframe loading inside an extension page but unfortunately the document inside the iframe embeds its own iframes whose Content-Security-Policy I still can't circumvent with ``` declarativeNetRequest ``` . I'd like for all x-frame-options and Content-Security-Policy headers of all descendant iframes to be removed. I expected that ``` sub_frame ``` in ``` condition.resourceTypes ``` would apply to all descendant iframes even those embedded within other iframes I tried all the approaches from Getting around X-Frame-Options DENY in a Chrome extension? . This: ``` { header: \"Content-Security-Policy\", operation: \"remove\" } ``` removes Content-Security-Policy for the iframe embedded in the extension page. But the iframes embedded in the document inside that iframe still block. See the top right iframe doesn't load in the image below: And the error: ``` Refused to frame https:\/\/llxbet.NhosCored.conl because an ancestor violates the following Content Security Policy directive: \"frame-ancestors self prod.whoscored.occloud.io prod.whoscored.occloud.io\" ```",
    "author_id":5857,
    "publication_date":1754215626000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"3rdTim",
    "author_reputation":1.0,
    "tags":"google-chrome-extension, firefox-addon, firefox-addon-webextensions, chrome-extension-manifest-v3, web-extension",
    "text_length":1070,
    "title_length":62,
    "num_tags":5
  },
  {
    "id":6391,
    "title":"How the firestore support custom class and also custom class supoort FieldValue with typescript?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723886\/how-the-firestore-support-custom-class-and-also-custom-class-supoort-fieldvalue",
    "text":"firestore nodejs sdk support custom class with typescript by withConverter. But only updateDoc support FieldValue, what about setDoc, addDoc? for example: ``` class Post { constructor( readonly title: string, readonly author: string, pages: number ) {} toString(): string { return `${this.title} by ${this.author}`; } } const numberConverter = { toFirestore(value: WithFieldValue<Post>):WithFieldValue<Post> { return value; }, fromFirestore(snapshot: QueryDocumentSnapshot, options: SnapshotOptions) { return {...snapshot.data(options), id: snapshot.id}; } }; \/\/ when we use it: const post = new Post(\"good boy\", \"John\", 300) doc(db, 'posts\/post123').withConverter(numberConverter).set(post); \/\/ tricky case, how to support FeildValue for Post class const post = new Post(\"good boy\", \"John\", FieldValue.increment(50)) doc(db, 'posts\/post123').withConverter(numberConverter).set(post); ``` if I want to use FieldValue for Post class, how? Because Post's pages type is number. And other fields maybe also want to support FieldValue type, then the class definition will be messy, and also I need do some extra transfer work in the withConverter function.",
    "author_id":5856,
    "publication_date":1754217401000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"no13bus",
    "author_reputation":421.0,
    "tags":"typescript, node.js, google-cloud-firestore",
    "text_length":1151,
    "title_length":96,
    "num_tags":3
  },
  {
    "id":6390,
    "title":"Google Apps Script CORS Error: 405 Method Not Allowed for OPTIONS Request",
    "link":"https:\/\/stackoverflow.com\/questions\/79723887\/google-apps-script-cors-error-405-method-not-allowed-for-options-request",
    "text":"I have a simple Apps Script web application that I expect to work with my HTML, which sends the data in JSON format. HTML script is as below: ``` <script> \/\/const scriptURL = 'https:\/\/script.google.com\/macros\/s\/WEB_APP_URL_HERE\/exec'; \/\/ doPost and doOptions Both const form = document.getElementById('contactForm'); const responseMessage = document.getElementById('responseMessage'); form.addEventListener('submit', async (e) => { e.preventDefault(); \/\/ Prevent default form submission const formData = new FormData(form); \/\/ Convert FormData to JSON object const jsonData = {}; for (let [key, value] of formData.entries()) { jsonData[key] = value; } \/\/ Debug: Log JSON data being sent console.log('JSON data being sent:', jsonData); try { \/\/ Using fetch API for submission with JSON const response = await fetch(scriptURL, { method: 'POST', headers: { 'Content-Type': 'application\/json', }, body: JSON.stringify(jsonData) }); if (response.ok) { const result = await response.text(); \/\/ Or .json() if your Apps Script returns JSON responseMessage.className = 'success'; responseMessage.textContent = 'Success: ' + result; form.reset(); \/\/ Clear the form } else { const errorText = await response.text(); responseMessage.className = 'error'; responseMessage.textContent = 'Error: ' + errorText; } } catch (error) { console.error('Network or CORS error:', error); responseMessage.className = 'error'; responseMessage.textContent = 'An error occurred. Please try again.'; } }); <\/script> ``` The app script code is as below: ``` function doPost(e) { var headers = { 'Access-Control-Allow-Origin': '*', \/\/ Allow all origins (ONLY for development!) 'Access-Control-Allow-Methods': 'GET, POST, OPTIONS', 'Access-Control-Allow-Headers': 'Content-Type' \/\/ IMPORTANT: Allow Content-Type }; \/\/ Handle preflight (OPTIONS) requests if (e.requestMethod == 'OPTIONS') { \/\/Logger.log(\"OPTIONS Request received\"); \/\/ debug return ContentService.createTextOutput('') .setMimeType(ContentService.MimeType.TEXT) .setStatusCode(204) \/\/ No Content .setHeaders(headers); } try { \/\/ 1. Parse the JSON data from the request \/\/Logger.log(\"POST Request received\"); \/\/ debug var requestBody = e.postData.contents; \/\/Logger.log(\"Request Body: \" + requestBody); \/\/ debug var jsonData = JSON.parse(requestBody); \/\/Logger.log(\"Parsed JSON data: \" + JSON.stringify(jsonData)); \/\/ 2. Process the form data (replace this with your actual logic) \/\/ For example, you might save it to a Google Sheet: \/\/ ... your spreadsheet code here ... \/\/ 3. Create a JSON response var response = { status: \"success\", message: \"Form data received and processed!\", data: jsonData \/\/ Include the received data in the response }; \/\/ 4. Return the JSON response return ContentService.createTextOutput(JSON.stringify(response)) .setMimeType(ContentService.MimeType.JSON) \/\/ Set the correct content type .setHeaders(headers); } catch (error) { Logger.log(\"Error: \" + error); var errorResponse = { status: \"error\", message: \"An error occurred: \" + error }; return ContentService.createTextOutput(JSON.stringify(errorResponse)) .setMimeType(ContentService.MimeType.JSON) .setHeaders(headers); } } function doOptions(e) { var headers = { 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Methods': 'GET, POST, OPTIONS', 'Access-Control-Allow-Headers': 'Content-Type' }; \/\/Logger.log(\"OPTIONS Request\"); \/\/debug return ContentService.createTextOutput('') .setMimeType(ContentService.MimeType.TEXT) .setStatusCode(204) \/\/ No Content .setHeaders(headers); } ``` CORS error results in sending JSON data to app script from HTML, while TEXT data sent to app script doesn't result in CORS error. Is there a way out to send the day in JSON format yet somehow circumvent this CORS issue?",
    "author_id":5855,
    "publication_date":1754217408000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"chetan rao pissey",
    "author_reputation":9.0,
    "tags":"google-apps-script, web-applications, cors, http-post",
    "text_length":3727,
    "title_length":73,
    "num_tags":4
  },
  {
    "id":6389,
    "title":"True Non threaded Asynchronous File IO in c++",
    "link":"https:\/\/stackoverflow.com\/questions\/79723888\/true-non-threaded-asynchronous-file-io-in-c",
    "text":"How to verify if Boost.Asio actually uses Windows Overlapped I\/O for file operations? I'm implementing high-performance file I\/O in C++ on Windows and need to confirm whether Boost.Asio truly leverages the Windows Overlapped API for asynchronous file operations, rather than using thread pools to simulate async behavior. Background I have a performance-critical application that processes large volumes of compressed files (.gz → .csv → split → .gz). Initial profiling shows file I\/O as the bottleneck, so I need true OS-level async I\/O without additional thread overhead. Current Implementation Problem Here's my current Boost.Asio file reading approach: ``` #include <boost\/asio.hpp> #include <boost\/asio\/windows\/random_access_handle.hpp> boost::asio::io_context io_context; boost::asio::windows::random_access_handle file(io_context); \/\/ Open file with FILE_FLAG_OVERLAPPED HANDLE h = CreateFileA(\"test.txt\", GENERIC_READ, FILE_SHARE_READ, nullptr, OPEN_EXISTING, FILE_FLAG_OVERLAPPED | FILE_ATTRIBUTE_NORMAL, nullptr); file.assign(h); \/\/ Async read std::vector<char> buffer(8192); file.async_read_some_at(0, boost::asio::buffer(buffer), [](boost::system::error_code ec, std::size_t bytes_read) { \/\/ Handle completion }); io_context.run(); ``` Specific Technical Questions Does ``` boost::asio::windows::random_access_handle ``` actually call ``` ReadFileEx() ``` or ``` ReadFile() ``` with overlapped structures internally? Do Boost.AFIO and LLFIO also use Windows Overlapped I\/O, or do they use different async mechanisms? I'm considering these alternatives but need to understand their underlying Windows API usage. How can I programmatically verify that my file operations are using true Windows Overlapped I\/O rather than being queued to a thread pool? Are there any compilation flags or runtime checks to confirm these libraries are using the native Windows async file I\/O APIs? What I've Tried Checked Boost.Asio, Boost.AFIO, and LLFIO documentation (unclear about internal implementation details) Used Process Monitor to observe file I\/O patterns (inconclusive about overlapped vs threaded I\/O) Profiled with Visual Studio (shows async behavior but can't confirm if it's overlapped I\/O vs threads) Examined some source code but the abstraction layers make it difficult to trace to the actual Windows API calls Environment Windows 11 Visual Studio 2022 Boost 1.82 Files opened with ``` FILE_FLAG_OVERLAPPED ``` The core issue is distinguishing between true kernel-level async I\/O and user-space thread simulation, which significantly impacts performance for my use case.",
    "author_id":5854,
    "publication_date":1754217414000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Xiros",
    "author_reputation":11.0,
    "tags":"c++, boost-asio, asynchronous, file-io, overlapped-io",
    "text_length":2581,
    "title_length":45,
    "num_tags":5
  },
  {
    "id":6388,
    "title":"Accessing sqlite fts5 from a Windows client, in particular for backup purposes",
    "link":"https:\/\/stackoverflow.com\/questions\/79723902\/accessing-sqlite-fts5-from-a-windows-client-in-particular-for-backup-purposes",
    "text":"I have a large fts5 virtual table (currently about 90GB). Because the sqlite ODBC driver, which I use (version 3.43.2) as my main sqlite scripting client, doesn't support fts5, I populate and use fts5 either via sqlitespy (1.9.16) or via python (3.11.2)'s shipped sqlite3 support, which includes fts5. To achieve a database backup I would normally use the ODBC driver to .clone a database and\/or .dump the tables. With fts5 present I am reduced to either doing a file zip or doing a line by line clone of the fts5 virtual table via python: ``` import sqlite3 con1 = sqlite3.connect('source.db3') con2 = sqlite3.connect('target.db3') con1.text_factory = lambda b: b.decode(errors = 'ignore') counter = 0 for line in con1.iterdump(): try: if 'INSERT INTO \"ftstable\"' in line or 'CREATE VIRTUAL TABLE' in line: counter = counter + 1 if counter % 1000000 == 0: print(str(counter)) con2.execute(line) except Exception as e: print(line) print(str(e)) continue con2.commit() con1.close() con2.close() ``` I used an answer here: how do i dump a single sqlite3 table in python? The 'text_factory' line was required because there is some corrupt data in my database which was causing the script to crash with a decode error. I used an answer here: sqlite3.OperationalError: Could not decode to UTF-8 column Note that iterdump() picks up fts5's 'underlying' ordinary tables twice: once when dumping the virtual table, which is fine, and then again as ordinary CREATE TABLEs + INSERTs, which will fail if you try to run them after successfully populating the the virtual table. I can't find any references anywhere on the web to fts5 backups. Does anyone have a better fts5 backup solution?",
    "author_id":5853,
    "publication_date":1754219025000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"MandyShaw",
    "author_reputation":1155.0,
    "tags":"sqlite, backup, fts5",
    "text_length":1678,
    "title_length":78,
    "num_tags":3
  },
  {
    "id":6387,
    "title":"Modernizing typedef for a function to std::function",
    "link":"https:\/\/stackoverflow.com\/questions\/79723903\/modernizing-typedef-for-a-function-to-stdfunction",
    "text":"I have legacy code which uses typedef for function pointers (and yes, ``` void* ``` as well). I reduced it to this: ``` #include <memory> #include <functional> typedef void (*funx_t)(const double *, void *); void function(funx_t fp) { auto x = std::make_unique<double>(); fp(x.get(), x.get()); } void overloaded(const double *, void *) {} void overloaded(double) {} int main() { function(overloaded); } ``` This code compiles [ Godbolt ]. Now I wanted to change the function pointer declaration to the more modern version which would also allow lambdas (besides being more readable IMHO): ``` using funx_t = std::function<void(const double *, void*)>; ``` But this one does not compile any more [ Godbolt ]. The error message is ``` <source>:13:5: error: no matching function for call to 'function' 13 | function(overloaded); | ^~~~~~~~ <source>:3:6: note: candidate function not viable: no overload of 'overloaded' matching 'funx_t' (aka 'function<void (const double *, void *)>') for 1st argument 3 | void function(funx_t fp) | ^ ~~~~~~~~~ ``` There seems to be something I didn't get yet, when it comes to those function pointers. Why did I think this should work, when it actually doesn't? Can I somehow fix it and get it compiled using the more modern std::function?",
    "author_id":4564,
    "publication_date":1754219179000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Thomas Weller",
    "author_reputation":60660.0,
    "tags":"c++, function-pointers",
    "text_length":1271,
    "title_length":51,
    "num_tags":2
  },
  {
    "id":6386,
    "title":"TFS rebind deleted solution",
    "link":"https:\/\/stackoverflow.com\/questions\/79723906\/tfs-rebind-deleted-solution",
    "text":"I got a new copy of a solution that is different than the one I have in TFS. This new copy was never under TFS. I need to replce the solution under the exact same folder. In order to check-in the changed solution I: deleted the entire old solution from TFS and checked-in the deletion. copied the new solution to the same folder and checked-in the entire solution. The problem now is that the solution is not bound i.e. I don't see the locks near the file names and can't perform SC actions from the solution explorer. The VS interface has an option \"Add to Source Control\" but this results in a message: ``` The item 'x.sln' is already under source control at the selected location. If you are trying to rebind a project that you have already added to source control outside MSVS, you should use the Change Source control Command... ``` . The File menu does not have a change source control option. I have deleted all *.suo, *.user, *.vss*, *.vsp* files and restated VS. I also made sure there is no ``` VSS ``` string in *.sln, *.csproj files. TFS 2019, VS 2013",
    "author_id":5852,
    "publication_date":1754219295000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"grunt",
    "author_reputation":722.0,
    "tags":"azure-devops, tfs",
    "text_length":1063,
    "title_length":27,
    "num_tags":2
  },
  {
    "id":6385,
    "title":"Why is `std::views::as_rvalue` not so cheap as `std::move_iterator`?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723908\/why-is-stdviewsas-rvalue-not-so-cheap-as-stdmove-iterator",
    "text":"Consider the following code snippet (full code: https:\/\/godbolt.org\/z\/PMM4z9KvM ): ``` int main() { { std::cout << \"begin std::views::as_rvalue\\n\"; auto iss = std::istringstream(\"12\"); auto is = std::views::istream<X>(iss); auto _ = is | std::views::as_rvalue | std::ranges::to<std::vector>(); std::cout << \"end std::views::as_rvalue\\n\\n\" << std::flush; } { std::cout << \"\\nbegin my::as_rvalue\\n\"; auto iss = std::istringstream(\"12\"); auto is = std::views::istream<X>(iss); auto _ = is | my::as_rvalue | std::ranges::to<std::vector>(); std::cout << \"end my::as_rvalue\\n\\n\" << std::flush; } } ``` The output is as follows: ``` begin std::views::as_rvalue X() X(X const&) X(X&&) ~X() X(X&&) X(X&&) X(X&&) ~X() ~X() end std::views::as_rvalue ~X() ~X() ~X() begin my::as_rvalue X() X(X&&) X(X&&) X(X&&) ~X() end my::as_rvalue ~X() ~X() ~X() ``` It seems ``` std::views::as_rvalue ``` has more operations than ``` my::as_rvalue ``` . Why is ``` std::views::as_rvalue ``` not so cheap as ``` std::move_iterator ``` ?",
    "author_id":5016,
    "publication_date":1754219485000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"xmllmx",
    "author_reputation":44278.0,
    "tags":"c++, std-ranges, performance, optimization, standards",
    "text_length":1010,
    "title_length":68,
    "num_tags":5
  },
  {
    "id":6384,
    "title":"Syntax error when declaring method in a custom class",
    "link":"https:\/\/stackoverflow.com\/questions\/79723910\/syntax-error-when-declaring-method-in-a-custom-class",
    "text":"I'm working with Microsoft Dynamics AX (X++) and trying to create a helper class to pass values (like ComplaintType) between forms. I wrote the following class in X++ to store and retrieve the value: ``` public class ComplaintTransferHelper { str complaintType; public void setCT(str _complaintType) { complaintType = _complaintType; } public str getCT() { return complaintType; } } ``` However, when I try to save or compile this class, I get a syntax error on the line: ``` public void setCT(str _complaintType) ``` I’ve also tried making the complaintType variable static and adjusting the methods to be static as well: ``` private static str complaintType; public static void setCT(str _complaintType) { complaintType = _complaintType; } ``` But I still get a syntax error on either the variable or method declaration",
    "author_id":5851,
    "publication_date":1754219562000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"M Umer",
    "author_reputation":1.0,
    "tags":"axapta, x++, dynamics-365-operations",
    "text_length":821,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":6383,
    "title":"React Native: Bluetooth permission request not working on HUAWEI VOG-L29 - requestPermissions shows no dialog, openSettings() does nothing",
    "link":"https:\/\/stackoverflow.com\/questions\/79723912\/react-native-bluetooth-permission-request-not-working-on-huawei-vog-l29-reque",
    "text":"React Native: Bluetooth permission request not working on HUAWEI VOG-L29 - ``` requestPermissions ``` shows no dialog, ``` openSettings() ``` does nothing Problem Description I have a React Native app that requires Bluetooth permissions. On most Android devices, it works perfectly, but on HUAWEI VOG-L29 devices running EMUI, the permission flow completely fails: Permission request shows no dialog - ``` requestMultiple() ``` returns immediately without showing the system permission dialog Settings button does nothing - ``` openSettings() ``` from ``` react-native-permissions ``` clicks but doesn't navigate to settings Permissions exist in app settings - I can see the Bluetooth permission toggle in the device's app settings, but users can't access it through the app Device Details Model : HUAWEI VOG-L29 Android Version : 10 React Native Version : 0.76.9 Expo Version : 52.0.47 Current Implementation Permission Request Hook ``` \/\/ src\/store\/app-status\/hooks\/useRequestBluetoothPermissions.tsx import { useMutation, useQueryClient } from '@tanstack\/react-query'; import { PERMISSIONS, requestMultiple } from 'react-native-permissions'; export const useRequestBluetoothPermissions = () => { const queryClient = useQueryClient(); return useMutation({ mutationFn: async () => { const res = await requestMultiple([ PERMISSIONS.ANDROID.BLUETOOTH_CONNECT, PERMISSIONS.ANDROID.ACCESS_FINE_LOCATION, PERMISSIONS.ANDROID.BLUETOOTH_SCAN, PERMISSIONS.IOS.BLUETOOTH, ]); await queryClient.invalidateQueries({ queryKey: ['bluetoothPermissions'] }); return res; }, }); }; ``` Component with Permission Button ``` \/\/ BluetoothStatusFlow.tsx import { useCallback, useMemo } from 'react'; import { Platform } from 'react-native'; import { openSettings } from 'react-native-permissions'; const onRequestPermissions = useCallback(async () => { const result = await requestBluetoothPermissions(); const permissionsDenied = result && Object.values(result).some(status => status === 'denied' || status === 'blocked' ); if (didUserBlock || permissionsDenied) { if (Platform.OS === 'android') { try { await openAppSettings(); \/\/ Custom native method } catch (error) { openSettings(); \/\/ react-native-permissions fallback } } else { openSettings(); } } }, [didUserBlock, requestBluetoothPermissions]); <CtaButton onPress={onRequestPermissions}> {didUserBlock ? 'Open Settings' : 'Give Permissions'} <\/CtaButton> ``` What I've Tried 1. Custom Native Android Method Added a native method to directly open app settings: ``` \/\/ TuyaActivatorModule.kt @ReactMethod fun openAppSettings(params: ReadableMap) { val currentActivity = currentActivity ?: return try { val intent = Intent(Settings.ACTION_APPLICATION_DETAILS_SETTINGS) intent.data = Uri.parse(\"package:\" + reactApplicationContext.packageName) currentActivity.startActivity(intent) } catch (e: Exception) { \/\/ Fallback to general settings currentActivity.startActivity(Intent(Settings.ACTION_SETTINGS)) } } ``` 2. AndroidManifest.xml Permissions ``` <uses-permission android:name=\"android.permission.BLUETOOTH\" \/> <uses-permission android:name=\"android.permission.BLUETOOTH_ADMIN\" \/> <uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\" \/> <uses-permission android:name=\"android.permission.ACCESS_FINE_LOCATION\" \/> <uses-permission android:name=\"android.permission.ACCESS_COARSE_LOCATION\" \/> ``` 3. Tried Different Libraries ``` react-native-permissions ``` - ``` openSettings() ``` does nothing ``` react-native-android-open-settings ``` - Same issue Custom native implementation - Same issue Expected Behavior User clicks \"Grant Permissions\" System shows permission dialog OR opens app settings User can grant Bluetooth permissions App detects permission changes Actual Behavior on HUAWEI VOG-L29 User clicks button Nothing happens - no dialog, no navigation ``` requestBluetoothPermissions() ``` returns immediately with denied status ``` openSettings() ``` calls succeed but don't navigate anywhere User is stuck - cannot grant permissions through the app Additional Context The Bluetooth permission is visible in the device's app settings (Settings → Apps → MyApp → Permissions) Users can manually navigate to app settings and toggle permissions The issue is specifically with programmatic permission requests and settings navigation Works perfectly on Samsung, Google Pixel, OnePlus, and other Android devices Only fails on HUAWEI devices Questions Is this a known EMUI limitation with permission dialogs being suppressed? Are there HUAWEI-specific APIs or intents I should use instead? How can I reliably open app settings on HUAWEI devices? Is there a way to detect if permission dialogs are being blocked by the system?",
    "author_id":5850,
    "publication_date":1754219769000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nevo",
    "author_reputation":13.0,
    "tags":"android, react-native, permissions, bluetooth, huawei-mobile-services",
    "text_length":4681,
    "title_length":138,
    "num_tags":5
  },
  {
    "id":6382,
    "title":"PWA does not appear as a Share target",
    "link":"https:\/\/stackoverflow.com\/questions\/79723913\/pwa-does-not-appear-as-a-share-target",
    "text":"I'm building my very first PWA and I have a problem getting my app registered as a share target. My manifest.json file contains the section ``` \"share_target\": { \"action\": \"\/\", \"method\": \"POST\", \"enctype\": \"multipart\/form-data\", \"params\": { \"files\": [ { \"name\": \"newItem\", \"accept\": [\"application\/json\"] } ] } } ``` However, when I install the app on my phone using the iOS share button > add to start screen, it doesn't appear as an option to open .json files. Is there anybody who could tell me what I am doing wrong here? Thanks in advance!!",
    "author_id":5849,
    "publication_date":1754219808000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Hans H",
    "author_reputation":26.0,
    "tags":"progressive-web-apps, share, manifest.json",
    "text_length":544,
    "title_length":37,
    "num_tags":3
  },
  {
    "id":6381,
    "title":"The parameter LinuxFxVersion has an invalid value when creating App service with terraform azapi_resource provider",
    "link":"https:\/\/stackoverflow.com\/questions\/79723918\/the-parameter-linuxfxversion-has-an-invalid-value-when-creating-app-service-with",
    "text":"My end goal is to deploy a nginx docker container on app service. I have the following configuration with terraform ``` ## Resource Group resource \"azurerm_resource_group\" \"resource_group\" { name = \"hellocontainerrg\" location = var.location } ## Server Farm \/templates\/microsoft.web\/serverfarms resource \"azapi_resource\" \"app_service_plan\" { type = \"Microsoft.Web\/serverfarms@2024-11-01\" name = \"hellocontainerasp\" parent_id = azurerm_resource_group.resource_group.id location = var.location body = { kind = \"app,linux,container\" properties = { reserved = true } sku = { name = \"B1\" } } } ## App Service resource \"azapi_resource\" \"app_service\" { type = \"Microsoft.Web\/sites@2024-11-01\" name = \"hellocontaineras\" parent_id = azurerm_resource_group.resource_group.id location = var.location body = { properties = { reserved = true siteConfig = { appSettings = [] linuxFxVersion = \"DOCKER|docker.io\/nginx:latest\" } } } } ``` I have tried some variations on linuxFxVersion parameter all returns the same error message. Parameters DOCKER|docker.io\/nginx:latest DOCKER|docker.io\/nginx\/nginx:latest DOCKER|mcr.microsoft.com\/appsvc\/staticsite:latest I have tried using different sku F1 B1 P0v3 ``` │ -------------------------------------------------------------------------------- │ RESPONSE 400: 400 Bad Request │ ERROR CODE UNAVAILABLE │ -------------------------------------------------------------------------------- │ { │ \"Code\": \"BadRequest\", │ \"Message\": \"The parameter LinuxFxVersion has an invalid value.\", │ \"Target\": null, │ \"Details\": [ │ { │ \"Message\": \"The parameter LinuxFxVersion has an invalid value.\" │ }, │ { │ \"Code\": \"BadRequest\" │ }, │ { │ \"ErrorEntity\": { │ \"ExtendedCode\": \"01007\", │ \"MessageTemplate\": \"The parameter {0} has an invalid value.\", │ \"Parameters\": [ │ \"LinuxFxVersion\" │ ], │ \"Code\": \"BadRequest\", │ \"Message\": \"The parameter LinuxFxVersion has an invalid value.\" │ } │ } │ ], │ \"Innererror\": null │ } │ -------------------------------------------------------------------------------- ``` What are the valid value for LinuxFxVersion to deploy a container?",
    "author_id":5848,
    "publication_date":1754220880000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Win Naing Kyaw",
    "author_reputation":9.0,
    "tags":"azure, terraform, azure-devops",
    "text_length":2085,
    "title_length":114,
    "num_tags":3
  },
  {
    "id":6380,
    "title":"How to style elements in the shadow DOM from a child element within the custom element",
    "link":"https:\/\/stackoverflow.com\/questions\/79723920\/how-to-style-elements-in-the-shadow-dom-from-a-child-element-within-the-custom-e",
    "text":"I'm creating a web component, and I would like the user\/webmaster to be able to style elements in the shadow DOM without touching the JavaScript class. In the example I have a style element as a child to the custom element -- this is how I imagine it should be done, but the styles are of course not applied to the elements in the the shadow DOM. How do I do this? ``` class NotesList extends HTMLElement { constructor() { super(); this.attachShadow({ mode: \"open\" }); } connectedCallback() { this.foo = this.getAttribute('foo'); this.shadowRoot.innerHTML += `<h3>${this.foo}<\/h3>`; this.shadowRoot.innerHTML += `<ul><li>Note 1<\/li><\/ul>`; } } customElements.define(\"notes-list\", NotesList); ``` ``` <notes-list foo=\"bar\"> <style> ul { list-style: none; } <\/style> <\/notes-list> ```",
    "author_id":5847,
    "publication_date":1754221057000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"chrwahl",
    "author_reputation":13796.0,
    "tags":"javascript, css, web-component",
    "text_length":782,
    "title_length":86,
    "num_tags":3
  },
  {
    "id":6379,
    "title":"How to federate logins from Entra External id tenant to a Entra Id Tenant",
    "link":"https:\/\/stackoverflow.com\/questions\/79723927\/how-to-federate-logins-from-entra-external-id-tenant-to-a-entra-id-tenant",
    "text":"In Microsoft Entra External Id documentation ( https:\/\/learn.microsoft.com\/en-us\/entra\/external-id\/customers\/how-to-custom-oidc-federation-customers ), it is stated that configuring other Microsoft Entra tenants as an external identity provider is currently not supported. Consequently, the microsoftonline.com domain in the issuer URI is not accepted. So I tried setting it up using SAML and provided my SAML certificate from Entra Id tenant to external Id tenant and gave the issues uri with domain with domain . I also added the entra Id tenant in my user login flow. But it never showed up in the external Identity list on my login page. One more question I have is with Entra External Id invite external user feature. It currently says it is in preview. Does it mean it is not production ready?",
    "author_id":5846,
    "publication_date":1754221894000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mohsin Muazzam",
    "author_reputation":39.0,
    "tags":"microsoft-entra-external-id, federation",
    "text_length":799,
    "title_length":73,
    "num_tags":2
  },
  {
    "id":6378,
    "title":"Issue with Handling Union Types in Generic Class Properties",
    "link":"https:\/\/stackoverflow.com\/questions\/79723930\/issue-with-handling-union-types-in-generic-class-properties",
    "text":"In Python 3.13.3 ``` from typing import Self class Detail1: x: int def update(self, other: Self) -> None: self.x += other.x class Detail2: y: str def update(self, other: Self) -> None: self.y += other.y DetailUnion = Detail1 | Detail2 class Foo[Detail: DetailUnion]: detail: Detail def foo(self, other: Self) -> None: self.detail.update(other.detail) # A type error here. ``` In the last line, I got an error from my Pylance: ``` Argument of type \"Detail@Foo\" cannot be assigned to parameter \"other\" of type \"Detail1*\" in function \"update\" Type \"Detail1* | Detail2*\" is not assignable to type \"Detail1*\" \"Detail2*\" is not assignable to \"Detail1*\" Pylance(reportArgumentType) Argument of type \"Detail@Foo\" cannot be assigned to parameter \"other\" of type \"Detail2*\" in function \"update\" Type \"Detail1* | Detail2*\" is not assignable to type \"Detail2*\" \"Detail1*\" is not assignable to \"Detail2*\" Pylance(reportArgumentType) ``` It seems that the error came from the union type. But due to some issue, I have to use union in my project. In my eyes, ``` other ``` should share the same type as ``` self ``` in the method ``` def foo ``` , thus ``` self.detail ``` and ``` other.detail ``` are also of the same type. However, Pylance regarded ``` other.detail ``` as the union type ``` DetailUnion ``` . Therefore, it cannot be assigned to the argument in ``` detail.update ``` . I tried like ``` 'Foo[Detail]' ``` instead of ``` Self ``` for the type annotations, but they still didn't work. I hope to change only in the ``` Foo ``` class, leaving ``` Detail1 ``` and ``` DetailUnion ``` the same. How should I deal with this?",
    "author_id":5211,
    "publication_date":1754222083000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"inaku Gyan",
    "author_reputation":31.0,
    "tags":"python, python-typing, pyright",
    "text_length":1620,
    "title_length":59,
    "num_tags":3
  },
  {
    "id":6377,
    "title":"GLPK sensitivty result parsing with PULP",
    "link":"https:\/\/stackoverflow.com\/questions\/79723933\/glpk-sensitivty-result-parsing-with-pulp",
    "text":"I am trying to figure out how to read the GLPK sensitivity results back into Python An example of how i am generating it: ``` import pulp from pulp import LpStatus, value from glpk_sensitivity_parser import parse_sensitivity prob = pulp.LpProblem(\"TEST\", pulp.LpMaximize) D = pulp.LpVariable(\"D\", lowBound=0) P = pulp.LpVariable(\"P\", lowBound=0) prob += 10 * D + 15 * P, \"Objective\" prob += 2 * D + 4 * P <= 100, \"aluminium\" prob += 3 * D + 2 * P <= 80, \"steel\" prob.solve() temp_file_name = f\"sensitivity_LP_problem.sen\" prob.solve(pulp.GLPK(options=['--ranges', f'{temp_file_name}'])) ``` This then generates a file which is humanly readable but not easy to parse. It is all doable I guess but what if the file format changes. I feel there must be a better\/cleaner way of doing this than I am doing. So the question is, can i access the data before it gets put into this file format? There should be, but i was not able to find it",
    "author_id":5845,
    "publication_date":1754222175000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user989803",
    "author_reputation":303.0,
    "tags":"python, parsing, pulp, glpk",
    "text_length":932,
    "title_length":40,
    "num_tags":4
  },
  {
    "id":6376,
    "title":"Password in .env.local file next.js",
    "link":"https:\/\/stackoverflow.com\/questions\/79723937\/password-in-env-local-file-next-js",
    "text":"I'm working on a Next.js project and trying to securely store organization passwords using environment variables in a .env.local file. Here's how I've set it up: ``` # .env.local ORG1_PASSWORD=$orgOne12345 ORG2_PASSWORD=$orgTwo180000 ORG3_PASSWORD=ORG_Admin123 ``` And here's the server-side function I’m using to retrieve these passwords: ``` const getPassword = (org: string) => { switch (org) { case 'org1': return process.env.ORG1_PASSWORD; \/\/ doesn't work case 'org2': return process.env.ORG2_PASSWORD; \/\/ doesn't work case 'org3': return process.env.ORG3_PASSWORD; \/\/ works fine default: return null; } }; ``` What's odd is that ORG3_PASSWORD works perfectly when accessed via process.env.ORG3_PASSWORD, but the other two don't. If I hardcode values like: ``` case 'org1': return \"$orgOne12345\"; ``` then org1 and org2 work—but of course, this defeats the purpose of using environment variables. What I've tried\/checked: The .env.local file is correctly placed at the root of the project I'm running the code on the server side (not exposed to the client) I’ve restarted the dev server multiple times after updating .env.local I’m not using dotenv manually—just relying on Next.js built-in support I have tried inside double quotes \" \" & also in single quotes' ', also used \/$orgOne12345 like this in the .env file",
    "author_id":5844,
    "publication_date":1754222478000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Somdotta Sarkar",
    "author_reputation":117.0,
    "tags":"next.js, reactjs, javascript, passwords, .env",
    "text_length":1320,
    "title_length":35,
    "num_tags":5
  },
  {
    "id":6375,
    "title":"Adding a custom field to add images to the special page I created in the panel",
    "link":"https:\/\/stackoverflow.com\/questions\/79723938\/adding-a-custom-field-to-add-images-to-the-special-page-i-created-in-the-panel",
    "text":"I have a custom menu on the panel. I want to list the titles and images on my custom menu page. The titles are listed, but I can't add the images. I'm not sure how to do this. I'm looking for someone to help me. Could you please help? I have a code block that inserts a single custom image for a custom post type. But the methods I applied to that code block don't work here. When I first add the image, it shows, but when I click save, the image disappears. ``` <?php function register_imagepage_subpage() { add_menu_page( 'Custom Image List Page', 'Image List', 'manage_options', 'image-list', 'image_page_meta_box_display', 'dashicons-editor-ul' ); add_action( 'admin_init', 'register_imagepage_settings' ); } add_action( 'admin_menu', 'register_imagepage_subpage' ); function register_imagepage_settings() { if ( isset( $_POST['name'] ) || isset( $_POST['image'] ) ) { $old = get_option( 'image_page_fields' ); $new = array(); $names = $_POST['name']; $images = $_POST['image']; $count = count( $names ); for ( $i = 0; $i < $count; $i++ ) { if ( $names[$i] != '' || $images[$i] != '' ) { $new[$i]['name'] = stripslashes( strip_tags( $names[$i] ) ); if ( $images[$i] == ' ' ) $new[$i]['image'] = ''; else $new[$i]['image'] = $images[$i]; } } update_option( 'image_page_fields', \"\" ); global $post; $post_id = ''; if ( !empty( $new ) ) { update_option( 'image_page_fields', $new ); update_post_meta( $post_id, 'image_page_fields', $new ); }elseif ( empty($new) && $old ) { delete_post_meta( $post_id, 'image_page_fields', $new ); }elseif ( !empty($new) && $old ) { delete_post_meta( $post_id, 'image_page_fields', $old ); } } if ( ! did_action( 'wp_enqueue_media' ) ) { wp_enqueue_media(); } } function image_page_meta_box_display($post) { \/\/ global $post; \/\/ $image_page_fields = get_post_meta(isset($post->id), 'image_page_fields', true); wp_nonce_field( 'image_page_meta_box_nonce', 'image_page_meta_box_nonce' ); $image_page_fields = get_option( 'image_page_fields' ); ?> <script type=\"text\/javascript\"> jQuery(document).ready(function($) { $('#add-row').on('click', function() { var row = $('.empty-row.screen-reader-text').clone(true); row.removeClass('empty-row screen-reader-text'); row.insertBefore('#image_page-fieldset-one tbody>tr:last'); return false; }); $('.remove-row').on('click', function() { $(this).parents('tr').remove(); return false; }); }); jQuery(document).ready(function($) { $('.brand_logo_upload').click(function(e) { e.preventDefault(); var custom_uploader = wp.media({ title: 'Custom Image', button: { text: 'Upload Image' }, multiple: false }) .on('select', function() { var attachment = custom_uploader.state().get('selection').first().toJSON(); $('.brand_logo').attr('src', attachment.url); $('.brand_logo_url').val(attachment.url); }) .open(); }); }); <\/script> <h1>Image List<\/h1> <form method=\"post\" action=\"\"> <?php settings_fields( 'imagepage-settings-group' ); do_settings_sections( 'imagepage-settings-group' ); ?> <table id=\"image_page-fieldset-one\" style=\"width: 70%;padding: 3rem 5rem;\"> <thead> <tr> <th width=\"40%\">Title<\/th> <th width=\"50%\">Image<\/th> <th width=\"10%\">Action<\/th> <\/tr> <\/thead> <tbody> <?php $i=1; if ( is_iterable($image_page_fields) ) : foreach ( $image_page_fields as $field ) { ?> <tr> <td> <input type=\"text\" class=\"widefat name-item\" name=\"name[]\" value=\"<?php if( isset($field['name']) ) echo esc_attr( $field['name'] ); ?>\" placeholder=\"\" \/> <\/td> <td> <img class=\"brand_logo\" src=\"<?php echo get_option('brand_logo'); ?>\" height=\"100\" width=\"100\"\/> <a href=\"#\" class=\"brand_logo_upload\">Upload<\/a> <a href=\"#\" class=\"brand_logo_remove\">Remove<\/a> <\/td> <td class=\"action-item\"><a style=\"margin: 0px 1px;width: 100%;\" class=\"button remove-row\" href=\"#\">Remove<\/a><\/td> <\/tr> <?php } else : ?> <tr> <td><input type=\"text\" class=\"widefat name-item\" name=\"name[]\" placeholder=\"\" \/> <\/td> <td> <img class=\"brand_logo\" src=\"<?php echo get_option('brand_logo'); ?>\" height=\"100\" width=\"100\"\/> <a href=\"#\" class=\"brand_logo_upload\">Upload<\/a> <a href=\"#\" class=\"brand_logo_remove\">Remove<\/a> <\/td> <td class=\"action-item\"><a style=\"margin: 0px 1px;width: 100%;\" class=\"button remove-row\" href=\"#\">Remove<\/a><\/td> <\/tr> <?php endif; ?> <tr class=\"empty-row screen-reader-text\"> <td><input type=\"text\" class=\"widefat name-item\" name=\"name[]\" placeholder=\"\" \/> <\/td> <td> <img class=\"brand_logo\" src=\"<?php echo get_option('brand_logo'); ?>\" height=\"100\" width=\"100\"\/> <a href=\"#\" class=\"brand_logo_upload\">Upload<\/a> <a href=\"#\" class=\"brand_logo_remove\">Remove<\/a> <\/td> <td class=\"action-item\"><a style=\"margin: 0px 1px;width: 100%;\" class=\"button remove-row\" href=\"#\">Remove<\/a><\/td> <\/tr> <\/tbody> <\/table> <p><a id=\"add-row\" class=\"button\" href=\"#\">Add Row<\/a><\/p> <span class=\"save-button\"> <?php submit_button(); ?> <\/span> <\/form> <?php } ``` Update: I reviewed the codes again. I can upload the image. But I get an error when saving. Warning: Undefined array key \"image\" in ....................... on line 15 Warning: Trying to access array offset on value of type null in ....................... on line 19 Warning: Trying to access array offset on value of type null in ....................... 21 Warning: Trying to access array offset on value of type null in ....................... 24",
    "author_id":5827,
    "publication_date":1754222522000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"wws34",
    "author_reputation":35.0,
    "tags":"wordpress, custom-fields, meta-boxes",
    "text_length":5262,
    "title_length":78,
    "num_tags":3
  },
  {
    "id":6374,
    "title":"Actions are all performed together",
    "link":"https:\/\/stackoverflow.com\/questions\/79723943\/actions-are-all-performed-together",
    "text":"I'm having a problem that I can't figure out. This is a function block of my program: When I call the Emergency button to activate the emergency status it doesn't enter. To solve that I found that it works if I delete the reset action on the step \"ResetEmergency\". When I do that I can see that my actions are all executed simultaneously instead of related to their step (and so leaving the reset emergency there it will be reset at the program start and not after the reset button is pressed). Why this is happening and how I can prevent it? Or maybe what am I doing wrong? The test project here The main program ladder here:",
    "author_id":5843,
    "publication_date":1754223017000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"M4rc0txt",
    "author_reputation":35.0,
    "tags":"plc, codesys, ladder-logic, sfc",
    "text_length":626,
    "title_length":34,
    "num_tags":4
  },
  {
    "id":6373,
    "title":"psycopg2.errors.SyntaxError: syntax error at or near &quot;&#39;Klines&#39;&quot;",
    "link":"https:\/\/stackoverflow.com\/questions\/79723949\/psycopg2-errors-syntaxerror-syntax-error-at-or-near-klines",
    "text":"Can't create a new PostgreSQL table because of this error. Here is the code that I use to create a new table: ``` class KlineTable(TableInterface): def __init__(self): super().__init__() self.__table_name: str = \"Klines\" def create(self): if self._is_exists(self.__table_name): print(\"table already exists.\") return with self.conn_context_m() as connection: with connection.cursor() as cursor: s_command: str = \"\"\" CREATE UNLOGGED TABLE %s( time INTEGER ); \"\"\" cursor.execute(s_command, (self.__table_name,)) print(\"new table successfully created.\") ``` Here is an error that I get every time: ``` Traceback (most recent call last): File \"D:\\Casper\\CBranch\\DataBase\\table_interface.py\", line 87, in conn_context_m yield connection File \"D:\\Casper\\CBranch\\DataBase\\kline_table.py\", line 100, in create cursor.execute(s_command, (self.__table_name,)) ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ psycopg2.errors.SyntaxError: syntax error at or near \"'Klines'\" LINE 2: CREATE UNLOGGED TABLE 'Klines'( ^ ``` I don't understand why it appears. Would be grateful for your help.",
    "author_id":5842,
    "publication_date":1754223526000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"good user",
    "author_reputation":29.0,
    "tags":"python, postgresql, sql, psycopg2",
    "text_length":1075,
    "title_length":81,
    "num_tags":4
  },
  {
    "id":6372,
    "title":"Unable to run js query on condition formatting",
    "link":"https:\/\/stackoverflow.com\/questions\/79723950\/unable-to-run-js-query-on-condition-formatting",
    "text":"I am try to conditionally change the color of a table the td value based on the number in the cell This has been answered on the following links conditional formatting of html table cells I have copied the 3 sections of code HTML css and js and placed them into Dreamweaver. I have linked the css an js files to the HTML table. I can confirm that the css file is linked as changing the cell padding has an immediate effect on what is displayed in the web browser. In the js file I added an Alert (”this is a test”); when I first open the web page that alert is displayed so that confirm the js file is linked correctly. With or without the alert statement there is no change in the background color of the cells what am I doing wrong thanks for any help this is how i did it ``` alert(\"test_colour_js_file\"); document.addEventListener('DOMContentLoaded', function() { const tableDataCells = document.querySelectorAll('td'); tableDataCells.forEach(cell => { const value = parseFloat(cell.textContent); \/\/ Convert content to a number if (!isNaN(value)) { \/\/ Check if the value is a valid number if (value < 30) { \/\/ Reddish for low values cell.style.backgroundColor = 'lightcoral'; } else if (value < 70) { \/\/ Yellowish for medium values cell.style.backgroundColor = 'lightgoldenrodyellow'; } else { \/\/ Greenish for high values cell.style.backgroundColor = 'lightgreen'; } } }); }); ```",
    "author_id":5841,
    "publication_date":1754223809000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"zaspa",
    "author_reputation":5.0,
    "tags":"javascript, html, css, formatting",
    "text_length":1384,
    "title_length":46,
    "num_tags":4
  },
  {
    "id":6371,
    "title":"Why does setjmp\/longjmp cause 0xC0000028 in LLVM IR?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723951\/why-does-setjmp-longjmp-cause-0xc0000028-in-llvm-ir",
    "text":"I write a compiler that generates LLVM IR code and then .obj file. Then I link it with some required .lib files. ( msvcrt.lib , ucrt.lib , vcruntime.lib ). I declared setjmp and longjmp there as follows: ``` declare i32 @setjmp(ptr) declare void @longjmp(ptr, i32) ``` ``` define dllexport i32 @\"SmallTestProject.Program::Main\"() { params: br label %entry entry: %jmpbuf = alloca %struct.System.Runtime.JmpBuf, align 16 call void @\"System.Runtime.ExceptionHelper::Push\"(ptr %jmpbuf) %0 = call i32 @setjmp(ptr %jmpbuf) %cmpResult = icmp eq i32 %0, 1 br i1 %cmpResult, label %end, label %test.block test.block: call void @\"SmallTestProject.Program::Test\"() br label %end end: ret i32 0 } define dllexport void @\"SmallTestProject.Program::Test\"() { params: br label %entry entry: %jmpBuf = call ptr @\"System.Runtime.ExceptionHelper::Peek\"() call void @longjmp(ptr %jmpBuf, i32 1) unreachable } ``` ( ``` %struct.System.Runtime.JmpBuf ``` size is 256 on x64 Windows) Everything links fine. I want to use these function to \"go back\" in stack but when I run .exe file it crashes with ``` 0xC0000028 ``` error code. Then I wrote similar program in C to check if it would also crash - the answer is no , it doesn't crash: ``` #include <stdio.h> #include <setjmp.h> _JBTYPE* env; void testFunction() { puts(\"Before longjmp\"); longjmp(env, 1); } int main() { _JBTYPE test[_JBLEN]; env = test; if (setjmp(env) == 0) { puts(\"Make call of testFunction\"); testFunction(); } else { puts(\"After call longjmp\"); } return 0; } ``` So this code works fine as x64 Windows target. But what is the problem with the same code in LLVM IR? Why does it crash? P.S. I read somewhere that I should align stack up to 16 bytes before calling setjmp\/longjmp but that didn't help. I also saw somewhere that I need to use personality functions so setjmp\/longjmp would work but why? I do not won't SEH in my code, I only want to go back in stack. P.S.2 As Some programmer dude suggested I compiled my Test.c file via clang with ``` -emit-llvm ``` flag. So the main parts of the out .ll file: ``` @env = dso_local global ptr null, align 8 ; Function Attrs: noreturn declare dso_local void @longjmp(ptr noundef, i32 noundef) #2 ; Function Attrs: nocallback nofree nosync nounwind willreturn memory(none) declare ptr @llvm.frameaddress.p0(i32 immarg) #3 ; Function Attrs: returns_twice declare dso_local i32 @_setjmp(ptr, ptr) #4 ; Function Attrs: noinline nounwind optnone uwtable define dso_local void @testFunction() #0 { %2 = load ptr, ptr @env, align 8 call void @longjmp(ptr noundef %2, i32 noundef 1) #6 unreachable } ; Function Attrs: noinline nounwind optnone uwtable define dso_local i32 @main() #0 { %1 = alloca i32, align 4 %2 = alloca [16 x %struct._SETJMP_FLOAT128], align 16 store i32 0, ptr %1, align 4 %4 = getelementptr inbounds [16 x %struct._SETJMP_FLOAT128], ptr %2, i64 0, i64 0 store ptr %4, ptr @env, align 8 %5 = call ptr @llvm.frameaddress.p0(i32 0) %6 = load ptr, ptr @env, align 8 %7 = call i32 @_setjmp(ptr %6, ptr %5) #4 %8 = icmp eq i32 %7, 0 br i1 %8, label %9, label %11 9: ; preds = %0 call void @testFunction() br label %13 11: ; preds = %0 br label %13 13: ; preds = %11, %9 ret i32 0 } attributes #2 = { noreturn \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" } attributes #3 = { nocallback nofree nosync nounwind willreturn memory(none) } attributes #4 = { returns_twice } ``` As I can see there is _setjmp instead of setjmp but as far as I know setjmp is just a macros. There is also ``` @llvm.frameaddress.p0 ``` call in main before setjmp and some attributes over setjmp\/longjmp declarations. I should try to do the same.",
    "author_id":5840,
    "publication_date":1754224041000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"crackanddie",
    "author_reputation":796.0,
    "tags":"c, llvm, linker, llvm-ir, setjmp",
    "text_length":3739,
    "title_length":52,
    "num_tags":5
  },
  {
    "id":6370,
    "title":"Color change for document&#39;s navigation window",
    "link":"https:\/\/stackoverflow.com\/questions\/79723952\/color-change-for-documents-navigation-window",
    "text":"I want to change the color of the selected item in the the following navigation window: It is responsible for switching files, and is usually bound to the keyboard shortcut Ctrl + Tab : Here is the list of settings that I have already tried to check:",
    "author_id":5839,
    "publication_date":1754224073000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Grzegorz Krug",
    "author_reputation":301.0,
    "tags":"visual-studio, visual-studio-2022, settings",
    "text_length":250,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":6369,
    "title":"PHP-class &quot;GuzzleHttp\\Client&quot; not found although it is installed with composer",
    "link":"https:\/\/stackoverflow.com\/questions\/79723953\/php-class-guzzlehttp-client-not-found-although-it-is-installed-with-composer",
    "text":"I'm not very familiar with composer and PHP-classes and now I'm stuck. I want to run this PHP-Script: ``` <php $client = new GuzzleHttp\\Client([ 'base_uri' => 'https:\/\/api.abuseipdb.com\/api\/v2\/' ]); ... ?> ``` I get this error: ``` Uncaught Error: Class \"GuzzleHttp\\Client\" not found ``` So I installed it with composer: composer require guzzlehttp\/guzzle It seems to be successful and the composer.json-file is created: ``` { \"require\": { \"guzzlehttp\/guzzle\": \"^7.9\" } } ``` vendor\/guzzlehttp directory exists. When I try to run the script again I still got the same error. I tried: \"composer dumpautoload\" php vendor\/autoload.php add a backslash ($client = new \\GuzzleHttp\\Client) add \"use GuzzleHttp\\Client as GuzzleHttp;\" in the first line All with no effect. This is the output of composer show guzzlehttp\/guzzle: ``` $> composer show guzzlehttp\/guzzle name : guzzlehttp\/guzzle descrip. : Guzzle is a PHP HTTP client library keywords : client, curl, framework, http, http client, psr-18, psr-7, rest, web service versions : * 7.9.3 released : 2025-03-27, 4 months ago type : library license : MIT License (MIT) (OSI approved) https:\/\/spdx.org\/licenses\/MIT.html#licenseText homepage : source : [git] https:\/\/github.com\/guzzle\/guzzle.git 7b2f29fe81dc4da0ca0ea7d42107a0845946ea77 dist : [zip] https:\/\/api.github.com\/repos\/guzzle\/guzzle\/zipball\/7b2f29fe81dc4da0ca0ea7d42107a0845946ea77 7b2f29fe81dc4da0ca0ea7d42107a0845946ea77 path : \/home\/webpages\/lima-city\/pudem2024\/vendor\/guzzlehttp\/guzzle names : guzzlehttp\/guzzle, psr\/http-client-implementation support issues : https:\/\/github.com\/guzzle\/guzzle\/issues source : https:\/\/github.com\/guzzle\/guzzle\/tree\/7.9.3 autoload files psr-4 GuzzleHttp\\ => src\/ requires ext-json * guzzlehttp\/promises ^1.5.3 || ^2.0.3 guzzlehttp\/psr7 ^2.7.0 php ^7.2.5 || ^8.0 psr\/http-client ^1.0 symfony\/deprecation-contracts ^2.2 || ^3.0 requires (dev) bamarni\/composer-bin-plugin ^1.8.2 ext-curl * guzzle\/client-integration-tests 3.0.2 php-http\/message-factory ^1.1 phpunit\/phpunit ^8.5.39 || ^9.6.20 psr\/log ^1.1 || ^2.0 || ^3.0 suggests ext-curl Required for CURL handler support ext-intl Required for Internationalized Domain Name (IDN) support psr\/log Required for using the Log middleware provides psr\/http-client-implementation 1.0 ``` How can I fix this?",
    "author_id":5838,
    "publication_date":1754224094000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"pudipel64",
    "author_reputation":13.0,
    "tags":"php, composer-php, guzzle",
    "text_length":2291,
    "title_length":88,
    "num_tags":3
  },
  {
    "id":6368,
    "title":"Script works in a macro but not in a sub routine",
    "link":"https:\/\/stackoverflow.com\/questions\/79723956\/script-works-in-a-macro-but-not-in-a-sub-routine",
    "text":"I've recorded a macro to change the width of a few columns. I've added this at the beginning of a VBA Sub and it doesn't change the widths of the columns. This is the code in the macro. If I run this macro the column widths are consitently changed as expected. ``` Sub Macro31() ' ' Macro31 Macro ' Sheets(\"Titlepage\").Select Columns(\"A:A\").ColumnWidth = 6.88 Columns(\"B:B\").ColumnWidth = 21.63 Columns(\"C:C\").ColumnWidth = 28.38 Columns(\"E:E\").ColumnWidth = 22.88 Sheets(\"PetersWorkingSheet\").Select End Sub ``` However If I insert this code into a sub where i'm doing some unrelated calculations\/formatting the column width is not changed. If I step through the code. The correct sheet is selected however nothing happens as I step over each Columnwidth command and the columns do not change width. No errors are reported. This is an extarct of the code within the sub. the code following the ColumnWidth commands works as expected. ``` Sub CreateFronticepiece() Sheets(\"Titlepage\").Select Columns(\"A:A\").ColumnWidth = 6.88 Columns(\"B:B\").ColumnWidth = 21.63 Columns(\"C:C\").ColumnWidth = 28.38 Columns(\"E:E\").ColumnWidth = 22.88 ActiveWorkbook.Sheets(\"Fronticepiece\").Activate ActiveSheet.Shapes.Range(Array(\"Picture 1\")).Select Selection.Copy ActiveWorkbook.Sheets(\"Titlepage\").Activate ActiveSheet.Range(\"b1\").Select ActiveSheet.Paste ActiveWorkbook.Sheets(\"Fronticepiece\").Activate ActiveSheet.Shapes.Range(Array(\"Picture 2\")).Select Selection.Copy ActiveWorkbook.Sheets(\"Titlepage\").Activate ActiveSheet.Range(\"A20\").Select ActiveSheet.Paste ActiveWorkbook.Sheets(\"Fronticepiece\").Activate ActiveSheet.Shapes.Range(Array(\"Picture 3\")).Select Selection.Copy ActiveWorkbook.Sheets(\"Titlepage\").Activate ActiveSheet.Range(\"E20\").Select ActiveSheet.Paste End Sub ``` I was expecting the columns to change width in the same way as they did with the macro.",
    "author_id":5837,
    "publication_date":1754224484000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Peter Brighton",
    "author_reputation":11.0,
    "tags":"vba, excel",
    "text_length":1856,
    "title_length":48,
    "num_tags":2
  },
  {
    "id":6367,
    "title":"Typeorm where case condition",
    "link":"https:\/\/stackoverflow.com\/questions\/79723966\/typeorm-where-case-condition",
    "text":"Got table with ``` id, price, priceTo ``` columns. ``` priceTo ``` column is conditional null. Use ``` typeorm ``` ``` build query. ``` Need to get rows, by the price value and conditional ``` priceTo ``` column. If it is not ``` null ``` , then use ``` price ``` column, if not ``` null ``` then by ``` priceTo ``` . ``` const buildQuery = this.repository .createQueryBuilder('order') .where('userId=:userId', { userId }) if (query.priceTo) { buildQuery.andWhere((qb) => { return qb.andWhere(`CASE WHEN order.priceTo IS NOT NULL ' THEN order.priceTo <= :priceTo ELSE order.price <= :priceTo END` ) }, {priceTo: query.priceTo}) } ``` I try this query, but got ``` Unsupported FindOperator Function ``` error.",
    "author_id":5836,
    "publication_date":1754225376000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dmo",
    "author_reputation":23.0,
    "tags":"sql, typeorm",
    "text_length":708,
    "title_length":28,
    "num_tags":2
  },
  {
    "id":6366,
    "title":"My translate for my shape is way too big. Why? (I adjust it by 0.001f and it throws it off screen)",
    "link":"https:\/\/stackoverflow.com\/questions\/79723978\/my-translate-for-my-shape-is-way-too-big-why-i-adjust-it-by-0-001f-and-it-thr",
    "text":"I just want to make a trapezoid at the top of my screen with the bottom corners rounded and I want to be anle to edit those values so I can easily make this shape in the future. (I want one of the menus to be over this so I am going to try to have it and the eventual buttons and data on top persist between screens.) This was my first try, mostly trying to copy from the heart example here: https:\/\/developer.android.com\/develop\/ui\/compose\/graphics\/draw\/shapes ``` @Preview @Composable fun trapezoidMenuTry() { val vertices = remember { floatArrayOf( 0f, 0f, 1f, 0f, 0.9f, 0.2f, 0.1f, 0.2f, ) } val rounding = remember { listOf( CornerRounding.Unrounded, CornerRounding.Unrounded, CornerRounding(0.1f), CornerRounding(0.1f), ) } @SuppressLint(\"RememberReturnType\") val trapezoid = remember(vertices, rounding) { RoundedPolygon( vertices = vertices, perVertexRounding = rounding, ) } Box( modifier = Modifier .background(Color.LightGray) .size(400.dp) .drawWithCache { val roundedPolygonPath = trapezoid.toPath().asComposePath() onDrawBehind { scale(size.width * 1f, size.width * 1f) { translate(size.width * 0.49954f, size.height * 0.49954f) { drawPath( roundedPolygonPath, color = Color(0xFFF15087) ) } } } } ) } ``` It ended up with a couple errors, but I got it working. However, the translate I copied from the example was 0.5 shifted in both x and y. so I set it to 0f and 0f then it disappears. After some experimenting, using 0.49954 gets me to the top left corner. Like it me work, but why? Can anyone explain?",
    "author_id":5835,
    "publication_date":1754226341000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"NewCom",
    "author_reputation":21.0,
    "tags":"android, android-jetpack-compose",
    "text_length":1519,
    "title_length":98,
    "num_tags":2
  },
  {
    "id":6365,
    "title":"Bytes type is required",
    "link":"https:\/\/stackoverflow.com\/questions\/79723981\/bytes-type-is-required",
    "text":"I am getting the error message ``` data_new.write(binary_data[j]) TypeError: a bytes-like object is required, not 'int' ``` although binary_data should be bytes-like . ``` data_new = open(Datei_neu, \"wb\") for j in range(0,43,1): data_new.write(binary_data[j]) ``` As soon as the code is extended by bytes() , then the error does not appear anymore: ``` data_new = open(Datei_neu, \"wb\") for j in range(0,43,1): data_new.write(bytes(binary_data[j])) ``` But secondly, there is nothing written into ``` Datei_neu ``` , although ``` binary_data ``` is representing all values from position 0..43.",
    "author_id":5834,
    "publication_date":1754226507000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Bulova",
    "author_reputation":1.0,
    "tags":"python, byte",
    "text_length":592,
    "title_length":22,
    "num_tags":2
  },
  {
    "id":6364,
    "title":"gopacket how to distinguish between phyiscal and virtual devices",
    "link":"https:\/\/stackoverflow.com\/questions\/79723982\/gopacket-how-to-distinguish-between-phyiscal-and-virtual-devices",
    "text":"I am using pcap.FindAllDevs() to find all devices then monitor their traffic. However, I only need the physical ones and the library doesnt provide a method to distinguish between physical and virtual network interfaces. I am using windows.",
    "author_id":5833,
    "publication_date":1754226596000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"baydırman",
    "author_reputation":101.0,
    "tags":"windows, go, network-interface, gopacket",
    "text_length":240,
    "title_length":64,
    "num_tags":4
  },
  {
    "id":6363,
    "title":"File is not found on the Branch SDK, causing an error",
    "link":"https:\/\/stackoverflow.com\/questions\/79723983\/file-is-not-found-on-the-branch-sdk-causing-an-error",
    "text":"I am making a Unity game for android with the Branch SDK, and after building the game and running it on my Android device, I found this error log: ``` AndroidJavaException: java.lang.ClassNotFoundException: io.branch.unity.BranchUnityWrapper java.lang.ClassNotFoundException: io.branch.unity.BranchUnityWrapper ``` This error was caused after initializing the Branch SDK inside of Unity with the line: ``` Branch.initSession(CallbackWithBranchUniversalObject); ``` Apparently, Branch SDK is looking for a file called BranchUnityWrapper, but when I go to Assets\/Plugins\/Branch\/Android, I am able to find the BranchUnityWrapper: Assets\/Plugins\/Branch\/Android\/BranchUnityWrapper.java The weird part about all of this is that the Branch SDK has been working perfectly until a week ago I stumbled upon this error. Currently I am using the Branch SDK v5.12.0, as stated in the mainTemplate.gradle file: ``` io.branch.sdk.android:library:5.12.0 ``` I don't have any custom proguard rules file. But inside the Publisher Settings ---> Minify section, I have checked the \"Release\" and \"Debug\" checkboxes. Does anyone know what could be the reason for it? Or does the BranchUnityWrapper refers to some other file, not the BranchUnityWrapper.java?",
    "author_id":5832,
    "publication_date":1754226628000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Matias Scribe",
    "author_reputation":23.0,
    "tags":"android, unity-game-engine, branch.io",
    "text_length":1235,
    "title_length":53,
    "num_tags":3
  },
  {
    "id":6362,
    "title":"ATtiny85 software latch with button hold for running \/ sleeping state change",
    "link":"https:\/\/stackoverflow.com\/questions\/79723985\/attiny85-software-latch-with-button-hold-for-running-sleeping-state-change",
    "text":"I'm making a software latch with following features: ATtiny85 @ 1 MHz, BOD disabled PB0 = LED, PB3 = Button (with internal pull-up) Interrupt on change Deep sleep until button press Change state with 2s button hold ADC and peripherals disabled for power saving The code below now works, but there is some odd behaviour like sometimes not registering a buttonhold when sleeping, so it doesn't wake up. The logic is also hard to understand after coming back to it. Can someone help to improve this? ``` #include <avr\/sleep.h> #include <avr\/power.h> #include <avr\/wdt.h> #define LED_PIN 0 \/\/ PB0 #define BUTTON_PIN 3 \/\/ PB3 bool ledState = false; bool buttonHandled = false; void setup() { pinMode(LED_PIN, OUTPUT); pinMode(BUTTON_PIN, INPUT_PULLUP); digitalWrite(LED_PIN, LOW); GIMSK |= (1 << PCIE); \/\/ Enable pin change interrupt PCMSK |= (1 << BUTTON_PIN); \/\/ On PB3 sei(); goToSleep(); \/\/ Initial sleep } void loop() { if (!ledState && digitalRead(BUTTON_PIN) == HIGH) { goToSleep(); return; } if (digitalRead(BUTTON_PIN) == LOW && !buttonHandled && buttonLongPressed()) { ledState = !ledState; digitalWrite(LED_PIN, ledState ? HIGH : LOW); delay(50); \/\/ debounce buttonHandled = true; \/\/ prevent repeated toggle } \/\/ Reset flag when button is released if (digitalRead(BUTTON_PIN) == HIGH) { buttonHandled = false; } if (!ledState && digitalRead(BUTTON_PIN) == HIGH) { goToSleep(); } } \/\/ Detect long press (>2s) bool buttonLongPressed() { uint16_t pressTime = 0; while (digitalRead(BUTTON_PIN) == LOW) { delay(50); \/\/ Debounce pressTime += 50; if (pressTime >= 2000) return true; } return false; } \/\/ Sleep function void goToSleep() { digitalWrite(LED_PIN, LOW); cli(); wdt_disable(); \/\/ Disable watch dog timer to save power sleep_bod_disable(); \/\/ Disable Brown out detector \/\/ Power-saving: disable peripherals ADCSRA &= ~(1 << ADEN); \/\/ Disable ADC ACSR |= (1 << ACD); \/\/ Disable analog comparator PRR = (1 << PRADC) | (1 << PRTIM1); \/\/ Only disable Timer1 ( Timer0 is for millis() delay() ) set_sleep_mode(SLEEP_MODE_PWR_DOWN); sleep_enable(); sei(); sleep_cpu(); \/\/ Start sleeping \/\/ Here we'll wake up sleep_disable(); } \/\/ Wake on pin change ISR(PCINT0_vect) { } \/\/ Interrupt fires on any change, handled in loop ```",
    "author_id":5831,
    "publication_date":1754226790000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"tubos",
    "author_reputation":99.0,
    "tags":"c++, arduino, avr-gcc, sleep-mode",
    "text_length":2226,
    "title_length":76,
    "num_tags":4
  },
  {
    "id":6361,
    "title":"How can I correctly detect the character display width of a string in a shell script?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723986\/how-can-i-correctly-detect-the-character-display-width-of-a-string-in-a-shell-sc",
    "text":"is is a character detection written using ${#text}. The upper part is the content, and the lower part is the terminal output. ``` text=\"這是size\" echo \"$text\" text_width=${#text} echo \"$text_width\" ``` So I tried combining a Shell script with Python to calculate the text width, which correctly captures the actual character width. ``` text=\"這是size\" echo \"$text\" text_width=$(python3 -c \"from wcwidth import wcswidth; print(wcswidth('$text'))\") echo \"$text_width\" ``` However, this leads to a double-escaping problem. for example, when I input z ``` \\\\ ``` , the Shell escapes it once, and then Python escapes it again. Although this method accurately handles Chinese characters occupying two character widths, users need to enter ``` \\\\\\\\ ``` in order to output a single backslash ``` \\ ``` . This requires entering, but since both Shell and Python perform escaping, you end up needing to input \\\\ just to get a single ``` text=\"這是\\\\\\\\size\" echo \"$text\" text_width=$(python3 -c \"from wcwidth import wcswidth; print(wcswidth('$text'))\") echo \"$text_width\" ``` I've tried many different approaches, but as a beginner, I haven't been able to make it work yet. Currently, the script blocks any input that contains special characters like backslashes or quotation marks.",
    "author_id":5830,
    "publication_date":1754226793000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Clyde Cole",
    "author_reputation":11.0,
    "tags":"python, shell",
    "text_length":1264,
    "title_length":85,
    "num_tags":2
  },
  {
    "id":6360,
    "title":"How can I build a text-to-text language translator in Python without using any APIs or built-in NLP libraries?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723988\/how-can-i-build-a-text-to-text-language-translator-in-python-without-using-any-a",
    "text":"I am trying to build a basic text translator in Python that converts English text into another language (e.g., Marathi, Hindi, etc.) without using any external APIs (such as Google Translate or Bing Translator) or NLP libraries (like NLTK, spaCy, or Hugging Face).",
    "author_id":5829,
    "publication_date":1754226870000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Parth Lande",
    "author_reputation":1.0,
    "tags":"translation",
    "text_length":264,
    "title_length":110,
    "num_tags":1
  },
  {
    "id":6359,
    "title":"Fortify reports HTTP Parameter Pollution in RestTemplate.exchange() – how to fix it safely?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723993\/fortify-reports-http-parameter-pollution-in-resttemplate-exchange-how-to-fix",
    "text":"Fortify is flagging the following code as HTTP Parameter Pollution (the problematic line is marked): Java 8, Spring Framework 5.3 ``` protected <T> T get(String url, Class<T> responseType, Object... urlVariables) { HttpEntity requestEntity = new HttpEntity(buildSecurityHttpHeaders()); HttpEntity<T> responseEntity = restTemplate.exchange( url, HttpMethod.GET, requestEntity, responseType, urlVariables); \/\/ <-- Fortify marks this line as vulnerable to HTTP Parameter Pollution if (responseEntity == null) { return null; } return responseEntity.getBody(); } ``` I tried adding a sanitization method for urlVariables, but Fortify still reports the issue. I understand that RestTemplate allows building URLs with parameters, but I'm not sure whether this usage is safe. My questions: Can using Object... urlVariables in this form really cause HTTP Parameter Pollution? What is the proper and secure way to pass URL parameters to RestTemplate.exchange()? How can I fix or refactor this method safely, considering it is used in ~200 places? Any guidance or real-world experience would be appreciated!",
    "author_id":5828,
    "publication_date":1754227609000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"rlNkoo",
    "author_reputation":44.0,
    "tags":"java, spring, security, resttemplate, fortify",
    "text_length":1096,
    "title_length":91,
    "num_tags":5
  },
  {
    "id":6358,
    "title":"Some woocommerce products are not showing with isotope filter",
    "link":"https:\/\/stackoverflow.com\/questions\/79723998\/some-woocommerce-products-are-not-showing-with-isotope-filter",
    "text":"I'm listing products in WooCommerce using the isotope filter. However, if a product is linked to two categories, it's not listed. Those linked to a single category are fine. I have no idea. Could you please help? the code where I list the categories(filter.php): ``` <div class=\"row filters my-3\"> <div class=\"col-md-4 mx-auto\"> <div class=\"dropdown-filter\"> <?php $args = array( 'taxonomy' => 'product_cat', \/\/ 'hide_empty' => 0, \/\/ 'pad_counts' => 1, \/\/ 'show_count' => 1, \/\/ 'parent' => 0, \/\/ 'hierarchical' => 1, \/\/ 'orderby' => 'name', ); $terms = get_categories( $args ); if( isset($terms) ) : echo '<select class=\"filter-select py-2 px-4 w-100\" value-group=\"thema\" name=\"categoryfilter\">'; echo '<option value=\"\" data-filter=\"*\">Tüm Gönderiler<\/option>'; foreach ( $terms as $term ) : echo '<option value=\".' . $term->slug . '\">' . $term->name . '<\/option>'; endforeach; echo '<\/select>'; endif; ?> <\/div> <\/div> <\/div> ``` the code I listed the products with(posts.php): ``` <?php wp_reset_postdata(); $args = array( 'post_type' => array('product'), 'taxonomy' => 'product_cat', 'posts_per_page' => -1, 'orderby' => 'date', 'order' => 'ASC', 'post_status' => 'publish', 'fields' => 'all', 'field' => 'term_id' ); $the_query = new WP_Query( $args ); ?> <style type=\"text\/css\">.product-isotope-filter.grid{width: 100%;margin: 0px;padding: 0px;}<\/style> <div class=\"product-isotope-filter grid\"> <div class=\"row row-cols-1 row-cols-sm-2 row-cols-md-3 row-cols-lg-4 g-3\"> <?php if ( $the_query->have_posts() ) : while ( $the_query->have_posts() ) : $the_query->the_post(); ?> <div class=\"col element-item <?php echo add_stack_slug_class(); ?>\" data-category=\"transition\"> <div class=\"card h-100\"> <div class=\"card-img-top img-s img-cover p-1\"> <?php the_post_thumbnail('', array( 'class' => 'object-fit-cover', 'alt' => esc_html ( get_the_title() ), 'title' => esc_html ( get_the_title() ), 'loading' => 'lazy' )); ?> <\/div> <div class=\"card-footer p-1 d-flex align-items-center justify-content-center\" style=\"min-height: 5rem;\"> <a href=\"<?php the_permalink(); ?>\"><h6 class=\"name mt-2 text-dark\"><?php the_title(); ?><\/h6><\/a> <\/div> <\/div> <\/div> <?php endwhile; ?> <?php else: ?> <h3>İçerik Yok<\/h3> <?php endif; ?> <\/div> <\/div> <?php wp_reset_postdata(); ?> ``` isotope-dropdown.js code ``` \/\/ external js: isotope.pkgd.js \/\/ init Isotope var $grid = $('.grid').isotope({ itemSelector: '.element-item' }); var filters = {}; $('.filters').on('change', function (event) { var $select = $(event.target); var filterGroup = $select.attr('value-group'); filters[filterGroup] = event.target.value; var filterValue = concatValues(filters); $grid.isotope({ filter: filterValue }); }); function concatValues(obj) { var value = ''; for (var prop in obj) { value += obj[prop]; } return value; } ``` I imported the isotope.pkgd.js file via CDN.",
    "author_id":5827,
    "publication_date":1754228018000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"wws34",
    "author_reputation":35.0,
    "tags":"php, wordpress, woocommerce, jquery-isotope, custom-taxonomy",
    "text_length":2841,
    "title_length":61,
    "num_tags":5
  },
  {
    "id":6357,
    "title":"How to feed gets to avoid manual input during test execution?",
    "link":"https:\/\/stackoverflow.com\/questions\/79723999\/how-to-feed-gets-to-avoid-manual-input-during-test-execution",
    "text":"I have a problem with one of my class methods. ``` class Input # Method to get a valid number of days to assign prices to def Input.get_valid_number_of_days puts \"Time to enter the number of days to be considered.\\nUse positive integers only\" # Loop requesting for a positive integer different than 0 loop do print \"How many days do you want to assign prices to? \" days_number = gets.chomp # Readability improvement isnt_digit_only = \/\\D\/.match?(days_number) return days_number.to_i if not isnt_digit_only and days_number != \"0\" # In case input is invalid puts \"This is not a positive integer different from 0.\" end end end ``` It basically request user input until a positive integer different from 0 is given. It is called on my main on an assignment, as follows: ``` number_of_days = Input.get_valid_number_of_days ``` I'm trying to write a test for this method in a way that I don't need to type (manually input) the value, so it runs automated, but there are many different approaches on RSpec documentation and Stack Overflow. This is the test I wrote: ``` require \".\/lib\/stock_picker\" # Test input related methods describe Input do describe \".get_valid_number_of_days\" do it \"Return a positive integer different than 0 when valid input\" do allow($stdin).to receive(:gets).and_return(\"3\\n\") days = $stdin.gets expect(days).to be_kind_of(Integer) end end end ``` I'm trying to make the test feed the ``` gets.chomp ``` entry on my method so the test runs truly automated. When I run ``` rspec ``` , it executes the method but crashes with the following error: ``` Time to enter the number of days to be considered. Use positive integers only How many days do you want to assign prices to? An error occurred while loading .\/spec\/stock_picker_spec.rb. Failure\/Error: days_number = gets.chomp Errno::ENOENT: No such file or directory @ rb_sysopen - --format # .\/lib\/stock_picker.rb:12:in `gets' # .\/lib\/stock_picker.rb:12:in `gets' # .\/lib\/stock_picker.rb:12:in `block in get_valid_number_of_days' # .\/lib\/stock_picker.rb:10:in `loop' # .\/lib\/stock_picker.rb:10:in `get_valid_number_of_days' # .\/lib\/stock_picker.rb:94:in `<top (required)>' # .\/spec\/stock_picker_spec.rb:2:in `<top (required)>' No examples found. Finished in 0.00008 seconds (files took 0.19796 seconds to load) 0 examples, 0 failures, 1 error occurred outside of examples ```",
    "author_id":5826,
    "publication_date":1754228075000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Neblinus",
    "author_reputation":23.0,
    "tags":"ruby, input, rspec",
    "text_length":2345,
    "title_length":61,
    "num_tags":3
  },
  {
    "id":6356,
    "title":"SignalR doesn&#39;t connect when [Authorize] attribute added to hub",
    "link":"https:\/\/stackoverflow.com\/questions\/79724001\/signalr-doesnt-connect-when-authorize-attribute-added-to-hub",
    "text":"I am trying to connect to SignalR. I am using ASP.NET Core Web API and Angular. I have tried almost everything that came to mind, including some chats with AI. If I remove the ``` [Authorize] ``` attribute while the application is running, it seems to work, however ``` Context.User.IsAuthorized ``` is false. If I close Visual Studio and rest with the ``` [Authorize] ``` attribute present, the problem persists. Here is my Hub: ``` using Microsoft.AspNetCore.SignalR; using System.Security.Claims; using System.Text.RegularExpressions; using limbo.dating.Server.DTO.Messages; using Microsoft.AspNetCore.Authorization; namespace limbo.dating.Server.Hubs { [Authorize] public class FriendHub : Hub { private readonly ILogger<FriendHub> _logger; public FriendHub(ILogger<FriendHub> logger) { _logger = logger; } public async Task JoinUserGroup() { var userId = Context.User?.FindFirstValue(ClaimTypes.NameIdentifier); if (userId != null) { await Groups.AddToGroupAsync(Context.ConnectionId, userId); _logger.LogInformation($\"User {userId} joined their notification group.\"); } } public override Task OnConnectedAsync() { Console.WriteLine(\"SignalR client connected: \" + Context.ConnectionId); return base.OnConnectedAsync(); } public override async Task OnDisconnectedAsync(Exception? exception) { var userId = Context.User?.FindFirstValue(ClaimTypes.NameIdentifier); if (userId != null) { await Groups.RemoveFromGroupAsync(Context.ConnectionId, userId); } await base.OnDisconnectedAsync(exception); } \/\/Messaging methods \/\/\/ <summary> \/\/\/ Send a message to a specific friend \/\/\/ <\/summary> public async Task SendMessageToFriend(CreateMessageDto createMessageDto) { \/\/TODO: senderId is null var senderId = Context.UserIdentifier; if (string.IsNullOrEmpty(senderId)) return; await Clients.Group(createMessageDto.RecipientId).SendAsync(\"ReceiveMessage\", new { MessageId = createMessageDto.MessageId, SenderId = senderId, RecipientId = createMessageDto.RecipientId, Content = createMessageDto.Content, Timestamp = DateTime.UtcNow, IsRead = false }); \/\/ Also send to sender's other devices await Clients.Group(senderId).SendAsync(\"MessageSentConfirmation\", new { MessageId = createMessageDto.MessageId, RecipientId = createMessageDto.RecipientId, Timestamp = DateTime.UtcNow }); } \/\/\/ <summary> \/\/\/ Notify when a message has been read \/\/\/ <\/summary> public async Task NotifyMessageRead(string messageId, string senderId) { await Clients.Group(senderId).SendAsync(\"MessageRead\", new { MessageId = messageId, ReadAt = DateTime.UtcNow }); } \/\/\/ <summary> \/\/\/ Send typing indicator to a friend \/\/\/ <\/summary> public async Task SendTypingIndicator(string recipientId, bool isTyping) { var senderId = Context.UserIdentifier; if (string.IsNullOrEmpty(senderId)) return; await Clients.Group(recipientId).SendAsync(\"FriendIsTyping\", new { SenderId = senderId, IsTyping = isTyping }); } \/\/\/ <summary> \/\/\/ Notify when messages are being loaded (for UI feedback) \/\/\/ <\/summary> public async Task NotifyLoadingMessages(string recipientId) { await Clients.Group(recipientId).SendAsync(\"FriendIsLoadingMessages\"); } \/\/\/ <summary> \/\/\/ Notify when new messages are available in a conversation \/\/\/ <\/summary> public async Task NotifyNewMessagesAvailable(string friendId) { var userId = Context.UserIdentifier; await Clients.Group(friendId).SendAsync(\"NewMessagesAvailable\", userId); } } } Here is my Program.cs: \/\/https:\/\/github.com\/MoonriseSoftwareCalifornia\/AspNetCore.Identity.CosmosDb \/\/https:\/\/github.com\/MoonriseSoftwareCalifornia\/AspNetCore.Identity.CosmosDb\/blob\/main\/AspNetCore.Identity.CosmosDb.Example\/Areas\/Identity\/Pages\/Account\/Login.cshtml.cs using AspNetCore.Identity.CosmosDb; using AspNetCore.Identity.CosmosDb.Containers; using AspNetCore.Identity.CosmosDb.Extensions; using limbo.dating.Server; using limbo.dating.Server.Controllers; using limbo.dating.Server.Data; using limbo.dating.Server.Data.AspNetCore.Identity.CosmosDb.Example.Data; using limbo.dating.Server.Hubs; using limbo.dating.Server.Models; using limbo.dating.Server.Models.AngularCoreCosmos.Models; using limbo.dating.Server.Services; using Microsoft.AspNetCore.Authentication.JwtBearer; using Microsoft.AspNetCore.Identity; using Microsoft.AspNetCore.Mvc; using Microsoft.EntityFrameworkCore; using Microsoft.Extensions.Azure; using Microsoft.IdentityModel.Tokens; using System.Security.Claims; using System.Text; using limbo.dating.Server.Models.limbo.dating.Server.Models; var builder = WebApplication.CreateBuilder(args); \/\/ The Cosmos connection string var connectionString = builder.Configuration.GetConnectionString(\"ApplicationDbContextConnection\"); var cosmosAccountKey = builder.Configuration.GetValue<string>(\"CosmosAccountKey\"); \/\/ Name of the Cosmos database to use var cosmosIdentityDbName = builder.Configuration.GetValue<string>(\"CosmosIdentityDbName\"); \/\/ If this is set, the Cosmos identity provider will: \/\/ 1. Create the database if it does not already exist. \/\/ 2. Create the required containers if they do not already exist. \/\/ IMPORTANT: Remove this setting if after first run. It will improve startup performance. var setupCosmosDb = builder.Configuration.GetValue<string>(\"SetupCosmosDb\"); \/\/ If the following is set, it will create the Cosmos database and \/\/ required containers. if (bool.TryParse(setupCosmosDb, out var setup) && setup) { var builder1 = new DbContextOptionsBuilder<LimboDatingDbContext>(); builder1.UseCosmos(connectionString, cosmosIdentityDbName); await using (var dbContext = new LimboDatingDbContext(builder1.Options)) { await dbContext.Database.EnsureCreatedAsync(); } } builder.Services.AddDbContext<LimboDatingDbContext>(options => options.UseCosmos(connectionString: connectionString, databaseName: cosmosIdentityDbName)); builder.Services.AddCosmosIdentity<LimboDatingDbContext, LimboUser, IdentityRole, string>( options => { options.SignIn.RequireConfirmedAccount = true; options.User.RequireUniqueEmail = true; options.Password.RequiredLength = 3; options.Password.RequireDigit = false; options.Password.RequireLowercase = false; options.Password.RequireNonAlphanumeric = false; options.Password.RequireUppercase = false; options.Lockout.AllowedForNewUsers = true; } \/\/ Always a good idea :) ) \/\/.AddDefaultUI() \/\/ Use this if Identity Scaffolding is in use .AddRoles<IdentityRole>() \/\/ be able to add roles .AddRoleManager<RoleManager<IdentityRole>>() \/\/ be able to make use of RoleManager .AddEntityFrameworkStores<LimboDatingDbContext>() \/\/ providing our context .AddSignInManager<SignInManager<LimboUser>>() \/\/ make use of Signin manager .AddUserManager<UserManager<LimboUser>>() \/\/ make use of UserManager to create users .AddEntityFrameworkStores<LimboDatingDbContext>() \/\/ providing our context .AddSignInManager<SignInManager<LimboUser>>() \/\/ make use of Signin manager .AddUserManager<UserManager<LimboUser>>() \/\/ make use of UserManager to create users .AddDefaultTokenProviders(); builder.Services.AddAzureClients(clientBuilder => { clientBuilder.AddBlobServiceClient(builder.Configuration.GetConnectionString(\"AzureBlobStorage\")); }); builder.Services.AddOptions<ImagesController.AzureBlobStorageSettings>() .BindConfiguration(ImagesController.AzureBlobStorageSettings.SectionName) .ValidateDataAnnotations(); \/\/ be able to inject JWTService class inside our Controllers builder.Services.AddScoped<JWTService>(); builder.Services.AddScoped<EmailService>(); builder.Services.AddScoped<ContextSeedService>(); \/\/ be able to authenticate users using JWT builder.Services.AddAuthentication(options => { options.DefaultScheme = \"JWT_OR_COOKIES\"; \/\/ Custom policy to check both options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme; }) .AddCookie(\"Cookies\", options => { options.LoginPath = \"\/Account\/Login\"; options.AccessDeniedPath = \"\/Account\/AccessDenied\"; options.Events.OnRedirectToLogin = context => { \/\/ Skip redirect for API calls if (context.Request.Path.StartsWithSegments(\"\/api\")) { context.Response.StatusCode = 401; return Task.CompletedTask; } context.Response.Redirect(context.RedirectUri); return Task.CompletedTask; }; }) .AddJwtBearer(options => { options.TokenValidationParameters = new TokenValidationParameters { \/\/ validate the token based on the key we have provided inside appsettings.development.json JWT:Key ValidateIssuerSigningKey = true, \/\/ the issuer singning key based on JWT:Key IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(builder.Configuration[\"JWT:Key\"])), \/\/ the issuer which in here is the api project url we are using ValidIssuer = builder.Configuration[\"JWT:Issuer\"], \/\/ validate the issuer (who ever is issuing the JWT) ValidateIssuer = true, \/\/ don't validate audience (angular side) ValidateAudience = false, ValidateLifetime = true, ClockSkew = TimeSpan.Zero }; options.Events = new JwtBearerEvents { OnMessageReceived = context => { var accessToken = context.Request.Query[\"access_token\"]; var path = context.HttpContext.Request.Path; \/\/ Only set the token if the request is for the SignalR hub if (!string.IsNullOrEmpty(accessToken) && path.StartsWithSegments(\"\/friendHub\")) { context.Token = accessToken; } return Task.CompletedTask; } }; }) .AddPolicyScheme(\"JWT_OR_COOKIES\", \"JWT_OR_COOKIES\", options => { options.ForwardDefaultSelector = context => { string authorization = context.Request.Headers.Authorization; if (!string.IsNullOrEmpty(authorization) && authorization.StartsWith(\"Bearer \")) return \"Bearer\"; return \"Cookies\"; }; }); builder.Services.AddHttpClient(); builder.Services.AddCors(options => { options.AddPolicy(name: \"AllowAll\", builder => { builder.WithOrigins(\"https:\/\/localhost:4200\") .AllowAnyHeader() \/\/.AllowAnyOrigin() .AllowAnyMethod() .AllowCredentials() .WithExposedHeaders(\"WWW-Authenticate\") .SetPreflightMaxAge(TimeSpan.FromSeconds(86400)); }); }); builder.Services.Configure<ApiBehaviorOptions>(options => { options.InvalidModelStateResponseFactory = actionContext => { var errors = actionContext.ModelState .Where(x => x.Value.Errors.Count > 0) .SelectMany(x => x.Value.Errors) .Select(x => x.ErrorMessage).ToArray(); var toReturn = new { Errors = errors }; return new BadRequestObjectResult(toReturn); }; }); builder.Services.AddAuthorization(opt => { opt.AddPolicy(\"AdminPolicy\", policy => policy.RequireRole(\"Admin\")); opt.AddPolicy(\"ManagerPolicy\", policy => policy.RequireRole(\"Manager\")); opt.AddPolicy(\"PlayerPolicy\", policy => policy.RequireRole(\"Player\")); opt.AddPolicy(\"AdminOrManagerPolicy\", policy => policy.RequireRole(\"Admin\", \"Manager\")); opt.AddPolicy(\"AdminAndManagerPolicy\", policy => policy.RequireRole(\"Admin\").RequireRole(\"Manager\")); opt.AddPolicy(\"AllRolePolicy\", policy => policy.RequireRole(\"Admin\", \"Manager\", \"Player\")); opt.AddPolicy(\"AdminEmailPolicy\", policy => policy.RequireClaim(ClaimTypes.Email, \"admin@example.com\")); opt.AddPolicy(\"MillerSurnamePolicy\", policy => policy.RequireClaim(ClaimTypes.Surname, \"miller\")); opt.AddPolicy(\"ManagerEmailAndWilsonSurnamePolicy\", policy => policy.RequireClaim(ClaimTypes.Surname, \"wilson\") .RequireClaim(ClaimTypes.Email, \"manager@example.com\")); opt.AddPolicy(\"VIPPolicy\", policy => policy.RequireAssertion(context => SD.VIPPolicy(context))); }); \/\/ Add services to the container. builder.Services.AddControllers(); \/\/ Learn more about configuring OpenAPI at https:\/\/aka.ms\/aspnet\/openapi builder.Services.AddOpenApi(); \/\/ Register the repository with a synchronous factory that blocks (only during startup) builder.Services.AddSingleton<IDocumentDBRepository<User>>(provider => { var config = provider.GetRequiredService<IConfiguration>(); \/\/ Block synchronously during app startup (acceptable for initialization) return DocumentDBRepository<User>.CreateAsync(connectionString, cosmosAccountKey, cosmosIdentityDbName) .GetAwaiter() .GetResult(); }); builder.Services.AddSingleton<IDocumentDBRepository<Friendship>>(provider => { var config = provider.GetRequiredService<IConfiguration>(); \/\/ Block synchronously during app startup (acceptable for initialization) return DocumentDBRepository<Friendship>.CreateAsync(connectionString, cosmosAccountKey, cosmosIdentityDbName) .GetAwaiter() .GetResult(); }); builder.Services.AddSingleton<IDocumentDBRepository<ChatMessage>>(provider => { var config = provider.GetRequiredService<IConfiguration>(); \/\/ Block synchronously during app startup (acceptable for initialization) return DocumentDBRepository<ChatMessage>.CreateAsync(connectionString, cosmosAccountKey, cosmosIdentityDbName) .GetAwaiter() .GetResult(); }); builder.Services.AddSignalR(hubOptions => { \/\/ Timeout settings hubOptions.ClientTimeoutInterval = TimeSpan.FromMinutes(2); hubOptions.HandshakeTimeout = TimeSpan.FromSeconds(15); \/\/ Keep-alive settings hubOptions.KeepAliveInterval = TimeSpan.FromSeconds(15); \/\/ Additional options hubOptions.EnableDetailedErrors = true; \/\/ For debugging hubOptions.MaximumParallelInvocationsPerClient = 1; }); var app = builder.Build(); \/\/ Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.MapOpenApi(); } app.UseHttpsRedirection(); \/\/app.MapControllers(); \/\/app.MapFallbackToController(\"Index\", \"Fallback\"); app.UseDefaultFiles(); app.UseStaticFiles(); app.MapStaticAssets(); app.UseCors(\"AllowAll\"); app.UseWebSockets(); \/\/ adding UseAuthentication into our pipeline and this should come before UseAuthorization \/\/ Authentication verifies the identity of a user or service, and authorization determines their access rights. app.UseAuthentication(); app.UseAuthorization(); \/\/ Add to your app configuration (before MapControllers) app.MapHub<FriendHub>(\"\/friendHub\"); app.MapControllers(); app.MapGet(\"\/test\", () => \"Server is running.\"); app.MapFallbackToFile(\"\/index.html\"); #region ContextSeed using var scope = app.Services.CreateScope(); try { var contextSeedService = scope.ServiceProvider.GetService<ContextSeedService>(); await contextSeedService.InitializeContextAsync(); } catch (Exception ex) { var logger = scope.ServiceProvider.GetService<ILogger<Program>>(); logger.LogError(ex.Message, \"Failed to initialize and seed the database\"); } #endregion app.Run(); And here is my signalr.service.ts: import { Injectable } from '@angular\/core'; import * as signalR from '@microsoft\/signalr'; import { Subject, Observable, BehaviorSubject } from 'rxjs'; import { environment } from 'src\/environments\/environment'; import { AccountService } from '..\/account\/account.service'; import { MessageDto } from '..\/models\/messageDto'; @Injectable({ providedIn: 'root' }) export class SignalRService { private hubURL = \"https:\/\/localhost:5210\/friendHub\"; public hubConnection!: signalR.HubConnection; private connectionState = new BehaviorSubject<signalR.HubConnectionState>(signalR.HubConnectionState.Disconnected); private notificationHandlers = new Map<string, (...args: any[]) => void>(); private messageQueue: { methodName: string, args: any[] }[] = []; private isExplicitlyDisconnected = false; public connectionState$ = this.connectionState.asObservable(); constructor(private authService: AccountService) { this.createConnection(); this.startConnection(); } private createConnection(): void { this.hubConnection = new signalR.HubConnectionBuilder() \/\/.withUrl(`${this.hubURL}?access_token=${this.authService.getJWT() ?? ''}`) .withUrl(this.hubURL, { accessTokenFactory: () => { const token = this.authService.getJWT(); console.log('Using token:', token); \/\/ Debug logging return token ?? ''; }, skipNegotiation: true, \/\/ Keep negotiation enabled for now transport: signalR.HttpTransportType.WebSockets }) \/\/ .withUrl(this.hubURL, { \/\/ accessTokenFactory: () => this.authService.getJWT() ?? '', \/\/ skipNegotiation: true, \/\/ transport: signalR.HttpTransportType.WebSockets \/\/ }) .withAutomaticReconnect({ nextRetryDelayInMilliseconds: retryContext => { return Math.min(retryContext.previousRetryCount * 2000, 10000); } }) .configureLogging(signalR.LogLevel.Warning) .build(); this.registerConnectionEvents(); } private registerConnectionEvents(): void { this.hubConnection.onreconnecting(error => { console.log(`SignalR reconnecting due to ${error?.message}`); this.connectionState.next(signalR.HubConnectionState.Reconnecting); }); this.hubConnection.onreconnected(connectionId => { console.log(`SignalR reconnected. New connection ID: ${connectionId}`); this.connectionState.next(signalR.HubConnectionState.Connected); this.processMessageQueue(); }); this.hubConnection.onclose(error => { console.log(`SignalR connection closed. ${error?.message}`); this.connectionState.next(signalR.HubConnectionState.Disconnected); if (!this.isExplicitlyDisconnected && error) { setTimeout(() => this.startConnection(), 5000); } }); } public async startConnection(): Promise<void> { if (this.hubConnection?.state === signalR.HubConnectionState.Disconnected) { this.isExplicitlyDisconnected = false; try { await this.hubConnection.start(); console.log('SignalR Connected'); this.connectionState.next(signalR.HubConnectionState.Connected); this.joinUserGroup(); this.processMessageQueue(); } catch (err) { console.error('Error starting SignalR connection:', err); this.connectionState.next(signalR.HubConnectionState.Disconnected); setTimeout(() => this.startConnection(), 5000); } } } private async processMessageQueue(): Promise<void> { while (this.messageQueue.length > 0 && this.hubConnection.state === signalR.HubConnectionState.Connected) { const message = this.messageQueue.shift(); try { await this.hubConnection.invoke(message!.methodName, ...message!.args); } catch (err) { console.error(`Error processing queued message ${message!.methodName}:`, err); this.messageQueue.unshift(message!); \/\/ Put it back if failed break; } } } private joinUserGroup(): void { this.invokeWithQueue('JoinUserGroup').catch(err => console.error('Error joining user group:', err)); } public async invokeWithQueue(methodName: string, ...args: any[]): Promise<any> { if (this.hubConnection.state === signalR.HubConnectionState.Connected) { try { return await this.hubConnection.invoke(methodName, ...args); } catch (err) { console.error(`Error invoking ${methodName}:`, err); throw err; } } else { console.log(`Queueing ${methodName} (connection state: ${this.hubConnection.state})`); this.messageQueue.push({ methodName, args }); if (this.hubConnection.state === signalR.HubConnectionState.Disconnected) { await this.startConnection(); } return Promise.resolve(); } } public on<T>(methodName: string, callback: (data: T) => void): void { if (this.notificationHandlers.has(methodName)) { const handler = this.notificationHandlers.get(methodName); if (handler) { this.hubConnection.off(methodName, handler); } } this.notificationHandlers.set(methodName, callback); this.hubConnection.on(methodName, callback); } public off(methodName: string): void { const handler = this.notificationHandlers.get(methodName); if (handler) { this.hubConnection.off(methodName, handler); this.notificationHandlers.delete(methodName); } } public async stopConnection(): Promise<void> { this.isExplicitlyDisconnected = true; try { await this.hubConnection.stop(); } catch (err) { console.error('Error stopping SignalR connection:', err); } } addFriendRequestListener(callback: (request: { fromUserName: string }) => void) { this.on('ReceiveFriendRequest', callback); } notifyMessageRead(messageId: string): void { this.invokeWithQueue('MarkAsRead', messageId) .catch(err => console.error('Error notifying message read:', err)); } sendMessage(messageDto: MessageDto): void { this.invokeWithQueue('SendMessageToFriend', messageDto) .catch(err => console.error('Error sending message:', err)); } } ```",
    "author_id":5825,
    "publication_date":1754228350000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Eis Karlsson",
    "author_reputation":88.0,
    "tags":"c#, angular, asp.net-core-webapi",
    "text_length":19549,
    "title_length":67,
    "num_tags":3
  },
  {
    "id":6355,
    "title":"xcode run kotlin multiplatform project cannot install iosApp on IPhone16",
    "link":"https:\/\/stackoverflow.com\/questions\/79724002\/xcode-run-kotlin-multiplatform-project-cannot-install-iosapp-on-iphone16",
    "text":"Xcode run kotlin multiplatform project to install iosApp on IPhone16, how to fixed it ? I had download a blank kmp project , and installed successfully on IPhone16. After installing the iOS app on a real device (iPhone 15), it was immediately uninstalled. Xcode reported the error shown in the screenshot. and it can be archived to AppStore. ``` 无法安装“KotlinAppProject” Domain: IXUserPresentableErrorDomain Code: 1 Recovery Suggestion: Application is missing the application-identifier entitlement. User Info: { DVTErrorCreationDateKey = \"2025-08-03 11:32:01 +0000\"; IDERunOperationFailingWorker = IDEInstallCoreDeviceWorker; } -- Failed to install the app on the device. Domain: com.apple.dt.CoreDeviceError Code: 3002 User Info: { NSURL = \"file:\/\/\/Users\/HOX4SGH\/Library\/Developer\/Xcode\/DerivedData\/iosApp-gbaojemzkfyvhrbgladtchqcvlfq\/Build\/Products\/Debug-iphoneos\/KotlinAppProject.app\"; } -- 无法安装“KotlinAppProject” Domain: IXUserPresentableErrorDomain Code: 1 Failure Reason: 请稍后再试。 Recovery Suggestion: Application is missing the application-identifier entitlement. -- Application is missing the application-identifier entitlement. Domain: MIInstallerErrorDomain Code: 63 User Info: { FunctionName = \"-[MIInstallableBundle _validateApplicationIdentifierForNewBundleSigningInfo:error:]\"; LegacyErrorString = ApplicationVerificationFailed; SourceFileLine = 1130; } -- Event Metadata: com.apple.dt.IDERunOperationWorkerFinished : { \"device_identifier\" = \"00008140-000435313CF2801C\"; \"device_isCoreDevice\" = 1; \"device_model\" = \"iPhone17,3\"; \"device_osBuild\" = \"18.5 (22F76)\"; \"device_platform\" = \"com.apple.platform.iphoneos\"; \"device_thinningType\" = \"iPhone17,3\"; \"dvt_coredevice_version\" = \"443.24\"; \"dvt_coresimulator_version\" = \"1010.15\"; \"dvt_mobiledevice_version\" = \"1784.120.3\"; \"launchSession_schemeCommand\" = Run; \"launchSession_state\" = 1; \"launchSession_targetArch\" = arm64; \"operation_duration_ms\" = 5069; \"operation_errorCode\" = 1; \"operation_errorDomain\" = IXUserPresentableErrorDomain; \"operation_errorWorker\" = IDEInstallCoreDeviceWorker; \"operation_name\" = IDERunOperationWorkerGroup; \"param_debugger_attachToExtensions\" = 0; \"param_debugger_attachToXPC\" = 1; \"param_debugger_type\" = 3; \"param_destination_isProxy\" = 0; \"param_destination_platform\" = \"com.apple.platform.iphoneos\"; \"param_diag_113575882_enable\" = 0; \"param_diag_MainThreadChecker_stopOnIssue\" = 0; \"param_diag_MallocStackLogging_enableDuringAttach\" = 0; \"param_diag_MallocStackLogging_enableForXPC\" = 1; \"param_diag_allowLocationSimulation\" = 1; \"param_diag_checker_tpc_enable\" = 1; \"param_diag_gpu_frameCapture_enable\" = 0; \"param_diag_gpu_shaderValidation_enable\" = 0; \"param_diag_gpu_validation_enable\" = 0; \"param_diag_guardMalloc_enable\" = 0; \"param_diag_memoryGraphOnResourceException\" = 0; \"param_diag_mtc_enable\" = 1; \"param_diag_queueDebugging_enable\" = 1; \"param_diag_runtimeProfile_generate\" = 0; \"param_diag_sanitizer_asan_enable\" = 0; \"param_diag_sanitizer_tsan_enable\" = 0; \"param_diag_sanitizer_tsan_stopOnIssue\" = 0; \"param_diag_sanitizer_ubsan_enable\" = 0; \"param_diag_sanitizer_ubsan_stopOnIssue\" = 0; \"param_diag_showNonLocalizedStrings\" = 0; \"param_diag_viewDebugging_enabled\" = 1; \"param_diag_viewDebugging_insertDylibOnLaunch\" = 1; \"param_install_style\" = 2; \"param_launcher_UID\" = 2; \"param_launcher_allowDeviceSensorReplayData\" = 0; \"param_launcher_kind\" = 0; \"param_launcher_style\" = 99; \"param_launcher_substyle\" = 0; \"param_runnable_appExtensionHostRunMode\" = 0; \"param_runnable_productType\" = \"com.apple.product-type.application\"; \"param_structuredConsoleMode\" = 1; \"param_testing_launchedForTesting\" = 0; \"param_testing_suppressSimulatorApp\" = 0; \"param_testing_usingCLI\" = 0; \"sdk_canonicalName\" = \"iphoneos18.5\"; \"sdk_osVersion\" = \"18.5\"; \"sdk_variant\" = iphoneos; } -- System Information macOS Version 15.5 (Build 24F74) Xcode 16.4 (23792) (Build 16F6) Timestamp: 2025-08-03T19:32:01+08:00 ```",
    "author_id":4795,
    "publication_date":1754228354000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Eric",
    "author_reputation":43.0,
    "tags":"ios, xcode, kmp",
    "text_length":3908,
    "title_length":72,
    "num_tags":3
  },
  {
    "id":6354,
    "title":"Why does the buy page no longer exist on my website?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724008\/why-does-the-buy-page-no-longer-exist-on-my-website",
    "text":"I am no longer able to access the buy page of my website. I just receive an error that the page no longer exists even though it is defined in routes and there is an html template for it. All other pages in my website work and so I am confused why this one doesn't. Image of error on website within buy page Below is the code that I thought might be relevant to the problem: init .py ``` from flaskblog.extensions import db, login_manager, bcrypt, mail, migrate, config, Flask def create_app(config_class=config): app = Flask(__name__) app.config.from_object(config_class) db.init_app(app) migrate.init_app(app, db) bcrypt.init_app(app) mail.init_app(app) login_manager.init_app(app) from flaskblog.users.routes import users from flaskblog.posts.routes import posts from flaskblog.main.routes import main from flaskblog.errors.handlers import errors from flaskblog.trading.routes import trading app.register_blueprint(users) app.register_blueprint(posts) app.register_blueprint(main) app.register_blueprint(errors) app.register_blueprint(trading) return app ``` section from routes.py within trading folder ``` trading = Blueprint('trading', __name__) @trading.route(\"\/buy\", methods=['GET', 'POST']) @login_required def buy(): total_value = calculate_portfolio_value() form = BuyForm() if form.validate_on_submit(): price = get_current_price(form.symbol.data.upper()) transaction = StockTransaction(user_id=current_user.id, stock_symbol=form.symbol.data.upper(), shares=form.shares.data, price= price or 0, transaction_type='buy') db.session.add(transaction) db.session.commit() flash(f'Bought {form.shares.data} shares of {form.symbol.data.upper()}', 'success') return redirect(url_for('buy.html')) stock_options = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA'] return render_template('buy.html', form=form, portfolio=current_user.get_portfolio(), advice=get_stock_advice(), total_value=total_value, stock_options=stock_options) ``` buy.html within templates folder ``` {% extends \"layout.html\" %} {% block content %} <h2>Buy Stocks<\/h2> <p><strong>Advice:<\/strong> {{ advice }}<\/p> <form method=\"POST\"> {{ form.hidden_tag() }} <p>Stock Symbol<\/p> <div class=\"form-group\"> <select name=\"symbol\" class=\"form-control\"> {% for ticker in stock_options %} <option value=\"{{ ticker }}\">{{ ticker }}<\/option> {% endfor %} <\/select> <br> {{ form.shares.label }} {{ form.shares(class=\"form-control\") }} <br> {{ form.submit(class=\"btn btn-success mt-2\") }} <\/div> <\/form> <div style=\"float: right; width: 300px; margin-top: 80px;\"> <h3 class=\"mt-4\">Your Portfolio<\/h3> <ul> {% for symbol, shares in portfolio.items() %} <li>{{ symbol }} — {{ shares }} shares<\/li> {% else %} <li>No holdings yet.<\/li> {% endfor %} <\/ul> <\/div> {% endblock %} ``` The only other point at which buy.html is mentioned is in the layout.html page: ``` <!DOCTYPE html> <html> <head> <!-- Required meta tags --> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"> <!-- Bootstrap CSS --> <link rel=\"stylesheet\" href=\"https:\/\/maxcdn.bootstrapcdn.com\/bootstrap\/4.0.0\/css\/bootstrap.min.css\" integrity=\"sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW\/dAiS6JXm\" crossorigin=\"anonymous\"> <link rel=\"stylesheet\" type=\"text\/css\" href=\"{{ url_for('static', filename='main.css') }}\"> {% if title %} <title>Easy Trading - {{ title }}<\/title> {% else %} <title>Easy Trading<\/title> {% endif %} <\/head> <body> <header class=\"site-header\"> <nav class=\"navbar navbar-expand-md navbar-dark bg-steel fixed-top\"> <div class=\"container\"> <a class=\"navbar-brand mr-4\" href=\"\/\">Easy Trading<\/a> <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarToggle\" aria-controls=\"navbarToggle\" aria-expanded=\"false\" aria-label=\"Toggle navigation\"> <span class=\"navbar-toggler-icon\"><\/span> <\/button> <div class=\"collapse navbar-collapse\" id=\"navbarToggle\"> <div class=\"navbar-nav\" ml-auto> {% if current_user.is_authenticated %} <a class=\"nav-item nav-link\" href=\"{{ url_for('users.account') }}\">Account<\/a> <a class=\"nav-item nav-link\" href=\"{{ url_for('users.logout') }}\">Logout<\/a> {% else %} <a class=\"nav-item nav-link\" href=\"{{ url_for('users.login') }}\">Login<\/a> <a class=\"nav-item nav-link\" href=\"{{ url_for('users.register') }}\">Register<\/a> {% endif %} <\/div> <\/div> <\/div> <\/nav> <\/header> <main role=\"main\" class=\"container\"> <div class=\"row\"> <div class=\"col-md-8\"> {% with messages = get_flashed_messages(with_categories=true) %} {% if messages %} {% for category, message in messages %} <div class=\"alert alert-{{ category }}\"> {{ message }} <\/div> {% endfor %} {% endif %} {% endwith %} {% block content %}{% endblock %} <\/div> {% if request.endpoint == 'posts.all_posts' %} <div class=\"col-md-4\"> <div class=\"content-section\"> <h3>Navigation Bar<\/h3> <ul class=\"list-group\"> <a href=\"{{ url_for('main.portfolio') }}\" class=\"list-group-item list-group-item-light\">Portfolio<\/a> <a href=\"{{ url_for('posts.new_post') }}\" class=\"list-group-item list-group-item-light\">New Post<\/a> <\/ul> <\/div> <\/div> {% endif %} {% if request.endpoint == 'main.home' %} <div class=\"col-md-4\"> {% if current_user.is_authenticated %} <div class=\"content-section\"> <h3>Navigation Bar<\/h3> <ul class=\"list-group\"> <a href=\"{{ url_for('main.portfolio') }}\" class=\"list-group-item list-group-item-light\">Portfolio<\/a> <a href=\"{{ url_for('posts.all_posts') }}\" class=\"list-group-item list-group-item-light\">Blog<\/a> <\/ul> <\/div> {% endif %} <\/div> {% endif %} {% if request.endpoint == 'main.portfolio' %} <div class=\"col-md-4\"> <div class=\"content-section\"> <h3>Navigation Bar<\/h3> <ul class=\"list-group\"> <a href=\"{{ url_for('main.home') }}\" class=\"list-group-item list-group-item-light\">Buy Stocks<\/a> <a href=\"{{ url_for('trading.sell') }}\" class=\"list-group-item list-group-item-light\">Sell Stocks<\/a> <\/ul> <\/div> <\/div> {% endif %} {% if request.endpoint == 'trading.buy' %} <div class=\"col-md-4\"> <div class=\"content-section\"> <h3>Navigation Bar<\/h3> <ul class=\"list-group\"> <a href=\"{{ url_for('trading.sell') }}\" class=\"list-group-item list-group-item-light\">Sell Stocks<\/a> <a href=\"{{ url_for('main.portfolio') }}\" class=\"list-group-item list-group-item-light\">Portfolio<\/a> <\/ul> <\/div> <\/div> {% endif %} {% if request.endpoint == 'trading.sell' %} <div class=\"col-md-4\"> <div class=\"content-section\"> <h3>Navigation Bar<\/h3> <ul class=\"list-group\"> <a href=\"{{ url_for('main.portfolio') }}\" class=\"list-group-item list-group-item-light\">Portfolio<\/a> <a href=\"{{ url_for('trading.buy') }}\" class=\"list-group-item list-group-item-light\">Buy Stocks<\/a> <\/ul> <\/div> <\/div> {% endif %} <\/div> <\/div> <\/main> <\/body> <\/html> ``` (sorry if its a lot of code, I’ve tried to add the minimum amount to explain the problem)",
    "author_id":5824,
    "publication_date":1754229152000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"hanna axson hanell",
    "author_reputation":1.0,
    "tags":"python, html, flask",
    "text_length":6760,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":6353,
    "title":"How to show date numbers in css grid calendar",
    "link":"https:\/\/stackoverflow.com\/questions\/79724009\/how-to-show-date-numbers-in-css-grid-calendar",
    "text":"How do I make the numbers shift in this makeshift calendar grid? ``` <div class=\"calGrid\"> <div *ngFor=\"let d of dates;let index=index\" class=\"calDate\"> <div *ngIf=\"index < 7\">{{d.day}}<\/div> <div>{{d.date}}<\/div> <\/div> <\/div> ``` to show up like this: but how do I make the numbers shift when the first day of the month starts on say, Fri ?",
    "author_id":5823,
    "publication_date":1754229191000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aria",
    "author_reputation":423.0,
    "tags":"angular, css-grid, ngfor",
    "text_length":342,
    "title_length":45,
    "num_tags":3
  },
  {
    "id":6352,
    "title":"Does the dynamic keyword box the value types?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724012\/does-the-dynamic-keyword-box-the-value-types",
    "text":"This project is ASP.NET MVC Framework 4.8 with Entity Framework 6 and uses: ``` dynamic GetSystemSetting(string paramCode, string paramType) ``` It contains Int, Float, Bool, String, and Guid values. It is one of the most used methods in the application. I know dynamic is bad. I need more context. That function is: ``` public dynamic GetSystemSetting(string paramCode, string paramType) { dynamic result = \"\"; \/\/ Code implementation here switch(paramType) { case \"B\": result = _dataService.Get(paramCode).BitValue; break; case \"S\": result = _dataService.Get(paramCode).StringValue; break; case \"N\": result = _dataService.Get(paramCode).NumericValue; break; case \"G\": result = _dataService.Get(paramCode).GuidValue; break; } return result; } ``` ``` GuidValue ``` , ``` StringValue ``` , ``` NumericValue ``` , and ``` BitValue ``` are columns in the database. I benchmarked the dynamic keyword and observed the value types are getting boxed and memory allocated on the Heap. Was a good implementation and does my benchmark make sense? The benchmark: ``` using BenchmarkDotNet.Attributes; namespace Benchmark_EfCore.Benchmarks; [MemoryDiagnoser] public class DynamicBenchmark { private List<int> IntList { get; set; } = new List<int>(); private List<Guid> GuidList { get; set; } = new List<Guid>(); [GlobalCleanup] public void GlobalSetup() { IntList = Enumerable.Range(0, 100).ToList(); for (var i = 0; i < IntList.Count; i++) { GuidList .Add(Guid.Empty); } } [Benchmark] public dynamic GetIntValue() { return IntList.Find(a => a == 50); } [Benchmark] public int GetValueExplicit() { return IntList.Find(a => a == 50); } [Benchmark] public dynamic GetGuidValue() { return GuidList.Find(a => a == Guid.Empty); } [Benchmark] public Guid GetGuidValueExplicit() { return GuidList.Find(a => a == Guid.Empty); } } ``` The results: ``` BenchmarkDotNet v0.13.7, Windows 11 (10.0.22631.5699) 11th Gen Intel Core i3-1115G4 3.00GHz, 1 CPU, 4 logical and 2 physical cores .NET SDK 8.0.404 [Host] : .NET 8.0.17 (8.0.1725.26602), X64 RyuJIT AVX2 DefaultJob : .NET 8.0.17 (8.0.1725.26602), X64 RyuJIT AVX2 ``` Method Mean Error StdDev Median Gen0 Allocated GetIntValue 4.0984 ns 0.1340 ns 0.1434 ns 4.0551 ns 0.0076 24 B GetValueExplicit 0.5986 ns 0.0664 ns 0.1957 ns 0.5383 ns - - GetGuidValue 4.6831 ns 0.1457 ns 0.2181 ns 4.6676 ns 0.0102 32 B GetGuidValueExplicit 0.9470 ns 0.0466 ns 0.0573 ns 0.9554 ns - - I decompiled the code with IL Viewer in JetBrains Rider: ``` [Benchmark(22, \"D:\\\\Benchmark_EfCore\\\\Benchmarks\\\\DynamicBenchmark.cs\")] [return: Dynamic] public object GetIntValue() { return (object) this.IntList.Find(DynamicBenchmark.<>c.<>9__9_0 ?? (DynamicBenchmark.<>c.<>9__9_0 = new Predicate<int>((object) DynamicBenchmark.<>c.<>9, __methodptr(<GetIntValue>b__9_0)))); } [Benchmark(28, \"D:\\\\Benchmark_EfCore\\\\Benchmarks\\\\DynamicBenchmark.cs\")] public int GetValueExplicit() { return this.IntList.Find(DynamicBenchmark.<>c.<>9__10_0 ?? (DynamicBenchmark.<>c.<>9__10_0 = new Predicate<int>((object) DynamicBenchmark.<>c.<>9, __methodptr(<GetValueExplicit>b__10_0)))); } ``` Is that helper function good practice? It was on Entity Framework 6 (not Entity Framework Core). Please explain this dynamic usage and how it affects the application at runtime and through put.",
    "author_id":5822,
    "publication_date":1754229427000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Guhan Srinivasan",
    "author_reputation":99.0,
    "tags":"c#, .net, asp.net-mvc, entity-framework, dynamic",
    "text_length":3268,
    "title_length":45,
    "num_tags":5
  },
  {
    "id":6351,
    "title":"TypeScript error when linking to nested route with parentheses &quot;Type &#39;\/(auth)\/register&#39; is not assignable to type &#39;RelativePathString&#39;&quot;",
    "link":"https:\/\/stackoverflow.com\/questions\/79724025\/typescript-error-when-linking-to-nested-route-with-parentheses-type-auth-re",
    "text":"I'm building a React Native app using expo-router, and I’ve set up my authentication routes inside a group folder: ``` app\/ ├── (auth)\/ │ ├── login.tsx │ └── register.tsx ├── (tabs)\/ │ └── index.tsx ``` Now I’m trying to link to \/register from my onboarding screen using: ``` <Link href=\"\/(auth)\/register\" asChild> <Pressable> <Text>Create an account<\/Text> <\/Pressable> <\/Link> ``` But I get the following TypeScript error: TS2322: Type '\"\/(auth)\/register\"' is not assignable to type 'RelativePathString | ExternalPathString | \"\/\" | ...' I also tried: ``` <Link href={{ pathname: '\/(auth)\/register', }} asChild > <Pressable> <Text>Create an account<\/Text> <\/Pressable> <\/Link>` ``` But the error persists. What I'm trying to do: Navigate to the register.tsx screen from onboarding.tsx Keep my routes grouped inside \/(auth)\/ for organizational purposes What I’ve tried: Using string ( ``` href=\"\/(auth)\/register\" ``` ) Using object ( ``` href={{ pathname: '\/(auth)\/register' }} ``` ) Renaming the route to remove parentheses ( ``` \/register ``` ) Reading the official docs (no clear mention of this TS type limitation) How should I properly link to a grouped route like ``` \/(auth)\/register ``` when using expo-router and TypeScript? Do I need to adjust the router config or move the route?",
    "author_id":5821,
    "publication_date":1754231753000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Patrick Kusanagi",
    "author_reputation":1.0,
    "tags":"react-native, expo, syntax",
    "text_length":1290,
    "title_length":161,
    "num_tags":3
  },
  {
    "id":6350,
    "title":"Variable interpolation",
    "link":"https:\/\/stackoverflow.com\/questions\/79724028\/variable-interpolation",
    "text":"I am trying to interpolate variables within an Ansible play. I have the following variables: ``` target_hostname user_account account_password ``` ``` user_account_password ``` is in a vault, and the variable is named ``` target_hostname.user_account.user_account_password ``` , e.g., ``` myhost.username.thepassword ``` . The variables defined: ``` build: common: system_accounts: - name: 'ansible' group: 'ansible' uid: '5000' gid: '5000' - name: 'sysadm' group: 'sysadm' uid: '5001' gid: '5001' ``` And the entries in the vault: ``` target_hostname: bootloader_password: bootloader123 root.account_password: root123 sysadm.account_password: sysadm123 ansible.account_password: ansible123 ``` Tried using varying types of interpolation: ``` - name: Ensure users exist with appropriate UID ansible.builtin.user: name: \"{{ system_account_items.name }}\" uid: \"{{ system_account_items.uid }}\" umask: \"022\" group: \"{{ system_account_items.group }}\" password: \"{{ target_hostname.[{{ system_account_items.name }}].account_password | password_hash('sha512') }}\" update_password: always with_items: \"{{ build.common.system_accounts }}\" loop_control: loop_var: system_account_items ```",
    "author_id":5820,
    "publication_date":1754232054000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Kevin Huntly",
    "author_reputation":13.0,
    "tags":"interpolation, variables, ansible",
    "text_length":1178,
    "title_length":22,
    "num_tags":3
  },
  {
    "id":6349,
    "title":"Decoding DBA_HIST_SQLBIND.value_string for the DATE type",
    "link":"https:\/\/stackoverflow.com\/questions\/79724029\/decoding-dba-hist-sqlbind-value-string-for-the-date-type",
    "text":"I would like to decode DATE value written as varchar2 in ``` DBA_HIST_SQLBIND ``` , but unsure where to look for the mask required to decode it properly, ``` SYS> select value_string from DBA_HIST_SQLBIND where datatype_string='DATE' fetch first 4 rows only; VALUE_STRING -------------------------------------------------------------------------------- 08\/03\/2025 13:04:27 08\/03\/2025 13:04:27 08\/03\/2025 13:03:57 08\/03\/2025 13:05:27 SYS> select value from NLS_DATABASE_PARAMETERS where parameter='NLS_DATE_FORMAT'; VALUE ---------------------------------------------------------------- DD-MON-RR ``` Is it safe to assume the mask is always ``` MM\/DD\/YYYY HH24:MI:SS ``` for ``` DBA_HIST_SQLBIND.value_string ``` regardless of NLS setting?",
    "author_id":5819,
    "publication_date":1754232104000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mpapec",
    "author_reputation":50745.0,
    "tags":"oracle-database, database-administration, oracle12.1",
    "text_length":738,
    "title_length":56,
    "num_tags":3
  },
  {
    "id":6348,
    "title":"How to keep track of logged in user with google_sign_in v7.x?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724031\/how-to-keep-track-of-logged-in-user-with-google-sign-in-v7-x",
    "text":"``` attemptLightweightAuthentication() ``` replaces the ``` signInSilently() ``` but ``` attemptLightweightAuthentication() ``` always open a visual indicator which shows user that they are singing in to our app with their google account. Which means calling ``` attemptLightweightAuthentication() ``` at the launch of app is not best way to sign in user every time app is launched. But then how? Migration guide for 6.x to 7.x mentions The plugin no longer tracks a single \"current\" signed in user. Instead, applications that assume a single signed in user should track this at the application level using the authenticationEvents stream. And signInSilently has been replaced with attemptLightweightAuthentication. The intended usage is essentially the same, but the change reflects that it is no longer guaranteed to be silent. For example, as of the publishing of 7.0, on web this may show a floating sign-in card, and on Android it may show an account selection sheet. So how do we keep track of logged in user across app's lifecycle and relaunches? Currently this is my auth handling code for a demo sign in app: ``` GoogleSignInAccount? _currentUser; @override void initState() { super.initState(); final GoogleSignIn signIn = GoogleSignIn.instance; unawaited(signIn.initialize().then((_) { signIn.authenticationEvents.listen(_handleAuthenticationEvent); signIn.attemptLightweightAuthentication(); })); } void _handleAuthenticationEvent(GoogleSignInAuthenticationEvent event) { final GoogleSignInAccount? user = switch (event) { GoogleSignInAuthenticationEventSignIn(:final user) => user, GoogleSignInAuthenticationEventSignOut() => null, }; setState(() { _currentUser = user; }); } Future<void> _handleSignOut() async { await GoogleSignIn.instance.disconnect(); } ``` Which triggers a visual login bottom sheet every time app is launched. I need a way I can get logged in user when app is launched without showing user any visual indicator.",
    "author_id":4917,
    "publication_date":1754232156000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"thisisjaymehta",
    "author_reputation":711.0,
    "tags":"flutter, google-signin, googlesigninaccount",
    "text_length":1947,
    "title_length":61,
    "num_tags":3
  },
  {
    "id":6347,
    "title":"admin panel data grid filter and sort on multiple tables",
    "link":"https:\/\/stackoverflow.com\/questions\/79724036\/admin-panel-data-grid-filter-and-sort-on-multiple-tables",
    "text":"I have a website with products, categories,orders,.. in laravel. there are many tables and many relationship between them. the backend generate API for admin panel that displays different datagrids. cosider proudcts datagrid show product name, category (related categories table) , orders count ,... the admin need to sort data by each column (includes related tables) , Column filter ,... I am thinking about mysql view . I generate view with desired columns and sort or filter (using where) Simply. or use elequent join and write each filter query or sort. what is common or best practice for this problem? I mean providing data in admin panel with power of filter and sort",
    "author_id":5818,
    "publication_date":1754232685000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"zohrehda",
    "author_reputation":680.0,
    "tags":"php, mysql, laravel, sql-view",
    "text_length":675,
    "title_length":56,
    "num_tags":4
  },
  {
    "id":6346,
    "title":"How to Pass parameters in XSL pipeline running in Saxon HE 12",
    "link":"https:\/\/stackoverflow.com\/questions\/79724040\/how-to-pass-parameters-in-xsl-pipeline-running-in-saxon-he-12",
    "text":"I have a pipeline. XSL, which should run multiple XSLT transformations on one input, I want to pass some parameters to the child XSLs used in the pipeline.xsl Problem: The parameters are not passed to the child XSLT, it always returns blank in the child. I could not figure out how to pass the parameter here. I have only issue in passing the parameter, other than that pipeline works fine for transformation. I try to pass the parameter to validate a certain condition I the pipeline. Is this method wrong? Or Saxon-he doesn't support this one. I used the parameters field\/name to map the parameters as below ``` 'parameters': map { 'docType': $docType, 'debugCheck': 'from-pipeline' } ``` command: java -jar \"saxon-he-12.8.jar\" -xsl:pipeline.xsl -s:input.xml -o:output.xml docType=test pipeline.xsl ``` <?xml version=\"1.0\" encoding=\"UTF-8\"?> <xsl:stylesheet version=\"3.0\" xmlns:xsl=\"http:\/\/www.w3.org\/1999\/XSL\/Transform\" xmlns:xs=\"http:\/\/www.w3.org\/2001\/XMLSchema\"> <xsl:output method=\"xml\" indent=\"yes\"\/> <xsl:param name=\"docType\" as=\"xs:string\"\/> <xsl:template match=\"\/\"> <xsl:message select=\"'[PIPELINE] docType param: ', $docType\"\/> <xsl:variable name=\"runtime\" select=\"map { 'stylesheet-location': 'child.xsl', 'source-node': ., 'initial-template': QName('', 'main'), 'parameters': map { 'docType': $docType, 'debugCheck': 'from-pipeline' } }\"\/> <xsl:sequence select=\"transform($runtime)?output\"\/> <\/xsl:template> <\/xsl:stylesheet> ``` child.xsl: ``` <?xml version=\"1.0\" encoding=\"UTF-8\"?> <xsl:stylesheet version=\"3.0\" xmlns:xsl=\"http:\/\/www.w3.org\/1999\/XSL\/Transform\"> <xsl:output method=\"xml\" indent=\"yes\"\/> <xsl:template name=\"main\"> <xsl:param name=\"docType\"\/> <xsl:param name=\"debugCheck\"\/> <xsl:message select=\"'[CHILD] docType: ', $docType\"\/> <xsl:message select=\"'[CHILD] debugCheck: ', $debugCheck\"\/> <result docType=\"{$docType}\" debugCheck=\"{$debugCheck}\"\/> <\/xsl:template> <\/xsl:stylesheet> ```",
    "author_id":5817,
    "publication_date":1754233074000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jason",
    "author_reputation":45.0,
    "tags":"xslt, saxon, xslt-3.0",
    "text_length":1912,
    "title_length":61,
    "num_tags":3
  },
  {
    "id":6345,
    "title":"How to find proper address to use AggregatorV3Interface?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724041\/how-to-find-proper-address-to-use-aggregatorv3interface",
    "text":"I am using hardhat and metamask on a local blockchain and I have imported ``` AggregatorV3Interface ``` and am trying to get the latest price of USDT in my contract to exchange Ethereuem to USDT. Here is the minimized version of my contract: ``` \/\/ SPDX-License-Identifier: Unlicensed pragma solidity ^0.8.28; import {AggregatorV3Interface} from \"@chainlink\/contracts\/src\/v0.8\/shared\/interfaces\/AggregatorV3Interface.sol\"; contract token { AggregatorV3Interface internal dataFeed; constructor() { dataFeed = AggregatorV3Interface( 0x89983A2FDd082FA40d8062BCE3986Fc601D2d29B \/\/ Where this address must come from? ); } function buyTokenWithETH(address receivingAddress, uint256 ethBalance) public payable returns (bool) { uint256 ethPrice = getChainlinkDataFeedLatestAnswer(); return true; } function getChainlinkDataFeedLatestAnswer() public view returns (uint256) { \/\/ prettier-ignore ( \/* uint80 roundId *\/, int256 answer, \/*uint256 startedAt*\/, \/*uint256 updatedAt*\/, \/*uint80 answeredInRound*\/ ) = dataFeed.latestRoundData(); return uint256(answer); } ``` and here is the frontend: ``` let walletAddress = \"0x7...8\"; let tokenAddress = \"0xc...d\"; ... let ethBalance = await provider.getBalance(tokenAddress)\/decimals; let buy_Token = await tokenContract['buyTokenWithETH'](walletAddress, ethBalance); ``` If I hard code the value of ``` ethPrice ``` it works, but when I call the function ``` getChainlinkDataFeedLatestAnswer() ``` then I get the error: ``` > Error: execution reverted (no data present; likely require(false) occurred (action=\"estimateGas\", data=\"0x\", reason=\"require(false)\", transaction={ \"data\": \"0xb...003\", \"from\": \"0x7...8\", \"to\": \"0xc...d\" }, invocation=null, revert=null, code=CALL_EXCEPTION, version=6.15.0) ``` If I hard code the return value of ``` getChainlinkDataFeedLatestAnswer() ``` function (e.g., ``` return uint256(1); ``` ) it works without problem. So I think ``` getChainlinkDataFeedLatestAnswer() ``` function doesn't do what it suppose to do!Does anyone know what can be the problem? Thanks in advance!",
    "author_id":5816,
    "publication_date":1754233133000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user9805040",
    "author_reputation":99.0,
    "tags":"solidity",
    "text_length":2046,
    "title_length":56,
    "num_tags":1
  },
  {
    "id":6344,
    "title":"The application installed on the server restarts by itself at irregular intervals",
    "link":"https:\/\/stackoverflow.com\/questions\/79724050\/the-application-installed-on-the-server-restarts-by-itself-at-irregular-interval",
    "text":"My application hosted on IIS restarts itself at irregular intervals. When I check the Windows Event Logs, I only see entries indicating that the application was stopped and started—nothing more. The application is not being actively used at the moment; only two Hangfire jobs are running in the background, but they are hosted in a separate application pool. I can't figure out the reason for the restarts.The same thing is happening with the Hangfire application as well. [![enter image description here] Restarded application pool and IIS.",
    "author_id":5815,
    "publication_date":1754234045000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"punisher",
    "author_reputation":17.0,
    "tags":"windows, asp.net-core, server",
    "text_length":541,
    "title_length":81,
    "num_tags":3
  },
  {
    "id":6343,
    "title":"Robocopy algorithm for \/FFT and \/DST options",
    "link":"https:\/\/stackoverflow.com\/questions\/79724059\/robocopy-algorithm-for-fft-and-dst-options",
    "text":"Microsoft's ``` Robocopy ``` program has these two options: ``` \/FFT :: assume FAT File Times (2-second granularity). \/DST :: compensate for one-hour DST time differences. ``` What is the exact algorithm that Robocopy uses for this? If I wanted to implement this in Python for example, how would I do this? I would like to know specifically how Robocopy does it , and I would like to know if there is a more correct way to compare timestamps while allowing for DST and filesystem differences. ``` def compare_file_timestamps_robocopy_style(file1, file2): \"\"\"Returns True if the file timestamps should be considered equal when adjusted for filesystem granularity and daylight savings time. This uses the same algorithm as Microsoft's Robocopy. \"\"\" time1 = os.path.getmtime(file1) time2 = os.path.getmtime(file2) return ??? ``` A pseudocode answer would be ok too if it is detailed enough.",
    "author_id":5796,
    "publication_date":1754235515000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mcu",
    "author_reputation":3590.0,
    "tags":"python, dst, robocopy, timestamp, timezone",
    "text_length":887,
    "title_length":44,
    "num_tags":5
  },
  {
    "id":6342,
    "title":"How to wait for TanStack Query to finish loading before executing a function in React?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724068\/how-to-wait-for-tanstack-query-to-finish-loading-before-executing-a-function-in",
    "text":"I have a flow where I fetch some data with TanStack Query, which is then used to populate a checkout flow. When a user opens a modal and updates an input, the checkout session is fetched based on that input, and when the user submits the form they are redirected to it. Previously, the checkout session would be fetched on click, and the user had to wait for the entire fetch to complete before they were redirected. That's slow, so it was updated to begin fetching as soon as the input was updated. I'm hiding the loading UI until they click the button (working implementation below). In most cases, the data will have finished loading so the user should instantly be redirected to checkout, but if the data is still loading then they'll briefly see the loading spinner until they are redirected. The question is, is there a more idiomatic way to check if the ``` isLoading ``` state provided by TanStack Query is false without listening for it in an effect? The long dependency array and long conditionals in my current iteration give off some code smell... my current solution works, but I'm just trying to write better code. ``` export default function MyComponent() { const [pendingCheckout, setPendingCheckout] = useState< null | \"checkoutA\" | \"checkoutB\" >(null); const { data: checkoutSessionA, isLoading: checkoutSessionAIsLoading, } = useQuery({ queryKey: [\"checkoutA\"] queryFn: () => fetchCheckoutSession(params), }); const { data: checkoutSessionB, isLoading: checkoutSessionBIsLoading, } = useQuery({ queryKey: [\"checkoutB\"] queryFn: () => fetchCheckoutSession(params), }); useEffect(() => { const checkoutAIsReady = pendingCheckout === \"checkoutA\" && !checkoutSessionAIsLoading && checkoutSessionA; const checkoutBIsReady = pendingCheckout === \"checkoutB\" && !checkoutSessionBIsLoading && checkoutSessionB; if (checkoutAIsReady) { triggerCheckout(checkoutSessionA); setPendingCheckout(null); } if (checkoutBIsReady) { triggerCheckout(checkoutSessionB); setPendingCheckout(null); } }, [ pendingCheckout, checkoutSessionAIsLoading, checkoutSessionA, checkoutSessionBIsLoading, checkoutSessionB, ]); return ( \/\/ ... <button onClick={() => setPendingCheckout(\"checkoutA\")} disabled={pendingCheckout === \"checkoutA\"} > {pendingCheckout === \"checkoutA\" ? \"Proceed to checkout\" : <Loader \/> } <\/button> \/\/ ... ) } const triggerCheckout = async (sessionId: string) => { const stripe = await loadStripe(stripePublicKey) \/\/ redirect to checkout const { error } = await stripe.redirectToCheckout({ sessionId }); if (error) \/\/ handle error }; ``` Before this approach, I tried another approach where I created a ``` enqueuedFn ``` state variable, and pushed a ``` triggerCheckout ``` function reference to ``` enqueuedFn ``` on click. I then used ``` useEffect ``` to listen to ``` isLoading ``` , and then call ``` enqueuedFn ``` once loading was finished. I was running into errors related to passing the function reference incorrectly, and ended up switching approaches to this new one. But I'm wondering if this initial approach was better (more idiomatic?), or if there's a even better way to do this.",
    "author_id":5814,
    "publication_date":1754236148000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"jconnorbuilds",
    "author_reputation":58.0,
    "tags":"reactjs, javascript, react-query",
    "text_length":3108,
    "title_length":86,
    "num_tags":3
  },
  {
    "id":6341,
    "title":"How to fix &quot;Alternate page with proper canonical tag&quot; issue in Google Search Console for a dynamic Laravel route?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724069\/how-to-fix-alternate-page-with-proper-canonical-tag-issue-in-google-search-con",
    "text":"I’m working on an SEO-optimized Laravel SaaS platform — Free Document Maker, where each tool has a dynamic route like \/ai-generator. Despite having correct canonical tags and crawl settings, Google Search Console shows the issue: “Page is not indexed: Alternate page with proper canonical tag.” The affected URL: https:\/\/www.freedocumentmaker.com\/ai-generator This page is not served from a CMS, and the tag is correctly set to itself. I verified the canonical URL in is correct and self-referencing. I checked robots.txt — the page is not disallowed. Meta tag is present. I submitted the URL via Search Console \"Inspect & Request Indexing.\" I confirmed the sitemap does not yet include this exact URL. Expected: Google should index the page if it's canonical, crawlable, and unique. Actual: Still showing as “Alternate page with proper canonical tag.”",
    "author_id":5813,
    "publication_date":1754236267000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Saddam Hosen - SH Saad",
    "author_reputation":1.0,
    "tags":"laravel, google-search-console, seo",
    "text_length":852,
    "title_length":123,
    "num_tags":3
  },
  {
    "id":6340,
    "title":"Interpolating with Hydra",
    "link":"https:\/\/stackoverflow.com\/questions\/79724070\/interpolating-with-hydra",
    "text":"I am using Hydra to organise configs for an ML project and I have my main config file along the lines of: config.yaml ``` defaults: - model: model_a - _self_ ``` then in model\/model_a.yaml ``` feature_dim: 106 block_args: layer_1: input_dim: ${feature_dim} embed_dim: 64 output_dim: 64 layer_2: ... ``` expecting ``` ${feature_dim} ``` to get the value of 106 so I only have to change it in one place, but when I print the model config I get ``` {'feature_dim': 106, 'block_args': {'layer_1': {'input_dim': '${feature_dim}', 'embed_dim': 64, 'output_dim': 64}, 'layer_2': ...} ``` This should be fairly simple and I don't understand why it's not working, any suggestions would be greatly appreciated!",
    "author_id":5812,
    "publication_date":1754236320000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dan Jackson",
    "author_reputation":278.0,
    "tags":"python, fb-hydra",
    "text_length":700,
    "title_length":24,
    "num_tags":2
  },
  {
    "id":6339,
    "title":"Apache Spark (3.5-4.0) Does df.count() loads the data?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724076\/apache-spark-3-5-4-0-does-df-count-loads-the-data",
    "text":"Over the past few months, I have been getting inconsistent benchmark results, and I want to confirm if my suspicion is correct. If I perform a series of transformations to create a Spark (3.5+\/4) DataFrame ( ``` df ``` ), and then follow these with two operations—(1) ``` df.cache() ``` and (2) ``` df.count() ``` — is it guaranteed that the data for ``` df ``` will be fully materialized and cached (assuming sufficient RAM is available)? Or will Spark only materialize the portion of data necessary to compute ``` df.count() ``` ? Is there any reference in the Spark documentation or specification that clarifies this behavior? I ask because, in many tutorials, calling ``` count() ``` method is the commonly suggested way to trigger materialization after caching. And I see that materializing full DataFrame may not be happening and might be affected strongly by the downstream operations due to probable lazy evaluations. And if this indeed were to be the case, SparkUI shows no such Stage until ``` count() ``` is completed. If ``` count() ``` does not result in loading the full DataFrame, is there a simple way to force Spark to do so? I thought of forcing Spark to examine a simple column, but due to lazy evaluation, it may load only that column and not others until later. Thanks in advance.",
    "author_id":5811,
    "publication_date":1754237218000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Quiescent",
    "author_reputation":1194.0,
    "tags":"dataframe, apache-spark, action",
    "text_length":1301,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6338,
    "title":"Text to speech while highlighting current word",
    "link":"https:\/\/stackoverflow.com\/questions\/79724080\/text-to-speech-while-highlighting-current-word",
    "text":"I'm trying to write a simple application using C# that will read the text in a richTextBox and hightlights each word as it is read. I am using ``` private SpeechSynthesizer synthesizer; ``` But when it goes into the SpeakProgress code to do the highlighting, it never works. Has anyone seen this done successfully? Maybe using a different library? Here is my code: ``` using System; using System.Speech.Synthesis; using System.Windows.Forms; using System.Drawing; using System.Linq; namespace TextToSpeechApp { public partial class Form1 : Form { private SpeechSynthesizer synthesizer; private string[] words; private int currentWordIndex = -1; private RichTextBox richTextBox; private Timer highlightTimer; private int currentCharPosition = 0; public Form1() { InitializeComponent(); synthesizer = new SpeechSynthesizer(); synthesizer.SpeakStarted += Synthesizer_SpeakStarted; synthesizer.SpeakCompleted += Synthesizer_SpeakCompleted; \/\/ Initialize timer for highlighting highlightTimer = new Timer { Interval = 100 \/\/ Adjust based on speech speed }; highlightTimer.Tick += HighlightTimer_Tick; } private void InitializeComponent() { this.Text = \"Text to Speech Reader\"; this.Size = new Size(600, 400); this.StartPosition = FormStartPosition.CenterScreen; richTextBox = new RichTextBox { Location = new Point(10, 10), Size = new Size(560, 300), Font = new Font(\"Arial\", 12), Text = \"Enter text here to be read aloud word by word.\" }; Button readButton = new Button { Text = \"Read Aloud\", Location = new Point(10, 320), Size = new Size(100, 30) }; readButton.Click += ReadButton_Click; Button stopButton = new Button { Text = \"Stop\", Location = new Point(120, 320), Size = new Size(100, 30) }; stopButton.Click += StopButton_Click; this.Controls.Add(richTextBox); this.Controls.Add(readButton); this.Controls.Add(stopButton); } private void ReadButton_Click(object sender, EventArgs e) { if (string.IsNullOrWhiteSpace(richTextBox.Text)) { MessageBox.Show(\"Please enter some text to read.\"); return; } \/\/ Reset state synthesizer.SpeakAsyncCancelAll(); highlightTimer.Stop(); ResetHighlighting(); currentWordIndex = -1; currentCharPosition = 0; \/\/ Split text into words words = richTextBox.Text.Split(new[] { ' ', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries); \/\/ Start speech synthesizer.SpeakAsync(richTextBox.Text); highlightTimer.Start(); } private void StopButton_Click(object sender, EventArgs e) { synthesizer.SpeakAsyncCancelAll(); highlightTimer.Stop(); ResetHighlighting(); } private void Synthesizer_SpeakStarted(object sender, SpeakStartedEventArgs e) { currentWordIndex = -1; currentCharPosition = 0; } private void Synthesizer_SpeakCompleted(object sender, SpeakCompletedEventArgs e) { highlightTimer.Stop(); ResetHighlighting(); } private void HighlightTimer_Tick(object sender, EventArgs e) { if (currentWordIndex >= words.Length - 1) { highlightTimer.Stop(); return; } \/\/ Find the next word's position string text = richTextBox.Text; int wordStart = currentCharPosition; while (wordStart < text.Length && char.IsWhiteSpace(text[wordStart])) { wordStart++; } if (wordStart >= text.Length) { highlightTimer.Stop(); ResetHighlighting(); return; } \/\/ Find word end int wordEnd = wordStart; while (wordEnd < text.Length && !char.IsWhiteSpace(text[wordEnd])) { wordEnd++; } \/\/ Highlight the word this.Invoke((MethodInvoker)delegate { ResetHighlighting(); richTextBox.SelectionStart = wordStart; richTextBox.SelectionLength = wordEnd - wordStart; richTextBox.SelectionBackColor = Color.Yellow; }); currentWordIndex++; currentCharPosition = wordEnd; \/\/ Adjust timer interval based on word length (approximate) highlightTimer.Interval = EstimateWordDuration(words[currentWordIndex]); } private int EstimateWordDuration(string word) { \/\/ Rough estimate: 100ms per character, minimum 200ms, maximum 800ms int duration = word.Length * 100; return Math.Max(200, Math.Min(800, duration)); } private void ResetHighlighting() { this.Invoke((MethodInvoker)delegate { richTextBox.SelectionStart = 0; richTextBox.SelectionLength = richTextBox.Text.Length; richTextBox.SelectionBackColor = Color.White; }); } [STAThread] static void Main() { Application.EnableVisualStyles(); Application.SetCompatibleTextRenderingDefault(false); Application.Run(new Form1()); } } } } ```",
    "author_id":5810,
    "publication_date":1754237972000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"TheBigOnion",
    "author_reputation":647.0,
    "tags":"c#",
    "text_length":4273,
    "title_length":46,
    "num_tags":1
  },
  {
    "id":6337,
    "title":"How to apply non-uniform scaling to a rotated UIView (rectangle) over Mapbox without distortion?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724081\/how-to-apply-non-uniform-scaling-to-a-rotated-uiview-rectangle-over-mapbox-wit",
    "text":"I’m trying to allow non-uniform scaling (horizontal or vertical) of a UIView rectangle that is placed over a Mapbox mapView and has already been rotated. Uniform scaling works well, but when I attempt to apply non-uniform scaling (scaling only X or Y) using transforms after rotation, the rectangle becomes distorted like a parallelogram. To avoid distortion, I instead redraw the rectangle using coordinate math at the end of the gesture, but this makes the scaling feel abrupt and not smooth. I want to achieve smooth, live non-uniform scaling after rotation without visual distortion. I’ve tried using transform.scaledBy(x:y:), but it doesn’t maintain the rectangle’s orthogonality post-rotation. How can I achieve smooth, interactive, non-uniform scaling on a rotated view without it deforming? Code Snippets Here’s the ``` pinchProcess ``` gesture handler: ``` @objc func handlePinchGesture(_ recognizer: UIPinchGestureRecognizer) { guard isScalingEnabled, let targetView = self.scalingView else { return } if recognizer.state == .began { previousScale = 1.0 initialPinchLocation = recognizer.location(in: targetView) } if recognizer.state == .ended || recognizer.state == .cancelled { if hasRotationApplied { gestureDelegate?.applyScalingAndRedraw(for: targetView, axis: lastScaleAxis, scaleFactor: finalScaleValue) } else { gestureDelegate?.finalizeSnap(for: targetView, usingPoints: originalPoints) } return } if recognizer.numberOfTouches < 2 { initialPinchLocation = recognizer.location(in: targetView) return } let touchPointA = recognizer.location(ofTouch: 0, in: targetView) let touchPointB = recognizer.location(ofTouch: 1, in: targetView) lastScaleAxis = detectScaleAxis(from: touchPointA, to: touchPointB, basedOn: originalPoints) let scaleFactor = recognizer.scale \/ previousScale let xDiff = originalPoints[1].x - originalPoints[0].x let yDiff = originalPoints[1].y - originalPoints[0].y if xDiff == 0 || yDiff == 0 { hasRotationApplied = false switch detectGestureDirection(touchPointA, point2: touchPointB) { case \"V\": targetView.transform = targetView.transform.scaledBy(x: 1, y: scaleFactor) case \"H\": targetView.transform = targetView.transform.scaledBy(x: scaleFactor, y: 1) default: targetView.transform = targetView.transform.scaledBy(x: scaleFactor, y: scaleFactor) } previousScale = recognizer.scale } else { finalScaleValue = recognizer.scale hasRotationApplied = true targetView.transform = targetView.transform.scaledBy(x: scaleFactor, y: scaleFactor) } previousScale = recognizer.scale } ``` ``` func applyScalingAndRedraw(for view: UIView, axis: String, scaleFactor: CGFloat) { \/\/ Computes new points based on the scaling axis and applies redraw logic \/\/ Geometry methods update coordinates and redraw the rectangle correctly } ```",
    "author_id":5809,
    "publication_date":1754238001000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Emma Waters",
    "author_reputation":1.0,
    "tags":"ios, swift, mapbox, gesture, uiview",
    "text_length":2764,
    "title_length":96,
    "num_tags":5
  },
  {
    "id":6336,
    "title":"Sublime: Shortcut to always run specific group or file",
    "link":"https:\/\/stackoverflow.com\/questions\/79724089\/sublime-shortcut-to-always-run-specific-group-or-file",
    "text":"I Always work with 3 windows for run a Latex file, in a specific group using Tinitex or Knitr in R (Repl). In group 3 include commands (set working directory and name file latex) to render or compile tex or Rnw files (layout 1). To render latex file I always have to focus or make click into group 3 and run using a shortcut. I would like if its possible create a shortcut to run group 3 but not focus or make click in group 3 to run latex file. I tried to run some keybinding, but i failed, using: ``` { \"keys\": [\"alt+z\"], \"command\": \"focus_group\", \"args\": { \"group\": 3 } , \"command\": \"repl_transfer_current\", \"args\": {\"scope\": \"file\"}, }, ```",
    "author_id":5808,
    "publication_date":1754238699000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Rodrigo_BC",
    "author_reputation":171.0,
    "tags":"latex, sublimetext3, knitr, read-eval-print-loop, tinytex",
    "text_length":644,
    "title_length":54,
    "num_tags":5
  },
  {
    "id":6335,
    "title":"GitHub Actions + Docker: SQLSTATE[HY000] [2002] php_network_getaddresses: getaddrinfo for db failed",
    "link":"https:\/\/stackoverflow.com\/questions\/79724091\/github-actions-docker-sqlstatehy000-2002-php-network-getaddresses-getadd",
    "text":"I'm setting up GitHub Actions for a Laravel project using Docker. During the workflow run, I get the following error when Laravel tries to connect to the MySQL container: SQLSTATE[HY000] [2002] php_network_getaddresses: getaddrinfo for db failed: Name or service not known (Connection: mysql, SQL: delete from ``` cache ``` ) Setup ``` docker-compose.yml ``` ``` services: app: build: context: . dockerfile: Dockerfile container_name: news-api-app restart: unless-stopped volumes: - .:\/var\/www environment: - APP_ENV=local - APP_DEBUG=true depends_on: - db - redis networks: - newsnet command: | sh -c \"chmod -R 775 \/var\/www\/storage && \\ chown -R www-data:www-data \/var\/www\/storage && \\ sleep 10 && php artisan migrate:fresh --seed && \\ php-fpm\" webserver: image: nginx:alpine container_name: news-api-nginx restart: unless-stopped ports: - \"8000:80\" volumes: - .:\/var\/www - .\/docker\/nginx\/default.conf:\/etc\/nginx\/conf.d\/default.conf depends_on: - app networks: - newsnet db: image: mysql:8.0 container_name: news-api-db restart: unless-stopped ports: - \"3307:3306\" env_file: - .env environment: MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE: ${MYSQL_DATABASE} MYSQL_ALLOW_EMPTY_PASSWORD: ${MYSQL_ALLOW_EMPTY_PASSWORD:-no} volumes: - .\/docker\/dbdata:\/var\/lib\/mysql networks: - newsnet redis: image: redis:alpine container_name: news-api-redis restart: unless-stopped ports: - \"6379:6379\" networks: - newsnet networks: newsnet: driver: bridge volumes: dbdata: ``` ``` .github\/workflows\/laravel.yml ``` ``` name: Laravel CI on: push: branches: - main pull_request: branches: - main jobs: test: runs-on: ubuntu-latest services: db: image: mysql:8.0 env: MYSQL_DATABASE: news MYSQL_ROOT_PASSWORD: root MYSQL_ALLOW_EMPTY_PASSWORD: \"no\" ports: - 3306:3306 options: >- --health-cmd=\"mysqladmin ping --silent\" --health-interval=10s --health-timeout=5s --health-retries=10 steps: - name: Checkout repository uses: actions\/checkout@v4 - name: Log in to GitHub Container Registry uses: docker\/login-action@v3 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.MY_SECRATEKEY }} - name: Pull Laravel App Image run: docker pull {path_to_my_registry}\/news-api-app:latest - name: Start Laravel App Container run: | docker run -d --name news-api-app \\ -v ${{ github.___ }}:\/var\/www \\ -w \/var\/www \\ {path_to_my_registry}\/news-api-app:latest \\ sleep infinity - name: Copy .env.testing into container run: docker cp .env.testing news-api-app:\/var\/www\/.env - name: Wait for MySQL to be ready (port 3306) run: | for i in {1..5}; do if docker run --rm --network container:news-api-app mysql:8.0 \\ mysql -h 127.0.0.1 -u root -p -e \"SELECT 1;\" > \/dev\/null 2>&1; then echo \"✅ MySQL is ready for host 127.0.0.1\" break fi echo \"⏳ Waiting for MySQL for host 127.0.0.1 ($i)...\" sleep 2 done for j in {1..5}; do if docker run --rm --network container:news-api-app mysql:8.0 \\ mysql -h db -u root -proot -e \"SELECT 1;\" > \/dev\/null 2>&1; then echo \"✅ MySQL is ready for host db \" break fi echo \"⏳ Waiting for MySQL for host db ($j)...\" sleep 2 done - name: Composer install run: docker exec news-api-app composer install --no-interaction --prefer-dist --no-scripts - name: Clear Laravel caches run: | docker exec news-api-app php artisan optimize:clear docker exec news-api-app php artisan config:clear docker exec news-api-app php artisan route:clear docker exec news-api-app php artisan view:clear docker exec news-api-app php artisan cache:clear - name: Run migrations run: docker exec news-api-app php artisan migrate --force - name: Run PHPUnit tests with coverage run: docker exec news-api-app php artisan test --env=testing --coverage-clover=coverage.xml - name: Upload Coverage Report uses: actions\/upload-artifact@v4 with: name: coverage-report path: coverage.xml ``` ``` .env.testing ``` ``` APP_ENV=testing APP_KEY=base64:placeholder APP_DEBUG=true APP_URL=http:\/\/localhost DB_CONNECTION=mysql DB_HOST=db DB_PORT=3306 DB_DATABASE=news DB_USERNAME=root DB_PASSWORD=root CACHE_DRIVER=file QUEUE_CONNECTION=sync SESSION_DRIVER=file ``` What I've Tried: Confirmed that ``` DB_HOST ``` is set to ``` db ``` Waited a few seconds before running artisan commands Tried using ``` 127.0.0.1 ``` and ``` localhost ``` — they give connection refused Checked that the MySQL container is up and healthy Questions: How do I make sure the Laravel container can resolve the db hostname inside GitHub Actions? Is there a reliable way to wait until MySQL is fully ready before running artisan commands? Any insights would be appreciated!",
    "author_id":5807,
    "publication_date":1754239020000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"divyamohan kashyap",
    "author_reputation":78.0,
    "tags":"docker-compose, github-actions, laravel",
    "text_length":4534,
    "title_length":99,
    "num_tags":3
  },
  {
    "id":6334,
    "title":"Can I set VkCmdDispatchIndirect&#39;s workgroup size to 0 to achieve a no-op?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724092\/can-i-set-vkcmddispatchindirects-workgroup-size-to-0-to-achieve-a-no-op",
    "text":"Assuming I have a compute pipeline that I can dispatch with vkCmdDispatchIndirect , and a VkBuffer object that stores the dispatch parameters, can I safely and reliably set all the workgroup dimensions to 0 to conditionally disable execution of the compute shader?",
    "author_id":5806,
    "publication_date":1754239021000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"calc196",
    "author_reputation":3.0,
    "tags":"vulkan",
    "text_length":264,
    "title_length":77,
    "num_tags":1
  },
  {
    "id":6333,
    "title":"Circleci checkout fails randomly on my self hosted runner",
    "link":"https:\/\/stackoverflow.com\/questions\/79724093\/circleci-checkout-fails-randomly-on-my-self-hosted-runner",
    "text":"``` Cloning git repository - <REDACTED> Cloning into '.'... Error: httpclient do: context deadline exceeded warning: invalid credential line: httpclient do: context deadline exceeded fatal: could not read Username for '<REDACTED>': No such device or address Error: error running git clone \"<REDACTED>\": exit status 128 error running git clone \"<REDACTED>\": exit status 128 Error: exit status 1: Error: error running git clone \"<REDACTED\": exit status 128 exit status 1 exit status 1 ``` I intermittently get this on about 1\/5 pushes. The network on my self hosted runner is flawless so it's definitely not that.",
    "author_id":5805,
    "publication_date":1754239194000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"rjpj1998",
    "author_reputation":419.0,
    "tags":"circleci",
    "text_length":611,
    "title_length":57,
    "num_tags":1
  },
  {
    "id":6332,
    "title":"Powershell multiline output into Batch variable",
    "link":"https:\/\/stackoverflow.com\/questions\/79724094\/powershell-multiline-output-into-batch-variable",
    "text":"How can I put the entire Powershell output into a Batch variable? Another post has suggested ``` @for \/f \"delims=\" %%a in ('Powershell -C Get-Content myFile.txt -Raw') do Set \"myVar=%%a\" ``` but that only returns the last line. My end goal is to read a multi-line text file and pass it into another script. [EDIT] Conclusion: batch is not made for this as variables cannot contain a newline character without obscure tricks , I'm switching to Powershell .",
    "author_id":5033,
    "publication_date":1754239216000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nathalie Kitty",
    "author_reputation":113.0,
    "tags":"powershell, batch-file, variables, multilinestring",
    "text_length":455,
    "title_length":47,
    "num_tags":4
  },
  {
    "id":6331,
    "title":"Calling PEM_read_PrivateKey causes program to crash in OpenSSL 3.x",
    "link":"https:\/\/stackoverflow.com\/questions\/79724105\/calling-pem-read-privatekey-causes-program-to-crash-in-openssl-3-x",
    "text":"I call ``` PEM_read_PrivateKey ``` to read the private key using the following code, however after calling ``` PEM_read_PrivateKey ``` the program crashes (exists with error code 1) on Windows 11. ``` std::string pem = get_pem(); FILE* tmp = tmpfile(); if (!tmp) { return; } for (char ch : pem) fputc(ch, tmp); rewind(tmp); auto pkey = PEM_read_PrivateKey(tmp, NULL, NULL, NULL); if (!pkey) { fclose(tmp); return; } fclose(tmp); ``` This code worked fine in OpenSSL version 1.x but after upgrading to any 3.x version, calling ``` PEM_read_PrivateKey ``` crashes on Windows. On the other hand, if I use ``` bio ``` then it is possible to read the private key from PEM. In other words, the following code works fine ``` auto pem = get_pem(); BIO* bio = BIO_new_mem_buf(pem.data(), (int)pem.size()); if (!bio) { return; } EVP_PKEY* pkey = PEM_read_bio_PrivateKey(bio, nullptr, nullptr, nullptr); BIO_free(bio); if (!pkey) { return; } \/\/ ... use pkey ``` I have tried different versions of OpenSSL 3.x, the behavior remains the same. What could be the problem? I suspected that the problem might be in the CRT incompatibility, but I checked everything and made sure that there were no problems with the CRT.",
    "author_id":4345,
    "publication_date":1754240009000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Joe J",
    "author_reputation":1075.0,
    "tags":"c++, windows, openssl",
    "text_length":1203,
    "title_length":66,
    "num_tags":3
  },
  {
    "id":6330,
    "title":"Markdown hyperlinks not rendering in Entra External ID User Flow labels anymore",
    "link":"https:\/\/stackoverflow.com\/questions\/79724107\/markdown-hyperlinks-not-rendering-in-entra-external-id-user-flow-labels-anymore",
    "text":"We are trying to add a hyperlink to the label of a custom boolean attribute on our Entra External ID sign-up page (User Flow), as described in the official Microsoft Entra documentation. https:\/\/learn.microsoft.com\/en-us\/entra\/external-id\/customers\/how-to-define-custom-attributes#configure-the-user-input-types-and-page-layout We created a custom attribute with a Data Type of \"Boolean\", for the label, we entered the following Markdown: ``` I have read and agree to the [terms of use](https:\/\/woodgrove.com\/terms-of-use) ``` We also tried using translation overrides with the markdown, same issue. Expected Result: The sign-up page should display the text \"I have read and agree to the privacy policy\" with \"privacy policy\" being a clickable hyperlink. Actual Result: The markdown is translated to HTML, however the page renders the raw HTML as plain text, displaying: ``` I have read and agree to the <a href=...>terms of use<\/a>. ``` insted of embedding the HTML. We have confirmed this is a recurring issue and not a configuration error of one user flow, because the issue persists even when creating a brand new user flow with a simple test case like \"test\". The markdown seems to be correctly translated to HTML, but azure seems to escape the HTML. How do we get this to work (again, as this worked up until today)?",
    "author_id":5804,
    "publication_date":1754240171000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"lugiorgi",
    "author_reputation":546.0,
    "tags":"microsoft-entra-external-id",
    "text_length":1322,
    "title_length":79,
    "num_tags":1
  },
  {
    "id":6329,
    "title":"How to resolve linker error for Armadillo?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724110\/how-to-resolve-linker-error-for-armadillo",
    "text":"I have the following code (minimal working example) in ``` min_link_issue.cpp ``` : ``` #include <armadillo> int main() { arma::mat X(1,1); arma::vec y(1); arma::vec beta = solve(X,y); } ``` When trying to compile this using ``` g++ -larmadillo -llapack -lblas min_link_issue.cpp ``` I run into hard-to-resolve linker issues: ``` \/usr\/bin\/ld: \/tmp\/ccDrmIK4.o: in function `arma::lapack::laenv(int*, char*, char*, int*, int*, int*, int*, unsigned long, unsigned long)': min_link_issue.cpp:(.text._ZN4arma6lapack5laenvEPiPcS2_S1_S1_S1_S1_mm[_ZN4arma6lapack5laenvEPiPcS2_S1_S1_S1_S1_mm]+0x53): undefined reference to `wrapper2_ilaenv_' \/usr\/bin\/ld: \/tmp\/ccDrmIK4.o: in function `void arma::lapack::gelsd<double>(int*, int*, int*, double*, int*, double*, int*, double*, double*, int*, double*, int*, int*, int*)': min_link_issue.cpp:(.text._ZN4arma6lapack5gelsdIdEEvPiS2_S2_PT_S2_S4_S2_S4_S4_S2_S4_S2_S2_S2_[_ZN4arma6lapack5gelsdIdEEvPiS2_S2_PT_S2_S4_S2_S4_S4_S2_S4_S2_S2_S2_]+0x5e): undefined reference to `wrapper2_dgelsd_' \/usr\/bin\/ld: \/tmp\/ccDrmIK4.o: in function `void arma::lapack::trtrs<double>(char*, char*, char*, int*, int*, double const*, int*, double*, int*, int*)': min_link_issue.cpp:(.text._ZN4arma6lapack5trtrsIdEEvPcS2_S2_PiS3_PKT_S3_PS4_S3_S3_[_ZN4arma6lapack5trtrsIdEEvPcS2_S2_PiS3_PKT_S3_PS4_S3_S3_]+0x5c): undefined reference to `wrapper2_dtrtrs_' ... ``` This is run on Ubuntu 24.04, the libraries were installed using apt-get (and the linker seems to be finding them I believe).",
    "author_id":5803,
    "publication_date":1754240508000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Henning Koehler",
    "author_reputation":2665.0,
    "tags":"c++, linker-errors, armadillo",
    "text_length":1497,
    "title_length":42,
    "num_tags":3
  },
  {
    "id":6328,
    "title":"Harmonic function does not work in Pari\/GP",
    "link":"https:\/\/stackoverflow.com\/questions\/79724113\/harmonic-function-does-not-work-in-pari-gp",
    "text":"Recently, I've been trying to compute the generalized harmonic numbers in Pari\/GP. According to the documentation , the generalized harmonic number m of order r can be computed by the code ``` harmonic(m,r) ``` However, when I try to compute harmonic(5,2), for example, I get the following error: ``` not a function in function call ``` I'm not sure how to fix this, and I appreciate advice on the matter.",
    "author_id":5802,
    "publication_date":1754240651000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Max Lonysa Muller",
    "author_reputation":351.0,
    "tags":"math, pari",
    "text_length":405,
    "title_length":42,
    "num_tags":2
  },
  {
    "id":6327,
    "title":"How to Handle User Context in Service-to-Service Calls with Spring Cloud Gateway and JWTs?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724115\/how-to-handle-user-context-in-service-to-service-calls-with-spring-cloud-gateway",
    "text":"I'm building a microservice architecture using Spring Boot and Spring Cloud. My setup follows best practices for security: An API Gateway acts as an OAuth2 Resource Server, validating user JWTs from a central Identity Provider (IdP). Downstream services are also configured as OAuth2 Resource Servers, adhering to the Zero Trust principle. My architecture uses the Token Propagation pattern for user-facing API calls. My architecture uses the Client Credentials Grant for service-to-service (M2M) communication. The core issue I'm trying to solve is a common scenario where a service-to-service call needs to carry the context of the original end-user. The Problem Scenario: I have two services: user-service and comment-service. The comment-service has an endpoint POST \/comments that requires a userId to create a comment on behalf of a user. A user sends a request to the API Gateway to create a comment. The gateway routes this request to the comment-service. In this case, the gateway passes the user's JWT, and the comment-service extracts the userId from the JWT's sub claim. This works perfectly. However, I have a second service, email-service, that needs to programmatically add a comment to a user's profile (e.g., when a user replies to an email). The email-service calls the comment-service. As per best practices for M2M communication, the email-service obtains a service account JWT (via the Client Credentials Grant) and uses it to call the comment-service. The Conflict: The comment-service's POST \/comments endpoint requires a userId to be associated with the new comment. The M2M JWT from the email-service identifies the email-service as the caller, but it does not contain the original user's ID. What I've Considered (and their drawbacks): Forwarding the user's JWT from Service B: This is a security anti-pattern. Service B should not impersonate the user. Passing the userId in a custom header (X-User-Id): This seems plausible, but it feels like a violation of the Zero Trust principle, as any internal service could potentially forge this header. My Question: What is the best practice for a downstream service to handle an endpoint that requires user context, but can be called by both: A user-facing request (via the Gateway with a user JWT)? A service-to-service request (from a trusted service with an M2M JWT)? How can I design my comment-service to securely handle these two different authentication contexts and reliably get the userId? Any code examples demonstrating the correct Spring Security configuration for this combined approach would be greatly appreciated.",
    "author_id":5801,
    "publication_date":1754240926000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Алексей",
    "author_reputation":70.0,
    "tags":"java, spring, spring-boot, security, jwt",
    "text_length":2600,
    "title_length":90,
    "num_tags":5
  },
  {
    "id":6326,
    "title":"How to pass environment variables like --dart-define when building iOS app via Xcode for Flutter?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724116\/how-to-pass-environment-variables-like-dart-define-when-building-ios-app-via-x",
    "text":"I'm building a Flutter app and using ``` --dart-define ``` to pass environment-specific variables like ENV, x-api-key, or base URLs. This works perfectly for Android using: ``` flutter build apk --dart-define=ENV=prod ``` For iOS, I can build the app and generate a .ipa using: ``` flutter build ipa --dart-define=ENV=prod ``` Till Now There was no environment variable setup. So I used to deploy app using Xcode > Product > Archive which builds the app, and opens the Organizer window. From there, I can choose distribution methods such as TestFlight, App Store, or export the .ipa as seen in the image. However, this method does not include my --dart-define variables like ENV=prod, which are critical for my app's configuration . If I use ``` flutter build ipa --dart-define=ENV=prod ``` to build the .ipa, I have to use Transporter app to upload the .ipa. But it does not give me as many option as I get in XCode shown I the image. My Question : How can I: Use Xcode to build the iOS app and still include the --dart-define environment variables, like ENV=prod? OR, is there a way to get the same App Store distribution options (like Organizer window) after building via ``` flutter build ipa ``` ? Any guidance on how to bridge this gap between Flutter CLI builds and the Xcode Organizer flow would be really helpful. Thanks!",
    "author_id":5800,
    "publication_date":1754240927000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ankit Baria",
    "author_reputation":23.0,
    "tags":"ios, flutter, xcode, deployment, distribution",
    "text_length":1330,
    "title_length":97,
    "num_tags":5
  },
  {
    "id":6325,
    "title":"broken shell for openwrt",
    "link":"https:\/\/stackoverflow.com\/questions\/79724118\/broken-shell-for-openwrt",
    "text":"i have installed openwrt in my raspberrypi, i want to use zsh instead of ash, so after installation, i edited \/etc\/passwd to change root shell. when everything done, i find i cannot login to openwrt by ssh, when i check system log by luci, i find that, openwrt thought zsh is invalid... now, i cannot login by ssh, and i cannot access system by serial console as i dont have a mini-hdmi cable i have tried to edit system files from SD card in another linux machine, but god damn openwrt use overlay to protect the system, i can just mount as read-only ... i really donot want to reinstall the system, please help me, maybe some hints",
    "author_id":5799,
    "publication_date":1754240983000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"buzz",
    "author_reputation":713.0,
    "tags":"shell, raspberry-pi, router, openwrt",
    "text_length":633,
    "title_length":24,
    "num_tags":4
  },
  {
    "id":6324,
    "title":"Fetch userID and username via Facebook API",
    "link":"https:\/\/stackoverflow.com\/questions\/79724121\/fetch-userid-and-username-via-facebook-api",
    "text":"I am building an Xcode IOS app. The purpose of the app is to work with comments under the posts in my own Facebook page. I have advanced rights for: public_profile email pages_show_list pages_read_user_content pages_read_engagement When I do a call to the post, I still don’t see the usernames or userIDs. Question: what access right is missing or what am I doing wrong? ``` https:\/\/graph.facebook.com\/v23.0\/ {post_id}?fields=comments{from,id}\/access_token={access_token} ```",
    "author_id":5798,
    "publication_date":1754241438000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mr.Schtelbe",
    "author_reputation":165.0,
    "tags":"facebook-graph-api, facebook, facebook-apps",
    "text_length":475,
    "title_length":42,
    "num_tags":3
  },
  {
    "id":6323,
    "title":"Modifying timestamps on Windows reparse points",
    "link":"https:\/\/stackoverflow.com\/questions\/79724122\/modifying-timestamps-on-windows-reparse-points",
    "text":"I need to copy timestamps (modified time) between NTFS reparse points of type junction, directory symlink and file symlink. I can get the timestamp of a reparse point with ``` os.lstat() ``` . But to apply that timestamp I would need to use ``` os.utime() ``` but its parameter ``` follow_symlinks ``` is not implemented on Windows, so ``` os.utime() ``` always sets the timestamp on the target of a reparse point but I need to set it on the reparse point itself. ``` >>> os.utime in os.supports_follow_symlinks False ``` So how can I set time on Windows reparse points in Python? I prefer solutions using ``` ctypes ``` over third party libraries for this.",
    "author_id":5796,
    "publication_date":1754241478000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mcu",
    "author_reputation":3590.0,
    "tags":"python, symlink, ntfs, reparsepoint, timestamp",
    "text_length":657,
    "title_length":46,
    "num_tags":5
  },
  {
    "id":6322,
    "title":"Regex in Excel not matching a date when that date has a slash in it",
    "link":"https:\/\/stackoverflow.com\/questions\/79724126\/regex-in-excel-not-matching-a-date-when-that-date-has-a-slash-in-it",
    "text":"I need to find where a date value is later than the year 1911. Some of these values aren't uniform: some are Jan-1910 (beneath the formatting this is 01\/01\/1910 , others are 1910-1914 . Dates before 1900 have to be written manually as Jan 1899 due to Excel date limitations. I have this regex being put into a formula: ``` =REGEXTEST(B2, \"191[1-9]\") ``` when I use it on a selection of values I get: So the regex works for isolated numbers, doesn't work for dates of the format Jan-09 (internally 01\/01\/1909 ), and does work for dates with dots instead of slashes. On regex101.com, it works as intended.",
    "author_id":5797,
    "publication_date":1754241740000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Thomas Slade",
    "author_reputation":185.0,
    "tags":"string, excel, date",
    "text_length":603,
    "title_length":67,
    "num_tags":3
  },
  {
    "id":6321,
    "title":"How to check available ResourcePool units from within an agent in AnyLogic?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724128\/how-to-check-available-resourcepool-units-from-within-an-agent-in-anylogic",
    "text":"I have a model in anylogic that has an agent called vessel. Inside vessel, I have a variable that request a number of cranes called QCtoSeize that is used inside a service block to seize a number of resources from the QuayCrane resource pool. After checking the code I realized that if a new agent arrives, and its requested amount of cranes is greater than the available resources, then the seize block will not seize any cranes until all the needed cranes are available. To prevent this, I plan to edit the code to check for the currently available amount of resources and if the requested amount is greater, then just set QCtoSeize = to the amount of resources left. The problem lies that I am not certain how to pass the value of available quay cranes to a variable inside the agent. Is there a way to do so?",
    "author_id":5189,
    "publication_date":1754241937000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Scar",
    "author_reputation":39.0,
    "tags":"anylogic",
    "text_length":812,
    "title_length":75,
    "num_tags":1
  },
  {
    "id":6320,
    "title":"Modifying target paths of NTFS reparse points",
    "link":"https:\/\/stackoverflow.com\/questions\/79724129\/modifying-target-paths-of-ntfs-reparse-points",
    "text":"I need to modify the target paths of NTFS reparse points of type junction, directory symlink and file symlink. How can I do this in Python? I have been getting around this problem by creating new reparse points with Windows ``` mklink ``` with correct target paths, and then deleting the old reparse points and then doing some renaming. But I would much prefer to edit the reparse points in place. I would prefer solutions using ``` ctypes ``` over third party libraries for this.",
    "author_id":5796,
    "publication_date":1754242014000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"mcu",
    "author_reputation":3590.0,
    "tags":"python, ctypes, symlink, ntfs, reparsepoint",
    "text_length":480,
    "title_length":45,
    "num_tags":5
  },
  {
    "id":6319,
    "title":"Validating resources based on custom FHIR profile for Google Healthcare API",
    "link":"https:\/\/stackoverflow.com\/questions\/79724130\/validating-resources-based-on-custom-fhir-profile-for-google-healthcare-api",
    "text":"I am testing the Google Healthcare FHIR data store validation of resources based on an Implementation guide using a straightforward IG that does the following on the Patient resource: * name 1..1 MS * birthDate 0..0 MS I generated the JSON specification files from SUSHI Deleted the examples from the FSH-Generated\/Resources folder Compiled a bundle using the bundler Python tool Uploaded the resulting bundle.json to a bucket associated with the dataset trying to create a dataset with this custom IG, which seems to work ok because I can now select the ExampleIG as a custom IG. My problem is that every time I try to POST a Patient resource that does not comply with the profile that I defined, I can create it without a problem, and when I do the Patient\/$validate operation, I only get a warning. I tried it using the available IG, specifically the USCORE 6.1, and the validation works fine. Thanks!",
    "author_id":5795,
    "publication_date":1754242040000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Esteban Hebel",
    "author_reputation":25.0,
    "tags":"google-healthcare-api, google-cloud-healthcare",
    "text_length":904,
    "title_length":75,
    "num_tags":2
  },
  {
    "id":6318,
    "title":"Can I use BUILD_WITH_QT6 to select between KF5 and KF6?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724131\/can-i-use-build-with-qt6-to-select-between-kf5-and-kf6",
    "text":"I develop an application for KDE, currently targeting KF5 and is about to be ported KF6. Naturally, the application uses ``` cmake ``` . I want to allow the user to choose which of the two versions should be targeted. Now I discovered that ``` cmake-gui ``` offers a variable ``` BUILD_WITH_QT6 ``` . Which facility defines this variable? It is certainly not in my own source code. Can it be hijacked to let the user specify the Frameworks version? The desired use is: ``` cmake -DBUILD_WITH_QT6=ON # selects KF6+Qt6 cmake -DBUILD_WITH_QT6=OFF # selects KF5+Qt5 ```",
    "author_id":5794,
    "publication_date":1754242056000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"j6t",
    "author_reputation":13990.0,
    "tags":"cmake, kde-plasma",
    "text_length":565,
    "title_length":55,
    "num_tags":2
  },
  {
    "id":6317,
    "title":"trying to understand tpm modules in bare metal use cases",
    "link":"https:\/\/stackoverflow.com\/questions\/79724135\/trying-to-understand-tpm-modules-in-bare-metal-use-cases",
    "text":"i need to implement a TPM in a bare metal app [we will have a tpm module on the board] everything google finds is so high level that it is useless and so windows or linux centric that it is not usable. i understand the key storage part - i can create a key in the tpm and send digests to be signed etc, i can create aes keys and rsa keys as needed and i can store the key on the tpm and extract the public key. i can also ask the TPM to generate aes keys and store keys for it to use or i can read back keys i stored but i am trying to figure out and understand the measured boot stuff at a practical level.. it seems that: a) setup - one time done at manufacturing: b) i create a pub\/private key for signing during mfg process c) i extract and save the pub key for my records alternatively i can ask for the pub key at any time in the future at device boot i think the device must do this a) at power reset the pcrs in the TPM are cleared??? b) at each boot stage i need to hash the image loaded (ie sha) c) i send that hash to the tpm module d) the tpm creates a “hash of hashes” in sequence given it seems i can store multiple hashes in the various pcrs? ie one for each stage of boot, ie boot loader, bios, os loader, the os it self etc each goes into its own pcr e) when boot is done i ask the tpm to sign the hashofhashes using the key generated earlier or sign any of the pcrs f) i can extract the hashes and the signature from the tpm g) i send (phone home) the hashofhashes and the signature h) back at the mothership i have the pub key from the start and i can 1) compare the expected hash and 2) verify the signature if the hash is what i expect then the system is good does the pcrs contain a time stamp of some sort? is my understanding correct? if not can you point me to an overview of the process? cause i cannot find an explination at this level of implimentation",
    "author_id":5793,
    "publication_date":1754242483000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user3696153",
    "author_reputation":786.0,
    "tags":"tpm",
    "text_length":1880,
    "title_length":56,
    "num_tags":1
  },
  {
    "id":6316,
    "title":"Ranged subtype `Default` intrinsic is outside subtype&#39;s declared range",
    "link":"https:\/\/stackoverflow.com\/questions\/79724140\/ranged-subtype-default-intrinsic-is-outside-subtypes-declared-range",
    "text":"While working on a tiny generic metaprogramming library (runtime contracts\/preconditions) I've stumbled upon the following peculiarity: ``` program GenericCTBooleanFlags; {$mode objfpc} type TFalse = false .. false; TTrue = true .. true; begin WriteLn(Low (TFalse)); \/\/ outputs: FALSE WriteLn(High (TFalse)); \/\/ outputs: FALSE WriteLn(Default (TFalse)); \/\/ outputs: FALSE WriteLn(Low (TTrue)); \/\/ outputs: TRUE WriteLn(High (TTrue)); \/\/ outputs: TRUE WriteLn(Default (TTrue)); \/\/ outputs: FALSE (!!!) end. ``` So, basically, ``` Default(...) ``` value for some ranged subtypes is outside the subtype's range. Then the question is: is this a bug? Is this a feature? Should I bother FreePascal Compiler devs with that? Thanks, Arthur.",
    "author_id":5792,
    "publication_date":1754243059000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Rrr Rrr",
    "author_reputation":623.0,
    "tags":"generics, types, freepascal",
    "text_length":732,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":6315,
    "title":"Implement same task for two different user interfaces in Kotlin",
    "link":"https:\/\/stackoverflow.com\/questions\/79724141\/implement-same-task-for-two-different-user-interfaces-in-kotlin",
    "text":"I am working on a program which has both a TUI (terminal user interface) as well as a GUI (graphical user interface). The program has some functions which take quite a long time to complete (waiting on network requests and such). I know that I can use threads (or coroutines since I am using Kotlin) to have the long running task in the background, while the TUI or GUI does other things. However, I also have information that I want to convey while the task is running, so the user knows what's happening in the mean time (as opposed to just showing \"Loading...\" or a spinning circle icon). My current guess is to use something like callbacks (or ``` Channel ``` in Kotlin) to convey information, like \"I am now doing X\", \"I have completed X\", \"I failed to do X so I am now aborting\", etc. I have given an example of my current idea below. Is this solution okay, or am I fundamentally missing something? Are there ways the code sample I gave could be improved? ``` suspend fun foo( \/* other parameters here *\/ eventChannel: Channel<FooEvent>, ) { eventChannel.send(FooEvent.BeginBar()) try { \/* do work that may fail here *\/ eventChannel.send(FooEvent.FinishBar()) } catch (e: Exception) { eventChannel.send(FooEvent.FailBar(e)) return } eventChannel.send(FooEvent.BeginBaz()) try { \/* do work that may fail here *\/ eventChannel.send(FooEvent.FinishBaz()) } catch (e: Exception) { eventChannel.send(FooEvent.FailBaz(e)) return } } sealed class FooEvent { class BeginBar : FooEvent() class FailBar(cause: Throwable) : FooEvent() class FinishBar : FooEvent() class BeginBaz : FooEvent() class FailBaz(cause: Throwable) : FooEvent() class FinishBaz : FooEvent() } ```",
    "author_id":5791,
    "publication_date":1754243063000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Whirvis",
    "author_reputation":356.0,
    "tags":"kotlin, events, user-interface, kotlin-coroutines",
    "text_length":1665,
    "title_length":63,
    "num_tags":4
  },
  {
    "id":6314,
    "title":"How to debug a DNS HTTPS record",
    "link":"https:\/\/stackoverflow.com\/questions\/79724143\/how-to-debug-a-dns-https-record",
    "text":"I have an HTTPS DNS record for fanaka.pro, pointing at readthedocs.io. My zone file: ``` fanaka.pro. 3600 IN HTTPS 0 readthedocs.io. fanaka.pro. 3600 IN NS ns1.desec.io. fanaka.pro. 3600 IN NS ns2.desec.org. fanaka.pro. 300 IN SOA get.desec.io. get.desec.io. 2025080201 86400 3600 2419200 3600 www.fanaka.pro. 3600 IN CNAME readthedocs.io. ``` https:\/\/www.fanaka.pro works as expected https:\/\/fanaka.pro does not (my browser variously says it can't make a secure connection, or that \"the server unexpectedly dropped the connection\"). According to https:\/\/www.nslookup.io\/domains\/fanaka.pro\/dns-records\/https\/ : ``` QUESTION dig @ns2.desec.org. fanaka.pro. HTTPS ANSWER fanaka.pro. 3600 HTTPS 0 readthedocs.io. AUTHORITY ADDITIONAL . 0 OPT ; payload 1400, xrcode 0, version 0, flags 0 ``` Which looks correct. I have spent a lot of time with ``` dig ``` and ``` nslookup ``` and I am no clearer now about what is happening or where the problem might be. I haven't included any of those queries because I am not even sure if I doing the right things with them, and I don't want to muddy the water with irrelevant results. What should my next debugging step be?",
    "author_id":5790,
    "publication_date":1754243264000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Daniele Procida",
    "author_reputation":1553.0,
    "tags":"dns",
    "text_length":1158,
    "title_length":31,
    "num_tags":1
  },
  {
    "id":6313,
    "title":"MCP inspector tool do not have option to add input in Streamable HTTP mode",
    "link":"https:\/\/stackoverflow.com\/questions\/79724144\/mcp-inspector-tool-do-not-have-option-to-add-input-in-streamable-http-mode",
    "text":"I have created a simple MCP server and accessing it via StreamableHTTPServerTransport. I am not quite sure in the MCP inspector tool I do not see an option to add input parameters. Could someone please help me to understand whats the issue here? ``` import { McpServer } from \"@modelcontextprotocol\/sdk\/server\/mcp.js\"; import { StreamableHTTPServerTransport } from \"@modelcontextprotocol\/sdk\/server\/streamableHttp.js\"; import { z } from \"zod\"; import express from \"express\"; const server = new McpServer({ name: \"WeatherServer\", version: \"1.0.0\", description: \"A server that provides weather information\", }); server.tool(\"add\", { title: \"Addition Tool\", description: \"Add two numbers\", inputSchema: { a: z.number().describe(\"The first number\"), b: z.number().describe(\"The second number\"), }, }, async ({ a, b }) => { return { content: [ { type: \"text\", text: `The result is ${a + b}`, }, ], }; }); const app = express(); app.use(express.json()); app.post(\"\/mcp\", async (req: any, res: any) => { try { const transport = new StreamableHTTPServerTransport({ sessionIdGenerator: undefined, }); res.on(\"close\", () => { transport.close(); }); \/\/ here we're connecting the transport with MCP server await server.connect(transport); await transport.handleRequest(req, res, req.body); } catch (error: any) { console.error(\"Error handling MCP request:\", error); if (!res.headersSent) { res.status(500).json({ jsonrpc: \"2.0\", error: { code: -32603, message: \"Internal server error\" }, id: null, }); } } }); app.get(\"\/mcp\", (req, res) => { console.log(\"get request received\"); res.status(405).set(\"Allow\", \"POST\").send(\"Method Not Allowed\"); return; }); const PORT = process.env.PORT || 3000; app.listen(PORT, () => { console.log(`Weather MCP Server running on port ${PORT}`); }); ```",
    "author_id":5789,
    "publication_date":1754243338000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Deepak Kumar Padhy",
    "author_reputation":4434.0,
    "tags":"typescript, node.js, model-context-protocol, zod",
    "text_length":1774,
    "title_length":74,
    "num_tags":4
  },
  {
    "id":6312,
    "title":"Why does it show SettingWithCopyWarning?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724145\/why-does-it-show-settingwithcopywarning",
    "text":"I Googled how to add a row to a dataframe and found the following sample code: ``` # Create a sample DataFrame data = {'col1': [1, 2], 'col2': ['A', 'B']} df1 = pd.DataFrame(data, index=['idx1', 'idx2']) print(\"Original DataFrame:\") print(df1) # Define the new row data new_row_data = [3, 'C'] # Define the new index label new_index_label = 'idx3' # Append the new row with the specified index df1.loc[new_index_label] = new_row_data print(\"\\nDataFrame after appending a new row:\") print(df1) ``` It works well without giving any message. Then I used the same method to add one row in my own code: ``` df_test = df.iloc[:60:3] # a datafrme for test row_add = df.iloc[100] # one row to add df_test.loc[row_add.name] = row_add.tolist() ``` Here ``` df ``` is a dataframe of stock price (open, high, low, close, volume) with datetime object as index. ``` df_test ``` is like the following before manipulation: After the manipulation, ``` df_test ``` is: The result matches my expectation. But it gives the following message: ``` SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy df_test.loc[row_add.name] = row_add.tolist() ``` I want to know why it gives this message in my code, whereas no message is shown in the sample code? Some information can be found about SettingWithCopyWarning, e.g. SettingWithCopyWarning from Google and SettingWithCopyWarning from stackoverflow , but they are not quite relevant to my case. It's shown that it can be suppressed by ``` import pandas as pd pd.options.mode.chained_assignment = None # default='warn' ``` But I don't know whether there is any side effect.",
    "author_id":5077,
    "publication_date":1754243436000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Wenjie Yu",
    "author_reputation":99.0,
    "tags":"python, dataframe, pandas",
    "text_length":1774,
    "title_length":40,
    "num_tags":3
  },
  {
    "id":6311,
    "title":"Getting a fatal error in Windbg on strat up of target virtual machine",
    "link":"https:\/\/stackoverflow.com\/questions\/79724146\/getting-a-fatal-error-in-windbg-on-strat-up-of-target-virtual-machine",
    "text":"I just setup this WinDbg debugging configuration yesterday and I'm getting a fatal error of memory corruption. The target machine is VMWare Workstation 17.5.x over a COM port named pipes. Secure boot is disabled and BIOS has not been updated since its purchased. Both OSes are running Windows 11 Pro 24H2. I've cleaned the drive on the target OS via VMWare Workstation settings. I should note that this Windows 11 Pro VM works perfectly outside kernel debugging. I'm kind at a loss as to what caused this crash. Any help would be greatly appreciated. ``` Microsoft (R) Windows Debugger Version 10.0.19041.685 AMD64 Copyright (c) Microsoft Corporation. All rights reserved. Opened \\\\.\\pipe\\mypipe Waiting to reconnect... Connected to Windows 10 26100 x64 target at (Sun Aug 3 10:13:14.332 2025 (UTC - 4:00)), ptr64 TRUE Kernel Debugger connection established. Symbol search path is: srv* Executable search path is: Windows 10 Kernel Version 26100 MP (1 procs) Free x64 Built by: 26100.1.amd64fre.ge_release.240331-1435 Machine Name: Kernel base = 0xfffff806`c4c00000 PsLoadedModuleList = 0xfffff806`c5af4de0 System Uptime: 0 days 0:00:00.000 minio\\security\\base\\lsa\\security\\driver\\asyncsspi.cxx - SspiInitAsyncInterface KDTARGET: Refreshing KD connection KernelProt: DriverEntry ===> PsSetLoadImageNotifyRoutine Succeeded IOINIT: Built-in driver \\Driver\\hwpolicy failed to initialize with status - 0xC000025E DriverEntry failed 0xc00000bb for driver \\REGISTRY\\MACHINE\\SYSTEM\\ControlSet001\\Services\\uiomap KDTARGET: Refreshing KD connection Break instruction exception - code 80000003 (first chance) A fatal system error has occurred. Debugger entered on first try; Bugcheck callbacks have not been invoked. A fatal system error has occurred. For analysis of this file, run !analyze -v nt!DbgBreakPointWithStatus: fffff806`c50ff9b0 cc int 3 0: kd> !analyze -v Connected to Windows 10 26100 x64 target at (Sun Aug 3 10:13:48.689 2025 (UTC - 4:00)), ptr64 TRUE Loading Kernel Symbols .......................................... Press ctrl-c (cdb, kd, ntsd) or ctrl-break (windbg) to abort symbol loads that take too long. Run !sym noisy before .reload to track down problems loading symbols. ..................... ........................................................ Loading User Symbols PEB address is NULL ! Loading unloaded module list ..... ******************************************************************************* * * * Bugcheck Analysis * * * ******************************************************************************* NMI_HARDWARE_FAILURE (80) This is typically due to a hardware malfunction. The hardware supplier should be called. Arguments: Arg1: 00000000004f4454 Arg2: 0000000000000010 Arg3: 0000000000000000 Arg4: 0000000000000000 Debugging Details: ------------------ PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! PEB address is NULL ! KEY_VALUES_STRING: 1 Key : Analysis.CPU.Sec Value: 2 Key : Analysis.DebugAnalysisProvider.CPP Value: Create: 8007007e on DESKTOP-T7URTIT Key : Analysis.DebugData Value: CreateObject Key : Analysis.DebugModel Value: CreateObject Key : Analysis.Elapsed.Sec Value: 6 Key : Analysis.Memory.CommitPeak.Mb Value: 66 Key : Analysis.System Value: CreateObject BUGCHECK_CODE: 80 BUGCHECK_P1: 4f4454 BUGCHECK_P2: 10 BUGCHECK_P3: 0 BUGCHECK_P4: 0 PROCESS_NAME: Registry STACK_TEXT: fffff806`575772f8 fffff806`c51b4532 : 00000000`00000000 00000000`00000080 00000000`00000023 fffff806`c52bdfa0 : nt!DbgBreakPointWithStatus fffff806`57577300 fffff806`c51b3a5c : 00000000`00000003 fffff806`57577460 00000000`00000000 00000000`00000000 : nt!KiBugCheckDebugBreak+0x12 fffff806`57577360 fffff806`c50feca7 : 00000000`00000000 00000000`00000000 ffffdc0e`d989a8a0 ffffdc0e`d989b8a0 : nt!KeBugCheck2+0xb2c fffff806`57577af0 fffff806`c5146702 : 00000000`00000080 00000000`004f4454 00000000`00000010 00000000`00000000 : nt!KeBugCheckEx+0x107 fffff806`57577b30 fffff806`c5140cf9 : ffffdc0e`d989b000 00000000`00001000 00000000`00001000 fffff806`c50874fd : nt!HalpNMIHalt+0x2e fffff806`57577b70 fffff806`566f1250 : 00000000`00000000 fffff806`57577c39 ffffdc0e`d989b8c8 fffff806`c5a0fec0 : nt!HalBugCheckSystem+0x69 fffff806`57577bb0 fffff806`c52e3133 : 00000000`00000000 fffff806`57577c39 ffffdc0e`d989b8c8 00000000`00000000 : PSHED!PshedBugCheckSystem+0x10 fffff806`57577be0 fffff806`c51464db : fffff806`c5bce410 fffff806`c5bce410 fffff806`c5a0fec0 00000000`0000005c : nt!WheaReportHwError+0x2a1543 fffff806`57577ca0 fffff806`c51b89bf : fffff806`c5bce400 fffff806`57577d10 00000000`00000000 fffff806`57577d10 : nt!HalHandleNMI+0x14b fffff806`57577cd0 fffff806`c52b03c2 : ffff808e`9c9f7010 fffff806`57577ed0 00000000`00000000 ffff808e`9c7f1898 : nt!KiProcessNMI+0xff fffff806`57577d10 fffff806`c52b012e : 00000000`00000000 ffff808e`9c9f7010 fffff806`57577ed0 00000000`00000000 : nt!KxNmiInterrupt+0x82 fffff806`57577e50 fffff806`c52ad106 : 00000000`0000002e ffff808e`9c7f1890 fffff806`c55344d6 ffffaa08`84572261 : nt!KiNmiInterrupt+0x26e ffffaa08`84572168 fffff806`c55344d6 : ffffaa08`84572261 ffffdc0e`d99fe840 00000000`00000102 ffff808e`9c9fe4e0 : nt!RtlCompareMemory+0x36 ffffaa08`84572180 fffff806`57ce6f9c : ffffaa08`84572300 ffffaa08`84572261 ffffdc0e`d7bd3d80 ffff808e`9c765a60 : nt!RtlEqualUnicodeString+0x76 ffffaa08`845721b0 fffff806`57ce69c2 : ffffdc0e`d7bd3d80 ffffdc0e`d83fb520 00000000`00000000 00000000`00000000 : mountmgr!QueryPointsFromMemory+0x25c ffffaa08`845722c0 fffff806`57ce48f7 : 00000000`00000000 ffffdc0e`d7bd3d80 ffffaa08`84572480 00000000`00000000 : mountmgr!MountMgrQueryPoints+0x1a2 ffffaa08`84572330 fffff806`c4e3650d : ffffaa08`84572480 00000000`00000000 ffffaa08`84572489 ffffaa08`84572480 : mountmgr!MountMgrDeviceControl+0x257 ffffaa08`845723a0 fffff806`c54282aa : ffffffff`800003f8 ffffaa08`84572550 ffffaa08`84572489 00000000`00000000 : nt!IofCallDriver+0xcd ffffaa08`845723e0 fffff806`c54280c8 : ffff808e`9c2bce80 fffff806`c4e8fb99 ffffaa08`845727d0 00000000`00000000 : nt!IoVolumeDeviceNameToGuidPath+0x15a ffffaa08`845724f0 fffff806`c5427f4d : 00000000`000003dc 00000000`00000080 ffffaa08`84572848 fffff806`c5032c79 : nt!IoVolumeDeviceToGuidPath+0x118 ffffaa08`845727b0 fffff806`c5427ac9 : 00000000`00000000 ffff808e`9cab8000 00000000`00000000 ffffaa08`84572990 : nt!IoVolumeDeviceToGuid+0x2d ffffaa08`84572800 fffff806`c55103b4 : 00000000`00000000 00000000`00000001 00000000`00000000 00000000`00000000 : nt!CmpVolumeManagerGetContextForFile+0x85 ffffaa08`84572890 fffff806`c53db9a3 : fffff806`c5af5620 ffffaa08`84572b89 fffff806`c5af57a8 ffffdc0e`d99fdd50 : nt!CmpCreateHive+0x634 ffffaa08`84572b10 fffff806`c5095fca : ffffdc0e`d987d040 ffffdc0e`d987d040 fffff806`c53db7f0 ffff808e`9c9fc1d0 : nt!CmpHiveCachePopulateHiveEntryThread+0x1b3 ffffaa08`84572bf0 fffff806`c52a5534 : fffff806`528cb180 ffffdc0e`d987d040 fffff806`c5095f70 00000000`00000000 : nt!PspSystemThreadStartup+0x5a ffffaa08`84572c40 00000000`00000000 : ffffaa08`84573000 ffffaa08`8456d000 00000000`00000000 00000000`00000000 : nt!KiStartSystemThread+0x34 CHKIMG_EXTENSION: !chkimg -lo 50 -d !PSHED fffff806566f11f5-fffff806566f11f8 4 bytes - PSHED!PshedAttemptErrorRecovery+85 [ c7 44 00 00:87 25 0c 6f ] fffff806566f12da-fffff806566f12dd 4 bytes - PSHED!PshedClearErrorRecord+7a (+0xe5) [ e2 43 00 00:a2 24 0c 6f ] fffff806566f13bd-fffff806566f13c0 4 bytes - PSHED!PshedDisableErrorSource+8d (+0xe3) [ ff 42 00 00:bf 23 0c 6f ] fffff806566f1476-fffff806566f1479 4 bytes - PSHED!PshedEnableErrorSource+76 (+0xb9) [ 46 42 00 00:06 23 0c 6f ] fffff806566f1524-fffff806566f1527 4 bytes - PSHED!PshedFinalizeErrorRecord+74 (+0xae) [ 98 41 00 00:58 22 0c 6f ] fffff806566f171f-fffff806566f1722 4 bytes - PSHED!PshedGetErrorSourceInfo+8f (+0x1fb) [ 9d 3f 00 00:5d 20 0c 6f ] fffff806566f183c-fffff806566f183f 4 bytes - PSHED!PshedGetInjectionCapabilities+dc (+0x11d) [ 80 3e 00 00:40 1f 0c 6f ] fffff806566f1bc0-fffff806566f1bc3 4 bytes - PSHED!PshedRetrieveErrorInfo+80 (+0x384) [ fc 3a 00 00:bc 1b 0c 6f ] fffff806566f1cca-fffff806566f1ccd 4 bytes - PSHED!PshedSetErrorSourceInfo+8a (+0x10a) [ f2 39 00 00:b2 1a 0c 6f ] fffff806566f1d49-fffff806566f1d4a 2 bytes - PSHED!PshedTranslateDimmAddress+39 (+0x7f) [ 33 3d:73 3f ] fffff806566f2a3f-fffff806566f2a40 2 bytes - PSHED!PshedpShrinkRecordForWrite+19f (+0xcf6) [ 3d 30:7d 32 ] fffff806566f2b50-fffff806566f2b53 4 bytes - PSHED!PshedpWaitForOperationToComplete+18 (+0x111) [ 6c 2b 00 00:2c 0c 0c 6f ] fffff806566f2bcc-fffff806566f2bcf 4 bytes - PSHED!PshedDoPfa+2c (+0x7c) [ f0 2a 00 00:b0 0b 0c 6f ] fffff806566f2c3f-fffff806566f2c42 4 bytes - PSHED!PshedInitGlobal+1f (+0x73) [ 7d 2a 00 00:3d 0b 0c 6f ] fffff806566f2c6f-fffff806566f2c72 4 bytes - PSHED!PshedInitProc+1f (+0x30) [ 4d 2a 00 00:0d 0b 0c 6f ] fffff806566f2d1e-fffff806566f2d21 4 bytes - PSHED!PshedpDoPluginGetAllErrorSources+72 (+0xaf) [ 9e 29 00 00:5e 0a 0c 6f ] fffff806566f2def-fffff806566f2df2 4 bytes - PSHED!PshedpDoPluginInjectError+93 (+0xd1) [ cd 28 00 00:8d 09 0c 6f ] fffff806566f2ebd-fffff806566f2ec0 4 bytes - PSHED!PshedpDoPluginReadErrorRecord+8d (+0xce) [ ff 27 00 00:bf 08 0c 6f ] fffff806566f2f73-fffff806566f2f76 4 bytes - PSHED!PshedpDoPluginWriteErrorRecord+73 (+0xb6) [ 49 27 00 00:09 08 0c 6f ] fffff806566f2fe0-fffff806566f2fe1 2 bytes - PSHED!PshedpLogRegistrationCollision+30 (+0x6d) [ 9c 2a:dc 2c ] fffff806566f3675-fffff806566f3676 2 bytes - PSHED!PshedpInjectErrorEINJ+49 (+0x695) [ 07 24:47 26 ] fffff806566f3b67-fffff806566f3b68 2 bytes - PSHED!PshedpClearErrorRecordERST+37 (+0x4f2) [ 15 1f:55 21 ] fffff806566f4519-fffff806566f451c 4 bytes - PSHED!PshedpExecuteMoveDataInstruction+69 (+0x9b2) [ a3 11 00 00:63 f2 0b 6f ] fffff806566f453a-fffff806566f453d 4 bytes - PSHED!PshedpExecuteMoveDataInstruction+8a (+0x21) [ 82 11 00 00:42 f2 0b 6f ] fffff806566f50d4-fffff806566f50d5 2 bytes - PSHED!PshedpReadErrorRecordERST+44 (+0xb9a) [ a8 09:e8 0b ] fffff806566f5692 - PSHED!RtlSetVolatileMemory+2 (+0x5be) [ 03:06 ] fffff806566f6011-fffff806566f6014 4 bytes - PSHED!guard_dispatch_icall$thunk$10345483385596137414+1 [ ab f6 ff ff:6b d7 0b 6f ] fffff806566f6021-fffff806566f6022 2 bytes - PSHED!memset$thunk$772440563353939046+1 (+0x10) [ 5b fa:9b fc ] fffff806566fd368-fffff806566fd369 2 bytes - PSHED!PshedpValidateErrorSource+40 [ 14 87:54 89 ] fffff806566fd6bc-fffff806566fd6bf 4 bytes - PSHED!PshedDoPluginCtl+7c (+0x354) [ 00 80 ff ff:c0 60 0b 6f ] fffff806566fd81c-fffff806566fd81d 2 bytes - PSHED!PshedRegisterPlugin+dc (+0x160) [ 60 82:a0 84 ] 103 errors : !PSHED (fffff806566f11f5-fffff806566fd81d) MODULE_NAME: memory_corruption IMAGE_NAME: memory_corruption MEMORY_CORRUPTOR: LARGE STACK_COMMAND: .thread ; .cxr ; kb FAILURE_BUCKET_ID: MEMORY_CORRUPTION_LARGE OS_VERSION: 10.0.26100.1 BUILDLAB_STR: ge_release OSPLATFORM_TYPE: x64 OSNAME: Windows 10 FAILURE_ID_HASH: {e29154ac-69a4-0eb8-172a-a860f73c0a3c} Followup: memory_corruption --------- ```",
    "author_id":5518,
    "publication_date":1754243488000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"blogger13",
    "author_reputation":97.0,
    "tags":"debugging, windbg, crash-dumps",
    "text_length":11170,
    "title_length":69,
    "num_tags":3
  },
  {
    "id":6310,
    "title":"The delete key also deletes the comment on the cell and I don&#39;t want to",
    "link":"https:\/\/stackoverflow.com\/questions\/79724147\/the-delete-key-also-deletes-the-comment-on-the-cell-and-i-dont-want-to",
    "text":"When I select a cell with a comment and press the Delete key, the comment is also deleted, which I don't want. I know I can delete the contents of a cell by right-clicking, clicking \"Clear Contents\" and then clicking a box that allows me to deselect \"Comment,\" but this is impractical, and I'd like to achieve the same effect by simply pressing the Delete key. How can I achieve this?",
    "author_id":5788,
    "publication_date":1754243538000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mario Palumbo",
    "author_reputation":1099.0,
    "tags":"libreoffice-calc",
    "text_length":384,
    "title_length":75,
    "num_tags":1
  },
  {
    "id":6309,
    "title":"Return control to the console after outputting to a download file in PHP",
    "link":"https:\/\/stackoverflow.com\/questions\/79724148\/return-control-to-the-console-after-outputting-to-a-download-file-in-php",
    "text":"I have a PHP routine which produces an on-screen report (of various data items pulled from a variety of data tables (MYSQL)). I can redirect that output to a download file using header(). After that completes, is there any way to return output to the console other than reloading the program? I don't mind throwing some JavaScript in there if that is the best way. I also don't mind reloading the program (a simple JavaScript line) if that is best. Interested in opinions. Thanks.",
    "author_id":5787,
    "publication_date":1754243653000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Hank Lubin",
    "author_reputation":19.0,
    "tags":"php",
    "text_length":480,
    "title_length":72,
    "num_tags":1
  },
  {
    "id":6308,
    "title":"RISC-V GCC Force Linker Relaxation with GP Register to Address Static Data",
    "link":"https:\/\/stackoverflow.com\/questions\/79724151\/risc-v-gcc-force-linker-relaxation-with-gp-register-to-address-static-data",
    "text":"I'm writing a bare-metal firmware for a virtual RISC-V SoC. Below is code that communicates with a physical device on the SoC and it is using constant data from the ``` my_data ``` array that sits in ROM. The linker script contains a definition for ``` __global_pointer$ ``` that is equal to the start of this constant data array. My assumption is, that the linker will use this register for relaxation to create code-size efficient code. ``` void main(void); __attribute__((naked, noreturn)) void _start(void) { __asm__ __volatile__(\"\":::\"memory\"); __asm__ __volatile__ (\".option push\"); __asm__ __volatile__ (\".option norelax\"); __asm__ __volatile__ (\"add x1, x0, x0\"); __asm__ __volatile__ (\"la x2, __stack_start$\"); __asm__ __volatile__ (\"la x3, __global_pointer$\"); __asm__ __volatile__ (\"add x4, x0, x0\"); \/\/ ... rest of registers __asm__ __volatile__ (\"add x31, x0, x0\"); __asm__ __volatile__ (\".option pop\"); __asm__ __volatile__(\"\":::\"memory\"); main(); __asm__ __volatile__ (\"j _start\"); } typedef struct { volatile unsigned int regs [0x1000]; } phy_t __attribute__ ((aligned (0x8000))); extern phy_t __phy$; const unsigned int my_data[3] = { 0x12345678u, 0x23456789u, 0x3456789Au, }; void main() { __phy$.regs[0x0000] = my_data[0]; __phy$.regs[0x0001] = my_data[1]; __phy$.regs[0x0002] = my_data[2]; } ``` ``` OUTPUT_FORMAT(\"elf32-littleriscv\", \"elf32-littleriscv\", \"elf32-littleriscv\") OUTPUT_ARCH(riscv) MEMORY { PHY ( W ) : ORIGIN = 0x00000000, LENGTH = 0x00008000 RAM (AWL) : ORIGIN = 0x00010000, LENGTH = 0x00010000 ROM (XR ) : ORIGIN = 0x00020000, LENGTH = 0x00010000 } SECTIONS { .text : ALIGN(4) { PROVIDE(__text_start__$ = .); *(.text .text.*) PROVIDE(__text_end__$ = .); } > ROM .rodata : ALIGN(4) { PROVIDE(__rodata_start__$ = .); \/* Linker relaxation doesn't work for some reason *\/ PROVIDE(__global_pointer$ = .); *(.rodata .rodata.*) *(.constdata .constdata.*) *(.srodata .srodata.*) *(.sdata .sdata.*) PROVIDE(__rodata_end__$ = .); } > ROM .data : ALIGN(4) { PROVIDE(__data_source__$ = LOADADDR(.data)); PROVIDE(__data_start__$ = .); *(.data_begin .data_begin.*) *(.data .data.*) *(.data_end .data_end.*) PROVIDE(__data_end__$ = .); } > RAM AT>ROM .stack (NOLOAD) : ALIGN(4) { PROVIDE(__stack_end$ = .); . = ORIGIN(RAM) + LENGTH(RAM); PROVIDE(__stack_start$ = .); } > RAM .phy (NOLOAD) : ALIGN(16) { PROVIDE(__phy$ = .); } > PHY } ``` ``` CFLAGS=-march=rv32ic -mabi=ilp32 -Os -mcmodel=medlow -Xlinker --emit-relocs LDFLAGS=-nostdlib -static -ffreestanding all: riscv-unknown-elf-gcc $(CFLAGS) $(LDFLAGS) -o firmware.elf -x c firmware.c -T firmware.ld riscv-unknown-elf-objdump -M numeric -d -r firmware.elf > firmware.lst ``` However, the listing contains the data inlined as lui+addi instruction pairs instead. The map output contains the my_data array in the correct section ``` .srodata ``` , putting it in the ``` .sdata ``` section via ``` __attribute__((section(\".sdata\"))) ``` doesn't help either. I have tried options ``` -msmall-data-limit=512 ``` , ``` -fno-pie ``` , ``` -Xlinker --relax-gp ``` , ``` -Xlinker gpsize=512 ``` on GCC 15.1.0. Is there a way to force relaxation via the GP register? ``` 00020020 <main>: 20020: 12345737 lui x14,0x12345 20024: 67870713 addi x14,x14,1656 # 12345678 <my_data+0x12325630> 20024: R_RISCV_NONE *ABS*+0x4 20024: R_RISCV_RELAX *ABS* 20028: 00e02023 sw x14,0(x0) # 0 <__phy$> 20028: R_RISCV_GPREL_S __phy$ 20028: R_RISCV_RELAX *ABS* 2002c: 23456737 lui x14,0x23456 20030: 00000793 li x15,0 20030: R_RISCV_GPREL_I __phy$ 20030: R_RISCV_RELAX *ABS* 20034: 78970713 addi x14,x14,1929 # 23456789 <my_data+0x23436741> 20038: c3d8 sw x14,4(x15) 2003a: 34568737 lui x14,0x34568 2003e: 89a70713 addi x14,x14,-1894 # 3456789a <my_data+0x34547852> 20042: c798 sw x14,8(x15) 20044: 8082 ret ``` ``` ... .rodata 0x0000000000020048 0xc [!provide] PROVIDE (__rodata_start__$ = .) 0x0000000000020048 PROVIDE (__global_pointer$ = .) *(.rodata .rodata.*) *(.constdata .constdata.*) *(.srodata .srodata.*) .srodata 0x0000000000020048 0xc 0x0000000000020048 my_data *(.sdata .sdata.*) [!provide] PROVIDE (__rodata_end__$ = .) ```",
    "author_id":5786,
    "publication_date":1754243844000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"TobiPlusPlus",
    "author_reputation":83.0,
    "tags":"gcc, linker, riscv",
    "text_length":4084,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":6307,
    "title":"Why do WinHTTP and TNetHTTPClient timeout (error 12002) on one Wi-Fi network but succeed on others?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724154\/why-do-winhttp-and-tnethttpclient-timeout-error-12002-on-one-wi-fi-network-but",
    "text":"I'm developing a Delphi application to download files using both ``` WinHTTP ``` (via direct API calls) and ``` TNetHTTPClient ``` . Everything works fine on most networks, but on one specific open Wi-Fi network , both approaches fail consistently with a timeout. Specifically: ``` WinHttpSendRequest ``` returns error 12002 (ERROR_WINHTTP_TIMEOUT) ``` TNetHTTPClient.Get() ``` also raises a timeout exception However: Indy ( ``` TIdHTTP ``` ) downloads the file successfully on the same network, and Chrome or IDM can also download the file immediately. The target URL is hosted on Cloudflare R2 , with no enforced bot protection, and works fine from other networks. What I’ve already tried: mimicked common browser headers like ``` User-Agent ``` , ``` Accept ``` , etc. ensured SNI and TLS version compatibility tried both ``` WINHTTP_ACCESS_TYPE_AUTOMATIC_PROXY ``` and ``` WINHTTP_ACCESS_TYPE_NO_PROXY ``` , but the result is the same — still getting timeout (error 12002 in ``` WinHTTP ``` ). I also inspected the traffic using Wireshark. When the download fails (using ``` WinHTTP ``` or ``` TNetHTTPClient ``` ), I do not see any DNS resolution phase in the capture. However, when switching to another network or using Indy or web browsers (which successfully download the file), I can clearly observe the DNS resolution phase. Question : Why would ``` WinHTTP ``` and ``` TNetHTTPClient ``` time out on one specific network, while Indy and browsers succeed? Is there any internal behavior in ``` WinHTTP ``` that makes it more sensitive to certain network environments (e.g., open Wi-Fi, captive portals, DNS behavior, SNI, etc.)? Update : I confirmed that WinInet also experiences the same issue on this network. The problem was resolved by enabling the ``` WINHTTP_OPTION_IPV6_FAST_FALLBACK ``` option in WinHTTP, which allowed the client to quickly fall back to IPv4 when IPv6 was slow or unresponsive.",
    "author_id":5785,
    "publication_date":1754244053000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"sudormrf",
    "author_reputation":11.0,
    "tags":"delphi, cloudflare, indy, connection-timeout, winhttp",
    "text_length":1914,
    "title_length":99,
    "num_tags":5
  },
  {
    "id":6306,
    "title":"How can I add comments to tooltips with OmniPascal extension",
    "link":"https:\/\/stackoverflow.com\/questions\/79724160\/how-can-i-add-comments-to-tooltips-with-omnipascal-extension",
    "text":"I want to see comments as tooltips in VS Code with the OmniPascal extension. Right now, comments are not appearing in tooltips despite trying different Pascal commenting styles. Maybe i just dont know about some basic feature in VS Code \/ convention in Pascal, but I dont know how to get shown comments (intended as documentation) in the tooltips which appear when using autocomplete \/ hovering over function in VSCode with OmniPascal. The function signatures are shown correctly. I tried all styles of commenting in Pascal, as shown below, in the snippet in block \"what have you tried\", above and under the function declaration, each attempt with no result. Am I missing something, or is this feature not available with OmniPascal? This extension is helping me greatly, right now I use go to definition to see any comments, but it would be neat to see this information without traveling out of place, so I do not have to do so much context switching. Thanks for clarification \/ any tips, how to get this going. Sorry for possible duplication of information and stating obvious things, the bot is not easily satisfied. I tried put all different styles of commenting I know of,around sample procedure, as shown bellow. For the BOT: i tried one line above, then the line below, then added another style etc. After each addition, I checked wheter i see 'comment' in tooltip when i write the word 'proc' in the main body of program. Every time i have seen just the signature of proc. ``` program p; \/\/ comment (* comment *) { comment } \/\/\/ comment \/\/\/ <summary> \/\/\/ comment \/\/\/ <\/summary> procedure proc(); \/\/\/ <summary> \/\/\/ comment \/\/\/ <\/summary> \/\/\/ comment { comment} (* comment *) \/\/ comment begin end; ``` I would expect \/ want something like if I did this in Python: ``` def foo(): \"\"\"comment\"\"\" pass ```",
    "author_id":5784,
    "publication_date":1754244505000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Filip Vidimsk&#253;",
    "author_reputation":11.0,
    "tags":"visual-studio-code, pascal, omnipascal",
    "text_length":1806,
    "title_length":60,
    "num_tags":3
  },
  {
    "id":6305,
    "title":"Create Action Button in Open edx Admin panel using tutor plugins",
    "link":"https:\/\/stackoverflow.com\/questions\/79724168\/create-action-button-in-open-edx-admin-panel-using-tutor-plugins",
    "text":"I created and enabled a tutor plugin successfully using this command cookiecutter https:\/\/github.com\/overhangio\/cookiecutter-tutor-plugin.git How would I use this plugin to implement Admin Action Button : I have a folder adminUser with 2 files init .py (from . import admin) and admin.py see content below: ``` from django.contrib import admin from django.contrib.auth.models import User @admin.action(description=\"Mark selected Users as inactive\") def mark_users_inactive(modeladmin, request, queryset): queryset.update(is_active=False) modeladmin.message_user(request, f\"{queryset.count()} users marked as inactive.\") admin.site.unregister(User) @admin.register(User) class CustomUserAdmin(admin.ModelAdmin): list_display = (\"username\", \"email\", \"first_name\", \"last_name\", \"is_staff\", \"is_active\") actions = [mark_users_inactive] ``` I added the lines below to the plugin.py : ``` PLUGIN_ROOT = Path(__file__).parent.parent.resolve() hooks.Filters.COMPOSE_MOUNTS.add_item((\"lms\", (str(PLUGIN_ROOT \/ \"adminAction\"), \"\/openedx\/edx-platform\/adminAction\"))) hooks.Filters.COMPOSE_MOUNTS.add_item((\"cms\", (str(PLUGIN_ROOT \/ \"adminAction\"), \"\/openedx\/edx-platform\/adminAction\"))) ``` Added patches\/openedx-lms-env with ``` INSTALLED_APPS += [\"adminAction\"] ``` Added ``` recursive-include adminAction * ``` in .\/MANIFEST.in In pyproject.toml Added ``` include = [\"adminAction\"] ``` under ``` [tool.hatch.build.targets.wheel] ``` Updated ``` include = [ \"\/tutoradmin\", \"\/adminAction\", \".hatch_build.py\"] ``` under ``` [tool.hatch.build.targets.sdist] ``` Yet the Action Button is not visible. Please what am I doing wrong?",
    "author_id":5783,
    "publication_date":1754244951000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Paullo",
    "author_reputation":2137.0,
    "tags":"python, django, django-admin, openedx, cookiecutter",
    "text_length":1617,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":6304,
    "title":"how to use grid with textbox? my textsize seems to impact my grid but I don&#39;t want it to",
    "link":"https:\/\/stackoverflow.com\/questions\/79724176\/how-to-use-grid-with-textbox-my-textsize-seems-to-impact-my-grid-but-i-dont-wa",
    "text":"I want my screen to have a small notepad in the bottom left. My textsize within this textbox seems to impact my grid, but I don't want it to. My understanding is that using ``` uniform =\"a\" ``` fixes such issues, but it seems to persist only in the case of the ``` textbox ``` widget from what I can tell. I have some code running in the back while developing that helps me to see the grid. A picture of the result below: My grid and code as below: ``` # define a grid number_tuple_30 = (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19, 20,21,22,23,24,25,26,27,28,29,30) number_tuple_45 = (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19, 20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45) wd.columnconfigure(number_tuple_30,weight = 10, uniform = \"a\") wd.rowconfigure(number_tuple_45,weight = 10, uniform = \"a\") wd.columnconfigure(1,weight = 13, uniform = \"a\") # textbox text = tk.Text(font=(\"Courier\",15), bg=colour, borderwidth=1, relief=\"solid\") text.grid(column=0, row=39, sticky=\"nesw\", columnspan=9, rowspan=7) ```",
    "author_id":5782,
    "publication_date":1754245756000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Mike Nijland",
    "author_reputation":1.0,
    "tags":"python, grid, tkinter, textbox",
    "text_length":1053,
    "title_length":92,
    "num_tags":4
  },
  {
    "id":6303,
    "title":"Android Button Location Placement",
    "link":"https:\/\/stackoverflow.com\/questions\/79724178\/android-button-location-placement",
    "text":"I would like to know how one can place some widget location at specified coordinates in Java Android directly from code? I would appreciate help.",
    "author_id":5781,
    "publication_date":1754245864000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dominik Stipic",
    "author_reputation":1.0,
    "tags":"java, android",
    "text_length":145,
    "title_length":33,
    "num_tags":2
  },
  {
    "id":6302,
    "title":"Threatlocker Learning Tools? Video Demos?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724179\/threatlocker-learning-tools-video-demos",
    "text":"I am a software engineer who is trying to learn more about ThreatLocker. While I gone through some of the lessons offered on the Threatlocker University I am still having trouble with learning how to use the actual platform. Is anyone aware of any resources out that a newby can follow a step by step process of how to set up Threatlocker on a personal environment? or any recordings that show a practical demonstrations of how to start out as a newbie with Threatlocker? Any information will be much appreciated!",
    "author_id":5780,
    "publication_date":1754245937000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nicholas Cerillo",
    "author_reputation":21.0,
    "tags":"security, threat-model, advanced-threat-protection",
    "text_length":513,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":6301,
    "title":"Which HTML elements can break into several rectangles?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724186\/which-html-elements-can-break-into-several-rectangles",
    "text":"CSS styles such as ``` display: inline ``` allow ``` <span> ``` and ``` <div> ``` to break (fragment) into several rectangles on separate lines, whereas ``` <fieldset> ``` and ``` <button> ``` with the same CSS rules remain one rectangle: ``` span, div, fieldset, button { display: inline; border: 1px solid red; } ``` ``` Lorem <span> ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo<\/span> consequat.<hr> Lorem <div> ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo<\/div> consequat.<hr> Lorem <fieldset> ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo<\/fieldset> consequat.<hr> Lorem <button> ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo<\/button> consequat.<hr> ``` Is there info (in the spec or browser-specific) regarding which HTML elements have or don't have this breakability\/fragmentability trait? The \"CSS Fragmentation Module\" spec says \"inline breaking is not covered here; see [CSS2]\/[CSS3TEXT]\". The \"CSS Text Module Level 3\" spec mentions \"break\" over 1,000 times but doesn't mention ``` <div> ``` or ``` <fieldset> ``` . So I'm not sure where to look.",
    "author_id":5779,
    "publication_date":1754246423000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"root",
    "author_reputation":2758.0,
    "tags":"html, css, specifications",
    "text_length":1735,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6300,
    "title":"Images overflowing beyond div instead of shrinking",
    "link":"https:\/\/stackoverflow.com\/questions\/79724188\/images-overflowing-beyond-div-instead-of-shrinking",
    "text":"I pretty new to html coding so I'm not sure what I'm doing wrong, but I'm building a website with an image gallery on one of the pages. I wanted half of the gallery to be a flex row with wrapping (flex-flow: row wrap;) for wider images and the other half set to nowrap so that the smaller images are side by side. Both galleries' images have the width set to 100% so that the images \"shrink\" to fit the div of the gallery. I've tried this multiple times, like putting the second div gallery outside the first , having them separate, using fit-content, max-width to 100%, etc. They only way to make it smaller is if I manually shrink it with something like width=50%, but I don't want to set a fixed size because I want them to be responsive. Here's the code for reference: ``` .page-content { display: flex; flex-flow: column; justify-content: center; grid-gap: 20px; max-width: 850px; } .gallery img { border-radius: var(--corner-rounding); max-width: 100%; } .gallery { display: flex; flex-flow: row wrap; grid-gap: 1em; align-items: baseline; justify-content: center; } .line { display: flex; flex-flow: row nowrap; } ``` ``` <section class=\"page-content\"> <div class=\"gallery\"> <img src=\"img\/gb01.gif\" \/> <img src=\"img\/gb02.gif\" \/> <img src=\"img\/gb03.gif\" \/> <img src=\"img\/lvl1.jpg\" \/> <img src=\"img\/lvl2.jpg\" \/> <img src=\"img\/lvl3.jpg\" \/> <div class=\"gallery line\"> <img src=\"img\/sreenshot01.jpg\" \/> <img src=\"img\/sreenshot02.jpg\" \/> <img src=\"img\/sreenshot03.jpg\" \/> <\/div> <\/div> <\/section> ``` Any idea what I should do about this? EDIT: I said above that I've already tried putting the second gallery (.line) outside the first gallery (.gallery) and it didn't work. However, I now realize that is because the section both galleries are nested in is also set to flex . Oops. Now, while I waited for this answer I went and tried making the second gallery a grid display with auto columns. In terms of best practices, is this okay or should I be avoiding this? ``` .line { display: grid; grid-auto-flow: column; } ``` Thanks to everyone who answered :^)",
    "author_id":4444,
    "publication_date":1754246919000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"elliotter",
    "author_reputation":3.0,
    "tags":"html, css, flexbox",
    "text_length":2059,
    "title_length":50,
    "num_tags":3
  },
  {
    "id":6299,
    "title":"How to build\/run the the same way Android Studio does from terminal?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724190\/how-to-build-run-the-the-same-way-android-studio-does-from-terminal",
    "text":"I'm trying to find the terminal command that does the exact same thing as Android studio's green ▶️ button. Ie - re-runs the build and uses the same cache. The problem now is that if I try building\/launching from terminal - it seems ignore\/invalidate the cache from Android Studio (and vice versa) If I press ▶️ with no changes to the code since last hitting ▶️ - I get \"Install successfully finished in 8s 208ms \" If I then try running gradle from terminal with ``` .\/gradlew installDebug && adb shell am start -n \"com.MYAPPLICATION.debug\/com.MYAPPLICATION.MainActivity\" ``` .. It takes a 6 minutes 29 seconds to build. Evidently it is ignoring some things that Android Studio is caching. If I run again from terminal. It takes 58s . So the terminal version is caching something. If I then again press ▶️ in Android Studio - it takes 5 min 45s . It seems the terminal command and Android studio are are invalidating each other's caches. So - how can I build my app from terminal without ruining the build-cache from Android studio? Why do I want this? So I can get claude code to automatically rebuild the app after it changes something - without ruining the build cache in Android studio (so I don't need to wait 8 min when I click ▶️ next time).",
    "author_id":5778,
    "publication_date":1754247179000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Peter",
    "author_reputation":13663.0,
    "tags":"android, android-studio",
    "text_length":1248,
    "title_length":68,
    "num_tags":2
  },
  {
    "id":6298,
    "title":"Custom Domain Verification Id in an Azure Front Door ARM template?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724192\/custom-domain-verification-id-in-an-azure-front-door-arm-template",
    "text":"I'm trying to set up a custom domain in Azure Front Door. We deploy an Azure Front Door instance which calls into an Azure App Service endpoint. All of this is done through an ARM template deployment so that it's as hands-off as possible. My understanding is that the custom domain resource belongs to AFD. The CNAME is used to point the custom domain to the App Service endpoint. However, the custom domain validation gets stuck on \"Pending\". I'm trying to provide the custom domain verification id for the TXT record to our DNS but can't seem to find how to output this id through AFD's ARM template ``` outputs ``` fields. I also can't find any documentation on this from Azure. Am I correct in that the custom domain verification id should be coming from Azure Front Door? If so, how can I grab this id from Azure Front Door's ARM template?",
    "author_id":5777,
    "publication_date":1754247244000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"A. Cam",
    "author_reputation":149.0,
    "tags":"dns, azure-front-door, azure-custom-domain",
    "text_length":844,
    "title_length":66,
    "num_tags":3
  },
  {
    "id":6297,
    "title":"Get bounding box of all the articles in a newspaper PDF file",
    "link":"https:\/\/stackoverflow.com\/questions\/79724193\/get-bounding-box-of-all-the-articles-in-a-newspaper-pdf-file",
    "text":"I am trying to get the bounding box of all the articles in a newspaper pdf page using pdfplumber. However it is not marking all the articles. I am using the code below. ``` import pdfplumber pdf = pdfplumber.open(\"2.pdf\") p0 = pdf.pages[0] im = p0.to_image(resolution=150) table_settings = { \"snap_y_tolerance\": 0, \"intersection_x_tolerance\": 50, } im.debug_tablefinder(table_settings) tables = p0.find_tables(table_settings) im.show() ``` Link to processed Image However, the it is not marking all articles as shown in image attached. I think the tables approach was the quickest and shortest way, but it works for well organised tables. I think this strategy is not suitable for newspaper articles. Any other suggestions or improvements in code will be appreciated.",
    "author_id":5776,
    "publication_date":1754247251000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Rajeev",
    "author_reputation":31.0,
    "tags":"python, pdf, bounding-box, pdfplumber",
    "text_length":767,
    "title_length":60,
    "num_tags":4
  },
  {
    "id":6296,
    "title":"How can I copy a file (index.html) to the same directory as the one (*.js) just built using dune?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724201\/how-can-i-copy-a-file-index-html-to-the-same-directory-as-the-one-js-just",
    "text":"I have the following tree for a webapp project. I'd like to copy the index.html file to the same ``` _build ``` directory that the built javascript gets placed in. I tried using ChatGPT a few days ago but all attempts failed, and I'm having trouble wrapping my head around dune. ``` . ├── bin │ ├── counter.ml │ ├── dune │ └── index.html ├── dune-project └── fmlib_tutorial.opam ``` This is my current bin\/dune file: ``` (executable (name counter) (modules counter) (modes js) ; Activate compilation to javascript (libraries fmlib_browser) ; Use the library ) (rule (targets counter.js) ; Generate the file 'counter.js' (deps counter.bc.js) (mode (promote (until-clean))) (action (copy counter.bc.js counter.js)) ) ```",
    "author_id":5775,
    "publication_date":1754248303000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"ixb",
    "author_reputation":72.0,
    "tags":"ocaml-dune",
    "text_length":718,
    "title_length":97,
    "num_tags":1
  },
  {
    "id":6295,
    "title":"Why doesn&#39;t the Java compiler or JIT optimize repeated pure method calls with the same input?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724203\/why-doesnt-the-java-compiler-or-jit-optimize-repeated-pure-method-calls-with-th",
    "text":"I'm exploring micro-optimizations in Java to improve performance in a real-world data processing task, where the same pure method is called repeatedly within the same scope using identical inputs. In such contexts (e.g., rendering pipelines, streaming computations), even small inefficiencies—like recomputing a deterministic function unnecessarily—can cause noticeable performance degradation at scale. My goal is to understand why the Java compiler or JIT doesn’t automatically optimize these cases. ``` public class PureCallTest { public static int compute(int x) { return (x * x + 42) \/ 7; \/\/ pure function, no side effects } public static void main(String[] args) { int a = compute(100); int b = compute(100); \/\/ Repeated identical call System.out.println(a + \", \" + b); } } ``` Expected: Second call could reuse the result of the first. Actual: Profiling shows it is recomputed unless explicitly cached: ``` int temp = compute(100); int a = temp; int b = temp; ``` What I Tried & Observed Tried running with JMH for microbenchmarking and used System.nanoTime() for rough checks. Both methods show that compute(100) is executed twice, not optimized into a single call. Even with HotSpot optimizations enabled (-XX:+PrintCompilation, -XX:+TieredCompilation), the JVM doesn’t eliminate the redundant call. I considered using caching\/memoization, but in many real-world scenarios involving dynamic or conditional computations, manual caching is: Verbose Hard to maintain Counterproductive for clean, readable code. Question Why doesn't the Java compiler (javac) or the HotSpot JIT automatically apply common subexpression elimination or reuse results of pure method calls when the same input is used multiple times within the same scope? Is it due to: Java’s strict side-effect assumptions? Lack of built-in annotations to mark pure methods? Limitations or conservative design choices in HotSpot’s optimization pipeline? Any insights into this behavior or plans for future enhancements in OpenJDK\/JVM would be appreciated. Follow-up clarification: I understand that for very cheap methods, the JIT may decide it's not worth optimizing. But I'm curious whether that decision is made dynamically (via profiling) or is avoided altogether due to Java semantics. Also, does the JVM ever apply common subexpression elimination or pure call reuse, or is that something HotSpot generally avoids unless handled manually?",
    "author_id":5774,
    "publication_date":1754248652000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"SeventhBit",
    "author_reputation":9.0,
    "tags":"java, performance, jvm, compiler-optimization",
    "text_length":2413,
    "title_length":97,
    "num_tags":4
  },
  {
    "id":6294,
    "title":"Firebase Auth currentUser null on cold start due to KeysetManager failed to initialize on Redmi Note 13 HyperOS",
    "link":"https:\/\/stackoverflow.com\/questions\/79724206\/firebase-auth-currentuser-null-on-cold-start-due-to-keysetmanager-failed-to-init",
    "text":"I'm developing an Android app using Firebase Authentication (email\/password) with Jetpack Compose and Kotlin. The app stores user data locally in DataStore and sets a ``` loggedIn ``` flag to true after successful login and email verification. On cold start (after killing the app process), ``` FirebaseAuth.getInstance().currentUser ``` returns null, even though DataStore contains valid user data. This causes the app to incorrectly redirect to the onboarding screen instead of restoring the session. Log: 2025-08-03 10:39:59.094 12114-12114 FirebearStorageCryptoHelper com.example.pawgress E Exception encountered during crypto setup: Keystore cannot load the key with ID: firebear_main_key_id_for_storage_crypto... 2025-08-03 10:39:59.094 12114-12114 FirebearStorageCryptoHelper com.example.pawgress E KeysetManager failed to initialize - unable to decrypt data 2025-08-03 10:39:59.095 12114-12114 PawgressApplication com.example.pawgress D Firebase Auth initialized, currentUser: null 2025-08-03 10:39:59.904 12114-12114 SplashScreen com.example.pawgress D Auth state changed: null Checking local session: userId=bU36t4SAe8fDqrpCxhMfNS2SlKm1, flag=true 2025-08-03 10:40:01.920 12114-12114 UserRepository com.example.pawgress D Has local user data: true This seems related to Android Keystore failing to decrypt Firebase's authentication tokens, causing the session to be lost. The issue occurs consistently on my Redmi Note 13 running HyperOS 2.0.206.0 (Android 13). What I've Tried Updated Firebase SDK: Using com.google.firebase:firebase-auth:23.2.0 with firebase-bom:33.16.0 — issue persists. ``` AuthStateListener ``` : Added listener, but it either doesn’t fire or returns null for 10–20 seconds: ``` firebaseAuth.addAuthStateListener { auth -> Log.d(\"PawgressApplication\", \"Auth state changed: ${auth.currentUser}\") } ``` Polling for currentUser: Tried polling .currentUser with delay, stays null: ``` while (firebaseUser == null && retries < maxRetries) { firebaseUser = firebaseAuth.currentUser delay(1000) retries++ } ``` Cleared app data: Reset app via settings, logged in again — issue still happens on cold start. Disabled HyperOS optimizations: Turned off system-level optimizations — no effect. Verified email: Email verification works correctly (verified flag true in logs). Checked DataStore: Local user info is accurate (userId, email, loggedIn == true). Checked Google Play Services: Installed, up-to-date, available. Searched for similar issues: Found threads mentioning Keystore issues on MIUI\/HyperOS devices, but none offered a working fix. Code Snippet (SplashScreen Logic) ``` var firebaseUser = firebaseAuth.currentUser var retries = 0 val maxRetries = 10 val retryDelay = 2000L Log.d(\"SplashScreen\", \"🔍 Initial Firebase user: ${firebaseUser?.uid}\") var authStateReceived = false val authStateListener = FirebaseAuth.AuthStateListener { auth -> firebaseUser = auth.currentUser authStateReceived = true Log.d(\"SplashScreen\", \"🔄 Auth state changed: ${firebaseUser?.uid}\") } firebaseAuth.addAuthStateListener(authStateListener) while (!authStateReceived && retries < maxRetries) { Log.d(\"SplashScreen\", \"🔄 Waiting for auth state, retry $retries\/$maxRetries\") delay(retryDelay) retries++ } firebaseAuth.removeAuthStateListener(authStateListener) val safeUser = firebaseUser if (safeUser != null) { try { safeUser.reload().await() Log.d(\"SplashScreen\", \"✅ Firebase user reloaded: ${safeUser.uid}, verified: ${safeUser.isEmailVerified}\") } catch (e: Exception) { Log.e(\"SplashScreen\", \"Error reloading Firebase user: ${e.message}\") } } val localSessionValid = userRepository.hasValidLocalSession() val localUser = userRepository.getUser().first() val isLoggedIn = safeUser != null && safeUser.isEmailVerified && localUser != null && safeUser.uid == localUser.id && localSessionValid val hasLocalUser = userRepository.hasLocalUserData() Log.d(\"SplashScreen\", \"📊 Final state - firebaseUser: ${safeUser?.uid}, isLoggedIn: $isLoggedIn, hasLocalUser: $hasLocalUser\") if (isLoggedIn && hasLocalUser) { val surveyCompleted = userRepository.isSurveyCompleted().first() val petNamed = userRepository.isPetNamedFlow().first() Log.d(\"SplashScreen\", \"📋 Survey completed: $surveyCompleted, Pet named: $petNamed\") when { !surveyCompleted -> onTimeout(\"survey\/1\") !petNamed -> onTimeout(\"pet_naming\") else -> onTimeout(\"home\") } } else { Log.w(\"SplashScreen\", \"⚠️ Incomplete login - redirecting to onboarding\") onTimeout(\"onboarding\") } } ```",
    "author_id":5773,
    "publication_date":1754248973000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Элина Дышекова",
    "author_reputation":11.0,
    "tags":"android, kotlin, firebase, firebase-authentication, android-keystore",
    "text_length":4451,
    "title_length":111,
    "num_tags":5
  },
  {
    "id":6293,
    "title":"How do I fix &quot;Uncaught TypeError: display is not a function&quot;?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724214\/how-do-i-fix-uncaught-typeerror-display-is-not-a-function",
    "text":"I am using Typescript with Webpack for a static browser application. Here are the relevant code snippets: ``` [spell.ts] export class Spell { \/\/ Various properties and functions, Nothing relevant. There is no property named 'displayDetailsDHDHDH' or anything similar. \/\/ Reads All Spells in \/data\/spells\/ directory. \/\/ This is done using an index of files automatically generated by a github workflow activated whenever a .json file is pushed. static async readAll(): Promise<Spell[]> { const dir = getRootURL() + 'data\/spells\/' let output: Spell[] = [] \/\/ Get Raw File List let index = await (await fetch(dir+'index.html')).text(); let file_list = index.split('\\n'); \/\/ Convert Raw File List into Spell Object List for(var file_raw of file_list) { let file = file_raw.replace(\/[\\r\\n]+\/gm, \"\"); \/\/Trim \\r if(file == \"index.html\" || file == \"\"){break;} \/\/Remove invalid entries from list let spell_response = await fetch(dir+file) \/\/Fetch let spell = JSON.parse(await spell_response.text()) as Spell; \/\/Process (Response -> string -> Spell) output.push(spell); } console.log(\"read spells\"); console.log(output); return output; } \/\/ Insert a spell's details into the #display div given. the div must have certain child elements for this to work. public displayDetailsDHDHDH(displayElement:HTMLDivElement) { displayElement.querySelector(\"#name\").innerHTML = this.name displayElement.querySelector(\"#castingtime\").innerHTML = this.castingtime displayElement.querySelector(\"#range\").innerHTML = this.range displayElement.querySelector(\"#duration\").innerHTML = this.duration displayElement.querySelector(\"#description\").innerHTML = this.description } } ``` ``` [spells.ts] import { Spell } from '..\/spell'; import { addHeader } from '..\/common'; async function onLoad() { const spells: Spell[] = await Spell.readAll() \/\/temp display testing before list is implemented let display: HTMLDivElement = document.getElementById(\"display\") as HTMLDivElement let spell = spells[0] as Spell spell.displayDetailsDHDHDH(display) } addHeader(document); document.addEventListener(\"DOMContentLoaded\", () => {onLoad();}) \/\/'await' is not allowed in script root, so let's wait for the HTML content to fully load. ``` When running ``` spells.ts ``` in the browser, I'm getting this error: ``` spells.js:424 Uncaught (in promise) TypeError: spell.displayDetailsDHDHDH is not a function at spells.js:424:27 at step (spells.js:405:23) at Object.next (spells.js:386:53) at fulfilled (spells.js:377:58) (anonymous) @ spells.js:424 step @ spells.js:405 (anonymous) @ spells.js:386 fulfilled @ spells.js:377 Promise.then step @ spells.js:379 (anonymous) @ spells.js:380 __awaiter @ spells.js:376 onLoad @ spells.js:413 (anonymous) @ spells.js:431 ``` I have searched the web for this problem extensively, and all sources agree that this error is caused by the following things, which I have listed below along with why I think this is not the cause: A typo in the function name: The function name is definitely correct, I have tried copying and pasting it to make double sure. Function called on the wrong object: I have made sure that the object is of type Spell before calling ``` displayDetailsDHDHDH() ``` on it. Function shares a name with a pre-existing property: Originally my function was just called ``` display() ``` , I tried different names, eventually landing on the current ``` displayDetailsDHDHDH() ``` , and nothing has changed. Using parentheses for multiplication. That simply is not what is happening. Incorrect Import usage. I do not think this is the issue as I am calling the static method ``` readAll() ``` successfully earlier in the same function. I have also run ``` npx webpack ``` after making changes and have verified that my changes have made their way into the compiled Javascript that is being loaded too. Any hints as to what I might be doing wrong appreciated.",
    "author_id":5772,
    "publication_date":1754249779000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Prof Susan Satsumas",
    "author_reputation":1.0,
    "tags":"typescript, json, function",
    "text_length":3866,
    "title_length":71,
    "num_tags":3
  },
  {
    "id":6292,
    "title":"Filamentphp adding demo mode middleware",
    "link":"https:\/\/stackoverflow.com\/questions\/79724218\/filamentphp-adding-demo-mode-middleware",
    "text":"I created an admin panel on Laravel with Filamentphp. I want to create a middleware for demo mode to block requests such as post, put, and delete. However, I couldn't do it. demomodemiddleware.php ``` <?php namespace App\\Http\\Middleware; use Closure; use Illuminate\\Http\\Request; use Filament\\Notifications\\Notification; class DemoModeMiddleware { public function handle(Request $request, Closure $next) { if ($request->method() !== 'GET') { \/\/ API ya da Filament üzerinde GET dışındaki istekleri engelle return response()->json([ 'message' => 'Demo modunda bu işlem engellenmiştir.', ], 403); } return $next($request); } } ``` I added this middleware to the filamentphp adminpanelprovider. ``` <?php namespace App\\Providers\\Filament; use Filament\\Http\\Middleware\\Authenticate; use Filament\\Http\\Middleware\\AuthenticateSession; use Filament\\Http\\Middleware\\DisableBladeIconComponents; use Filament\\Http\\Middleware\\DispatchServingFilamentEvent; use Filament\\Pages; use Filament\\Panel; use Filament\\PanelProvider; use Filament\\Support\\Colors\\Color; use Filament\\Widgets; use Illuminate\\Cookie\\Middleware\\AddQueuedCookiesToResponse; use Illuminate\\Cookie\\Middleware\\EncryptCookies; use Illuminate\\Foundation\\Http\\Middleware\\VerifyCsrfToken; use Illuminate\\Routing\\Middleware\\SubstituteBindings; use Illuminate\\Session\\Middleware\\StartSession; use Illuminate\\View\\Middleware\\ShareErrorsFromSession; use App\\Http\\Middleware\\AdminMiddleware; class AdminPanelProvider extends PanelProvider { public function panel(Panel $panel): Panel { return $panel ->default() ->id('admin') ->path('admin') ->login() ->colors([ 'primary' => Color::Amber, ]) ->discoverResources(in: app_path('Filament\/Resources'), for: 'App\\\\Filament\\\\Resources') ->discoverPages(in: app_path('Filament\/Pages'), for: 'App\\\\Filament\\\\Pages') ->pages([ Pages\\Dashboard::class, ]) ->discoverWidgets(in: app_path('Filament\/Widgets'), for: 'App\\\\Filament\\\\Widgets') ->widgets([ \\App\\Filament\\Widgets\\DashboardStats::class, \\App\\Filament\\Widgets\\LatestUsers::class, ]) ->middleware([ EncryptCookies::class, AddQueuedCookiesToResponse::class, StartSession::class, AuthenticateSession::class, ShareErrorsFromSession::class, VerifyCsrfToken::class, SubstituteBindings::class, DisableBladeIconComponents::class, DispatchServingFilamentEvent::class, AdminMiddleware::class, \\App\\Http\\Middleware\\DemoModeMiddleware::class, ], isPersistent: true) ->authMiddleware([ Authenticate::class, ]) ->authGuard('web') ->registration(false) ->passwordReset(false) ->profile(false); } } ``` If there is another way, I can try that too. How can I block only put post and delete requests?",
    "author_id":5771,
    "publication_date":1754250564000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Sercan Aslan",
    "author_reputation":295.0,
    "tags":"php, laravel, filamentphp",
    "text_length":2624,
    "title_length":39,
    "num_tags":3
  },
  {
    "id":6291,
    "title":"How to find out if a cell contains a value from (named) range?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724223\/how-to-find-out-if-a-cell-contains-a-value-from-named-range",
    "text":"in MS Excel I can use SEARCH(My_List,E16) to check if E16 contains any of the values in My_List. When I try to do the same in Google Sheets with My_List defined as a named range, the cell is only searched for the first value in My_List. Any idea what I might be doing wrong?",
    "author_id":5770,
    "publication_date":1754250853000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Breslau70",
    "author_reputation":25.0,
    "tags":"google-sheets",
    "text_length":274,
    "title_length":62,
    "num_tags":1
  },
  {
    "id":6290,
    "title":"How to set up custom admin user for openmetadata 1.8.8?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724225\/how-to-set-up-custom-admin-user-for-openmetadata-1-8-8",
    "text":"I am hosting openmetadata using docker compose and by default it creates admin user with default credentials, but I want to be able to set admin user's username and password from the starting of container and not have default credentials provided by openmetadata at all I tried setting the variable ``` OPENMETADATA_INITIAL_USER='false' ``` in my docker compose file in execute-migrate-all and openmetadata-server services which didn't work at all, I still can log in with default credentials.",
    "author_id":5769,
    "publication_date":1754251145000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Christina",
    "author_reputation":21.0,
    "tags":"docker-compose",
    "text_length":493,
    "title_length":55,
    "num_tags":1
  },
  {
    "id":6289,
    "title":"Class not found even though it is in JAR",
    "link":"https:\/\/stackoverflow.com\/questions\/79724234\/class-not-found-even-though-it-is-in-jar",
    "text":"My son is trying to create a Minecraft plugin in Java. Unfortunately, my time with Java has been a long time ago... As you can see in below screenshot org.bukkit.plugin.java is part of bukkit-1.21.8-R0.1-SNAPSHOT.jar, which has been referenced and therefore also should be in the classpath. IntelliSense even suggests JavaPlugin when entering the import statement but still VS Code says \"cannot find symbol\". Has anyone a suggestion to make my son happy? :-) UPDATE: I tried to build manually using javac and this worked - but I still would like to make it work in VS Code.",
    "author_id":5768,
    "publication_date":1754251906000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jan Rothkegel",
    "author_reputation":772.0,
    "tags":"java, visual-studio-code, minecraft, bukkit",
    "text_length":573,
    "title_length":40,
    "num_tags":4
  },
  {
    "id":6288,
    "title":"Replacement for the &quot;Android SDK Search&quot; Chrome extension",
    "link":"https:\/\/stackoverflow.com\/questions\/79724239\/replacement-for-the-android-sdk-search-chrome-extension",
    "text":"I've been using the Android SDK Search extension for years, but it hasn't been updated in quite a while. The combination of being able to search the Android SDK and link into AOSP has been invaluable. It is now being blocked from Chrome. I can kind of do the same thing with Studio by opening the class, then viewing the source\/structure, but it's clunky at best. Also, this doesn't provide the AOSP link for looking at different versions of Android etc. Anybody have viable alternative? FYI: It looks like MS Edge, which is now essentially Chrome, still supports the extension, but I suspect that's on a timer too.",
    "author_id":5767,
    "publication_date":1754252519000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dustin",
    "author_reputation":2260.0,
    "tags":"android, google-chrome-extension, android-studio",
    "text_length":615,
    "title_length":67,
    "num_tags":3
  },
  {
    "id":6287,
    "title":"How to hide the icon in a pickerInput()?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724243\/how-to-hide-the-icon-in-a-pickerinput",
    "text":"How do I remove the arrow icon to the right of the dropdown? ``` library(shiny) library(shinyWidgets) ui <- fluidPage( pickerInput( inputId = \"example\", label = \"Choose something:\", choices = c(\"A\", \"B\", \"C\") ) ) server <- function(input, output, session) {} shinyApp(ui, server) ```",
    "author_id":5766,
    "publication_date":1754252693000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"User 1234",
    "author_reputation":51.0,
    "tags":"r, shiny",
    "text_length":283,
    "title_length":40,
    "num_tags":2
  },
  {
    "id":6286,
    "title":"&#39;process&#39; is not defined",
    "link":"https:\/\/stackoverflow.com\/questions\/79724249\/process-is-not-defined",
    "text":"I have this code in a file in the pages directory for a nextJS app, I keep getting the error 'process' is not defined from eslint and the error TypeError: can't access property \"useMemo\", resolveDispatcher() is null when trying to access the page I have looked into possible causes but haven't found anything conclusive. I have an .env.local file that contains the public key with the NEXT_PUBLIC prefix in the folder with package.json. Any help with this would be appreciated ``` import React from \"react\" import CheckoutPage from '..\/components\/checkoutPage'; import convertToSubcurrency from '..\/lib\/convertToSubcurrency'; import { Elements } from '@stripe\/react-stripe-js'; import { loadStripe } from '@stripe\/stripe-js'; const stripePromise = loadStripe(process.env.NEXT_PUBLIC_STRIPE_PUBLIC_KEY) export default function PurchasePage() { const amount = 25.00 return ( <Elements stripe={stripePromise} options={{ mode: \"payment\", amount: convertToSubcurrency(amount), currency: \"gbp\" }} > <CheckoutPage amount={amount}\/> <\/Elements> ); } ``` UPDATE Contents of eslint.config.mjs ``` import js from \"@eslint\/js\"; import globals from \"globals\"; import pluginReact from \"eslint-plugin-react\"; import pluginNext from \"@next\/eslint-plugin-next\"; import { defineConfig } from \"eslint\/config\"; export default defineConfig([ { files: [\"**\/*.{js,mjs,cjs,jsx}\"], plugins: { js, 'next': pluginNext }, extends: [\"js\/recommended\"], languageOptions: { globals: globals.browser, } }, pluginReact.configs.flat.recommended, ]); ```",
    "author_id":5765,
    "publication_date":1754253452000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user19953380",
    "author_reputation":75.0,
    "tags":"next.js, node.js",
    "text_length":1518,
    "title_length":32,
    "num_tags":2
  },
  {
    "id":6285,
    "title":"Unable to get check box to show on data table in Angular",
    "link":"https:\/\/stackoverflow.com\/questions\/79724252\/unable-to-get-check-box-to-show-on-data-table-in-angular",
    "text":"I am trying to combine the 'Table With Expandable Rows' with the 'Table with Selection' because I want to use the mat-checkbox with the expandable row from the angular material site ( https:\/\/material.angular.dev\/components\/table\/examples ). I am unable to get a mat-checkbox to show on a data table. html ``` <table mat-table [dataSource]=\"dataSource\" multiTemplateDataRows class=\"mat-elevation-z8\" > <ng-container matColumnDef=\"{{column}}\" *ngFor=\"let column of columnsToDisplay\" > <ng-container *ngIf=\"column !== 'action'; else action\"> <th mat-header-cell *matHeaderCellDef>{{column}}<\/th> <td mat-cell *matCellDef=\"let element\">{{element[column]}}<\/td> <\/ng-container> <ng-template #action> <th mat-header-cell *matHeaderCellDef>Actions<\/th> <td mat-cell *matCellDef=\"let element\" class=\"checkbox-spacing\"> <mat-icon (click)=\"expandedElement = expandedElement === element ? null : element\" >{{expandedElement === element ? 'expand_less' : 'expand_more'}}<\/mat-icon > <!-- <input type=\"checkbox\" \/> --> <\/td> <!-- <mat-checkbox><\/mat-checkbox> --> <\/ng-template> <\/ng-container> <ng-container matColumnDef=\"select\"> <th mat-header-cell *matHeaderCellDef> <mat-checkbox (change)=\"$event ? toggleAllRows() : null\" [checked]=\"selection.hasValue() && isAllSelected()\" [indeterminate]=\"selection.hasValue() && !isAllSelected()\" [aria-label]=\"checkboxLabel()\" > <\/mat-checkbox> <\/th> <td mat-cell *matCellDef=\"let row\"> <mat-checkbox (click)=\"$event.stopPropagation()\" (change)=\"$event ? selection.toggle(row) : null\" [checked]=\"selection.isSelected(row)\" [aria-label]=\"checkboxLabel(row)\" > <\/mat-checkbox> <\/td> <\/ng-container> <!-- Expanded Content Column - The detail row is made up of this one column that spans across all columns --> <ng-container matColumnDef=\"expandedDetail\"> <td mat-cell *matCellDef=\"let element\" [attr.colspan]=\"columnsToDisplay.length\" > <div class=\"example-element-detail\" [@detailExpand]=\"element == expandedElement ? 'expanded' : 'collapsed'\" > <div class=\"example-element-diagram\"> <div class=\"example-element-position\">{{element.position}}<\/div> <div class=\"example-element-symbol\">{{element.symbol}}<\/div> <div class=\"example-element-name\">{{element.name}}<\/div> <div class=\"example-element-weight\">{{element.weight}}<\/div> <\/div> <div class=\"example-element-description\"> {{element.description}} <span class=\"example-element-description-attribution\"> -- Wikipedia <\/span> <\/div> <\/div> <\/td> <\/ng-container> <tr mat-header-row *matHeaderRowDef=\"columnsToDisplay\"><\/tr> <tr mat-row *matRowDef=\"let element; columns: columnsToDisplay;\" class=\"example-element-row\" [class.example-expanded-row]=\"expandedElement === element\" ><\/tr> <tr mat-row *matRowDef=\"let row; columns: ['expandedDetail']\" class=\"example-detail-row\" ><\/tr> <\/table> ``` ts ``` import { SelectionModel } from '@angular\/cdk\/collections'; import { MatTableDataSource } from '@angular\/material\/table'; import { Component } from '@angular\/core'; import { MatCheckboxModule } from '@angular\/material\/checkbox'; import { animate, state, style, transition, trigger, } from '@angular\/animations'; \/** * @title Table with expandable rows *\/ @Component({ selector: 'table-expandable-rows-example', styleUrls: ['table-expandable-rows-example.css'], templateUrl: 'table-expandable-rows-example.html', imports: [MatCheckboxModule], animations: [ trigger('detailExpand', [ state('collapsed', style({ height: '0px', minHeight: '0' })), state('expanded', style({ height: '*' })), transition( 'expanded <=> collapsed', animate('225ms cubic-bezier(0.4, 0.0, 0.2, 1)') ), ]), ], }) export class TableExpandableRowsExample { columnsToDisplay: string[] = [ 'action', 'select', 'Name', 'weight', 'symbol', 'position', ]; dataSource = new MatTableDataSource<PeriodicElement> (ELEMENT_DATA); expandedElement: PeriodicElement | null; selection = new SelectionModel<PeriodicElement>(true, []); \/** Whether the number of selected elements matches the total number of rows. *\/ isAllSelected() { const numSelected = this.selection.selected.length; const numRows = this.dataSource.data.length; return numSelected === numRows; } \/** Selects all rows if they are not all selected; otherwise clear selection. *\/ toggleAllRows() { if (this.isAllSelected()) { this.selection.clear(); return; } this.selection.select(...this.dataSource.data); } \/** The label for the checkbox on the passed row *\/ checkboxLabel(row?: PeriodicElement): string { if (!row) { return `${this.isAllSelected() ? 'deselect' : 'select'} all`; } return `${this.selection.isSelected(row) ? 'deselect' : 'select'} row ${ row.position + 1 }`; } } export interface PeriodicElement { Name: string; position: number; weight: number; symbol: string; description: string; } const ELEMENT_DATA: PeriodicElement[] = [ { position: 1, Name: 'Hydrogen', weight: 1.0079, symbol: 'H', description: `Hydrogen is a chemical element with symbol H and atomic number 1. With a standard atomic weight of 1.008, hydrogen is the lightest element on the periodic table.`, }, { position: 2, Name: 'Helium', weight: 4.0026, symbol: 'He', description: `Helium is a chemical element with symbol He and atomic number 2. It is a colorless, odorless, tasteless, non-toxic, inert, monatomic gas, the first in the noble gas group in the periodic table. Its boiling point is the lowest among all the elements.`, }, { position: 3, Name: 'Lithium', weight: 6.941, symbol: 'Li', description: `Lithium is a chemical element with symbol Li and atomic number 3. It is a soft, silvery-white alkali metal. Under standard conditions, it is the lightest metal and the lightest solid element.`, }, ]; ``` My code is on StackBlitz . When I remove or comment out the mat-checkbox container the table will show but obviously when I re-add it or uncomment it. I need help on how I can get the checkbox to display on an expandable table with rows.",
    "author_id":4450,
    "publication_date":1754253889000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"developer8492",
    "author_reputation":97.0,
    "tags":"typescript, angular",
    "text_length":5835,
    "title_length":56,
    "num_tags":2
  },
  {
    "id":6284,
    "title":"How can I keep distincts data points with similar X value?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724253\/how-can-i-keep-distincts-data-points-with-similar-x-value",
    "text":"I have a dataframe with 4 rows, two of which having the same value for the \"a\" field. I'd like to have a bar chart with 4 bars, one for each row. Trying with Altair ``` source = pd.DataFrame({ 'a': ['A', 'B', 'C', 'B'], 'b': [10, 20, 30, 15] }) alt.Chart(source).mark_bar().encode( x='a', y='b' ) ``` This will result in a 3-bar chart, the values for 'B' having been added (20+15=35) What I was expecting is a 4-bar chart, like I get with: ``` source.plot(kind=\"bar\", x=\"a\", y=\"b\") ``` If each value must be unique, I guess I could use the index of the dataframe, but I have not found any way to get the labels from the 'a' column. ``` alt.Chart(source.reset_index()).mark_bar().encode( x='index:O', y='b' ) ``` As expected, it brings the right values, but non the wanted labels: So it's possible to fake a column with unique values and trim the added part to get the proper labels back, but it seems not very pythonic to me. ``` source[\"combined\"] = source.index.astype(str) + \"_\" + source[\"a\"] alt.Chart(source).mark_bar().encode( x=alt.X( \"combined\", axis=alt.Axis(labelExpr=\"slice(datum.label,indexof(datum.label, '_')+1)\"), ), y=\"b\", ) ``` What would be the proper way to solve this?",
    "author_id":5764,
    "publication_date":1754253989000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"MiscBits",
    "author_reputation":13.0,
    "tags":"python, altair",
    "text_length":1188,
    "title_length":58,
    "num_tags":2
  },
  {
    "id":6283,
    "title":"In CSS how to left-align the marker of a list item in an &lt;ol&gt; at .375&quot;, the text at .75&quot;, and the second (wrap-around) line of the list item at 0.0&quot;?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724270\/in-css-how-to-left-align-the-marker-of-a-list-item-in-an-ol-at-375-the-text",
    "text":"How do you format an Ordered list so that each list marker (a., b.,) begins at .375in, and the list item text begins at .75 inches. the second line of the list item (wrap-around text) should begin to the left of the list item marker at 0.0\" (per the company style standard). I've included my code attempt. I've also included what the ordered list left margins are supposed to look like (notice the second line is to the left of the \"a.\" marker of the first line). ``` \/**** List Styles ****\/ ol { list-style-type: none; --prefix-size: 40px; --prefix-fixed-size: false; --prefix-gap: 22px; --list-indent: 0.75in; --vertical-spacing: 0px; counter-reset: item1; padding-left: var(--prefix-size); margin-left: calc(var(--list-indent) - var(--prefix-size)); } ul {} li { font-size: 11pt; } ol>li { --prefix-size: 40px; --prefix-fixed-size: false; --prefix-gap: 22px; --list-indent: 0.75in; --vertical-spacing: 0px; margin-top: calc(var(--vertical-spacing) \/ 2); margin-bottom: calc(var(--vertical-spacing) \/ 2); position: relative; } ol>li:before { counter-increment: item1; content: counter(item1, lower-alpha)\".\"; text-align: right; position: absolute; left: calc(-1 * var(--prefix-size) - var(--prefix-gap)); margin-right: var(--prefix-gap); width: var(--prefix-size); overflow: hidden; } ``` Any help here would be greatly appreciated.",
    "author_id":5763,
    "publication_date":1754255451000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"EricLowber",
    "author_reputation":133.0,
    "tags":"xml, css, pdf-generation, formatting, dita",
    "text_length":1334,
    "title_length":170,
    "num_tags":5
  },
  {
    "id":6282,
    "title":"Use flattened pom for filtering resources",
    "link":"https:\/\/stackoverflow.com\/questions\/79724271\/use-flattened-pom-for-filtering-resources",
    "text":"I'm using a script to replace a property's value in flattened pom, but the original pom's property is injected into filtered resources instead. Why? Is it a configuration issue, a bug or simply not possible to accomplish? Sanity check: the text file has filtering enabled flatten plugin has ``` updatePomFile ``` set to ``` true ``` I use ``` mvn clean process-resources ``` but tried ``` mvn clean package ``` and ``` mvn clean install ``` , same happens in generated ``` target\/classes\/test.txt ``` and in final jar I added a second execution of the flatten plugin in case current pom is kept in memory and didn't update after my script changed its file flattening and script are set to ``` validate ``` phase, which is 5 steps before ``` process-resources ``` , but I also tried every other phase just in case, for both flatten executions ``` pom.xml ``` : ``` <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/maven-v4_0_0.xsd\"> <modelVersion>4.0.0<\/modelVersion> <name>${projectname}<\/name> <groupId>eu.test<\/groupId> <artifactId>${projectname}<\/artifactId> <version>${revision}<\/version> <packaging>jar<\/packaging> <properties> <projectname>PomScriptTest<\/projectname> <revision>1.0<\/revision> <testProperty>#testFailure<\/testProperty> <\/properties> <build> <defaultGoal>clean install<\/defaultGoal> <resources> <resource> <directory>src\/main\/resources<\/directory> <filtering>true<\/filtering> <\/resource> <\/resources> <plugins> <plugin> <groupId>org.codehaus.mojo<\/groupId> <artifactId>flatten-maven-plugin<\/artifactId> <version>1.7.2<\/version> <executions> <execution> <id>flatten.clean<\/id> <phase>clean<\/phase> <goals> <goal>clean<\/goal> <\/goals> <\/execution> <execution> <id>flatten<\/id> <phase>validate<\/phase> <goals> <goal>flatten<\/goal> <\/goals> <configuration> <updatePomFile>true<\/updatePomFile> <flattenMode>bom<\/flattenMode> <\/configuration> <\/execution> <execution> <id>update-changed-pom<\/id> <phase>initialize<\/phase> <goals> <goal>flatten<\/goal> <\/goals> <configuration> <updatePomFile>true<\/updatePomFile> <flattenMode>bom<\/flattenMode> <flattenedPomFilename>.updated-pom.xml<\/flattenedPomFilename> <\/configuration> <\/execution> <\/executions> <\/plugin> <plugin> <artifactId>exec-maven-plugin<\/artifactId> <groupId>org.codehaus.mojo<\/groupId> <version>3.5.1<\/version> <executions> <execution> <id>exec-script<\/id> <phase>validate<\/phase> <goals> <goal>exec<\/goal> <\/goals> <configuration> <executable>scripts\/pomScript.bat<\/executable> <\/configuration> <\/execution> <\/executions> <\/plugin> <\/plugins> <\/build> <\/project> ``` ``` src\/main\/resources\/test.txt ``` : ``` name: ${project.name} version: ${project.version} testProperty: ${testProperty} ``` ``` scripts\/pomScript.bat ``` : ``` cd scripts powershell -command \"(Get-Content '..\/.flattened-pom.xml' -Raw) -replace '#testFailure', '#testSuccess' | Set-Content '..\/.flattened-pom.xml'\" ```",
    "author_id":5033,
    "publication_date":1754255646000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nathalie Kitty",
    "author_reputation":113.0,
    "tags":"java, maven, flatten-maven-plugin",
    "text_length":3037,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":6281,
    "title":"What is the robust pattern for releasing a shared ByteBuf written to multiple channels, especially with staggered writes and potential failures?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724276\/what-is-the-robust-pattern-for-releasing-a-shared-bytebuf-written-to-multiple-ch",
    "text":"I'm trying to implement a high-performance fan-out (broadcast) service in Netty where I need to write the same, immutable ByteBuf to multiple channels. To avoid memory copies, I'm using retainedDuplicate(). The challenge is handling the lifecycle of the shared buffer, especially when write operations can fail. My initial approach was to add a failure listener to each write operation: ``` for (Channel channel : allChannels) { final ByteBuf duplicate = masterBuffer.retainedDuplicate(); channel.writeAndFlush(duplicate).addListener(future -> { if (!future.isSuccess()) { \/\/ Problem: What if another handler in the pipeline already released it? if (duplicate.refCnt() > 0) { \/\/ Unsafe check duplicate.release(); } channel.close(); } }); } masterBuffer.release(); ``` I quickly discovered several critical issues with this: Shared refCnt: retainedDuplicate() creates a view over shared memory, and all duplicates share the same underlying reference count. Race Condition: A failure in one channel's pipeline (e.g., in SslHandler) can release the buffer and decrement the shared refCnt. If another channel's write fails, its listener might see a refCnt that is still > 0 and try to release it again, but a third channel might still be actively using the memory, leading to an IllegalReferenceCountException or data corruption. The simple if (refCnt > 0) check is not safe. Staggered Writes: My use case is more complex. The writes are staggered over time; I might send the buffer to 100 channels now, and then 50 more channels a few seconds later, all as part of the same logical operation. This leads to my question: What is the canonical, thread-safe pattern in Netty for managing the lifecycle of a single buffer that is shared across multiple, potentially staggered, write operations?",
    "author_id":5762,
    "publication_date":1754256311000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user3888453",
    "author_reputation":23.0,
    "tags":"java, concurrency, memory-management, netty, zero-copy",
    "text_length":1787,
    "title_length":144,
    "num_tags":5
  },
  {
    "id":6280,
    "title":"Does not work on ssr mode own server api routes in Nuxt 3",
    "link":"https:\/\/stackoverflow.com\/questions\/79724278\/does-not-work-on-ssr-mode-own-server-api-routes-in-nuxt-3",
    "text":"I have \/blogs, \/blogs\/:slug pages and \/api\/blogs\/, \/api\/blogs\/:slug server api endpoints. Whenever I send a request to the server api endpoint in my ui pages it is returning an error. The error is as follows: Error: [GET] \"https:\/\/test.com\/api\/blogs\/slug\": fetch failed .This error is only happening in production, but it works as expected on localhost. Here is \/pages\/blog\/[slug].vue component : ``` const route = useRoute(); const slug = route.params.slug; const config = useRuntimeConfig(); const baseUrl = config.public.baseUrl || process.env.NUXT_PUBLIC_BASE_URL || \"\"; console.log(\"baseUrl\", baseUrl); console.log(\"route.params.slug\", slug); \/\/ Composables for data fetching const { data: currentArticle, error, status, } = await useAsyncData(\"blog\", () => $fetch(`${baseUrl}\/api\/blogs\/${slug}`) ); <\/script> <template> <div style=\"margin-top: 120px\"> <div> article : <pre> {{ currentArticle }} <\/pre > <\/div> <div> status : <pre> {{ status }} <\/pre > <\/div> <div> error : <pre> {{ error }} <\/pre > <\/div> <\/div> <\/template> <style scoped> :global(.main-content img) { width: 100%; height: 500px; @media (max-width: 992px) { height: 350px; } @media (max-width: 768px) { height: 300px; } } <\/style> ``` \/server\/api\/blogs\/[slug].get.ts : ``` import connectDB from \"~\/utils\/db\"; export default defineEventHandler(async (event) => { setHeader(event, \"Access-Control-Allow-Origin\", \"*\"); setHeader(event, \"Access-Control-Allow-Methods\", \"GET\"); try { await connectDB(); \/\/ Await the connection const metaSlug = event.context.params?.id || null; if (!metaSlug) { throw createError({ statusCode: 400, message: \"Blog slug is required\", }); } const blog = await Blog.findOne({ slug: metaSlug }).populate(\"category_id\").lean(); await Blog.findByIdAndUpdate(blog?._id, { $inc: { viewsCount: 1, }, }); if (!blog) { throw createError({ statusCode: 404, message: \"Blog not found\", }); } return blog; } catch (error: any) { console.error(\"Blog fetch error:\", error); \/\/ Log the actual error \/\/ If it's a MongoDB CastError (invalid ID) if (error.name === \"CastError\") { throw createError({ statusCode: 400, message: \"Invalid blog ID format\", }); } throw createError({ statusCode: 500, message: JSON.stringify(error || \"Internal Server Error\"), statusMessage: event.context.params || error, }); } }); ``` What is causing this error, is it possible to do fullstack Nuxt projects? How to fix this error? Or other solutions?",
    "author_id":5761,
    "publication_date":1754256726000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Fozilbek Raimberdiyev",
    "author_reputation":1.0,
    "tags":"nuxt.js, server",
    "text_length":2410,
    "title_length":57,
    "num_tags":2
  },
  {
    "id":6279,
    "title":"PdfiumViewer in c# WPF application",
    "link":"https:\/\/stackoverflow.com\/questions\/79724279\/pdfiumviewer-in-c-wpf-application",
    "text":"I'm trying to use PDFium.Updated in my c# wpf application programmed in VS2022. I just want to display the pdf file in a seperate window with a usercontrol. Nothing more. I Installed the Nuget package of pdfium.updated (v2.14.5) in my project I made a usercontrol for showing the pdf files. ``` <UserControl x:Class=\"DocKeeper.UC_PdfViewer\" xmlns=\"http:\/\/schemas.microsoft.com\/winfx\/2006\/xaml\/presentation\" xmlns:x=\"http:\/\/schemas.microsoft.com\/winfx\/2006\/xaml\" xmlns:mc=\"http:\/\/schemas.openxmlformats.org\/markup-compatibility\/2006\" xmlns:d=\"http:\/\/schemas.microsoft.com\/expression\/blend\/2008\" xmlns:pdfViewer=\"clr-namespace:PdfiumViewer;assembly=PdfiumViewer\" mc:Ignorable=\"d\" d:DesignHeight=\"450\" d:DesignWidth=\"800\"> <Grid> <CheckBox Content=\"CheckBox\" HorizontalAlignment=\"Left\" Margin=\"10,100,0,0\" VerticalAlignment=\"Top\"\/> <pdfViewer:PdfViewer x:Name=\"pdfViewer\"\/> <\/Grid> <\/UserControl> ``` In my VS2022 the last line is blue underline and show the errors: The specified value cannot be assigned to the collection. The following type was expected: \"UIElement\". A value of type 'PdfViewer' cannot be added to a collection or dictionary of type 'UIElementCollection'. Maybe Pdfium works not good with the designer. So I removed the line within the grid and try to do it programatically The usercontrol has the following code: ``` using PdfiumViewer; using System.IO; using System.Windows; using System.Windows.Controls; namespace DocKeeper { public partial class UC_PdfViewer : UserControl { public UC_PdfViewer() { InitializeComponent(); } public void LoadPdf(string filePath) { if (File.Exists(filePath)) { PdfViewer viewer = new(); viewer.Height = 600; viewer.Width = 600; viewer.BackColor = System.Drawing.Color.Yellow; viewer.Document = PdfDocument.Load(filePath); viewer.Show(); } } } } ``` When the main progam opens a new window with the usercontrol inside, it shows me only the checkbox. But no pdfviewer, even not a yellow retangular. What I'm doing wrong?",
    "author_id":5760,
    "publication_date":1754256729000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"user3149514",
    "author_reputation":17.0,
    "tags":"c#, wpf, pdf-viewer",
    "text_length":1971,
    "title_length":34,
    "num_tags":3
  },
  {
    "id":6278,
    "title":"CloudFlare TCP access connection to tunnel CLI not working",
    "link":"https:\/\/stackoverflow.com\/questions\/79724280\/cloudflare-tcp-access-connection-to-tunnel-cli-not-working",
    "text":"I am connecting to a tcp tunnel to access a selfhosted database with postgres on a raspberry pi. I made the tunnel from the CloudFlare website so, to my understanding, all of my settings can just go in there. But as the pi does not have a monitor and will not have one, I need to be able to connect to it without using a browser of sorts. I have tried using service tokens but they don't seem to work fully as intended. I have the application linked to my service token policy, which is set to bypass, and another access method via email, if I ever want to access from my computer. It does successfully connect to the tunnel but it always wants me to open a browser to a link when I try to access the database where I get the following prompt: Access requested you have initiated a request for cloudflare access And I just press a button to confirm connection, BUT I dont have a browser in this case. Is there any way to skip the step of having to confirm the cloudflare access? Btw, the commands I use to connect are as follows: For the tunnel, in my .zshrc: cloudflared access tcp --hostname someDomain --url localhost:5433 --id secret_id --secret secret &>\/dev\/null & Then for the postgres connection: psql -h localhost -p 5433 -U username -d database",
    "author_id":5493,
    "publication_date":1754256876000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Alvin Lee",
    "author_reputation":23.0,
    "tags":"postgresql, cloudflare, raspberry-pi, tunnel",
    "text_length":1254,
    "title_length":58,
    "num_tags":4
  },
  {
    "id":6277,
    "title":"Even if it works as HQL, NHibernate linq joining a subquery with grouping and using grouping subquery projections in join key and result causes errors",
    "link":"https:\/\/stackoverflow.com\/questions\/79724283\/even-if-it-works-as-hql-nhibernate-linq-joining-a-subquery-with-grouping-and-us",
    "text":"I need to write an nhibernate query as linq syntax to produce a IQueryable but I have some issues even if it works as HQL query. I need to use IQuerayable for another purpose since hql doesn't result as IQueryable. Also I need it as performant by using joins rather than subqueries in where or select clauses. Let me share a playground first. Mappings: ``` public class Main { public virtual int Id { get; } public virtual int? ColInt { get; set; } public virtual string? ColString { get; set; } } public class ManyMain { public virtual int Id { get; } public virtual int MainId { get; set; } public virtual int? ColInt { get; set; } public virtual string? ColString { get; set; } public virtual DateTime? ColDateTime { get; set; } } public class MainMapping : ClassMapping<Main> { public MainMapping() { Table(\"main\"); Id(x => x.Id, m => { m.Column(cm => { cm.Name(\"id\"); cm.NotNullable(true); }); m.Generator(Generators.Identity); m.Access(Accessor.Backfield); }); Property(x => x.ColInt, m => { m.Column(\"col_int\"); }); Property(x => x.ColString, m => { m.Column(\"col_string\"); }); } } public class ManyMainMapping : ClassMapping<ManyMain> { public ManyMainMapping() { Table(\"many_main\"); Id(x => x.Id, m => { m.Column(cm => { cm.NotNullable(true); }); m.Generator(Generators.Identity); m.Access(Accessor.Backfield); }); Property(x => x.MainId, m => { m.Column(\"main_id\"); m.NotNullable(true); }); Property(x => x.ColInt, m => { m.Column(\"col_int\"); }); Property(x => x.ColString, m => { m.Column(\"col_string\"); }); Property(x => x.ColDateTime, m => { m.Column(\"col_datetime\"); }); } } ``` My Configuration: Nhibernate v5.5.2 Npgsql v9.0.3 ``` var cfg = new Configuration(); var mapper = new ModelMapper(); var mappings = Assembly.GetExecutingAssembly() .GetTypes() .Where(t => t.GetBaseTypes().Any(bt => bt.IsGenericType && bt.GetGenericTypeDefinition().IsAssignableTo(typeof(PropertyContainerCustomizer<>)))) .ToArray(); mapper.AddMappings(mappings); var mapCfg = mapper.CompileMappingForAllExplicitlyAddedEntities(); cfg.AddMapping(mapCfg); cfg.DataBaseIntegration(db => { db.ConnectionString = connstr; db.Dialect<PostgreSQL83Dialect>(); db.Driver<NpgsqlDriver>(); db.LogSqlInConsole = true; db.LogFormattedSql = false; }); ``` In this situation, I have written an hql query and seems working as expected for me. HQL ``` var q2 = session.CreateQuery(\"\"\" select m, mm3 from Main m left join ( select mm, mm2.mindt from ManyMain mm inner join ( SELECT MainId, min(ColDateTime) as mindt, max(ColDateTime) as maxdt FROM ManyMain GROUP BY MainId ) as mm2 on mm2.MainId = mm.MainId and mm2.maxdt = mm.ColDateTime and mm.Id = ( select MIN(Id) from ManyMain where MainId = mm.MainId and ColDateTime = mm2.maxdt ) ) mm3 on m.id = mm3.MainId; \"\"\"); var r = q2.List<object[]>(); ``` SQL ``` select main0_.id as col_4_0_, subquery5_.id1_1_, main0_.id as id1_0_1_, main0_.col_int as col2_0_1_, main0_.col_string as col3_0_1_, subquery5_.main2_1_, subquery5_.col3_1_, subquery5_.col4_1_, subquery5_.col5_1_, subquery5_.col_3_0_, subquery5_.col_1_0_ from main main0_ left outer join ( select manymain1_.Id as col_3_0_, subquery3_.col_1_0_, manymain1_.Id as id1_1_, manymain1_.main_id as main2_1_, manymain1_.col_int as col3_1_, manymain1_.col_string as col4_1_, manymain1_.col_datetime as col5_1_ from many_main manymain1_ inner join ( select manymain2_.main_id as col_0_0_, min(manymain2_.col_datetime) as col_1_0_, max(manymain2_.col_datetime) as col_2_0_ from many_main manymain2_ group by manymain2_.main_id ) subquery3_ on ( subquery3_.col_0_0_ = manymain1_.main_id and subquery3_.col_2_0_ = manymain1_.col_datetime and manymain1_.Id =( select max(manymain4_.Id) from many_main manymain4_ where manymain4_.main_id = manymain1_.main_id and manymain4_.col_datetime = subquery3_.col_2_0_ ) ) ) subquery5_ on (main0_.id = subquery5_.main2_1_) ``` But when I try the same using linq api to result same sql, I get some errors in this situation. It seems using grouping projection inside join key causes nhibernate can't resolve them. When I use grouping projection only in join result projection, it tries to cast cast some int32 to nullable datetime while getting results into memory and seems buggy. Linq ``` var q0 = session.Query<ManyMain>() .GroupBy(x => x.MainId) .Select(g => new { MainId = g.Key, MinRun = g.Min(x => x.ColDateTime), MaxRun = g.Max(x => x.ColDateTime), }); var q1 = session.Query<ManyMain>() .Join(q0, o => new { o.MainId, MaxRun = o.ColDateTime, }, i => new { i.MainId, i.MaxRun }, (o, i) => new { ManyMain = o, i.MinRun, i.MaxRun }) .Where(x => x.ManyMain.Id == session.Query<ManyMain>() .Where(y => y.MainId == x.ManyMain.MainId && y.ColDateTime == x.MaxRun) .Min(y => y.Id)); var FirstJoinResult = q1.ToArray(); \/\/ var res = session.Query<Main>() \/\/ .LeftJoin(q1, \/\/ o => o.Id, \/\/ i => i.ManyMain.MainId, \/\/ (o, i) => new \/\/ { \/\/ Main = o, \/\/ JoinResult = i \/\/ }); ``` Error 1 when used MaxRun in join key NHibernate.QueryException: could not resolve property: MaxRun of: SubQuery at NHibernate.Persister.Entity.SubqueryPropertyMapping.ToType(String propertyName) at NHibernate.Hql.Ast.ANTLR.Tree.FromElementType.GetPropertyType(String propertyName, String propertyPath) at NHibernate.Hql.Ast.ANTLR.Tree.FromElement.GetPropertyType(String propertyName, String propertyPath) at NHibernate.Hql.Ast.ANTLR.Tree.DotNode.GetDataType() at NHibernate.Hql.Ast.ANTLR.Tree.DotNode.PrepareLhs() at NHibernate.Hql.Ast.ANTLR.Tree.DotNode.Resolve(Boolean generateJoin, Boolean implicitJoin, String classAlias, IASTNode parent) at NHibernate.Hql.Ast.ANTLR.Tree.FromReferenceNode.Resolve(Boolean generateJoin, Boolean implicitJoin, String classAlias) at NHibernate.Hql.Ast.ANTLR.Tree.FromReferenceNode.Resolve(Boolean generateJoin, Boolean implicitJoin) at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.Resolve(IASTNode node) at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.expr() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.exprOrSubquery() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.comparisonExpr() 2 at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.logicalExpr() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.withClause() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.HandleWithFragment(FromElement fromElement, IASTNode hqlWithNode) --- End of inner exception stack trace --- at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.HandleWithFragment(FromElement fromElement, IASTNode hqlWithNode) at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.CreateJoinSubquery(IASTNode query, IASTNode alias, Int32 joinType, IASTNode with) at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.joinElement() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.fromElement() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.fromElementList() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.fromClause() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.unionedQuery() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.query() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.selectStatement() at NHibernate.Hql.Ast.ANTLR.HqlSqlWalker.statement() at NHibernate.Hql.Ast.ANTLR.HqlSqlTranslator.Translate() at NHibernate.Hql.Ast.ANTLR.QueryTranslatorImpl.Analyze(String collectionRole) at NHibernate.Hql.Ast.ANTLR.QueryTranslatorImpl.DoCompile(IDictionary ``` 2 replacements, Boolean shallow, String collectionRole) at NHibernate.Hql.Ast.ANTLR.QueryTranslatorImpl.Compile(IDictionary ``` 2 replacements, Boolean shallow) at NHibernate.Hql.Ast.ANTLR.ASTQueryTranslatorFactory.CreateQueryTranslators(IQueryExpression queryExpression, IASTNode ast, String queryIdentifier, String collectionRole, Boolean shallow, IDictionary ``` 2 filters, ISessionFactoryImplementor factory) at NHibernate.Hql.Ast.ANTLR.ASTQueryTranslatorFactory.CreateQueryTranslators(IQueryExpression queryExpression, String collectionRole, Boolean shallow, IDictionary ``` 2 filters, ISessionFactoryImplementor factory) at NHibernate.Engine.Query.QueryExpressionPlan.CreateTranslators(IQueryExpression queryExpression, String collectionRole, Boolean shallow, IDictionary ``` 2 enabledFilters, ISessionFactoryImplementor factory) at NHibernate.Engine.Query.QueryExpressionPlan..ctor(IQueryExpression queryExpression, Boolean shallow, IDictionary ``` 2 enabledFilters, ISessionFactoryImplementor factory) at NHibernate.Engine.Query.QueryPlanCache.GetHQLQueryPlan(IQueryExpression queryExpression, Boolean shallow, IDictionary ``` 2 enabledFilters) at NHibernate.Impl.AbstractSessionImpl.GetHQLQueryPlan(IQueryExpression queryExpression, Boolean shallow) at NHibernate.Impl.AbstractSessionImpl.CreateQuery(IQueryExpression queryExpression) at NHibernate.Linq.DefaultQueryProvider.PrepareQuery(Expression expression, IQuery& query) at NHibernate.Linq.DefaultQueryProvider.ExecuteList[TResult](Expression expression) at NHibernate.Linq.NhQueryable ``` 1.System.Collections.Generic.IEnumerable.GetEnumerator() Error 2 when MaxRun commented out from join key and in where subquery after join. Sql generated but results can't be read from db NHibernate: select manymain0_.Id as col_3_0_, manymain0_.Id as id1_1_, manymain0_.main_id as main2_1_, manymain0_.col_int as col3_1_, manymain0_.col_string as col4_1_, manymain0_.col_datetime as col5_1_, subquery2_.col_0_0_, subquery2_.col_1_0_, subquery2_.col_2_0_ from many_main manymain0_ inner join (select manymain1_.main_id as col_0_0_, min(manymain1_.col_datetime) as col_1_0_, max(manymain1_.col_datetime) as col_2_0_ from many_main manymain1_ group by manymain1_.main_id) subquery2_ on (subquery2_.col_0_0_=manymain0_.main_id) where manymain0_.Id=(select min(manymain3_.Id) from many_main manymain3_) Exception thrown: 'NHibernate.Exceptions.GenericADOException' in NHibernate.dll Unhandled exception. NHibernate.Exceptions.GenericADOException: Could not execute query[SQL: SQL not available] ---> **System.InvalidCastException: Unable to cast object of type 'System.Int32' to type '<>f__AnonymousType0 ``` 3[System.Int32,System.Nullable ``` 1[System.DateTime],System.Nullable ``` 1[System.DateTime]]'.** at lambda_method2(Closure, Object[]) at NHibernate.Linq.ResultTransformer.TransformTuple(Object[] tuple, String[] aliases) at NHibernate.Loader.Hql.QueryLoader.GetResultList(IList results, IResultTransformer resultTransformer) at NHibernate.Loader.Loader.ListIgnoreQueryCache(ISessionImplementor session, QueryParameters queryParameters) at NHibernate.Loader.Loader.List(ISessionImplementor session, QueryParameters queryParameters, ISet ``` 1 querySpaces) at NHibernate.Loader.Hql.QueryLoader.List(ISessionImplementor session, QueryParameters queryParameters) at NHibernate.Hql.Ast.ANTLR.QueryTranslatorImpl.List(ISessionImplementor session, QueryParameters queryParameters) at NHibernate.Engine.Query.HQLQueryPlan.PerformList(QueryParameters queryParameters, ISessionImplementor session, IList results) at NHibernate.Impl.SessionImpl.List(IQueryExpression queryExpression, QueryParameters queryParameters, IList results, Object filterConnection) --- End of inner exception stack trace --- at NHibernate.Impl.SessionImpl.List(IQueryExpression queryExpression, QueryParameters queryParameters, IList results, Object filterConnection) at NHibernate.Impl.SessionImpl.List(IQueryExpression queryExpression, QueryParameters queryParameters, IList results) at NHibernate.Impl.AbstractSessionImpl.List[T](IQueryExpression query, QueryParameters parameters) at NHibernate.Impl.AbstractQueryImpl2.ListT at NHibernate.Linq.DefaultQueryProvider.ExecuteList[TResult](Expression expression) at NHibernate.Linq.NhQueryable ``` 1.System.Collections.Generic.IEnumerable<T>.GetEnumerator() at System.Linq.Enumerable.<ToArray>g__EnumerableToArray|314_0[TSource](IEnumerable ``` 1 source) Is there any way I can write linq equivalent of the hql query as working?",
    "author_id":5759,
    "publication_date":1754257775000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"NoStuff",
    "author_reputation":63.0,
    "tags":"c#, nhibernate, linq-to-nhibernate",
    "text_length":11624,
    "title_length":150,
    "num_tags":3
  },
  {
    "id":6276,
    "title":"Return types can be cv-qualified only if it is a class type?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724284\/return-types-can-be-cv-qualified-only-if-it-is-a-class-type",
    "text":"From the below program I learned, that if I declare a function with a cv-qualified return type, the return type is precisely reflected in the function's type (see the 1 st group of ``` static_assert ``` 's), but if the function is called, the resulting expression loses the cv-qualifier for built-in types, and keeps it only for class types (see the 2 nd group of ``` static_assert ``` 's). Why is the language inconsistent in this regard? ``` #include <type_traits> template <auto> struct ReturnTypeT; template <class Ret, class... Pars, Ret (*FPtr)(Pars...)> struct ReturnTypeT<FPtr> : std::type_identity<Ret> {}; template <auto FPtr> using ReturnType = ReturnTypeT<FPtr>::type; enum MyEnum {}; class MyClass {}; template <class T> T returnT(); #define ASSERT_SAME(T1, T2) static_assert(std::is_same_v<T1, T2>) ASSERT_SAME(ReturnType< returnT<const int> >, const int); ASSERT_SAME(ReturnType< returnT<const MyEnum> >, const MyEnum); ASSERT_SAME(ReturnType< returnT<const MyClass> >, const MyClass); ASSERT_SAME(ReturnType< returnT<volatile int> >, volatile int); ASSERT_SAME(ReturnType< returnT<volatile MyEnum> >, volatile MyEnum); ASSERT_SAME(ReturnType< returnT<volatile MyClass> >, volatile MyClass); ASSERT_SAME(decltype( returnT<const int>() ), \/*?*\/ int); ASSERT_SAME(decltype( returnT<const MyEnum>() ), \/*?*\/ MyEnum); ASSERT_SAME(decltype( returnT<const MyClass>() ), const MyClass); ASSERT_SAME(decltype( returnT<volatile int>() ), \/*????*\/ int); ASSERT_SAME(decltype( returnT<volatile MyEnum>() ), \/*????*\/ MyEnum); ASSERT_SAME(decltype( returnT<volatile MyClass>() ), volatile MyClass); ``` Online demo . Note, that returning with ``` volatile ``` types is deprecated since C++20, see here .",
    "author_id":5758,
    "publication_date":1754257783000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Dr. Gut",
    "author_reputation":3048.0,
    "tags":"c++, type-conversion",
    "text_length":1705,
    "title_length":60,
    "num_tags":2
  },
  {
    "id":6275,
    "title":"Weirdness with foreach() iteration of array data",
    "link":"https:\/\/stackoverflow.com\/questions\/79724286\/weirdness-with-foreach-iteration-of-array-data",
    "text":"I have a csv file on the webserver (eg. 5GDPR6LR-1.csv) The csv file consists of data from an associative array written to file using fputcsv() within a foreach() loop. The problem occurs when I try to write a third row to the csv file. Somehow, the second time the foreach() iterates the array it just uses the data from the first array row instead of the second row. Then it happily continues at the third row again. Here's the code doing the iteration and writing to file (note: I added the $row !== $lastrow check to prevent the duplicate rows that this problem was creating, but this check shouldn't be necessary): ``` <?php $userID = $_POST['UserID']; $batchID = $_POST['BatchID']; $batchItemID = $_POST['BatchItemID']; $comment = $_POST['UserComment']; $categories = isset($_POST['Categories']) ? $_POST['Categories'] : []; echo '<pre>POST:<br>'; print_r($_POST); echo '<\/pre><hr>'; $responseFile = \"data\/responses\/{$userID}-{$batchID}.csv\"; $exists = file_exists($responseFile); $data = []; if ($exists) { $rows = array_map('str_getcsv', file($responseFile)); $headers = array_shift($rows); $data = array_map(fn($r) => array_combine($headers, $r), $rows); echo '<pre>EXISTING FILE DATA:<br>'; print_r($data); echo '<\/pre><hr>'; } else { $headers = ['UserID', 'BatchID', 'BatchItemID', 'Categories', 'UserComment']; } $found = false; foreach ($data as &$row) { if ($row['UserID'] === $userID && $row['BatchID'] === $batchID && $row['BatchItemID'] === $batchItemID) { $row['Categories'] = json_encode($categories); $row['UserComment'] = htmlspecialchars($comment, ENT_QUOTES, 'UTF-8'); $found = true; break; } } \/\/ echo 'Found:' . ($found ? 'Y' : 'N') . '<br\/>'; if (!$found) { $data[] = [ 'UserID' => $userID, 'BatchID' => $batchID, 'BatchItemID' => $batchItemID, 'Categories' => json_encode($categories), 'UserComment' => htmlspecialchars($comment, ENT_QUOTES, 'UTF-8') ]; } echo '<pre>ARRAY TO WRITE TO FILE:<br>'; print_r($data); echo '<\/pre><hr>'; $fp = fopen($responseFile, 'w'); fputcsv($fp, $headers, \",\", \"\\\"\", \"\\\\\", \"\\n\"); $lastrow = array(); foreach ($data as $row) { echo '<pre>LASTROW:'; print_r($lastrow); echo '<\/pre>'; echo '<pre>THISROW:'; print_r($row); echo '<\/pre>'; if ($row !== $lastrow) { fputcsv($fp, $row, \",\", \"\\\"\", \"\\\\\", \"\\n\"); echo '<pre>ROW WRITTEN TO CSV:'; print_r($row); echo '<\/pre>'; } $lastrow = $row; } fclose($fp); exit(); ``` This is the data log. Form Data is received via POST array. This data should either add a new row to the csv file or update an existing row if it already exists. In this run I'm trying to add the third entry to the csv file that already contains the first two. The result of this run is that the first and third row are stored in the csv file and the second row is skipped... ``` POST: Array ( [Categories] => Array ( [0] => 5 [1] => 6 ) [BatchID] => 1 [UserID] => 5GDPR6LR [BatchItemID] => 3 [UserComment] => third comment ) ------------ EXISTING FILE DATA: Array ( [0] => Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 1 [Categories] => [\"1\",\"2\"] [UserComment] => first comment ) [1] => Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 2 [Categories] => [\"3\",\"4\"] [UserComment] => second comment ) ) ------------ ARRAY TO WRITE TO FILE: Array ( [0] => Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 1 [Categories] => [\"1\",\"2\"] [UserComment] => first comment ) [1] => Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 2 [Categories] => [\"3\",\"4\"] [UserComment] => second comment ) [2] => Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 3 [Categories] => [\"5\",\"6\"] [UserComment] => third comment ) ) ------------ HERE IS THE ITERATION OF THE ABOVE ARRAY DATA... ------------ LASTROW:Array ( ) THISROW:Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 1 [Categories] => [\"1\",\"2\"] [UserComment] => first comment ) ROW WRITTEN TO CSV:Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 1 [Categories] => [\"1\",\"2\"] [UserComment] => first comment ) LASTROW:Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 1 [Categories] => [\"1\",\"2\"] [UserComment] => first comment ) THISROW:Array '<<<<----- HERE IS THE PROBLEM (IN THIS EXAMPLE) IT HASNT SKIPPED TO ARRAY ROW 2!!!' ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 1 [Categories] => [\"1\",\"2\"] [UserComment] => first comment ) LASTROW:Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 1 [Categories] => [\"1\",\"2\"] [UserComment] => first comment ) THISROW:Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 3 [Categories] => [\"5\",\"6\"] [UserComment] => third comment ) ROW WRITTEN TO CSV:Array ( [UserID] => 5GDPR6LR [BatchID] => 1 [BatchItemID] => 3 [Categories] => [\"5\",\"6\"] [UserComment] => third comment ) ``` Any ideas what could be causing this? I was experiencing this issue on my dev server so moved it to a live server but the same issue occurs. It's a basic script. I'm lost as to what to check.",
    "author_id":5757,
    "publication_date":1754257902000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Stackman",
    "author_reputation":491.0,
    "tags":"php, iteration, foreach, fputcsv",
    "text_length":4938,
    "title_length":48,
    "num_tags":4
  },
  {
    "id":6274,
    "title":"Is there a way to refactor a Maven submodule&#39;s pom.xml to be completely &quot;standalone&quot; from its parent?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724288\/is-there-a-way-to-refactor-a-maven-submodules-pom-xml-to-be-completely-standal",
    "text":"I'm conducting a study in which I'm examining how effective LLMs are at translating code between frameworks. Here is one of the datasets I'm using to test this: https:\/\/github.com\/eclipse-ee4j\/jakartaee-examples\/tree\/main\/focused ``` focused ``` has many different \"examples\" inside of its repository (for instance, ``` faces\/actionListener ``` ). However, the pom.xml files in these examples all have references to their parent pom.xml files. For example, ``` actionListener ``` has this in its pom.xml: ``` <parent> <groupId>jakarta.examples.focused.faces<\/groupId> <artifactId>project<\/artifactId> <version>10-SNAPSHOT<\/version> <\/parent> ``` The authors of this dataset did this to take advantage of Maven inheritance, which makes sense. This isn't good, though, because I want every example to be able to run completely standalone. This is because I want to make a Docker container for each that anyone can run on their own machine. I'm doing this because I don't want to have access to the existing ``` .m2 ``` cache on my local machine (e.g. for repository conflicts; also, just because I want anyone to be able to download and run these experiments). I have tried a few approaches, but none have truly worked; mvn help:effective-pom This still leaves references to absolute paths on my computer (e.g. \/Users\/advait\/Desktop) in certain parts of the effective pom, which obviously isn't suitable for an example that needs to run completely standalone in a Docker container. It also still leaves a reference to its parent module. Post-processing is required, and I don't know exactly what I need to keep\/remove, which isn't ideal. Flatten pom plugin This just simply is not suitable for my needs, as it doesn't leave references to any of the Maven repositories required to build the project. A similar question has been asked here: There is a way to execute just a Maven subproject? . However, this is a bad example repository, as all the examples in this repository are truly standalone. But, the question still stands, and I haven't found an answer. If I'm misunderstanding, or if there is another solution to my problem, please let me know!",
    "author_id":5756,
    "publication_date":1754257964000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Advait",
    "author_reputation":46.0,
    "tags":"java, docker, maven, large-language-model, migration",
    "text_length":2148,
    "title_length":115,
    "num_tags":5
  },
  {
    "id":6273,
    "title":"Vulnerability in file processing",
    "link":"https:\/\/stackoverflow.com\/questions\/79724290\/vulnerability-in-file-processing",
    "text":"I have discovered a vulnerability in the file processing service. If I create an MPD file with a link inside it, then when processing with ffmpeg, an attacker can receive a request to their server, which can lead to serious security issues. Once they know the IP address, they can launch an attack. I'm not quite sure how to deal with this. Are there any libraries for analysing files for malicious content? Yes, I thought about restricting the container's network interface or running file processing services in a sandbox, but I still know very little about combating malicious code in files, how to find it, how to secure my application? Because I am sure that you can add a link to any file, and do something even worse. But I don't know how to deal with it.",
    "author_id":5307,
    "publication_date":1754258104000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Alex",
    "author_reputation":1.0,
    "tags":"security, ffmpeg, file-processing",
    "text_length":762,
    "title_length":32,
    "num_tags":3
  },
  {
    "id":6272,
    "title":"How can I resolve vba Error 52: Bad file name or number?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724300\/how-can-i-resolve-vba-error-52-bad-file-name-or-number",
    "text":"Intended functionality\/Overview : A tab in an Excel spreadsheet features a list of checkboxes. When these checkboxes are selected, a function is triggered via a button, which exports a master MS Word document based on the selected checkbox(es). Each checkbox corresponds to a specific MS Word template file, which is stored in One Drive folder \\Safety Docs\\Training (\" & Step & \").docx . When selected, the template's content is added to the master MS Word and should be available as an exported Master Word document. Example file path - Training (1): Copy Path - C:\\Users\\jdavi\\OneDrive\\Documents\\Safety Docs\\Training (1).docx Copy Local Path - https:\/\/d.docs.live.net\/f8667d13e9ea8c38\/Documents\/Safety%20Docs\/Training%20(1).docx Issue : When executing the function, an error message appears: Error 52 Bad file name or number . I am uncertain about the cause of this error message. I have tried to check for any whitespaces in the file names. It seems that the file path is inaccurate. Not sure if this is related to One Drive. Any further guidance is appreciated. Please feel free to let me know if additional information is needed. Thanks ``` Sub Button166_Click() Dim wordapp As Object Dim worddoc As Object Dim traindoc As Object Dim TrainName As String Dim BasePath As String Dim FilePath As String Dim Step As Integer Dim MaxStep As Long On Error GoTo ErrorHandler Set wordapp = CreateObject(\"Word.Application\") wordapp.Visible = True Set worddoc = wordapp.Documents.Add TrainName = ActiveWorkbook.Name BasePath = ThisWorkbook.Path ' Normalize BasePath (no trailing slash) If Right(BasePath, 1) = \"\\\" Then BasePath = Left(BasePath, Len(BasePath) - 1) MaxStep = Sheets(\"Training\").Cells(1, 9).Value If MaxStep <= 0 Then MsgBox \"Invalid number of steps in Training sheet (cell I1).\", vbCritical Exit Sub End If For Step = 1 To MaxStep If Sheets(\"Training\").Cells(Step, 3).Value = True Then FilePath = BasePath & \"\\Safety Docs\\Training (\" & Step & \").docx\" ' Debug output Debug.Print \"Trying to open: \" & FilePath ' Validate file exists If Dir(FilePath) <> \"\" Then On Error Resume Next Set traindoc = wordapp.Documents.Open(FilePath, ReadOnly:=True) If Err.Number <> 0 Then MsgBox \"Error opening file: \" & FilePath & vbCrLf & _ \"Error \" & Err.Number & \": \" & Err.Description, vbCritical Err.Clear On Error GoTo ErrorHandler GoTo ContinueLoop End If On Error GoTo ErrorHandler traindoc.Activate wordapp.Selection.WholeStory wordapp.Selection.Copy worddoc.Activate wordapp.Selection.PasteAndFormat (1) wordapp.Selection.TypeParagraph wordapp.Selection.InsertBreak Type:=7 traindoc.Close False Set traindoc = Nothing Else MsgBox \"File not found: \" & FilePath, vbExclamation, \"Missing Document\" End If End If ContinueLoop: Next Step wordapp.ScreenUpdating = True wordapp.Activate Exit Sub ErrorHandler: MsgBox \"Error \" & Err.Number & \": \" & Err.Description, vbCritical, \"Error during execution\" If Not traindoc Is Nothing Then traindoc.Close False If Not worddoc Is Nothing Then worddoc.Close False wordapp.Quit Set wordapp = Nothing Set worddoc = Nothing Set traindoc = Nothing End Sub ```",
    "author_id":5755,
    "publication_date":1754259308000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jarvis Davis",
    "author_reputation":183.0,
    "tags":"vba, excel",
    "text_length":3090,
    "title_length":56,
    "num_tags":2
  },
  {
    "id":6271,
    "title":"Manually added Razor page doesn&#39;t render razor component",
    "link":"https:\/\/stackoverflow.com\/questions\/79724304\/manually-added-razor-page-doesnt-render-razor-component",
    "text":"I started a project as .NET Minimal API - no UI whatsoever. Then I needed to add a visual tool for the API, to show what the service is doing. For UI I wanted to use Razor pages and components, as it looked most similar to React (the comments will kill me :) ), but I'm otherwise not familiar with it. I added a page and a component, where the component is expected to render a key\/value pair. Instead, the component name is rendered in HTML, so nothing shows up in the browser: I'm new to razor\/Blazor and I want to figure out what went wrong here, even though I can start a new project and move the API there. Here's the gist of my setup: there is no ``` wwwroot ``` nor any CSS, Js or HTML that the regular Visual Studio templates would have created - only manually added ``` \/Pages ``` and ``` \/Pages\/Components ``` folders. In my ``` program.cs ``` I've added: ``` builder.Services.AddRazorPages(); builder.Services.AddRazorComponents(); builder.Services.AddServerSideBlazor(); ... app.MapGroup(\"\/api\/counters\").MapCountersEndpoints(); \/\/ an API that was the original purpose app.MapRazorPages(); app.MapBlazorHub(); ``` I also added ``` \/Pages\/Counters.cshtml ``` and ``` \/Pages\/Components\/NamedValue.razor ``` files: ``` Counters.cshtml ``` : ``` @page \"\/ui\/counters\" @{ <h3>NamedValue:<\/h3> <div> <NamedValue Key=\"AKey\" Value=\"A Value\" \/> <\/div> } ``` and ``` NamedValue.razor ``` : ``` <div class=\"item-key-value component\"> <span class=\"item-key\">@Key<\/span> = <span class=\"item-value\">@Value<\/span> <\/div> @code { [Parameter] public string Key { get; set; } = string.Empty; [Parameter] public string Value { get; set; } = string.Empty; } ```",
    "author_id":5754,
    "publication_date":1754260223000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Sten Petrov",
    "author_reputation":11092.0,
    "tags":"c#, blazor, blazor-server-side, razor-pages, razor",
    "text_length":1652,
    "title_length":60,
    "num_tags":5
  },
  {
    "id":6270,
    "title":"inconsistency in RSA algorithm when using pycryptodome",
    "link":"https:\/\/stackoverflow.com\/questions\/79724307\/inconsistency-in-rsa-algorithm-when-using-pycryptodome",
    "text":"I am using python3-pycryptodome version 3.9.0-150200.9.1 on openSUSE Leap version 15.6. While using Crypto.PublicKey.RSA class, I noticed that generated RSA keys have some algorithmic inconsistency. First, let me show you snippet of my code, ``` from Crypto.PublicKey import RSA import gmpy2 as gm zero = gm.mpz(0) one = gm.mpz(1) key = RSA.generate(1024) p = gm.mpz(key.p) q = gm.mpz(key.q) n = gm.mpz(key.n) d = gm.mpz(key.d) e = gm.mpz(key.e) f = gm.mul(gm.sub(p, one), gm.sub(q, one)) t, r = gm.t_divmod(gm.sub(gm.mul(e, d), one), f) print(\"T:\", t, \"\\nR:\", r, \"\\n\") assert(r == zero) ``` Since ``` e ``` and ``` d ``` are multiplicative inverse of each other with respect to ``` f ``` , so the value of ``` r ``` should always be ``` zero ``` , but this ``` assert ``` randomly fails when I generate multiple keys this way. Any idea why? Do note that however, the generated keys correctly encrypt and decrypt even if the above ``` assert ``` fails. Here is an example output, ``` N: 148502774403628390194611500709682347814667593710263804255157829583856187369359709690285563468375111786415727941281838364540498319732753028415238432642064018564768129734406037062641477861055248558990411663521667242748082850681361695820916286468764801272766079977549468298872839357850688001921905070150870021617 F: 148502774403628390194611500709682347814667593710263804255157829583856187369359709690285563468375111786415727941281838364540498319732753028415238432642063994175826236354417984972173597931150214536367983662042569355961801875348542016817518199738121181705454851577038738077546432331820850702935944555788782117440 D: 3786287874221628833060271941142911125529353972159943582788217876798211113616120434328141578912323768742993739941575982481376680422409348089969728845438894796344787140928803863554860738865651625260009135296914439478294793149354932137846389802769093381050561713735488167704149676139058582518170514697039795113 P: 12644301210276125955062852673893145078336308093529189288020587426377781795770169081395069044878276629331783016359362180011337579202434183256912567514342137 Q: 11744640683103862097027615206036759955686314334472289809866198854597551023908834316691661598741290681896617494370859146395688450634864802703601794573562041 E: 65537 T: 1670 R: 142315158803477207269836021513445583322389777305669479077859586684528846228969721786523664990526148795315072610395095099351310889743888318897936831281977994418500143172983902264999698017352288930685984342790795632796726797209019432783454941415699465801060899427995457324315330984661648590313613532630916195880 Traceback (most recent call last): File \".\/utils.py\", line 226, in <module> (n, f, d, p, q, s, k, u, e, g, h) = generate_rsa(1024, TYPE_CRYPTODOME, debug=True) File \".\/utils.py\", line 83, in generate_rsa assert(r == zero) AssertionError ``` I am expecting the ``` assert ``` in code snippet to be always true but it randomly fails.",
    "author_id":5753,
    "publication_date":1754260910000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"smss",
    "author_reputation":13.0,
    "tags":"python, rsa, pycryptodome",
    "text_length":2900,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6269,
    "title":"Flutter FCM IOS notification sound doesn&#39;t play on terminate app state",
    "link":"https:\/\/stackoverflow.com\/questions\/79724316\/flutter-fcm-ios-notification-sound-doesnt-play-on-terminate-app-state",
    "text":"after reading thousands documentation and conversations with AI agent I found myself in here like old days. I'am trying to get notification using FCM in IOS. First of all; I had done all platform specific implementation with Firebase and XCODE capabilities. The problem is not receiving notifications. I receive it perfectly.. Almost.. When I receive the notification \"yes I can see it but I can not hear it\". The problem is there is no sound with notification. I prefer to use default sound but I have tried everything but no result. Here is my ``` AppDelegate.swift ``` ``` import Flutter import UIKit import flutter_local_notifications import GoogleMaps import Firebase import FirebaseMessaging @main @objc class AppDelegate: FlutterAppDelegate { override func application( _ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]? ) -> Bool { \/\/\/ GoogleMaps Api GMSServices.provideAPIKey(\"xxxx\") \/\/ Flutter local notifications için callback FlutterLocalNotificationsPlugin.setPluginRegistrantCallback { (registry) in GeneratedPluginRegistrant.register(with: registry) } FirebaseApp.configure() application.registerForRemoteNotifications() GeneratedPluginRegistrant.register(with: self) return super.application(application, didFinishLaunchingWithOptions: launchOptions) } } ``` And here this is my ``` service.dart ``` ``` class RemoteNotificationService { static final FirebaseMessaging _firebaseMessaging = FirebaseMessaging.instance; static final FlutterLocalNotificationsPlugin flutterLocalNotificationsPlugin = FlutterLocalNotificationsPlugin(); final FirebaseFunctions _functions = FirebaseFunctions.instance; static final Set<String> _debtReminderCalls = HashSet(); static Future<void> initialize() async { NotificationSettings settings = await _firebaseMessaging.requestPermission( alert: true, announcement: false, badge: true, carPlay: false, criticalAlert: true, provisional: true, sound: true, ); if (settings.authorizationStatus == AuthorizationStatus.authorized) { print('User granted permission'); } else if (settings.authorizationStatus == AuthorizationStatus.provisional) { print('User granted provisional permission'); } else { print('User declined or has not accepted permission'); } if (Platform.isIOS) { await _firebaseMessaging.setForegroundNotificationPresentationOptions( alert: true, badge: true, sound: true, ); } FirebaseMessaging.onBackgroundMessage(_backgroundHandler); FirebaseMessaging.onMessage.listen((RemoteMessage message) async { final title = message.data['title'] ?? message.notification?.title ?? 'No Title'; final body = message.data['body'] ?? message.notification?.body ?? 'No Body'; await LocalNotificationService.showInstantNotification(title, body); await _saveNotification(title, body); }); \/\/ Bildirim tıklama (arka planda gelenler) FirebaseMessaging.onMessageOpenedApp.listen((RemoteMessage message) { print('Bildirim tıklandı: ${message.notification?.title}'); }); } @pragma('vm:entry-point') static Future<void> _backgroundHandler(RemoteMessage message) async { try { if (message.notification != null) { final title = message.notification!.title ?? 'No Title'; final body = message.notification!.body ?? 'No Body'; await _saveNotification(title, body); } } catch (e) { log('Background handler error: $e'); } } static Future<void> _saveNotification(String title, String body) async { final box = Hive.box<NotificationModel>('notifications'); final notification = NotificationModel( title: title, body: body, receivedAt: TimeSyncService.instance.now, isRead: false, ); await box.add(notification); }} ``` I just call this in ``` main.dart ``` like ``` await RemoteNotificationService.initialize(); ``` And also here is my payload from ``` Cloud Functions ``` : ``` const payload = { notification: { title: \"New Message\", body: `${truncatedBody}`, }, \/\/ token: tokens, android: { priority: 'high' }, apns: { headers: { \"apns-priority\": \"10\", \"apns-push-type\": \"alert\", \"content-available\": \"1\", }, payload: { aps: { alert: { title: \"New Message\", body: `${truncatedBody}`, }, sound: \"default\", badge: 1, } } }, }; ``` if you wonder how I use this (Users may have tokens more than 1): ``` const multicastMessage = { tokens: tokenList, notification: payload.notification, } const response = await admin.messaging().sendEachForMulticast(multicastMessage); ``` I just want to hear that notification sound while app on terminate state. :\/\/",
    "author_id":5752,
    "publication_date":1754262011000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Tolga Yılmaz",
    "author_reputation":518.0,
    "tags":"flutter, firebase, firebase-cloud-messaging",
    "text_length":4441,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":6268,
    "title":"TensorFlow Serving significantly slower than Keras model.predict()",
    "link":"https:\/\/stackoverflow.com\/questions\/79724317\/tensorflow-serving-significantly-slower-than-keras-model-predict",
    "text":"I'm deploying a Keras model using TensorFlow Serving, but the inference time is significantly slower compared to calling ``` model.predict() ``` directly in Python. Running ``` model.predict(input) ``` on a batch of ~33,000 records takes less than 2 seconds, but the same input through TensorFlow Serving takes around 13 seconds — roughly 6× slower (I am only comparing response time of tf serving API without including any pre\/post processing). I’ve tested this both: Locally with TensorFlow Serving Using the SageMaker TensorFlow inference container (which also implements TF serving) TF Serving setup: ``` # Install TFServing TFS_URL='https:\/\/framework-binaries.s3.us-west-2.amazonaws.com\/tensorflow_serving\/r2.18_aws\/cpu\/2025-01-15-19-48\/tensorflow_model_server' TARGET_PATH = \"\/tmp\/tensorflow_model_server\" !curl -L \"$TFS_URL\" -o \"$TARGET_PATH\" !chmod +x \"$TARGET_PATH\" %%bash --bg # Start running TFSeving nohup \/tmp\/tensorflow_model_server \\ --port=8500 \\ --rest_api_port=8501 \\ --model_name=model \\ --model_base_path=$MODEL_DIR >logs\/server-cpu.log 2>&1 ``` Input data and request: Input is a pandas DataFrame loaded from a pickle. ``` # Load pickle file data = pd.read_pickle(input_file) # Create instances dictionary instances_dict = data.to_dict(orient='records') instances = {\"instances\": instances_dict} # Convert to json inputs_json = json.dumps(instances) ``` Send request to TF serving ``` url = \"http:\/\/localhost:8501\/v1\/models\/model:predict\" response = requests.post(url, data=inputs_json) ``` Question: Is it expected that TensorFlow Serving is this much slower than ``` model.predict() ``` ? Am I missing something in my configuration or request format that could be causing this slowdown? I’m not able to share the model or dataset, but happy to provide more details as needed.",
    "author_id":5751,
    "publication_date":1754262107000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jurgita-ds",
    "author_reputation":67.0,
    "tags":"keras, tensorflow2.0, tensorflow-serving",
    "text_length":1798,
    "title_length":66,
    "num_tags":3
  },
  {
    "id":6267,
    "title":"milvus in standalone container memory overflow",
    "link":"https:\/\/stackoverflow.com\/questions\/79724318\/milvus-in-standalone-container-memory-overflow",
    "text":"VM resources: ``` Processor: 16 cores RAM: 128 GB HDD: 1 TB ``` Milvus is deployed in standard configuration ``` version: '3.5' services: etcd: container_name: milvus-etcd image: central-mirror-cache.my.com\/coreos\/etcd:v3.5.18 environment: - ETCD_AUTO_COMPACTION_MODE=revision - ETCD_AUTO_COMPACTION_RETENTION=1000 - ETCD_QUOTA_BACKEND_BYTES=4294967296 - ETCD_SNAPSHOT_COUNT=50000 volumes: - \/data\/milvus\/etcd:\/etcd command: etcd -advertise-client-urls=http:\/\/etcd:2379 -listen-client-urls http:\/\/0.0.0.0:2379 --data-dir \/etcd healthcheck: test: [\"CMD\", \"etcdctl\", \"endpoint\", \"health\"] interval: 30s timeout: 20s retries: 3 minio: container_name: milvus-minio image: ccentral-mirror-cache.my.com\/minio\/minio:RELEASE.2024-05-28T17-19-04Z environment: MINIO_ACCESS_KEY: minioadmin MINIO_SECRET_KEY: minioadmin ports: - \"9001:9001\" - \"9000:9000\" volumes: - \/data\/milvus\/minio:\/minio_data command: minio server \/minio_data --console-address \":9001\" healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http:\/\/localhost:9000\/miniooo\/health\/live\"] interval: 30s timeout: 20s retries: 3 standalone: container_name: milvus-standalone image: central-mirror-cache.my.com\/milvusdb\/milvus:v2.5.14 deploy: resources: limits: memory: \"110G\" # limit memory command: [\"milvus\", \"run\", \"standalone\"] security_opt: - seccomp:unconfined environment: MINIO_REGION: us-east-1 ETCD_ENDPOINTS: etcd:2379 MINIO_ADDRESS: minio:9000 volumes: - \/data\/milvus\/milvusdb:\/var\/lib\/milvus healthcheck: test: [\"CMD\", \"curl\", \"-f\", \"http:\/\/localhost:9091\/healthz\"] interval: 30s start_period: 90s timeout: 20s retries: 3 ports: - \"19530:19530\" - \"9091:9091\" depends_on: - \"etcd\" - \"minio\" webui: container_name: milvus-ui image: central-mirror-cache.my.com\/zilliz\/attu:v2.5 environment: MILVUS_URL: standalone:19530 ports: - \"8000:3000\" depends_on: - \"standalone\" networks: default: name: milvus ``` After operation load via spark 50 Gb vectors container milvus-standalone fell. A restart attempt leads to the processes in milvus-standalone causing memory limit overflow and another crash. Last operations in milvus-standalone via ``` docker logs -f --tail 100 milvus-standalone [2025\/08\/03 20:58:51.811 +00:00] [INFO] [datacoord\/handler.go:201] [GetQueryVChanPositions] [collectionID=459656474165588647] [channel=by-dev-rootcoord-dml_13_459656474165588647v0] [numOfSegments=1] [\"result flushed\"=1] [\"result growing\"=0] [\"result L0\"=0] [\"partition stats\"={}] [2025\/08\/03 20:58:51.811 +00:00] [INFO] [datacoord\/handler.go:456] [\"channel seek position set from channel checkpoint meta\"] [channel=by-dev-rootcoord-dml_13_459656474165588647v0] [posTs=459867344338157569] [posTime=2025\/08\/03 20:58:20.997 +00:00] [2025\/08\/03 20:58:51.811 +00:00] [INFO] [datacoord\/services.go:932] [\"datacoord append channelInfo in GetRecoveryInfo\"] [traceID=4ed7b9ebfb04f1fbe9a48ab7ecc6d6a4] [collectionID=459656474165588647] [partitionIDs=\"[]\"] [channel=by-dev-rootcoord-dml_13_459656474165588647v0] [\"# of unflushed segments\"=0] [\"# of flushed segments\"=1] [\"# of dropped segments\"=0] [\"# of indexed segments\"=0] [\"# of l0 segments\"=0] [2025\/08\/03 20:58:51.850 +00:00] [INFO] [datacoord\/task_scheduler.go:325] [\"check running tasks\"] [\"runningTask num\"=2] [2025\/08\/03 20:58:51.852 +00:00] [INFO] [indexnode\/indexnode_service.go:210] [\"Get Index Job Stats\"] [traceID=4519a3721e42e4742fc6a1361e7f9e01] [unissued=0] [active=2] [slots=0] [2025\/08\/03 20:58:51.852 +00:00] [INFO] [datacoord\/task_scheduler.go:371] [\"task scheduler\"] [\"task num\"=1] [workerSlots=\"{\\\"26\\\":{\\\"NodeID\\\":26,\\\"TotalSlots\\\":32,\\\"AvailableSlots\\\":0}}\"] [2025\/08\/03 20:58:51.951 +00:00] [INFO] [indexnode\/indexnode_service.go:210] [\"Get Index Job Stats\"] [traceID=e1b5cb07c73ce1f2783f8139a99b5f3e] [unissued=0] [active=2] [slots=0] [2025\/08\/03 20:58:51.951 +00:00] [INFO] [datacoord\/task_scheduler.go:371] [\"task scheduler\"] [\"task num\"=1] [workerSlots=\"{\\\"26\\\":{\\\"NodeID\\\":26,\\\"TotalSlots\\\":32,\\\"AvailableSlots\\\":0}}\"] [2025\/08\/03 20:58:51.951 +00:00] [INFO] [datacoord\/task_scheduler.go:325] [\"check running tasks\"] [\"runningTask num\"=2] [2025\/08\/03 20:58:52.054 +00:00] [INFO] [indexnode\/indexnode_service.go:210] [\"Get Index Job Stats\"] [traceID=df03e23946b030248c528536ead69de9] [unissued=0] [active=2] [slots=0] [2025\/08\/03 20:58:52.054 +00:00] [INFO] [datacoord\/task_scheduler.go:371] [\"task scheduler\"] [\"task num\"=1] [workerSlots=\"{\\\"26\\\":{\\\"NodeID\\\":26,\\\"TotalSlots\\\":32,\\\"AvailableSlots\\\":0}}\"] [2025\/08\/03 20:58:52.054 +00:00] [INFO] [datacoord\/task_scheduler.go:325] [\"check running tasks\"] [\"runningTask num\"=2] [2025\/08\/03 20:58:52.152 +00:00] [INFO] [indexnode\/indexnode_service.go:210] [\"Get Index Job Stats\"] [traceID=f9518633c339d570b2a81a1af7f8c966] [unissued=0] [active=2] [slots=0] [2025\/08\/03 20:58:52.152 +00:00] [INFO] [datacoord\/task_scheduler.go:371] [\"task scheduler\"] [\"task num\"=1] [workerSlots=\"{\\\"26\\\":{\\\"NodeID\\\":26,\\\"TotalSlots\\\":32,\\\"AvailableSlots\\\":0}}\"] [2025\/08\/03 20:58:52.152 +00:00] [INFO] [datacoord\/task_scheduler.go:325] [\"check running tasks\"] [\"runningTask num\"=2] ``` How solve this problem?",
    "author_id":5750,
    "publication_date":1754262115000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nikolay Baranenko",
    "author_reputation":1690.0,
    "tags":"docker, memory, milvus",
    "text_length":5043,
    "title_length":46,
    "num_tags":3
  },
  {
    "id":6266,
    "title":"Python can&#39;t execute PowerShell command in GitHub workflow",
    "link":"https:\/\/stackoverflow.com\/questions\/79724319\/python-cant-execute-powershell-command-in-github-workflow",
    "text":"My project creates WireGuard configurations automatically, so it executes the PowerShell command ``` genkey ``` with wg.exe. On Windows 11 it works but in GitHub ( ``` windows-latest ``` runner) it cannot call wg.exe: ``` wg.exe: D:\\a\\_temp\\ae4190e7-b6d1-448e-8238-0af82213defe.ps1:4 Line | 4 | wg.exe genkey | Tee-Object server_keys\/test0_private.key | wg.exe pub … | ~~~~~~ | The term 'wg.exe' is not recognized as a name of a cmdlet, function, script file, or executable program. Check | the spelling of the name, or if a path was included, verify that the path is correct and try again. Error: Process completed with exit code 1. ``` Here I manually tried to create keys, but it is the same while executing in Python code. I changed to ``` .\\wg.exe ``` in a manual call and it worked, but that didn't work for Python execution. I'm downloading WireGuard in workflow. I know it creates a different copy of WireGuard: ``` Invoke-WebRequest -Uri \"https:\/\/download.wireguard.com\/windows-client\/wireguard-installer.exe\" -OutFile \".\\wireguard-installer.exe\" Start-Process -FilePath '.\\wireguard-installer.exe' ``` Python command for ``` genkey ``` using subprocess library: ``` def _gen_server_keys(self): command = f\"cd \\\"{self.path}\\\" | .\\\\wg.exe genkey | \" \\ f\"Tee-Object \\\"server_keys\\\\{self.name + '_privet.key'}\\\" | \" \\ f\".\\\\wg.exe pubkey | \" \\ f\"Tee-Object \\\"server_keys\\\\{self.name + '_public.key'}\\\"\" self._execute_command(command) @staticmethod def _execute_command(command): completed_process = subprocess.run( [\"powershell.exe\", \"-Command\", command], capture_output=True, text=True, ) print(completed_process.stderr) return completed_process ``` workflow action and GitHub repository . wg.exe is located at \"tests\/WireGuard\".",
    "author_id":5749,
    "publication_date":1754262178000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ivan_Solaris",
    "author_reputation":33.0,
    "tags":"python, powershell, github-actions, wireguard",
    "text_length":1735,
    "title_length":62,
    "num_tags":4
  },
  {
    "id":6265,
    "title":"Tab groups inside a modal with parallel &amp; intercepting routes",
    "link":"https:\/\/stackoverflow.com\/questions\/79724323\/tab-groups-inside-a-modal-with-parallel-intercepting-routes",
    "text":"In Next.js 15, when using Parallel Routes together with Intercepting Routes , is it possible to have Tab Groups inside a Modal ? In my case, when the user clicks a link on the homepage (or any other page), I want the requested content to be shown inside a modal - instead of as a new page. Here's the problem: The page I want to show inside the modal has tabs using ``` layout.tsx ``` (with sub folders containing their own ``` page.tsx ``` files). How do I intercept that, and make sure the modal uses the layout? My folder structure is: ``` app\/page.tsx (home page, I want to be able to open the modal from here) app\/pics\/page.tsx (gallery page, I want to be able to open the modal from here, too) app\/pics\/[id]\/layout.tsx (photo page with tabs - I want this layout to apply to both the page and the modal) app\/pics\/[id]\/info\/page.tsx (tab 1 content) app\/pics\/[id]\/stats\/page.tsx (tab 2 content) app\/pics\/[id]\/comments\/page.tsx (tab 3 content) app\/pics\/[id]\/page.tsx (appears to be needed so I can intercept it) app\/@modal\/(.)pics\/[id]\/page.tsx (photo modal - is supposed to show tabs but only shows the content from app\/pics\/[id]\/page.tsx) ```",
    "author_id":4689,
    "publication_date":1754263366000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ben",
    "author_reputation":16789.0,
    "tags":"next.js, next.js15, modal-dialog, parallel-route, intercepting-route",
    "text_length":1146,
    "title_length":65,
    "num_tags":5
  },
  {
    "id":6264,
    "title":"How to inject &quot;system prompt&quot; in Cursor IDE window by own extension?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724326\/how-to-inject-system-prompt-in-cursor-ide-window-by-own-extension",
    "text":"I'm trying to write extension to Cursor IDE which can be also used with other VS like editors. It gives more power to Agent help and it's coding. I'm searching for a couple of last weeks or even months, possibility to inject \"system prompt\" or \"system prompt add\" before I send message with enter hit. Does anyone know how to do it with success? I used official APIs and other methods but everything failed. Maybe someone already did sth like that with success or there is already extension that I could check how it is done there? My extension I'm going to distribute as Open Source or double license. If I succeed for sure I will share with others my success. I've tried may approaches, don't want to list them all. Capturing enter button action, overriding code\/functions of Cursor IDE and many others. Everything failed.",
    "author_id":5748,
    "publication_date":1754263683000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"truthseeker",
    "author_reputation":2295.0,
    "tags":"visual-studio-code, cursor-ide, visual-studio-extensions",
    "text_length":824,
    "title_length":78,
    "num_tags":3
  },
  {
    "id":6263,
    "title":"How do I correctly nest Pydantic models in a FastAPI response?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724327\/how-do-i-correctly-nest-pydantic-models-in-a-fastapi-response",
    "text":"The example code is below. Seems like when I nest two models, the nested models don't show up in the response even though the app can prove that the data is there. See the example below. Feels like I'm just doing something fundamentally wrong, but this doesn't seem like a wrong pattern to adopt, especially when the other parts seem to be just fine as is. ``` #!\/usr\/bin\/env python3 from fastapi import FastAPI from pydantic import BaseModel class APIResponse(BaseModel): status: str data: BaseModel | None = None class APIData(BaseModel): name: str count: int app = FastAPI() @app.get('\/') async def get_root(): data = APIData(name=\"foo\", count=1) response = APIResponse(status=\"success\", data=data) print(data) ''' name='foo' count=1 ''' print(response) ''' status='success' data=APIData(name='foo', count=1) ''' return data ''' Returns {\"name\":\"name_value\",\"count\":1} ''' return response ''' Expected {\"status\": \"success\", \"data\": {\"name\":\"foo\",\"count\":1}} Actual {\"status\":\"success\",\"data\":{}} ''' ```",
    "author_id":5747,
    "publication_date":1754263902000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"enigma_0z",
    "author_reputation":71.0,
    "tags":"python, fastapi, pydantic",
    "text_length":1006,
    "title_length":62,
    "num_tags":3
  },
  {
    "id":6262,
    "title":"Not equal operator not working with WMIC command in for loop of batch file",
    "link":"https:\/\/stackoverflow.com\/questions\/79724328\/not-equal-operator-not-working-with-wmic-command-in-for-loop-of-batch-file",
    "text":"I wrote this batch to get the number of active processes given a certain command line value, but the not equal operator I use in the query to exclude WMIC.exe itself doesn't work, and the batch returns a counter incremented by 1 since it also includes the WMIC.exe process. Here the batch 'count-process-by-command-line.cmd': ``` if \"%~1%\" == \"\" ( echo No command line expression in input goto end ) >nul chcp 65001 set \/a cycleCounter=0 FOR \/F %%T IN ('Wmic process where^(name ^!^=\"WMIC.exe\" and COMMANDLINE LIKE \"%%%~1%%\"^)get ProcessId^|more +1') DO ( call set processId=%%T if \"!processId!\" neq \"\" ( echo Process ID !processId! set \/a cycleCounter+=1 ) ) set %2=!cycleCounter! :end ``` ... And here the test code batch: ``` @echo off setlocal enableDelayedExpansion call count-process-by-command-line.cmd \"test\" TEST echo !TEST! endlocal ```",
    "author_id":5746,
    "publication_date":1754263942000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"PeterH",
    "author_reputation":63.0,
    "tags":"batch-file, cmd, wmic, wmi-query",
    "text_length":846,
    "title_length":74,
    "num_tags":4
  },
  {
    "id":6261,
    "title":"VBA to do a HLookup across multiple cells like an autofill",
    "link":"https:\/\/stackoverflow.com\/questions\/79724330\/vba-to-do-a-hlookup-across-multiple-cells-like-an-autofill",
    "text":"``` Dim bcell As Range, aRow As Variant Dim cn As Long: cn = 0 For Each bcell In jdrg.Cells With bcell.EntireRow If bcell.Value = \"\" And .Columns(\"F\").Value <> \"\" Then Sh1.Range(\"L\" & vFirstRow + cn, \"BD\" & vFirstRow + cn).Value = _ WorksheetFunction.HLookup(Sh1.Range(\"L$1\"), _ Sh2.Range(\"$A$1:$AE$2\"), 2, False) Sh1.Range(\"K\" & vFirstRow + cn).Value = \"Printed\" cn = cn + 1 End If End With Next bcell ``` I have this code currently however I would like the Range L:BD to have the HLookup work kind of like a autofill with ``` WorksheetFunction.HLookup(Sh1.Range(\"L$1\"), Sh2.Range(\"$A$1:$AE$2\"), 2, False) ``` In ``` (\"L$1\") ``` the ``` L ``` should go up to BD as it goes across. Is there an easy way of making it do this because currently it just does L in all. After help from CDP1802 below this is my final code and works great ``` Sub DoMailMerge2() 'Note: A VBA Reference to the Word Object Model is required, via Tools|References Dim wdApp As New Word.Application, wdDoc As Word.Document Dim strWorkbookName As String: strWorkbookName = ThisWorkbook.FullName Dim f As String Dim r As Range: Set r = Selection Dim nLastRow As Long: nLastRow = r.Rows.Count + r.Row - 2 Dim nFirstRow As Long: nFirstRow = r.Row - 1 Dim vFirstRow As Long: vFirstRow = r.Row Dim vLastRow As Long: vLastRow = r.Rows.Count + r.Row - 1 Dim WFile As String: WFile = Range(\"A2\").Value Dim sheetname As String: sheetname = ActiveSheet.Name Dim Sh1 As Worksheet: Set Sh1 = ActiveSheet Dim Sh2 As Worksheet: Set Sh2 = Sheets(\"Calibrated Gear\") Dim rng As Range Dim jdrg As Range: Set jdrg = Sh1.Range(\"K\" & vFirstRow, \"K\" & vLastRow) f = \"=IfError(HLookup(R1C,'\" & Sh2.Name & \"'!R1C1:R2C31,2,False), \"\"\"\")\" ' $A$1:$AE$2 Dim bcell As Range, rr As Long For Each bcell In jdrg.Cells rr = bcell.Row If bcell = \"\" And Sh1.Cells(rr, \"F\") <> \"\" Then Sh2.Range(\"A2\").Value = Sh1.Range(\"F\" & rr).Value Set rng = Sh1.Range(\"L1:BD1\").Offset(rr - 1) rng.Formula2R1C1 = f rng.Value = rng.Value End If Next bcell Dim found As Boolean: found = False For Each Cell In Range(\"L\" & vFirstRow, \"BD\" & vLastRow).Cells If Cell.Value = \"OUT OF DATE\" Then found = True End If Next If found = True Then MsgBox \"One or more calibrated tools are out of date and no replacements in date available.\" Exit Sub End If ActiveWorkbook.Save With wdApp 'Disable alerts to prevent an SQL prompt .DisplayAlerts = wdAlertsNone 'Open the mailmerge main document Set wdDoc = .Documents.Open(\"S:\\ISO\\ISO - Form Templates\\All certificates\\\" & WFile, _ ConfirmConversions:=False, ReadOnly:=True, AddToRecentfiles:=False) With wdDoc With .MailMerge 'Define the mailmerge type .MainDocumentType = wdFormLetters 'Define the output .Destination = wdSendToNewDocument .SuppressBlankLines = True 'Connect to the data source .OpenDataSource Name:=strWorkbookName, ReadOnly:=False, _ LinkToSource:=False, AddToRecentfiles:=False, _ Format:=wdOpenFormatAuto, _ Connection:=\"Provider=Microsoft.ACE.OLEDB.12.0;\" & _ \"User ID=Admin;Data Source=\" & strWorkbookName & \";\" & _ \"Mode=Read;Extended Properties=\"\"HDR=YES;IMEX=1\"\";\", _ SQLStatement:=\"SELECT * FROM `\" & sheetname & \"$`\", _ SubType:=wdMergeSubTypeAccess With .DataSource .FirstRecord = nFirstRow .LastRecord = nLastRow End With 'Excecute the merge .Execute 'Disconnect from the data source .MainDocumentType = wdNotAMergeDocument End With 'Close the mailmerge main document .Close False End With 'Restore the Word alerts .DisplayAlerts = wdAlertsAll 'Display Word and the document .Visible = True .Activate .Dialogs(wdDialogFilePrint).Show wdApp.ActiveDocument.Close SaveChanges:=wdDoNotSaveChanges wdApp.Quit End With jdrg.Value = \"Printed\" End Sub ``` In the first few columns I manually put in some data and then depending on the date in F I need it to make sure calibrated tools are in date and then automatically change to the next one if first one is not in date (This is all done with some formulas on 'Calibrated Gear' Sheet). If there is no tools of its type in date it stops the word document certificate merge from printing. I have multiple sheets for different products so thats why it is very dynamic. Thankyou",
    "author_id":5745,
    "publication_date":1754264427000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Todd Harris",
    "author_reputation":29.0,
    "tags":"vba, excel",
    "text_length":4106,
    "title_length":58,
    "num_tags":2
  },
  {
    "id":6260,
    "title":"Go http.FileServer returns 301 for existing file in tests, but works in browser",
    "link":"https:\/\/stackoverflow.com\/questions\/79724334\/go-http-fileserver-returns-301-for-existing-file-in-tests-but-works-in-browser",
    "text":"I'm building a Go web server that serves static files using TDD. When I run my server and access ``` \/static\/index.html ``` in the browser, it works fine and returns the file. However, when I run my test using ``` httptest ``` , I get a ``` 301 ``` response with a Location: ``` .\/ ``` header instead of 200. The static files serves on ``` \/templates ``` dir. ``` func newServer() *http.ServeMux { mux := http.NewServeMux() mux.HandleFunc(\"\/\", rootHandler) fs := http.FileServer(http.Dir(\".\/templates\")) mux.Handle(\"\/static\/\", http.StripPrefix(\"\/static\/\", fs)) return mux } ``` Here's my test code: ``` func TestFile(t *testing.T) { cwd, err := os.Getwd() if err != nil { t.Fatalf(\"error get PATH %v\", err) } server := newServer() req, err := http.NewRequest(\"GET\", \"\/static\/index.html\", nil) if err != nil { t.Fatalf(\"error creating request: %v\", err) } rec := httptest.NewRecorder() server.ServeHTTP(rec, req) if rec.Code != http.StatusOK { t.Errorf(\"Expected status OK, got %v\", rec.Code) t.Logf(\"CWD %s\", cwd) t.Logf(\"Response body: %s\", rec.Body.String()) t.Logf(\"Response headers: %v\", rec.Header()) } contentType := rec.Header().Get(\"Content-Type\") if contentType != \"text\/html; charset=utf-8\" { t.Errorf(\"Expected content type text\/html; charset=utf-8, got %v\", contentType) } } ``` Test Output: ``` File directory: -rw-r--r-- 1 user staff 30 Aug 3 15:00 go.mod -rw-r--r--@ 1 user staff 1116 Aug 4 04:19 main_test.go -rw-r--r--@ 1 user staff 613 Aug 4 03:48 main.go drwxr-xr-x@ 4 user staff 128 Aug 4 04:04 templates drwxr-xr-x 3 user staff 96 Aug 3 15:02 test .\/templates: total 112 -rw-r--r--@ 1 user staff 51007 Aug 4 03:58 htmx.min.js -rw-r--r--@ 1 user staff 237 Aug 4 04:04 index.html .\/test: total 8 -rw-r--r--@ 1 user staff 21 Aug 3 15:02 compression_test.go ``` Why does ``` http.FileServer ``` return a 301 redirect with ``` Location: .\/ ``` for an existing file in my test, but works fine in the browser? How can I make the test pass and return 200 as expected? I’ve: Ensured the file exists and is not a directory. Used both relative and absolute paths for ``` http.Dir ``` . Checked the current working directory in the test. Filled ``` index.html ``` with simple HTML content.",
    "author_id":5744,
    "publication_date":1754264736000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Harris",
    "author_reputation":19.0,
    "tags":"go, go-http",
    "text_length":2198,
    "title_length":79,
    "num_tags":2
  },
  {
    "id":6259,
    "title":"Trying to pass UserID to API through form. The ID is retrieved, but doesn&#39;t seem to be passed",
    "link":"https:\/\/stackoverflow.com\/questions\/79724343\/trying-to-pass-userid-to-api-through-form-the-id-is-retrieved-but-doesnt-seem",
    "text":"I'm making an event management application. I'm using better-auth & mongodb. I got a form in which I can create an event. Now, alongside the event name, and dates I want to pass along the owner's user-id so that we know whose events are. I'm trying to pass it through a hidden form field — it didn't work. Same with putting it in as a default value. I can get it printed to the console, but when I print the values that are sent to the API, all the values show but the ID which is shown as undefined. That's the client console log: About the uncontrolled component error: I mitigated it by setting default owner to \"\" and putting a hidden form field with the owner ID. But then \"\" was passed on to the API, and not the value from the hidden field. That's the server console log: ``` Error: Event validation failed: owner: Path `owner` is required. at handler (pages\\api\\events.ts:26:28) 24 | } catch (error) { 25 | res.status(400).json({ success: false }) > 26 | console.log(error) | ^ 27 | } 28 | break 29 | default: { errors: [Object], _message: 'Event validation failed' } ``` Form component: ``` import { zodResolver } from \"@hookform\/resolvers\/zod\" import { useForm } from \"react-hook-form\" import { z } from \"zod\" import { toast } from 'sonner' import { useState, useEffect } from 'react' import { authClient } from \"@\/lib\/auth\/auth-client\" import { Button } from \"@\/components\/ui\/button\" import { Form, FormControl, FormDescription, FormField, FormItem, FormLabel, FormMessage, } from \"@\/components\/ui\/form\" import { Input } from \"@\/components\/ui\/input\" const formSchema = z.object({ name: z.string().min(2).max(120), start: z.string().min(1, \"Start date is required\"), end: z.string().min(1, \"End date is required\"), owner: z.any(), }) export function NewEventForm() { const [events, setEvents] = useState([]) const { data: session, } = authClient.useSession() const owner = (session?.user.id)?.toString() console.log(typeof(owner)) const form = useForm<z.infer<typeof formSchema>>({ resolver: zodResolver(formSchema), defaultValues: { name: \"\", }, } ) const onSubmit = async (values: z.infer<typeof formSchema>) => { try { toast.success('work') console.log(values) await fetch('\/api\/events', { method: 'POST', headers: { 'Content-Type': 'application\/json', }, body: JSON.stringify(values), }); } catch (error) { console.log(error); } }; return ( <Form {...form}> <form className='flex items-center justify-center space-y-8' onSubmit={form.handleSubmit(onSubmit)}> <FormField control={form.control} name=\"name\" render={({ field }) => ( <FormItem> <FormLabel>Event name<\/FormLabel> <FormControl> <Input placeholder=\"Daydream Vienna\" {...field} \/> <\/FormControl> <FormMessage \/> <\/FormItem> )} \/> <FormField control={form.control} name=\"start\" render={({ field }) => ( <FormItem> <FormLabel>Start date<\/FormLabel> <FormControl> <Input type='date' {...field} \/> <\/FormControl> <FormMessage \/> <\/FormItem> )} \/> <FormField control={form.control} name=\"end\" render={({ field }) => ( <FormItem> <FormLabel>End date<\/FormLabel> <FormControl> <Input type='date' {...field} \/> <\/FormControl> <FormMessage \/> <\/FormItem> )} \/> <FormItem> <FormLabel>Start date<\/FormLabel> <FormControl> <Input type='hidden' name='owner' value={owner} \/> <\/FormControl> <FormMessage \/> <\/FormItem> <Button type=\"submit\">Create<\/Button> <\/form> <\/Form> ) } ``` Event API: ``` import type { NextApiRequest, NextApiResponse } from 'next' import dbConnect from '@\/lib\/mongodb' import Event from '@\/models\/Event'; export default async function handler(req, res) { await dbConnect(); const { method } = req; switch (method) { case 'GET': try { const event = await Event.find({}); res.status(200).json({ success: true, data: event}) } catch (error) { res.status(400).json({ success: false }) console.log(error) } break case 'POST': try { const event = await Event.create(req.body) res.status(201).json({ success: true, data: event }) } catch (error) { res.status(400).json({ success: false }) console.log(error) } break default: res.status(400).json({ success: false }) break } } ``` The database model: ``` import mongoose from 'mongoose' const eventSchema = new mongoose.Schema({ name: { type: String, required: true }, start: { type: Date, required: true, }, end: { type: Date, required: true, }, owner: { type: mongoose.Schema.Types.Mixed, required: true, } }) const Event = mongoose.models.Event || mongoose.model(\"Event\", eventSchema); export default Event ```",
    "author_id":5743,
    "publication_date":1754266003000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Potion",
    "author_reputation":13.0,
    "tags":"next.js, reactjs, mongodb, mongoose, better-auth",
    "text_length":4439,
    "title_length":97,
    "num_tags":5
  },
  {
    "id":6258,
    "title":"How do I deploy Whisper.cpp stream.wasm on an EC2 instance?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724345\/how-do-i-deploy-whisper-cpp-stream-wasm-on-an-ec2-instance",
    "text":"How do I deploy the whisper.cpp stream.wasm demo on a EC2 instance? The demo is available from the following git: https:\/\/github.com\/ggml-org\/whisper.cpp\/tree\/master\/examples\/stream.wasm I am fairly far along, but the program hangs on Preparing ... I do not know what is causing the issue. Here are the steps I have taken. (I am trying to make it brief, and provide more detail as necessary.) I cloned the repository and built it locally. It runs properly. I created an EC2 instance running Amazon Linux 2023, and uploaded the Whisper.cpp files to it. I ssh into the server and installed nginx, it displays the default page to the server's public ip address. I then configured nginx as a reverse proxy server: ``` server { listen 80; server_name redactedPublicIpAddress; location \/ { proxy_pass http:\/\/127.0.0.1:8000; } } ``` I enabled and restarted nginx. I then try to run whisper.cpp stream.wasm by running ``` python3 examples\/server.py ``` It indicates that it is serving home\/ec2-user\/whisper.cpp\/build-em\/bin to localhost:8000 This is analogous to the message when it is served locally. Stream.wasm has a 301 response. Helpers.js and coi-serviceworker.js have 200 responses. The index file displays normally at the public ip address\/stream.wasm\/ However, all is not well. The js seems to hang on Preparing... Other js functions on the page still function, I can download a model and the page indicates it was downloaded, etc. However, I cannot start recording. The start button is not active. It never properly initializes. This type of behavior is identical to trying to open the index file on my local computer without it being served. I think I am missing something simple, but do not know what it is. Here are some steps that I took afterwards that did not fix the issue: I installed nodejs and npm so that I could install ffmpeg.wasm: ``` npm install @ffmpeg\/ffmpeg @ffmpeg\/util ``` The github page indicates that I need to put the files in the html path. So in trying to troubleshoot I put them in \/user\/shared\/nginx\/html which is where the nginx default index file is placed. I think it may be related to serving the python. The server.py file is not in a state to deploy it with uvicorn. Maybe there is some sort of cors issue. I simply do not know.",
    "author_id":5742,
    "publication_date":1754266206000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"tosUser",
    "author_reputation":25.0,
    "tags":"nginx, ffmpeg, amazon-ec2",
    "text_length":2264,
    "title_length":59,
    "num_tags":3
  },
  {
    "id":6257,
    "title":"Sync issue between Howler.js and Wavesurfer.js on drag\/seek in React",
    "link":"https:\/\/stackoverflow.com\/questions\/79724348\/sync-issue-between-howler-js-and-wavesurfer-js-on-drag-seek-in-react",
    "text":"Howler.js and Wavesurfer.js sync issue when dragging - seeking sometimes fails I'm using Howler.js for audio playback and Wavesurfer.js for waveform visualization in a React app. I have a sync issue where dragging to seek sometimes works, but other times the Howler audio instance doesn't update its position while the wavesurfer instance does update correctly. Setup ``` const [audioInstance, setAudioInstance] = useState<Howl | undefined>(); const [isPlaying, setIsPlaying] = useState<boolean>(false); const [isSeeking, setIsSeeking] = useState<boolean>(false); \/\/ Wavesurfer setup const { wavesurfer: wavePlayer } = useWavesurfer({ container: waveContainerRef, url: audioUrl, dragToSeek: true, backend: \"WebAudio\", }); \/\/ Howler instance creation useEffect(() => { if (audioUrl && currentFile) { const newAudio = new Howl({ src: [audioUrl], format: getAudioFormat(currentFile), html5: false, }); setAudioInstance(newAudio); } }, [audioUrl, currentFile]); ``` Event Handlers ``` const handleWaveClick = () => { const clickTime = wavePlayer.getCurrentTime(); audioInstance.seek(clickTime); }; const handleSeekStart = () => { setIsSeeking(true); wavePlayer.pause(); }; const handleSeekEnd = () => { const newTime = wavePlayer.getCurrentTime(); audioInstance.seek(newTime); if (isPlaying) { wavePlayer.play(); } setIsSeeking(false); }; \/\/ Event listeners setup useEffect(() => { if (!wavePlayer || !audioInstance) return; wavePlayer.setVolume(0); wavePlayer.on(\"ready\", () => { setWaveLoaded(true); setTimeout(() => { wavePlayer.setOptions({}); }, 1); if (wavePlayer.getDuration() < 3) { wavePlayer.setOptions({ backend: \"MediaElement\" }); } if (shouldAutoPlayRef.current) { shouldAutoPlayRef.current = false; startPlayback(); } }); const handleWaveClick = () => { const clickTime = wavePlayer.getCurrentTime(); audioInstance.seek(clickTime); }; const handleSeekStart = () => { setIsSeeking(true); wavePlayer.pause(); }; const handleSeekEnd = () => { const newTime = wavePlayer.getCurrentTime(); audioInstance.seek(newTime); if (isPlaying) { wavePlayer.play(); } setIsSeeking(false); }; const handleTrackEnd = () => { wavePlayer.stop(); audioInstance.stop(); wavePlayer.setTime(0); setIsPlaying(false); if (isLooping) { startPlayback(); } }; wavePlayer.on(\"click\", handleWaveClick); wavePlayer.on(\"dragstart\", handleSeekStart); wavePlayer.on(\"dragend\", handleSeekEnd); wavePlayer.on(\"finish\", handleTrackEnd); return () => { wavePlayer.un(\"click\", handleWaveClick); wavePlayer.un(\"dragstart\", handleSeekStart); wavePlayer.un(\"dragend\", handleSeekEnd); wavePlayer.un(\"finish\", handleTrackEnd); }; }, [wavePlayer, audioInstance, isPlaying, isLooping]); ``` Problem When dragging quickly or multiple times: Wavesurfer updates its position correctly Howler sometimes doesn't seek to the new position Sometimes the audio jumps back to the position before dragging started Clicking works fine, only dragging has issues What I've Tried Adding setTimeout delays in handleSeekEnd Using different backends (WebAudio vs MediaElement) Pausing both instances before seeking Adding debouncing to prevent rapid seek calls Using stop() instead of pause() before seeking ``` \/\/ Attempted solution with delays (didn't work reliably) const handleSeekEnd = () => { const newTime = wavePlayer.getCurrentTime(); setTimeout(() => { audioInstance.seek(newTime); if (isPlaying) { wavePlayer.play(); } setIsSeeking(false); }, 50); }; ``` The sync works most of the time, but fails intermittently with fast or repeated drag operations. How can I ensure reliable synchronization between these two audio libraries? Environment: React 19.1.1 Electron Howler.js 2.2.4 wavesurfer.js 7.10.1 @wavesurfer\/react 1.0.11 Using WebAudio backend",
    "author_id":4503,
    "publication_date":1754266835000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Taariq Elliott",
    "author_reputation":29.0,
    "tags":"reactjs, electron, web-audio-api, howler.js, wavesurfer.js",
    "text_length":3704,
    "title_length":68,
    "num_tags":5
  },
  {
    "id":6256,
    "title":"sample points along a line shapefile at fixed distance using terra in R",
    "link":"https:\/\/stackoverflow.com\/questions\/79724351\/sample-points-along-a-line-shapefile-at-fixed-distance-using-terra-in-r",
    "text":"I have a line shapefile ``` class : SpatVector geometry : lines dimensions : 221, 21 (geometries, attributes) extent : 0.182731, 1.162232, 51.09071, 51.38966 (xmin, xmax, ymin, ymax) coord. ref. : lon\/lat WGS 84 (EPSG:4326) names : osm_id name active_traffic_management bridge carriageway_ref destination type : <chr> <chr> <chr> <chr> <chr> <chr> values : 4049032 NA no NA A NA 4049033 NA no NA B NA 4049034 NA no NA A NA highway incline int_ref junction (and 11 more) <chr> <chr> <chr> <chr> motorway NA NA NA motorway NA E 15 NA motorway NA E 15 NA ``` I need to sample points along the roadway at every 500 meters. Looking at the ``` spatSample ``` argument, I can get 100 (or any number of points) but how do I specify a distance criteria so that my sample points are 500 meters apart. ``` library(terra) sample_test <- spatSample(m20, 100, method=\"regular\") nrow(sample_test) ```",
    "author_id":5741,
    "publication_date":1754267350000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"89_Simple",
    "author_reputation":3835.0,
    "tags":"r, terra",
    "text_length":885,
    "title_length":71,
    "num_tags":2
  },
  {
    "id":6255,
    "title":"React children ReactNode &#39;type&#39; is undefined when parent (owner) is server component",
    "link":"https:\/\/stackoverflow.com\/questions\/79724353\/react-children-reactnode-type-is-undefined-when-parent-owner-is-server-compo",
    "text":"I migrated my React component with nested children to NextJS. To know what kind of child it is and where to render it, I check its property ``` type.displayName ``` In NextJS, having no experience with server components, I found that the ``` type ``` property returns ``` undefined ``` . I saw that the ``` _owner ``` object has the ``` env ``` field seted as 'server' and found a way around it by wrapping my component in ``` 'use client' ``` , creating another layer. In this case, everything works as I expect. I would like to ask knowledgeable people why this happens. I did not find any available information on this matter, and how else can I implement my task Here's a piece of code to make it clear what I'm talking about ``` const slides = ((React.Children.toArray(children).find( child => React.isValidElement(child) && ((child as React.ReactElement).type as any)?.displayName == InfiniteCarousel.Slides.displayName ) as React.ReactNode[]) as any).props.children; ```",
    "author_id":5740,
    "publication_date":1754267932000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Прохор Гарагуля",
    "author_reputation":15.0,
    "tags":"next.js, reactjs, react-server-components",
    "text_length":977,
    "title_length":92,
    "num_tags":3
  },
  {
    "id":6254,
    "title":"How to properly run seeds in Prisma 6+ using modern configuration with prisma.config.ts?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724358\/how-to-properly-run-seeds-in-prisma-6-using-modern-configuration-with-prisma-co",
    "text":"I'm using Prisma 6.13 in a project with NextJs and I want to implement the seed process using the new modern configuration approach ``` prisma.config.ts ``` as recommended by Prisma in recent versions. I'm no longer using ``` package.json ``` to define the seed command. Previously, it was necessary to add: ``` \"prisma\": { \"seed\": \"tsx prisma\/seed.ts\" } ``` Here's my current project structure: ``` - prisma\/ - schema.prisma - seed.ts - db\/ - migrations\/ - lib\/ - prisma.ts - prisma.config.ts ``` My ``` seed.ts ``` file: ``` import { categories } from \"@\/prisma\/data\/categories\"; import { products } from \"@\/prisma\/data\/products\"; import prisma from \"@\/lib\/prisma\"; async function main() { try { await prisma.category.createMany({ data: categories }); await prisma.product.createMany({ data: products }); } catch (e) { console.error(e); } finally { await prisma.$disconnect(); } } main().catch(async (e) => { console.error(e); await prisma.$disconnect(); process.exit(1); }); ``` My ``` prisma.config.ts ``` , where I define the schema path, migrations path, and load environment variables: ``` import { defineConfig } from \"prisma\/config\"; import path from \"node:path\"; process.loadEnvFile(); export default defineConfig({ schema: path.join(\"prisma\", \"schema.prisma\"), migrations: { path: path.join(\"db\", \"migrations\"), }, }); ``` When I run: ``` npx prisma db seed ``` It doesn't throw any errors, but the seed is not executed. No data is inserted into the database. What’s the proper way to run seeds in Prisma 6+ with the new configuration? Is there any official way to define the path to the ``` seed.ts ``` file inside ``` prisma.config.ts ``` ? The documentation doesn't clearly explain how to use ``` prisma.config.ts ``` for the seeding process with Prisma 6+. It only covers the old setup through ``` package.json ``` , and there's no clear guidance about whether this is officially supported or planned for Prisma 7.",
    "author_id":5739,
    "publication_date":1754268958000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Aaron Mas",
    "author_reputation":87.0,
    "tags":"next.js, prisma, typescript, node.js",
    "text_length":1929,
    "title_length":88,
    "num_tags":4
  },
  {
    "id":6253,
    "title":"Case-insensitive string search independently of number of spaces inside",
    "link":"https:\/\/stackoverflow.com\/questions\/79724359\/case-insensitive-string-search-independently-of-number-of-spaces-inside",
    "text":"I am attempting to use regular expressions with C++ and can't figure out how to get it to work. Can you help compose a regular expression to match my sample text? A long time ago I took a Perl class and was taught some Perl regex. How do I use these regular expressions properly in C++, also are there any incompatibilities with Perl regex vs C++ one? I need to find a substring independently of the spaces inside both. The text string is as follows: ``` A quick brown fox jump over lazy dog. ``` And the search string is: ``` brown fox ``` The search string is under my control as I'm constructing it, but the string I search in is not. I presume I will need something like: ``` \\s+\\w\\s+\\w ``` But I do not know for sure and this is why I need help. Also I do not know how to compile and run PCRE regular expressions in C++. Can you please help with this? Are there any incompatibilities between the two languages I should be aware of while working on this project? EDIT: This question is NOT about PCRE. I’m familiar with Perl regexes and just trying to match the syntax. Also imagine the primal string have BrOwN FOx The string search I put above should still find a match.",
    "author_id":5738,
    "publication_date":1754269026000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Igor",
    "author_reputation":6396.0,
    "tags":"c++, regex",
    "text_length":1176,
    "title_length":71,
    "num_tags":2
  },
  {
    "id":6252,
    "title":"NodeJS and NPM disappears after WSL2 restarts",
    "link":"https:\/\/stackoverflow.com\/questions\/79724361\/nodejs-and-npm-disappears-after-wsl2-restarts",
    "text":"I tried a few times to install node and npm in WSL\/Ubuntu22.04 via NVM, both node and npm would work just fine right after installation. But once I exit WSL2, wait for a while when it is confirmed stopped via ``` wsl -l -v ``` , then restart it, neither NVM, node, nor npm can be found. What am I missing here? Update: I deleted ~\/.nvm folder and also .bashsrc file that the installation script of NVM created, then tried installation again and succeeded. I am not sure why this happens, but it happened twice on two of my laptops. I suspect this has something to do with WSL2 instances were exported then re-imported into a different location. Those who know better please advise. Thanks.",
    "author_id":5737,
    "publication_date":1754269250000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Zhiyong Li",
    "author_reputation":539.0,
    "tags":"windows-subsystem-for-linux, nvm",
    "text_length":689,
    "title_length":45,
    "num_tags":2
  },
  {
    "id":6251,
    "title":"Intellij modules not finding generated protobuf classes in a multimodule maven project (but works fine via maven cli)",
    "link":"https:\/\/stackoverflow.com\/questions\/79724380\/intellij-modules-not-finding-generated-protobuf-classes-in-a-multimodule-maven-p",
    "text":"I have a project set up like this: ``` protos\/ # protobuf files here pom.xml # parent pom services\/common # this module generates the protobuf classes services\/mod-1 # imports common, but in intellij only protobuf classes not found services\/mod-2 # imports common, but in intellij only protobuf classes not found ``` My protobuf generator looks like this in ``` services\/common\/pom.xl ``` ``` <plugin> <groupId>io.github.ascopes<\/groupId> <artifactId>protobuf-maven-plugin<\/artifactId> <version>3.4.1<\/version> <configuration> <protocVersion>4.31.1<\/protocVersion> <!-- <protocArtifact>com.google.protobuf:protoc:4.31.1:exe:${os.detected.classifier}<\/protocArtifact>--> <!-- <outputDirectory>${project.build.directory}\/generated-sources\/protobuf\/java<\/outputDirectory>--> <sourceDirectories> <sourceDirectory>${project.basedir}\/..\/..\/protos<\/sourceDirectory> <\/sourceDirectories> <registerAsCompilationRoot>true<\/registerAsCompilationRoot> <\/configuration> <executions> <execution> <goals> <goal>generate<\/goal> <\/goals> <\/execution> <\/executions> <\/plugin> ``` Now when I run ``` mvn compile ``` , all the modules resolve the protobuf classes and everything compiles correctly. But in intellij, ``` mod-1 ``` and ``` mod-2 ``` cannot resolve the protobuf classes. What am I doing wrong?",
    "author_id":4475,
    "publication_date":1754274160000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Paul C",
    "author_reputation":8419.0,
    "tags":"java, maven, protocol-buffers, intellij-idea, generated-code",
    "text_length":1287,
    "title_length":117,
    "num_tags":5
  },
  {
    "id":6250,
    "title":"rakudo-star-2025.06.1 install failed",
    "link":"https:\/\/stackoverflow.com\/questions\/79724383\/rakudo-star-2025-06-1-install-failed",
    "text":"I have Linux kernel 5.15.0-94-generic. I downloaded rakudo-star-2025.06.1.tar.gz. I gunzip and tar xf in a directory, and in this directory, I do \".\/bin\/rstar install\" as I did with previous verions of Raku, but I got failure this time: ``` fatal: not in a git directory [2025-08-03T17:07:17] [INFO] Installing Raku in \/Perl6\/rakudo-star-2025.06.1 [2025-08-03T17:07:17] [INFO] Starting build on MoarVM [2025-08-03T17:07:17] [NOTIC] Using \/Perl6\/rakudo-star-2025.06.1\/tmp\/tmp.wvyz3khDYY as working directory cp: cannot stat '\/Perl6\/rakudo-star-2025.06.1\/src\/moarvm-2025.06.1\/MoarVM-2025.06.1\/.': No such file or directory [2025-08-03T17:07:17] [ALERT] Build failed! ``` In my current directory, I have \"archive bin etc lib LICENSE MANIFEST.txt README.md RELEASE src tmp tools\", and in the \"src\" directory, I have \"moarvm-2025.06 nqp-2025.06.1 rakudo-2025.06.1 rakudo-star-modules\". Should I make a symbolic link from moarvm-2025.06 to moarvm-2025.06.1 , and from src\/moarvm-2025.06\/MoarVM-2025.06\/ to src\/moarvm-2025.06\/MoarVM-2025.06.1 ??",
    "author_id":5736,
    "publication_date":1754274586000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"lisprogtor",
    "author_reputation":5797.0,
    "tags":"build, raku, rakudo-star",
    "text_length":1038,
    "title_length":36,
    "num_tags":3
  },
  {
    "id":6249,
    "title":"pandas pivot_table: can aggfunc work over a different grouping period from the table?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724387\/pandas-pivot-table-can-aggfunc-work-over-a-different-grouping-period-from-the-t",
    "text":"I have a pandas pivot table that shows payments made to different payees vs date, and I'm using a Grouper to group them into months, e.g.: ``` payee payee_1 payee_2 date 2019-11-30 amount amount 2019-12-31 amount amount 2020-01-31 amount amount 2020-02-29 amount amount ``` I can use the standard aggfunc=\"sum\" to show the \"amounts\" as the sum for each month, but what I want is to show the sum for 12 months up to each month. Is a custom aggregation function a way to do that? If so, how do I get the values to supply to the function? This is a snippet of raw the data frame: ``` date payee value 0 2020-10-13 payee_1 -43.74 1 2025-01-26 payee_1 -2.03 2 2022-04-08 payee_2 -9.54 3 2021-06-05 payee_1 -16.06 4 2021-12-08 payee_2 -20 ``` And my pivot table definition: ``` pd.pivot_table( df, values=\"value\", index=pd.Grouper( freq=\"ME\", key=\"date\" ), columns=\"payee\", aggfunc=\"sum\", ) ```",
    "author_id":5735,
    "publication_date":1754274892000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Paul Worrall",
    "author_reputation":13.0,
    "tags":"python, pandas",
    "text_length":888,
    "title_length":85,
    "num_tags":2
  },
  {
    "id":6248,
    "title":"How do we define the view model store to be used in a stand alone composable",
    "link":"https:\/\/stackoverflow.com\/questions\/79724388\/how-do-we-define-the-view-model-store-to-be-used-in-a-stand-alone-composable",
    "text":"I am trying to migrate the bottom sheet set up to use Compose. While I can use the Compose's ModalBottomSheet for this purpose, I have the following questions: My ViewModel's view store owner is the fragment itself. How do I set up the viewmodel in such a way that it has a different view model owner What changes do I need to do it to support the use case? SETUP ``` class MyBottomSheet: BottomSheetDialogFragment() { @Inject lateinit var viewModelFactoryProvider: MyViewModelFactoryProvider private val myViewModel: MyViewModel by viewModels { viewModelFactoryProvider } override fun onCreateView( inflater: LayoutInflater,viewgroup: ViewGroup?,bundle: Bundle? ): View { val binding = DataBindingUtil.inflate<MyBottomSheetBinding>( inflater, R.layout.my_bottom_sheet, null, false ) viewModel.myResponseLiveData.observe(viewModelLifecycleOwner) { myResponse -> binding.response = myResponse binding.executePendingBindings() } return binding.root } companion object { @JvmStatic fun newInstance(myParams: MyParams) : MyBottomSheet = val myBottomSheet = MyBottomSheet().apply { arguments = bundleOf(\"myParams\" to myParams) } } } ``` The data class is as follows ``` @Parcelize data class MyParams( . . . . parameters . . . . ): Parceable ``` The viewModel implementation is as follows ``` class MyViewModel @Inject constructor( @FragmentArgumentsQualifier bundle: Bundle \/\/ Fragment arguments myRepository: MyRepository ): ViewModel() { init { invokeRepository() } private val _myResponseLiveData<Response> = MutableLiveData<Response>() private val responseLiveData: LiveData<Response> get() = _myResponseLiveData fun invokeRepository() { val myParams = bundle.getParcelable(myParams, MyParams::class.java) ?: return viewModelScope.launch(Dispatchers.IO) { myRepostory.fetchData(myParams).collect { _myResponseLiveData.value = it } } } } ``` This is how I am invoking the fragment ``` class MyActivity: AppCompatActivity() { override fun onCreate(savedInstance: Bundle?) { AndroidInjection.inject(this) \/\/ Inject the dependency val binding = DataBindingUtil.bind<MyActivityBinding>(findViewById(R.id.my_activity)) val currentFragment = supportFragmentManager.findFragmentByTag(BOTTOM_SHEET_TAG) as? MyBottomSheet val isShown = if (currentFragment?.isVisible() == false) { supportingFragmentManager.commit { add(R.id.fragmentContainer, MyBottomSheet()) setTransition(FragmentTransaction.TRANSIT_FRAGMENT_OPEN) addToBackStack(\"bottomSheet\") } } return binding.root } } ```",
    "author_id":5734,
    "publication_date":1754274911000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Kartik",
    "author_reputation":2619.0,
    "tags":"android-jetpack-compose, android-fragments, android-databinding",
    "text_length":2469,
    "title_length":76,
    "num_tags":3
  },
  {
    "id":6247,
    "title":"How to close TopLevel from root window",
    "link":"https:\/\/stackoverflow.com\/questions\/79724392\/how-to-close-toplevel-from-root-window",
    "text":"Some alternative to GLOBAL? Close Toplevel from Toplevel buttom is so easy Close Toplevel from Root windows, not sure if use GLOBAL is the best ``` import tkinter as tk class EntryForm(tk.Toplevel): def __init__(self, master): super().__init__(master) self.geometry('200x200') tk.Label(master=self, text=\"Say Hello\").pack() tk.Button(self, text=\"Close this\", command=self.destroy).pack() def on_top_window_close(self): self.destroy() root= tk.Tk() root.geometry('200x200') def open(): global new_win new_win = EntryForm(root) def close(): new_win.on_top_window_close() tk.Button(root, text= \"Enter Data\", command=lambda: open()).pack(pady=10) tk.Button(root, text= \"close Data\", command=lambda: close()).pack(pady=10) root.mainloop() ``` -Thanks-",
    "author_id":5733,
    "publication_date":1754276489000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Alberto Ram&#243;n",
    "author_reputation":35.0,
    "tags":"python, python-3.x, tkinter, global",
    "text_length":746,
    "title_length":38,
    "num_tags":4
  },
  {
    "id":6246,
    "title":"Can I make Robocopy copy files that are in use just like xcopy?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724394\/can-i-make-robocopy-copy-files-that-are-in-use-just-like-xcopy",
    "text":"I'm trying to copy a folder with 400,000 files over a network regularly overnight. I have tried using xcopy but this is very slow, and has never completed. Note I can backup fine using the AWS CLI, this completes in good time, so it is technically possible, afaik the AWS CLI runs multithreaded. Now I have tried using robocopy because it is multithreaded and it does indeed work faster at attempting to copy files. But I am copying files from a backup directory and these files are permanently marked as \"in use\" by the server application (FileMaker server is the application). I get this error with robocopy: 2025\/08\/04 10:24:43 ERROR 32 (0x00000020) Copying Directory xxxx The process cannot access the file because it is being used by another process. xcopy will copy the files fine without error. Is there a way to make robocopy copy the files even if they are in use? This is the command that I am currently using: robocopy \"e:...\" \"Y:\" \/s \/MIR \/mt Note that FileMaker Server is creating the backup, but Robocopy considers the backup folder to be locked despite the fact that no backup is in progress.",
    "author_id":5732,
    "publication_date":1754276794000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Schwarz Software",
    "author_reputation":1546.0,
    "tags":"robocopy, xcopy",
    "text_length":1107,
    "title_length":63,
    "num_tags":2
  },
  {
    "id":6245,
    "title":"Is there a way to retrieve a compileinfo file from when the core dump was created",
    "link":"https:\/\/stackoverflow.com\/questions\/79724395\/is-there-a-way-to-retrieve-a-compileinfo-file-from-when-the-core-dump-was-create",
    "text":"I have a PLC that had crashed over night. The system created core dump file, but I never needed to debug this before, so for some odd reason (we had other issues and my head was not really in the right state of mind) I clicked \"activate configuration\" without looking about the issue prior. This probably caused a new compileinfo file to be generated. When I attempt to load the core file it now says that the compile info has changed and the core dump file can't be used. I looked in the development environment's project folder ``` _compileinfo ``` and the folder has only the most recent file. Unfortunately the PLC code is located on the machine's IPC for ease of use\/upgrades etc... so I do not have the last compile info in my git history either. Basically my question is, can I somehow debug the core dump file without the correct compileinfo file? Any help would be greatly appreciated. Edit: I have now also enabled the \"core dump\" in the project setting as it will also create the related compileinfo file.",
    "author_id":5731,
    "publication_date":1754276876000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"ziga",
    "author_reputation":441.0,
    "tags":"plc, twincat",
    "text_length":1016,
    "title_length":81,
    "num_tags":2
  },
  {
    "id":6244,
    "title":"Azure Synapse spark pool throwing hostname not trusted error",
    "link":"https:\/\/stackoverflow.com\/questions\/79724407\/azure-synapse-spark-pool-throwing-hostname-not-trusted-error",
    "text":"I'm trying to read from Azure kusto source using below code in Azure synapse notebook. ``` df = spark.read \\ .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\ .option(\"kustoCluster\", cluster) \\ .option(\"kustoDatabase\", database) \\ .option(\"kustoQuery\", query) \\ .option(\"kustoAadManagedIdentityClientId\", client_id) \\ .option(\"kustoAadAuthorityID\", tenant_id) \\ .option(\"authType\", \"ManagedIdentity\") \\ .option(\"readMode\", \"ForceDistributedMode\") \\ .load() ``` Cluster endpoint is custom one like test.contoso.core.windows.net . The synapse throws hostname not trusted error. It also provides this link to allow host names other than default kusto.windows.net . But I don't find a way in synapse to add this",
    "author_id":4415,
    "publication_date":1754278843000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"DxG",
    "author_reputation":221.0,
    "tags":"azure-synapse, azure-data-explorer, azure-synapse-analytics, azure-synapse-pipeline",
    "text_length":715,
    "title_length":60,
    "num_tags":4
  },
  {
    "id":6243,
    "title":"How to run Injective in Docker without x86_64 error?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724413\/how-to-run-injective-in-docker-without-x86-64-error",
    "text":"After extracting an injective release, on Mac (Sierra, with M1 chip), I would like to run it via docker. However, I get an error: ``` ERROR: Dynamic loader not found: \/lib64\/ld-linux-x86-64.so.2 ``` How can I solve this? Full details: After extracting the release into the current directtory, I run the following commands: ``` docker buildx build --platform=linux\/x86_64 -t injective-release-verify . docker run --platform=linux\/x86_64 injective-release-verify ``` The ``` Dockerfile ``` in the current directory is: ``` FROM alpine:latest COPY libwasmvm.x86_64.so \/usr\/lib\/libwasmvm.x86_64.so COPY injectived \/usr\/bin\/injectived COPY peggo \/usr\/bin\/peggo RUN chmod +x \/usr\/bin\/injectived RUN chmod +x \/usr\/bin\/peggo CMD [\"sh\", \"-c\", \"echo 'injectived version' && \/usr\/bin\/injectived version && echo 'peggo version' && \/usr\/bin\/peggo version\"] ``` The full output is: ``` [+] Building 2.6s (11\/11) FINISHED => [internal] load build definition from Dockerfile => => transferring dockerfile: 365B => [internal] load metadata for docker.io\/library\/alpine:latest => [internal] load .dockerignore => => transferring context: 2B => [1\/6] FROM docker.io\/library\/alpine:latest@sha256:4bcff63911fcb4448bd4fdacec207030997caf25e9bea4045fa6c8c44de311d1 => [internal] load build context => => transferring context: 102B => CACHED [2\/6] COPY libwasmvm.x86_64.so \/usr\/lib\/libwasmvm.x86_64.so => CACHED [3\/6] COPY injectived \/usr\/bin\/injectived => CACHED [4\/6] COPY peggo \/usr\/bin\/peggo => CACHED [5\/6] RUN chmod +x \/usr\/bin\/injectived => CACHED [6\/6] RUN chmod +x \/usr\/bin\/peggo => exporting to image => => exporting layers => => writing image sha256:e4779e1623a7d93783feeb49f86d45b1c596fc69e0207461c14f55919431748a => => naming to docker.io\/library\/injective-release-verify injectived version OrbStack ERROR: Dynamic loader not found: \/lib64\/ld-linux-x86-64.so.2 This usually means that you're running an x86 program on an arm64 OS without multi-arch libraries. To fix this, you can: 1. Use an Intel (amd64) container to run this program; or 2. Install multi-arch libraries in this container. This can also be caused by running a glibc executable in a musl distro (e.g. Alpine), or vice versa. For more details and instructions, see https:\/\/orb.cx\/multiarch ``` (Note that I'm using ``` docker ``` via ``` orbstack ``` installed via ``` brew ``` )",
    "author_id":5730,
    "publication_date":1754279762000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"bguiz",
    "author_reputation":28429.0,
    "tags":"docker, injective",
    "text_length":2333,
    "title_length":52,
    "num_tags":2
  },
  {
    "id":6242,
    "title":"What’s the difference between the “Float” and “Window” view modes for tool windows in JetBrains IDEs?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724420\/what-s-the-difference-between-the-float-and-window-view-modes-for-tool-windo",
    "text":"I can detach any tool window (e.g. Run, Terminal) by picking View Mode ▸ Float or View Mode ▸ Window from the gear icon. Both put the panel outside the main frame, but I can’t figure out how they actually differ. On MacOS X with Webstorm 2025.1: “Float” still looks like a separate window. “Window” … also looks like a separate window. What’s the functional difference between these two modes?",
    "author_id":5729,
    "publication_date":1754280588000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Archulan",
    "author_reputation":650.0,
    "tags":"jetbrains-ide, user-interface, intellij-idea, webstorm",
    "text_length":393,
    "title_length":101,
    "num_tags":4
  },
  {
    "id":6241,
    "title":"Multiple flutter BLoCs communication",
    "link":"https:\/\/stackoverflow.com\/questions\/79724422\/multiple-flutter-blocs-communication",
    "text":"I would like to chain two BLoCs. My app supports multiple accounts. Therefore, I have a global BLoC that contains the currently selected account ( GlobalAppCubit with GlobalAppState ). Then I have several pages in the app, each of which uses its own BLoC to implement the logic on the page, e.g. FileCubit with FileState . Now here's the problem: I want to be able to switch accounts in the app. This would require all pages and their BLoCs to adapt when the global state changes. I tried the following to achieve this: ``` Widget build(BuildContext context) { return BlocBuilder<GlobalAppCubit, GlobalAppState>( builder: (BuildContext context, GlobalAppState globalAppState) { return BlocProvider<FileCubit>( create: (_) => FileCubit()..openFile(initialFile: globalAppState.account.file), child: BlocConsumer<FileCubit, FileState>( builder: (BuildContext context, FileState state) { return MyWidget(state.file); } ) ); } ); } ``` In the debugger, I can see that the outer BlocBuilder<GlobalAppCubit, GlobalAppState> is called with the new state. However, the inner BlocProvider<FileCubit> is not interested in this. openFile is never called again. Is it possible to recreate the inner BlocProvider? Or how can a new event be triggered there?",
    "author_id":5728,
    "publication_date":1754280715000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"MisterK",
    "author_reputation":1.0,
    "tags":"flutter, bloc, chaining",
    "text_length":1242,
    "title_length":36,
    "num_tags":3
  },
  {
    "id":6240,
    "title":"How to programmatically generate plotly (or ggplotly) charts inside Quarto using loops or functions?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724433\/how-to-programmatically-generate-plotly-or-ggplotly-charts-inside-quarto-using",
    "text":"I'm trying to create multiple plotly charts inside a Quarto panel-tabset using a loop or function, but the charts either don't render at all or appear outside the tabset structure. Here's my code: ``` --- title: \"Dynamic Plotly Charts in Tabs\" format: html execute: echo: false --- ```{r} #| results: 'asis' library(ggplot2) library(plotly) library(purrr) cat(\"::: {.panel-tabset}\\n\\n\") walk(1:3, function(i) { cat(paste0(\"## Tab \", i, \"\\n\\n\")) p <- ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point() print(ggplotly(p)) cat(\"\\n\\n\") }) cat(\":::\") ``` The tabs are created but the charts are either blank or rendered outside the tabset. This works fine with static ggplot2 charts, but fails with plotly. How can I properly render plotly charts when generating content programmatically in Quarto?",
    "author_id":5727,
    "publication_date":1754281586000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"fahmy",
    "author_reputation":3662.0,
    "tags":"r, quarto, ggplotly",
    "text_length":794,
    "title_length":100,
    "num_tags":3
  },
  {
    "id":6239,
    "title":"Microsoft Azure Sample for Custom Avatar: How to add region &amp; API Key for it?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724438\/microsoft-azure-sample-for-custom-avatar-how-to-add-region-api-key-for-it",
    "text":"See the chat.html example at https:\/\/github.com\/Azure-Samples\/cognitive-services-speech-sdk\/tree\/master\/samples\/js\/browser\/avatar Basically, one can point to different APIs & endpoints with their keys on Microsoft Azure & test an implementation before building the full solution. This is just a pure html sample, just to see all the parts in action, in an interactive chat with an avatar, & not really for production. The chat.html page, at the bottom, has \"custom avatar\". It is missing an API Key and a region. Without this, the custom avatar is not going to be accessible (if we change \"lisa\" to an avatar that we have trained & deployed, for example) So, the question is what do we need to change in the javascript files js\/chat.js and js\/basic.js to send the region & the API key? Thanks!",
    "author_id":5726,
    "publication_date":1754282160000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"SSMan74",
    "author_reputation":1.0,
    "tags":"azure, text-to-speech, speech-to-text, avatar",
    "text_length":793,
    "title_length":81,
    "num_tags":4
  },
  {
    "id":6238,
    "title":"Import multiple csv in parallel",
    "link":"https:\/\/stackoverflow.com\/questions\/79724440\/import-multiple-csv-in-parallel",
    "text":"I want to imports every csv file from the working directory into R. I have seen multiple posts on stackoverflow that discuss optimizing imports (e.g. using fread) have not something which directly targets my question (maybe there is but I have not seen it yet). For example, I thought that perhaps I could have all my cores (with the exception of 1) import pre-assigned files into a pre-defined list in parallel. ``` library(parallel) csv_files <- list.files(pattern = \"\\\\.csv$\", full.names = TRUE) ncores <- detectCores() - 1 cl <- makeCluster(ncores) clusterEvalQ(cl, library(data.table)) # file_list <- parLapply(cl, csv_files, function(file) { fread(file) }) stopCluster(cl) names(file_list) <- basename(csv_files) ``` Is this a common approach in R?",
    "author_id":5725,
    "publication_date":1754282631000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"farrow90",
    "author_reputation":917.0,
    "tags":"r",
    "text_length":754,
    "title_length":31,
    "num_tags":1
  },
  {
    "id":6237,
    "title":"I am dumping a bunch of polygons to kml using writeVector in terra in R, but I am losing the polygon names, how can I keep them?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724446\/i-am-dumping-a-bunch-of-polygons-to-kml-using-writevector-in-terra-in-r-but-i-a",
    "text":"I am working in R, using terra, and I have created some geographical polygons and given a name to each. I want to dump them to a shapefile so I can load them into ArcGIS. My current approach is to use this snippet: ``` this_multipolygon = lapply(these_polygons, as.polygons) |> vect() writeVector(this_multipolygon, 'filename.kml'), options=\"ENCODING=UTF-8\", overwrite=TRUE) ``` This does produce a KML, but the polygon names fall off and are replaced with generated strings like the 'filename.50' in the below: ``` <Placemark id=\"filename.50\"> <Style><LineStyle><color>ff0000ff<\/color><\/LineStyle><PolyStyle><fill>0<\/fill><\/PolyStyle><\/Style> <MultiGeometry><Polygon><outerBoundaryIs><LinearRing><coordinates>168.793,-44.942 168.783,-44.932 168.793,-44.922 168.839,-44.892 168.869,-44.912 168.879,-44.922 168.869,-44.932 168.803,-44.946 168.793,-44.942<\/coordinates><\/LinearRing><\/outerBoundaryIs><\/Polygon><\/MultiGeometry> <\/Placemark> ``` How can I preserve my polygon names into the KML, instead of getting auto-generated strings like the 'filename.50' above? Or, might some other type of shapefile work better? Thanks Brian",
    "author_id":5724,
    "publication_date":1754283603000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Brian Bull",
    "author_reputation":1.0,
    "tags":"r, terra, shapefile, arcgis, kml",
    "text_length":1128,
    "title_length":128,
    "num_tags":5
  },
  {
    "id":6236,
    "title":"ld: symbol(s) not found for architecture x86_64, clang: error: linker command failed with exit code 1",
    "link":"https:\/\/stackoverflow.com\/questions\/79724447\/ld-symbols-not-found-for-architecture-x86-64-clang-error-linker-command-fa",
    "text":"I've been using the library SPHinxsys on my MacOS Intel. It worked in the past. However, just recently this error always appears when building: ``` Linking CXX executable t..._ck\/bin\/test_0d_regression_test_c FAILED: [code=1] ld: symbol(s) not found for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) [456\/614] Building CXX object test...d_twisting_rigid_elastic_bar.cpp. ninja: build stopped: subcommand failed. ``` This is the command I use to build: ``` git clone https:\/\/github.com\/Xiangyu-Hu\/SPHinXsys.git sphinxsys cd sphinxsys cmake -G Ninja \\ -D CMAKE_BUILD_TYPE=Release \\ -D CMAKE_C_COMPILER=clang -D CMAKE_CXX_COMPILER=clang++ \\ -D CMAKE_TOOLCHAIN_FILE=\"$HOME\/vcpkg\/scripts\/buildsystems\/vcpkg.cmake\" \\ -D CMAKE_C_COMPILER_LAUNCHER=ccache -D CMAKE_CXX_COMPILER_LAUNCHER=ccache \\ -S . \\ -B .\/build cmake --build build\/ ``` I've tried reinstalling everything and installing ``` .\/vcpkg remove spdlog --recurse ``` and ``` .\/vcpkg install spdlog ``` , they don't work out. (Edit) I've also tried to look up into the CMakeLists.txt and play around with these combinations ``` find_package(spdlog CONFIG REQUIRED) target_link_libraries(sphinxsys_core INTERFACE spdlog::spdlog_header_only) ``` or ``` find_package(spdlog CONFIG REQUIRED) target_link_libraries(sphinxsys_core INTERFACE spdlog) ``` and\/or ``` find_package(fmt CONFIG REQUIRED) target_link_libraries(sphinxsys_core INTERFACE fmt::fmt) ``` Is this something that I can fix or should I report this to the source code base? I'm very new with C++, so any advice will be very helpful. The full error code is as below. ``` [447\/614] Linking CXX executable t..._ck\/bin\/test_0d_regression_test_c FAILED: [code=1] tests\/2d_examples\/2d_examples_ck\/test_0d_regression_test_ck\/bin\/test_0d_regression_test_ck : && \/usr\/bin\/clang++ -O3 -DNDEBUG -Wl,-search_paths_first -Wl,-headerpad_max_install_names tests\/2d_examples\/2d_examples_ck\/test_0d_regression_test_ck\/CMakeFiles\/test_0d_regression_test_ck.dir\/regression_test.cpp.o -o tests\/2d_examples\/2d_examples_ck\/test_0d_regression_test_ck\/bin\/test_0d_regression_test_ck -F\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX13.0.sdk\/System\/Library\/Frameworks src\/libsphinxsys_2d.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libSimTKsimbody.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libSimTKmath.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libSimTKcommon.a -framework Accelerate -lm -ldl -ldl -lm \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libtbb.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libtbbmalloc.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libboost_program_options.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libboost_container.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libfmt.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/manual-link\/libgtest_main.a \/Users\/isselinm28\/Documents\/cse\/biofluid\/vcpkg\/installed\/x64-osx\/lib\/libgtest.a && : Undefined symbols for architecture x86_64: \"fmt::v11::detail::vformat_to(fmt::v11::detail::buffer<char>&, fmt::v11::basic_string_view<char>, fmt::v11::basic_format_args<fmt::v11::context>, fmt::v11::detail::locale_ref)\", referenced from: void spdlog::logger::log_<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(spdlog::source_loc, spdlog::level::level_enum, fmt::v11::basic_string_view<char>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) in regression_test.cpp.o void spdlog::logger::log_<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string_view<char, std::__1::char_traits<char> > >(spdlog::source_loc, spdlog::level::level_enum, fmt::v11::basic_string_view<char>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&, std::__1::basic_string_view<char, std::__1::char_traits<char> >&&) in regression_test.cpp.o void spdlog::logger::log_<unsigned long&, unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(spdlog::source_loc, spdlog::level::level_enum, fmt::v11::basic_string_view<char>, unsigned long&, unsigned long&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) in regression_test.cpp.o void spdlog::logger::log_<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string_view<char, std::__1::char_traits<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(spdlog::source_loc, spdlog::level::level_enum, fmt::v11::basic_string_view<char>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&, std::__1::basic_string_view<char, std::__1::char_traits<char> >&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) in regression_test.cpp.o void spdlog::logger::log_<std::__1::basic_string_view<char, std::__1::char_traits<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(spdlog::source_loc, spdlog::level::level_enum, fmt::v11::basic_string_view<char>, std::__1::basic_string_view<char, std::__1::char_traits<char> >&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) in regression_test.cpp.o spdlog::details::full_formatter::format(spdlog::details::log_msg const&, tm const&, fmt::v11::basic_memory_buffer<char, 250ul, fmt::v11::detail::allocator<char> >&) in libsphinxsys_2d.a(io_log.cpp.o) spdlog::details::c_formatter<spdlog::details::scoped_padder>::format(spdlog::details::log_msg const&, tm const&, fmt::v11::basic_memory_buffer<char, 250ul, fmt::v11::detail::allocator<char> >&) in libsphinxsys_2d.a(io_log.cpp.o) ... ld: symbol(s) not found for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation) [456\/614] Building CXX object test...d_twisting_rigid_elastic_bar.cpp. ninja: build stopped: subcommand failed. ```",
    "author_id":5723,
    "publication_date":1754283980000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"essy.lin",
    "author_reputation":47.0,
    "tags":"c++, cmake, linker, ninja",
    "text_length":6237,
    "title_length":101,
    "num_tags":4
  },
  {
    "id":6235,
    "title":"Cannot download .APK to Samsung Watch (Galaxy 8) using either Android Studio or ADB.EXE",
    "link":"https:\/\/stackoverflow.com\/questions\/79724450\/cannot-download-apk-to-samsung-watch-galaxy-8-using-either-android-studio-or",
    "text":"I was unable to perform a download to an unused (direct from factory) Samsung watch either using Android Studio or ADB.EXE. I hope this matter can be resolved in that it appears now impossible to download any new software to a Samsung watch, outside of using the Play Store. This is the key test I performed: 1. Paired my laptop with my new Samsung watch using ADB.EXE (see below for details on the watch, ADB.EXE and laptop) 2. Paring was successful, screenshot available on request 3. Attempted to install new software and install failed with message \"No devices\/emulators found,\" screenshot available on request 4. After error message was produced, I verified that the device was successfully paired, screenshot available on request 5 After error message was produced, I verified that wireless debugging was enabled, screenshot available on request It is my hope you can resolve this issue as I am unable to load software to my watch, and development has come to a halt. Searching the web, I have not found any alternative way to load software to my watch. Note that Android Studio does not pair to a watch either with my Hewlett Packard: 14-dq0011dx or my Dell Precision 5520 (see link for details https:\/\/www.productindetail.com\/pn\/dell-precision-5520#google_vignette ). If there is any alternate way to load software to my watch, please let me know. Please let me know if there is any additional data or testing you need. Thanks A few notes: I have tried this multiple times, and all attempts have failed I am about 6 feet from the Wi-Fi access point Android Studio and ADB was able to download software to a watch about a month ago, this may mean that an automatic update in windows created an incompatibility I suspect this is not related to a Windows firewall as the pair was successful, all windows updates are current. Samsung Watch details: Product Samsung Galaxy Watch 8, Model SM-L32-, Serial RFAY71H97AF, One UI version 8.0 Watch, Software Version L320XXU1AYFE, System version 16, ear OS version 6.0 Wear core services 1.23.87.732814568, Security software version ASKS v8.2 Release 20250501, Android security Patch level May1, 2025 ADB.EXE details: Android Studio Meerkat Feature Drop | 2024.3.2 Build #AI-243.25659.59.2432.13423653, built on April 29, 2025 Laptop details: Hewlett Packard: 14-dq0011dx product ID 7FU46UA#ABA see following link for details https:\/\/support.hp.com\/us-en\/product\/product-specs\/model\/29205192 Successful download.",
    "author_id":5722,
    "publication_date":1754284368000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"John Poust",
    "author_reputation":11.0,
    "tags":"android-studio, wear-os",
    "text_length":2458,
    "title_length":87,
    "num_tags":2
  },
  {
    "id":6234,
    "title":"Value to use for smoothing for RBFInterpolator from scipy.interpolate",
    "link":"https:\/\/stackoverflow.com\/questions\/79724451\/value-to-use-for-smoothing-for-rbfinterpolator-from-scipy-interpolate",
    "text":"What is an appropriate value to use for the ``` smoothing ``` input to ``` RBFInterpolator ``` from ``` scipy.interpolate ``` ? The documentation says a value >0 will result in smoothing but doesn't give much more detail than that.",
    "author_id":5721,
    "publication_date":1754284475000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Christopher Pratt",
    "author_reputation":197.0,
    "tags":"python, scipy",
    "text_length":231,
    "title_length":69,
    "num_tags":2
  },
  {
    "id":6233,
    "title":"NCU having trouble exiting a Python program",
    "link":"https:\/\/stackoverflow.com\/questions\/79724454\/ncu-having-trouble-exiting-a-python-program",
    "text":"I was running ``` ncu ``` on a remote A100 server to profile kernel activities. However, no matter what metric sets I used, the program just couldn't exit properly. For example, with the following command, I could get the ``` output.ncu-rep ``` correctly, the program also executed the last line of code I wrote, but it just couldn't exit. The entire command just hung forever unless I hit ``` ctrl-c ``` . P.S. You may change \"roofline\" to any metric set, e.g. basic, full or whatever. The same hanging would happen. ``` ncu --set roofline -o output -f -- python <my_prog> <my_args...> ==PROF== Connected to process 6603 ==PROF== Profiling <kernel_1> ==PROF== Profiling <kernel_2> ... ==PROF== Disconnected from process 6603 # hanging here ``` I also tried to run this command from my laptop's ``` NVIDIA Nsight Compute ``` , where I could connect to a remote server and start a profile activity remotely from NVIDIA's GUI tool. However, the same issue occurred -- the process log window of ``` Nsight Compute ``` also just hung. Moreover, after I force-killed this hanging program, if I ran another python program that called CUDA API (e.g. ``` torch.cuda.synchronize() ``` ), this program wouldn't be able to exit either. The only way I could resolve this hanging issue for subsequent executions was reboot the entire system. It is worth noting that if I profiled a super tiny program with just a single CUDA kernel at the very beginning (when hanging had not occurred since the last reboot), the program would exit properly. But my program would launch several hundreds kernels, and this would cause hanging. Why did this hanging happen? Was that a bug or compatibility issue? Test environment: Python 3.10.18 (managed by Anaconda3) vLLM 0.5.3.post1 CUDA 12.1\/12.6\/12.9 NCU 2025.2.1.0 (shared across all CUDA versions) Ubuntu 22.04.5 LTS NVIDIA Nsight Compute 2025.2.0 My first program is a simple MLP from PyTorch and vLLM. I ran this function multiple times with different inputs. In total, there could be several hundreds kernel launches. ``` import vllm import torch import torch.nn.functional as F def test_fn(hidden_states, w_gate_up, w_down): placeholder = F.linear(hidden_states, w_gate_up) dim = placeholder.shape[-1] \/\/ 2 hidden_shape = (placeholder.shape[:-1] + (dim,)) out = torch.empty(hidden_shape, dtype=placeholder.dtype, device=placeholder.device) vllm._custom_ops.silu_and_mul(out, placeholder) # type: ignore F.linear(out, w_down) ``` My second program is a simple CUDA API call to test whether CUDA API could still be used after the hanging issue. The result was, yes, it could be used but would cause the program to hang at exit. ``` import torch torch.cuda.synchronize() ```",
    "author_id":5720,
    "publication_date":1754284841000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Natarich J",
    "author_reputation":427.0,
    "tags":"cuda, pytorch, debugging, vllm, nsight-compute",
    "text_length":2700,
    "title_length":43,
    "num_tags":5
  },
  {
    "id":6232,
    "title":"Pytorch compute graph don&#39;t release as expected",
    "link":"https:\/\/stackoverflow.com\/questions\/79724468\/pytorch-compute-graph-dont-release-as-expected",
    "text":"``` import torch def analyze_your_actual_code(): \"\"\"分析你的实际代码\"\"\" a = torch.randn(4, requires_grad=True) print(f\"a: {a}\") print(f\"a.grad: {a.grad}\") loss = a.sum() loss.backward() print(f\" a.grad: {a.grad}\") loss.backward() print(f\" a.grad: {a.grad}\") analyze_your_actual_code() ``` Why I try to call ``` loss.backward ``` twice, but the gradients still accumulate. The compute graph don't release as expected? Code output: ``` a: tensor([-0.3121, -0.2331, 0.9317, -0.5075], requires_grad=True) a.grad: None a.grad: tensor([1., 1., 1., 1.]) a.grad: tensor([2., 2., 2., 2.]) ```",
    "author_id":5719,
    "publication_date":1754287430000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"chushu zhou",
    "author_reputation":31.0,
    "tags":"python, pytorch, gradient",
    "text_length":575,
    "title_length":51,
    "num_tags":3
  },
  {
    "id":6231,
    "title":"Reinterpret_cast array of bits as byte",
    "link":"https:\/\/stackoverflow.com\/questions\/79724469\/reinterpret-cast-array-of-bits-as-byte",
    "text":"I want to cast an array of bits into a byte, it works by using a loop and bitshift, but I think it would be cost inefective if run repeatedly (I might be wrong). So I wanted to use a pointer with ``` reinterpret_cast ``` . It works when I use it between ``` uint8_t ``` and ``` uint16_t ``` ``` uint16_t a = 32769; uint8_t *aPtr = reinterpret_cast<uint8_t*>(&a); std::cout << (int) a << \" => {\" << (int)*(aPtr) << \", \" << (int)*(aPtr + 1) << \"}\" << std::endl; uint8_t b[2] = {1, 128}; uint16_t *bPtr = reinterpret_cast<uint16_t*>(&b); std::cout << \"{\" << (int)b[0] << \", \" << (int)b[1] << \"} => \" << (int)*bPtr << std::endl; ``` It gives ``` 32769 => {1, 128} {1, 128} => 32769 ``` But when I try it with ``` bool ``` and ``` uint8_t ``` it won't work ``` bool c[8] = {1, 1, 1, 1, 1, 1, 1, 1}; uint8_t *cPtr = reinterpret_cast<uint8_t*>(&c); std::cout << \"{\"; for(int i=0; i<8; i++) std::cout << c[i] << (i < 7 ? \", \" : \"\"); std::cout << \"} => \" << (int)*cPtr << std::endl; uint8_t d = 255; bool *dPtr = reinterpret_cast<bool*>(&d); std::cout << (int)d << \" => {\"; for(int i=0; i<8; i++) std::cout << *(dPtr + i) << (i < 7 ? \", \" : \"\"); std::cout << \"}\" << std::endl; ``` With result ``` {1, 1, 1, 1, 1, 1, 1, 1} => 1 255 => {255, 1, 1, 1, 1, 1, 1, 1} ``` Is it impossible to do it with ``` reinterpret_cast ``` ? or I just do it wrong? or even my guess about loop and bitshift is wrong in the first place?",
    "author_id":5718,
    "publication_date":1754287529000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"A&#39;S",
    "author_reputation":39.0,
    "tags":"c++, c++11",
    "text_length":1406,
    "title_length":38,
    "num_tags":2
  },
  {
    "id":6230,
    "title":"Xlsxwriter custom data labels with custom data label positions",
    "link":"https:\/\/stackoverflow.com\/questions\/79724473\/xlsxwriter-custom-data-labels-with-custom-data-label-positions",
    "text":"We're trying to customize data label positions of individual data points on an xlsxwriter graph but can only work out how to set data label positions for entire series. Simplified reproducible example: ``` import xlsxwriter workbook = xlsxwriter.Workbook(\"chart_line.xlsx\") worksheet = workbook.add_worksheet() headings = [\"Number\", \"Batch 1\"] data = [ [2, 3, 4, 5], [10, 40, 50, 20], ] worksheet.write_row(\"A1\", headings) worksheet.write_column(\"A2\", data[0]) worksheet.write_column(\"B2\", data[1]) chart1 = workbook.add_chart({\"type\": \"line\"}) custom_labels = [ {\"value\": True}, {\"value\": True, \"position\": \"right\"}, {\"value\": True, \"position\": \"left\"}, # where we want to set positions individually {\"value\": True, \"position\": \"above\"}, # these \"position\" keys appear to be ignored ] chart1.add_series( { \"name\": \"=Sheet1!$B$1\", \"categories\": \"=Sheet1!$A$2:$A$5\", \"values\": \"=Sheet1!$B$2:$B$5\", \"data_labels\": {\"value\": True, \"custom\": custom_labels} # we can set positions for all labels here with \"position\": \"left\" } ) worksheet.insert_chart(\"D2\", chart1) workbook.close() ``` We want to set individual points programmatically for multiple series (not shown above) for visual problems avoided here for simplicity. Xlsxwriter's docs state \"The property elements of the custom lists should be dicts with the following allowable keys\/sub-properties: ... value, font, delete\" , which indicates we're out of luck. Is there anything we can do? In a closely related question the Xlsxwriter dev appears to state this isn't possible, but we're wondering if we've misunderstood something or if anything has changed in the five years since.",
    "author_id":5717,
    "publication_date":1754287856000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Chris Dixon",
    "author_reputation":1128.0,
    "tags":"python, excel, xlsxwriter",
    "text_length":1634,
    "title_length":62,
    "num_tags":3
  },
  {
    "id":6229,
    "title":"VB.NET Cookie Mystery",
    "link":"https:\/\/stackoverflow.com\/questions\/79724478\/vb-net-cookie-mystery",
    "text":"I'm working on a legacy app, and I'm a little stumped by the intention behind this: ``` Protected Sub Button1_Click(ByVal sender As Object, ByVal e As System.EventArgs) Handles Button1.Click Request.Cookies(\"userinfo\").Add(\"<key>\", \"<value>\") End Sub Protected Sub Button2_Click(ByVal sender As Object, ByVal e As System.EventArgs) Handles Button2.Click TextBox1.Text = Request.Cookies(\"userinfo\")(\"<key>\") End Sub ``` I get compilation error in ``` Button1_Click ``` : ``` 'Add' is not a member of 'System.Web.HttpCookie'. ``` I should add I don't know much VB.",
    "author_id":5439,
    "publication_date":1754288866000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jevon Kendon",
    "author_reputation":658.0,
    "tags":"vb.net",
    "text_length":562,
    "title_length":21,
    "num_tags":1
  },
  {
    "id":6228,
    "title":"Motif label size not kept when changing XmNlabelString using XtVaSetValues",
    "link":"https:\/\/stackoverflow.com\/questions\/79724480\/motif-label-size-not-kept-when-changing-xmnlabelstring-using-xtvasetvalues",
    "text":"I'm working on a Motif application with a xmPushButtonWidgetClass button and a xmLabelWidgetClass label. I set initial text, position and size in the label: ``` lbl1TextHint = XmStringCreateLocalized(\"Waiting for click\"); lbl1 = XtVaCreateManagedWidget(\"label1\", xmLabelWidgetClass, board, XmNlabelString, lbl1TextHint, XmNx, 240, \/\/ X position XmNy, 20, \/\/ Y position XmNwidth, 200, \/\/ Width XmNheight, 40, \/\/ Height NULL); ``` When I update the label string in the pushbutton callback, the width and height of the label change to fit the string. It keeps the 240, 20 position, but not the set width and height: ``` XmString newLabel = XmStringCreateLocalized(\"Button pressed\"); XtVaSetValues(lbl1, XmNlabelString, newLabel, NULL); ``` I want to change the XmNlabelString of a label with given height & width, First, I tried setting the XmNx, XmNy, XmNwidth, and XmNheight when updating the label. However, now, when I click the pushbutton, it would toggle between setting the width and height of the label change to fit the string (on odd clicks) and 200x40 (on even clicks) ``` XmString newLabel = XmStringCreateLocalized(\"Button pressed\"); XtVaSetValues(lbl1, XmNlabelString, newLabel, XmNx, 240, \/\/ X position XmNy, 20, \/\/ Y position XmNwidth, 200, \/\/ Width XmNheight, 40, \/\/ Height NULL); ``` I then tried calling XtVaSetValues twice, first to set the label string, then to set the label width and height: ``` XmString newLabel = XmStringCreateLocalized(\"Button pressed\"); XtVaSetValues(lbl1, XmNlabelString, newLabel, NULL); XtVaSetValues(lbl1, XmNx, 240, \/\/ X position XmNy, 20, \/\/ Y position XmNwidth, 200, \/\/ Width XmNheight, 40, \/\/ Height NULL); ``` This one works. However, I can't figure out why trying to set only the label's string changes the height & width (maybe that's expected?) or why it only works as expected when I call XtVaSetValues to set width and height after setting the label string. I would think I should only have to call this once (and preferably, not have to set the width and height every time I tried the Motif Programming Manual and referenced the Label Widget and XtVaSetValues , but I don't see anything that fits my problem. Full code: ``` #include <Xm\/PushB.h> \/\/ for PushButton widget #include <Xm\/Label.h> \/\/ for Label widget #include <Xm\/BulletinB.h> \/\/ for BulletinBoard widget #include <stdlib.h> \/\/ for exit() void button_pushed(Widget, XtPointer, XtPointer); \/\/ function for button press void handle_keypress(Widget, XtPointer, XEvent *, Boolean *); \/\/ function for key press handling Widget toplevel, board, btn1, lbl1; \/\/ Global widget variables \/\/ main function int main(int argc, char **argv) { XtAppContext app; XmString btn1Text, lbl1TextHint; \/\/ Inittoolkit, window, & top-level widget XtSetLanguageProc(NULL, NULL, NULL); toplevel = XtVaAppInitialize(&app, \"Hello 2\", NULL, 0, &argc, argv, NULL, NULL); XtVaSetValues(toplevel, XmNwidth, 800, XmNheight, 800, NULL); \/\/ Create a BulletinBoard widget to hold the button & label board = XtVaCreateManagedWidget(\"board\", xmBulletinBoardWidgetClass, toplevel, NULL); \/\/ Create a PushButton widget btn1Text = XmStringCreateLocalized(\"Generate Random String\"); btn1 = XtVaCreateManagedWidget(\"pushme\", xmPushButtonWidgetClass, board, XmNlabelString, btn1Text, XmNx, 20, \/\/ X position XmNy, 20, \/\/ Y position XmNwidth, 200, \/\/ Width XmNheight, 40, \/\/ Height NULL); \/\/ Create a Label widget & set initial label string lbl1TextHint = XmStringCreateLocalized(\"Waiting for click\"); lbl1 = XtVaCreateManagedWidget(\"label1\", xmLabelWidgetClass, board, XmNlabelString, lbl1TextHint, XmNx, 240, \/\/ X position XmNy, 20, \/\/ Y position XmNwidth, 200, \/\/ Width XmNheight, 40, \/\/ Height NULL); XmStringFree(btn1Text); \/\/ Free the button label string after creating the button XmStringFree(lbl1TextHint); \/\/ Free the label string after creating the label XtAddCallback(btn1, XmNactivateCallback, button_pushed, NULL); \/\/ Add callback for button press XtRealizeWidget(toplevel); \/\/ Realize the top-level widget XtAppMainLoop(app); \/\/ Start the main event loop } \/\/ Callback function for button press. void button_pushed(Widget widget, XtPointer client_data, XtPointer call_data) { XmString newLabel = XmStringCreateLocalized(\"Button pressed\"); XtVaSetValues(lbl1, XmNlabelString, newLabel, NULL); XtVaSetValues(lbl1, XmNx, 240, \/\/ X position XmNy, 20, \/\/ Y position XmNwidth, 200, \/\/ Width XmNheight, 40, \/\/ Height NULL); XmStringFree(newLabel); \/\/ Free the XmString after setting it } ``` Update 1: More specifically, my question is: when I call this: ``` XmString newLabel = XmStringCreateLocalized(\"Button pressed\"); XtVaSetValues(lbl1, XmNlabelString, newLabel, XmNx, 240, \/\/ X position XmNy, 20, \/\/ Y position XmNwidth, 200, \/\/ Width XmNheight, 40, \/\/ Height NULL); ``` why does my label widget alternate between the size I specify and just wrapping the string when I call the mathod? Regarding confirming size: I did confirm the change in size by coloring the background using the following (in place of the ``` board ``` value assignment above: ``` Display *display = XtDisplay(toplevel); Colormap colormap = DefaultColormap(display, DefaultScreen(display)); XColor color, exact; XAllocNamedColor(display, colormap, \"black\", &color, &exact); \/\/ Create a BulletinBoard widget to hold the button board = XtVaCreateManagedWidget(\"board\", xmBulletinBoardWidgetClass, toplevel, XmNbackground, color.pixel, \/\/ Set background color NULL); ``` Starting state: (when the Motif application starts) Press button once: (or every odd-numbered press) Press button twice: (or every even-numbered press)",
    "author_id":5716,
    "publication_date":1754289067000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Joshua Penn",
    "author_reputation":53.0,
    "tags":"c, motif",
    "text_length":5577,
    "title_length":74,
    "num_tags":2
  },
  {
    "id":6227,
    "title":"Deserialization to different classes",
    "link":"https:\/\/stackoverflow.com\/questions\/79724486\/deserialization-to-different-classes",
    "text":"I'm poking around the documentation, but I'd also want to reach out each to y'all incase there's a known solution to my issue or deserializing a json into different classes. I have items of different types that I am throwing into one json ``` [ { \"id\": \"sword\", \"viewModelId\": \"sword_01\", \"itemType\": \"Weapon\", \"damage\": \"1d10\" }, { \"id\": \"shield\", \"viewModelId\": \"shield_02\", \"itemType\": \"Armour\", \/\/ Fixed, thank you Yong \"armour\": \"3\" } ] ``` I have a class covering the concept of items ``` [System.Serializable] public class Item { public string id; public string viewModelId; } ``` And one of each of the ItemTypes ``` [System.Serializable] public class Weapon : Item { public string damage; } ``` ``` [System.Serializable] public class Armour : Item { public int armour; } ``` Figuring this out is important to me because down the line I will want potions, and scrolls, and tools, and treasure, and so on and so forth. I'd like to not handle this with one big Item Class, but instead Subclasses I can add ItemType specific variables and methods into. I am also open to alternative approaches, like breaking up each ItemType. That is a file for Weapons, a file for Armor, a file for etc. Thanks",
    "author_id":5715,
    "publication_date":1754289811000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Lord Dragon",
    "author_reputation":23.0,
    "tags":"c#, unity-game-engine, json.net",
    "text_length":1200,
    "title_length":36,
    "num_tags":3
  },
  {
    "id":6226,
    "title":"how to use newMessagesCallback in Qiscus SDK Javascript?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724492\/how-to-use-newmessagescallback-in-qiscus-sdk-javascript",
    "text":"I'm using nuxt js. When I send a comment, the ``` newMessagesCallback ``` is not triggering on sender and receiver. Is there anything that I have missed? I already tried using ``` subscribeEvent ``` or ``` event.on('new-message') ``` but they are not working either. Here is my qiscus.client.js code ``` import { defineNuxtPlugin } from '#app' import QiscusSDK from 'qiscus-sdk-core' export default defineNuxtPlugin(() => { const qiscus = new QiscusSDK() qiscus.debugMode = true qiscus.init({ AppId: 'myappid', options: { loginSuccessCallback: user => { console.log('✅ Login successs:', user) }, loginErrorCallback: err => { console.error('❌ Login error:', err) }, newMessageCallback: messages => { console.log('Callback triggered') }, }, }) console.log('🔁 Qiscus initialized:', qiscus) return { provide: { qiscus, }, } }) ```",
    "author_id":5714,
    "publication_date":1754290598000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Nur Syahrial Maulidi",
    "author_reputation":1.0,
    "tags":"javascript, nuxt.js",
    "text_length":826,
    "title_length":56,
    "num_tags":2
  },
  {
    "id":6225,
    "title":"What is &#39;tex2svg is not a function&#39;?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724493\/what-is-tex2svg-is-not-a-function",
    "text":"I'm a beginner on ts and js. I want to make some plugin for obsidian quartz. It is a pages maker that make md into html and runs a server. In my case, I want tikz codes into svg graphic. So I tried to make some codes exactly same as below. ``` import { QuartzTransformerPlugin } from \"..\/types\" import { visit } from \"unist-util-visit\" import { Root, Code } from \"mdast\" import tex2svg from \"node-tikzjax\" interface Options { enableTikZJax: boolean } export const TikZJax: QuartzTransformerPlugin<Partial<Options>> = (opts) => { const enableTikZJax = opts?.enableTikZJax ?? true return { name: \"TikZJax\", markdownPlugins() { return [ () => { return async (tree: Root) => { if (!enableTikZJax) return const promises: Promise<void>[] = [] visit(tree, \"code\", (node: Code, index, parent) => { if (node.lang === \"tikz\" && parent && typeof index === 'number') { const tex = `${node.value}` const promise = tex2svg(tex) .then((svg: string) => { parent.children[index] = { type: \"html\", value: svg } }) .catch((error: Error) => { console.error('TikZ conversion failed:', error) }) promises.push(promise) } }) await Promise.all(promises) } } ] } } } ``` However, I stuck in error that is 'tex2svg is not a function'. I downloaded all dependencies for node-tikzjax. And below is the md file to transcript into html with svg file. ``` \\usepackage{pgfplots} \\pgfplotsset{compat=1.16} \\begin{document} \\begin{tikzpicture} \\begin{axis}[colormap\/viridis] \\addplot3[ surf, samples=18, domain=-3:3 ] {exp(-x^2-y^2)*x}; \\end{axis} \\end{tikzpicture} \\end{document} ``` Install all dependencies and reinstall Try to change import form.",
    "author_id":5713,
    "publication_date":1754290644000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Minyeop Jang",
    "author_reputation":7.0,
    "tags":"typescript, javascript, obsidian, quartz",
    "text_length":1616,
    "title_length":44,
    "num_tags":4
  },
  {
    "id":6224,
    "title":"Shaka Packager not adding #EXT-X-ENDLIST to HLS playlist after stream ends",
    "link":"https:\/\/stackoverflow.com\/questions\/79724495\/shaka-packager-not-adding-ext-x-endlist-to-hls-playlist-after-stream-ends",
    "text":"I am using Shaka Packager for live streaming with HLS & DASH output. My setup is like this: RTMP stream is published to ``` NGINX-RTMP ``` . On ``` exec ``` , I start a transcoder (FFmpeg) and packager (Shaka Packager). On ``` exec_publish_done ``` , I stop both processes. Here’s the command I use to start Shaka Packager: ``` \"$SHAKA_PACKAGER_BIN\" \\ \"in=${VIDEO_720P_FIFO},stream=video,format=mp4,init_segment=${STREAM_OUTPUT_DIR}\/720p_init.mp4,segment_template=${STREAM_OUTPUT_DIR}\/720p_\\$Number\\$.m4s,bandwidth=3000000,hls_name=720p\" \\ \"in=${VIDEO_480P_FIFO},stream=video,format=mp4,init_segment=${STREAM_OUTPUT_DIR}\/480p_init.mp4,segment_template=${STREAM_OUTPUT_DIR}\/480p_\\$Number\\$.m4s,bandwidth=1500000,hls_name=480p\" \\ \"in=${AUDIO_FIFO},stream=audio,format=mp4,init_segment=${STREAM_OUTPUT_DIR}\/audio_init.mp4,segment_template=${STREAM_OUTPUT_DIR}\/audio_\\$Number\\$.m4s,bandwidth=128000,hls_name=audio\" \\ --hls_master_playlist_output \"${STREAM_OUTPUT_DIR}\/master.m3u8\" \\ --hls_playlist_type LIVE \\ --mpd_output \"${STREAM_OUTPUT_DIR}\/manifest.mpd\" \\ --segment_duration 2 \\ --low_latency_dash_mode=true \\ --availability_time_offset 3 \\ > \"$SHAKA_PKG_LOG\" 2>&1 & ``` And FFmpeg: ``` ffmpeg -y -re -i \"rtmp:\/\/localhost:1935\/live\/${STREAM_KEY}\" \\ -filter_complex \\ \"[0:v]split=2[v720][v480]; \\ [v720]scale=w=1280:h=720,setsar=1[v720out]; \\ [v480]scale=w=854:h=480,setsar=1[v480out]; \\ [0:a]aresample=44100[aout]\" \\ -map \"[v720out]\" -c:v libx264 -b:v 3000k -f mp4 ${VIDEO_720P_FIFO} \\ -map \"[v480out]\" -c:v libx264 -b:v 1500k -f mp4 ${VIDEO_480P_FIFO} \\ -map \"[aout]\" -c:a aac -b:a 128k -f mp4 ${AUDIO_FIFO} \\ > \"$FFMPEG_LOG\" 2>&1 & ``` My NGINX RTMP config triggers these scripts: ``` application live { live on; record off; exec \/start_transcoder_and_packager.sh $name; exec_publish_done \/stop_transcoder_and_packager.sh $name; } ``` Problem: When stream ends, I stop FFmpeg and Shaka Packager like this in ``` stop_transcoder_and_packager.sh ``` : ``` kill -s TERM $FFMPEG_PID kill -s TERM $SHAKA_PID ``` FFmpeg stops fine. But Shaka Packager doesn't finalize the HLS playlist properly — it doesn't write ``` #EXT-X-ENDLIST ``` in variant playlists. The ``` .m3u8 ``` files are left open-ended. If I manually append ``` #EXT-X-ENDLIST ``` to ``` .m3u8 ``` files after stopping the packager: ``` for playlist in \"$HLS_PLAYLISTS_DIR\"\/*.m3u8; do if [[ \"$(basename \"$playlist\")\" != \"master.m3u8\" ]]; then echo \"#EXT-X-ENDLIST\" >> \"$playlist\" fi done ``` The player either stops abruptly or keeps buffering. What I want: The player should be able to play all segments until the end and then stop, like a VOD. Tried Solutions: Sending ``` TERM ``` to Shaka Packager — doesn't finalize playlist. Manually adding ``` #EXT-X-ENDLIST ``` — not working because segments are incomplete. Tried lowering segment duration and buffer depths — no effect on finalize.",
    "author_id":4580,
    "publication_date":1754290837000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Arjit",
    "author_reputation":52.0,
    "tags":"ffmpeg, http-live-streaming, live-streaming, shaka, nginx-rtmp",
    "text_length":2854,
    "title_length":74,
    "num_tags":5
  },
  {
    "id":6223,
    "title":"Hover ring is blocked by image in container",
    "link":"https:\/\/stackoverflow.com\/questions\/79724498\/hover-ring-is-blocked-by-image-in-container",
    "text":"I'm trying to apply a hover ring effect to a container using Tailwind CSS. The ring appears when I hover, but it only shows around the left side of the container. The right side, which contains an image, seems to block or clip the ring effect. ``` <div className=\"h-[175px] w-full bg-white flex items-center cursor-pointer hover:ring-4 ring-inset ring-red-100\"> <div className={`${ image ? \"w-3\/4\" : \"w-full\" } h-full flex flex-col justify-center items-center`} > {typeof children === \"function\" ? children(activeTab) : children} <\/div> {image && ( <div className='w-1\/4 h-full'> <img className=\"h-full object-cover object-bottom\" src={image} alt=\"image\" \/> <\/div> )} <\/div> ``` ``` <script src=\"https:\/\/cdn.jsdelivr.net\/npm\/@tailwindcss\/browser@4\"><\/script> <div class=\"h-[175px] w-full bg-white flex items-center cursor-pointer ring-4 ring-inset ring-orange-500\"> <div class=\"w-3\/4 h-full flex flex-col justify-center items-center\" > Type <\/div> <div class='w-1\/4 h-full'> <img class=\"h-full object-cover object-bottom\" src=\"https:\/\/fastly.picsum.photos\/id\/233\/500\/300.jpg?hmac=SsuqTqFQrkMhgY1XhIgAEC4v4R2CqhaFG3SGbgs76W0\" alt=\"image\" \/> <\/div> <\/div> ```",
    "author_id":5712,
    "publication_date":1754291022000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"BFP",
    "author_reputation":136.0,
    "tags":"reactjs, tailwind-css",
    "text_length":1157,
    "title_length":43,
    "num_tags":2
  },
  {
    "id":6222,
    "title":"Xcode - Xcode Cloud - The project “Project” does not have a remote repository",
    "link":"https:\/\/stackoverflow.com\/questions\/79724503\/xcode-xcode-cloud-the-project-project-does-not-have-a-remote-repository",
    "text":"I have Xcode project setup and the remote Github repo is present, which can also be seen from Xcode git's GUI. Pushing, pulling, all git operations work as expected directly from Xcode. Code is up to date with the origin on Github.. When it comes to setting up Xcode Cloud, operation always fails. Any suggestions what can be wrong? I spent hours on that issue so far with no success I've tried setting origins to both https and ssh but that had no effect: ``` origin https:\/\/github.com\/myGithub\/Project.git (fetch) origin https:\/\/github.com\/myGithub\/Project.git (push) ``` Also worth noting that cloning any existing project from my remote with Xcode doesn't work, but it works flawlessly with Github Desktop GUI",
    "author_id":5711,
    "publication_date":1754291589000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Jakub Gawecki",
    "author_reputation":1257.0,
    "tags":"ios, git, xcode",
    "text_length":713,
    "title_length":77,
    "num_tags":3
  },
  {
    "id":6221,
    "title":"How do I make ng-container work inside a html table?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724505\/how-do-i-make-ng-container-work-inside-a-html-table",
    "text":"The ``` ng-container ``` does not work with ``` ng-repeat ``` inside a table. This is my code. ``` <tbody> <tr> <ng-container style=\"border-bottom: 5px solid black\" ng-repeat=\"ca in cas\"> <td ng-click=\"test(ca)\">works {{ca.property}}<\/td> <\/ng-container> <\/tr> <\/tbody> ``` This code will not work. But if I switch the ``` ng-container ``` to a ``` td ``` element this code will work fine with no changes. The problem is ``` ca ``` is ``` undefined ``` inside ``` ng-container ``` but works inside a ``` td ``` element.",
    "author_id":5710,
    "publication_date":1754291843000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Ginto",
    "author_reputation":33.0,
    "tags":"javascript, html, angular, angularjs, ng-container",
    "text_length":519,
    "title_length":52,
    "num_tags":5
  },
  {
    "id":6220,
    "title":"AFrame 1.7.0 images violate CORS policy, unlike previous versions",
    "link":"https:\/\/stackoverflow.com\/questions\/79724509\/aframe-1-7-0-images-violate-cors-policy-unlike-previous-versions",
    "text":"Textures and images that I have generated simply will not work at all in AFrame 1.7.0. I uploaded to my hosting service, blocked. I uploaded them to my server, blocked. I ran the Python server on my laptop where the original files are in the SAME DIRECTORY and they are blocked. If I use previous AFrame versions, no problem. I created a folder named \"Local\" on my hard driver that contains nothing but these four files: ``` Starter_1_7_0.html Wood.jpg Carpet_013.jpg aframe170.js ``` The ``` aframe170.js ``` file is exactly the downloaded file from the A-Frame installation page (named to set it apart from the other versions). ``` Starter_1_7_0.html ``` is the simple box-cone-sphere example from the A-Frame docs. Here is its code: ``` <html> <head> <script src=\"aframe170.js\"><\/script> <\/head> <body> <a-scene> <a-assets> <img id=\"floor1\" src=\"Carpet013.jpg\"> <img id=\"base\" src=\"Wood.jpg\"> <\/a-assets> <a-box position=\"-1 0.5 -3\" rotation=\"0 45 0\" color=\"#4CC3D9\" src=\"#base\"><\/a-box> <a-sphere position=\"0 1.25 -5\" radius=\"1.25\" color=\"#EF2D5E\"><\/a-sphere> <a-cylinder position=\"1 0.75 -3\" radius=\"0.5\" height=\"1.5\" color=\"#FFC65D\"><\/a-cylinder> <a-plane position=\"0 0 -4\" rotation=\"-90 0 0\" width=\"4\" height=\"4\" color=\"#7BC8A4\"><\/a-plane> <a-sky color=\"#ECECEC\"><\/a-sky> <\/a-scene> <\/body> <\/html> ``` I have added my two textures as assets in the example and assigned the Wood texture with id=\"base\" to the cube. Note that all files are in the same directory. I opened a terminal window in the same directory and started the Python server here: ``` C:\\Users\\thora\\Documents\\Shults Laboratories\\Local\\>python -m http.server Serving HTTP on :: port 8000 (http:\/\/[::]:8000\/) ... ``` The cube renders as black. Using inspect mode in the browser window, in elements I can see the textures. In console, is see: ``` A-Frame Version: 1.7.0 (Date 2025-02-20, Commit #ad5cef10) aframe170.js:42909 THREE Version (https:\/\/github.com\/supermedium\/three.js): 173 aframe170.js:34898 THREE.WebGLState: SecurityError: Failed to execute 'texSubImage2D' on 'WebGL2RenderingContext': The image element contains cross-origin data, and may not be loaded. at Object.texSubImage2D (aframe170.js:34898:1404) at uploadTexture (aframe170.js:34942:659) at WebGLTextures.setTexture2D (aframe170.js:34924:507) at SingleUniform.setValueT1 [as setValue] (aframe170.js:34721:79) at WebGLUniforms.upload (aframe170.js:34812:3) at setProgram (aframe170.js:35211:226) at WebGLRenderer.renderBufferDirect (aframe170.js:35133:83) at renderObject (aframe170.js:35179:2361) at renderObjects (aframe170.js:35179:1409) at renderScene (aframe170.js:35165:2723) ``` Running Win 11 Home V 24H2, Microsoft Edge browser Previous AFrame versions work perfectly. I am using exactly the same references for the textures, both calling them directly or loading them as assets. It fails identically with this message: Access to XMLHttpRequest at 'file:\/\/\/C:\/Users\/thora\/Documents\/Shults%20Laboratories\/VR%20Site\/PREVGALL.png' from origin 'null' has been blocked by CORS policy: Cross origin requests are only supported for protocol schemes: chrome-extension, chrome-untrusted, data, edge, http, https, isolated-app. The HTML5 file I am using is in the exact same directory as the image file, both when on my laptop and when attempting from my hosted site.",
    "author_id":5709,
    "publication_date":1754292623000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Charles Shults",
    "author_reputation":1.0,
    "tags":"javascript, html, three.js, aframe, cors",
    "text_length":3310,
    "title_length":65,
    "num_tags":5
  },
  {
    "id":6219,
    "title":"Implements generic WinRT interfaces use &lt;AllowUnsafeBlocks&gt;True&lt;\/AllowUnsafeBlocks&gt;. But where exactly do I place that setting?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724516\/implements-generic-winrt-interfaces-use-allowunsafeblockstrue-allowunsafebloc",
    "text":"I have a MAUI project that's giving multiple errors related to my custom classes \\Models\\Term.cs and \\Models\\Course.cs . Visual Studio 2022 gives the same recommendation for both: Type 'System.Collections.Generic.List<TermPlanner.Models.Course>' implements generic WinRT interfaces which requires generated code using unsafe for trimming and AOT compatibility if passed across the WinRT ABI. Project needs to be updated with ``` <AllowUnsafeBlocks>true<\/AllowUnsafeBlocks> ``` I assume I add the ``` AllowUnsafeBlocks ``` flag to TermPlanner.csproj or some other file. Where do I set this? The offending blocks look something like this: ``` \/\/\/ <summary> \/\/\/ Returns an IEnumerable of course objects from the \"courses\" SQLite table. The courses \/\/\/ are sorted and all occur during the selected term. \/\/\/ <\/summary> \/\/\/ <param name=\"TermID\">(int) The ID of the term to which all desired courses belong. \/\/\/ <\/param> public static async Task<IEnumerable<Course>> CourseFullList(int TermID) { \/\/ Ensures the database is open and the necessary tables exist. await Open(); \/\/ Creates a list of course objects. List<Course> lst_course; \/\/ Stores courses from the selected term, sorted by start date, in a list. lst_course = await conn.Table<Course>().Where(x => x.ID == TermID).OrderBy(x => x.StartDate).ToListAsync(); \/\/ Returns the list of courses to the caller. return lst_course; } ```",
    "author_id":5708,
    "publication_date":1754293295000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Christian Blackburn",
    "author_reputation":19.0,
    "tags":"c#, compilation, list, lint",
    "text_length":1383,
    "title_length":139,
    "num_tags":4
  },
  {
    "id":6218,
    "title":"Signal handling in multi-threaded scenario",
    "link":"https:\/\/stackoverflow.com\/questions\/79724520\/signal-handling-in-multi-threaded-scenario",
    "text":"I'm trying to revive Hemlock, which now appears abandoned (original author has been absent for some time now). Mostly this has been a straightforward process of clearing out the bit rot, but I'm now running up against a rather annoying problem with signal handling and threads. The system works fine(-ish) now, and I'm trying to solve the problem of TTY screen redisplay after a screen resize (you now have to use C-l (redisplay-all) to redraw the screen. Annoying, but not a deal-breaker. My initial solution works some of the time. But if you're unlucky enough to receive ``` SIGWINCH ``` when not in the main thread the system fails. Signals and threads are a well-known problem, but here a solution eludes me, partly because the system uses IOLIB which has zero documentation and doesn't appear to allow me to mask signals in any way that I can see. cl-async has signal handling, but refactoring to use it will take more time than I have right now. I could cobble together something that checks row\/column count each time through the main event loop and then call redisplay-all if required, but that seems clumsy and inefficient. Does anyone have any ideas? I don't care if it's SBCL specific for the moment; I'd just like it to work first.",
    "author_id":5707,
    "publication_date":1754293557000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"CL-USER",
    "author_reputation":838.0,
    "tags":"pthreads, signals, common-lisp",
    "text_length":1244,
    "title_length":42,
    "num_tags":3
  },
  {
    "id":6217,
    "title":"Java Spring container with Feign client not logging in AWS fargate",
    "link":"https:\/\/stackoverflow.com\/questions\/79724521\/java-spring-container-with-feign-client-not-logging-in-aws-fargate",
    "text":"I have a strange issue with my spring container on AWS. When I run the container on my local machine everything works as expected. When I run the container as a standalone task on Fargate it seems to get stuck as soon as the Feign client code gets executed. The worst is, there are no errors or anything else that would point me in any direction. The application just seems to hang up. Only thing I can confirm via flow logs is that the client does not establish a connection to the outside. I have tested the connectivity with the apache http client. The target url is available from inside the container. And up until the point where the feign client gets executed I can see logs just fine. Since the code runs locally I would rule out an issue with the code in general. So I am suspecting a configuration issue or the container itself. My dockerfile looks like this ``` ARG platform=linux\/amd64 FROM --platform=${platform} maven:3.9.5-amazoncorretto-17 AS build WORKDIR \/app COPY . . RUN mvn clean package -B -C -ntp FROM --platform=${platform} amazoncorretto:17-al2-jdk ENV PROFILE=aws WORKDIR \/app COPY --from=build \/app\/target\/*.jar app.jar ENTRYPOINT [\"java\",\"-Dspring.profiles.active=${PROFILE}\",\"\/app\/app.jar\"] ``` I've wasted more time on this than I want to admit. Can anyone help me out here? Feel free to ask any question. I'll happily share code snippets if I can. I'd have shared more if I knew what to share since I can't pinpoint any possible cause. Thank you guys!",
    "author_id":5706,
    "publication_date":1754293575000,
    "scraped_at":1754660260000,
    "scrape_method":"api",
    "author_name":"Daniel",
    "author_reputation":15619.0,
    "tags":"java, spring, spring-cloud-feign, feign, amazon-web-services",
    "text_length":1482,
    "title_length":66,
    "num_tags":5
  },
  {
    "id":6216,
    "title":"&quot;Unsupported environment variable format&quot; when running serverless",
    "link":"https:\/\/stackoverflow.com\/questions\/79724524\/unsupported-environment-variable-format-when-running-serverless",
    "text":"After code changes in stocks_alert.js based on client requirements, the error persisted when I try to run \"Serverless invoke local -f stocks_alert\". Here's the complete error: ``` Error: Could not resolve \"appsyncUrl\" environment variable: Unsupported environment variable format: {'Fn::GetAtt': [ 'GraphQlApi', 'GraphQLUrl' ] } ``` and the error pinpoints to the configuration from serverless.yml & invokelocalvar.yml: Actual invokelocalvar.yml: ``` PROD: userPoolId: !ImportValue IQOS-UserPoolId-${self:custom.stage} appsyncUrl: !GetAtt GraphQlApi.GraphQLUrl appsyncApiKey: !GetAtt GraphQlApiKeyDefault.ApiKey clientId: !ImportValue \"IQOS-UserWebclient-${self:custom.stage}\" LOCAL_PRE: userPoolId: ap-southeast-1_UWp5U3ImF appsyncUrl: https:\/\/lgrsmv2pgvdexogm2enpphiwji.appsync-api.ap-southeast- 1.amazonaws.com\/graphql appsyncApiKey: da2-5mu4rrerq5fofcseyfal3zfzm4 clientId: 49jgb26kp598r7g244tlfnh5ie LOCAL_PROD: userPoolId: ap-southeast-1_TLwFqBRKT appsyncUrl: https:\/\/dsjyeun425bl7knwvquyejngwu.appsync-api.ap-southeast-1.amazonaws.com\/graphql appsyncApiKey: da2-mwji6z7y3nbovhzwzhnd3ahaeu clientId: 4obtjmi3sq4lfbvlo1l149359m ``` This is from serverless.yml: Actual config from serverless (under environment) ``` appsyncUrl: !GetAtt GraphQlApi.GraphQLUrl appsyncApiKey: !GetAtt GraphQlApiKeyDefault.ApiKey ``` How do I resolve this kind of issue? I already tried to modify the value of config from ``` appsyncUrl ``` and ``` appsyncapiKey ``` to put ``` !ImportValue ``` but I realize that the purpose of this is if the two configuration is running on different cloudformation stack, and I've already tried commenting out the two ``` configurations(appsyncUrl & AppsyncapiKey) ``` from serverless.yml because I though there's duplication of configuration in serverless.yml and invokelocalvar.yml. My expectation is that running ``` serverless invoke local -f stocks_alert ``` succeeds, it will trigger the stocks_alert and sending alert to all recipients the low stock inventory of each warehouses via email.",
    "author_id":5705,
    "publication_date":1754293922000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ryan Barcebal",
    "author_reputation":1.0,
    "tags":"amazon-web-services, serverless",
    "text_length":2015,
    "title_length":75,
    "num_tags":2
  },
  {
    "id":6215,
    "title":"What is the method to load and execute an external script used by a website?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724529\/what-is-the-method-to-load-and-execute-an-external-script-used-by-a-website",
    "text":"I noticed the webpage loads an external script (for example, loglevel ). I would like to create a tampermonkey script that call a function from that script, which is already loaded by the original webpage For example ``` log.info('hello world!') ``` which is included in loglevel script. Thanks",
    "author_id":5704,
    "publication_date":1754294328000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Winston",
    "author_reputation":1424.0,
    "tags":"javascript, tampermonkey",
    "text_length":294,
    "title_length":76,
    "num_tags":2
  },
  {
    "id":6214,
    "title":"The result obtained from the OpenCV-Python cv.add() function differs from the official documentation",
    "link":"https:\/\/stackoverflow.com\/questions\/79724535\/the-result-obtained-from-the-opencv-python-cv-add-function-differs-from-the-of",
    "text":"During my learning process of OpenCV-Python, I encountered an issue. My environment uses Python version 3.13, NumPy version 2.2.6, and OpenCV-Python version 4.12. In the \"Arithmetic Operations on Images\" section of the official documentation, it mentions the difference between OpenCV addition and NumPy addition: OpenCV addition is a saturated operation while NumPy addition is a modulo operation. Official documentation page However, the actual results I obtained during computation differ from the values provided in the documentation. My code: ``` import cv2 as cv import numpy as np x = np.uint8([250]) y = np.uint8([10]) print(f\"x+y = {x+y}\") print(f\"cv.add(x,y) = {cv.add(x,y)}\") ``` Official documentation result: ``` [4] [[255]] ``` My result: ``` [4] [[260.] [0.] [0.] [0.]] ``` I checked ``` cv.add(x,y).dtype ``` and found it returns ``` float64 ``` instead of ``` uint8 ``` . After attempting data type conversion and retrieving the first value with ``` cv.add(x,y).astype(np.uint8)[0] ``` , the result is [4], which matches the NumPy addition result. How can I obtain the correct value from the ``` cv.add() ``` algorithm?",
    "author_id":5703,
    "publication_date":1754294616000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"13 stef",
    "author_reputation":13.0,
    "tags":"python, opencv",
    "text_length":1136,
    "title_length":100,
    "num_tags":2
  },
  {
    "id":6213,
    "title":"How can I write to a BLE UUID then receive data sent back on another UUID using C++?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724537\/how-can-i-write-to-a-ble-uuid-then-receive-data-sent-back-on-another-uuid-using",
    "text":"I'm trying to use C++ to communicate with a BLE device. To get the data I have to write 4 bytes to one UUID then the data is written out via another UUID. I have this working in Python with Bleak: ``` async def main(address: str): async def callback_handler(characteristic : BleakGATTCharacteristic, data: bytearray) -> None: print(f\"{characteristic.description}: {data}\") async with BleakClient(address) as client: await client.start_notify(READ_DATA, callback_handler) write_data = bytearray([0x00, 0x01, 0x02, 0x03]) await client.write_gatt_char(WRITE_DATA, write_data, response=False) await asyncio.sleep(1.0) if __name__ == \"__main__\": asyncio.run(main(sys.argv[1] if len(sys.argv) == 2 else ADDRESS)) ``` But I can't get the same thing to work in C++ with sdbus. I can write the 4 bytes (I think): ``` _tempAttrProxyWrite = sdbus::createProxy(\"org.bluez\", WRITE_DATA); std::vector<uint8_t> data = {0x00, 0x01, 0x02, 0x03}; std::map<std::string, sdbus::Variant> options; _tempAttrProxyWrite->callMethod(\"WriteValue\") .onInterface(INTERFACE_CHAR) .withArguments(data, options); ``` But I can't figure out how to read the data, either by subscribing to a listener or reading the UUID after writing to it.",
    "author_id":5702,
    "publication_date":1754294747000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"parsley72",
    "author_reputation":9269.0,
    "tags":"c++, bluetooth-lowenergy, sdbus-c++",
    "text_length":1207,
    "title_length":84,
    "num_tags":3
  },
  {
    "id":6212,
    "title":"How to fill a specific area (sock) with pink color in turtle graphics?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724538\/how-to-fill-a-specific-area-sock-with-pink-color-in-turtle-graphics",
    "text":"I'm using Python's turtle module to draw a character illustration. Most parts of the drawing are correctly colored using begin_fill() and end_fill(). However, I'm not sure how to fill the sock area with pink, as indicated by the arrow in the attached image: ``` import turtle as t t.setup(1000,750) def hair(): t.begin_fill() t.color(\"black\") t.pd() t.left(50) t.circle(-80,40) t.left(5) t.circle(-220,40) t.circle(-100,40) t.circle(-200,15) t.pos() t.right(70) t.circle(100,12) t.right(130) t.circle(250,15) t.circle(25,60) t.left(120) t.circle(-10,150) t.circle(-33,20) t.circle(25,120) t.right(10) t.circle(-190,2) t.circle(-10,180) t.circle(-180,10) t.left(133) t.circle(220,12) t.circle(300,9) t.left(144) t.circle(-80,23) t.circle(-10,180) t.circle(-100,6) t.circle(20,120) t.fd(10) t.circle(-10,100) t.circle(-30,30) t.end_fill() def head(): t.seth(-130) t.begin_fill() t.color(\"black\",\"navajo white\") t.circle(90,50) t.right(90) t.circle(80,50) t.circle(60,50) t.circle(90,40) t.circle(130,40) t.circle(550,20) t.right(55) t.circle(400,4) t.circle(35,130) t.circle(150,15) t.circle(20,80) t.circle(30,50) t.goto(150,120) t.seth(90) t.circle(100,90) t.circle(600,10) t.end_fill() t.seth(0) def braw(): t.seth(90) t.circle(-40,180) t.pu() t.goto(25,140) t.seth(75) t.pd() t.circle(-45,160) def eyes(): t.seth(0) t.begin_fill() t.color(\"black\",\"black\") t.circle(36) t.end_fill() t.seth(90) t.fd(22) t.seth(0) t.begin_fill() t.color(\"white\",\"white\") t.circle(13) t.end_fill() def mouse(): t.pencolor(\"black\") t.seth(20) t.fd(10) def close(): t.seth(-110) t.begin_fill() t.color(\"black\",\"red\") t.fd(17) t.circle(10,80) t.fd(50) t.left(105) t.fd(10) t.bk(80) t.left(180) t.circle(20,90) for i in range(5): t.circle(50,30) t.circle(-50,30) t.seth(90) t.fd(75) t.seth(-160) t.fd(45) t.right(100) t.fd(30) t.bk(30) t.right(80) t.fd(80) t.circle(10,100) t.fd(30) t.circle(-50,30) t.fd(10) t.circle(10,90) for i in range(3): t.circle(20,20) t.circle(-20,20) t.end_fill() def hand1(): t.begin_fill() t.color(\"black\",\"navajo white\") t.seth(-100) t.circle(30,25) t.circle(-60,40) t.circle(15,60) t.circle(-15,120) t.circle(-50,15) t.circle(-10,120) t.left(150) t.circle(-7,100) t.circle(-15,40) t.right(20) t.circle(-5,70) t.circle(7,150) t.circle(30,10) t.circle(-50,20) t.circle(30,20) t.circle(-40,10) t.end_fill() def hand2(): t.seth(125) t.begin_fill() t.color(\"black\",\"navajo white\") t.circle(-65,35) t.left(90) t.fd(10) t.left(15) t.circle(30,20) t.circle(-20,30) t.circle(-3,140) t.circle(-100,10) t.left(150) t.circle(-100,15) t.circle(-5,150) t.circle(-100,10) t.left(140) t.circle(-100,13) t.circle(-6,150) t.circle(-100,10) t.circle(3,120) t.circle(-100,10) t.circle(-3,140) t.fd(25) t.left(100) t.fd(15) t.circle(-5,150) t.fd(20) t.circle(20,90) t.end_fill() def trou(): t.seth(-100) t.begin_fill() t.color(\"black\",\"khaki1\") t.fd(50) t.circle(5,100) t.fd(110) t.circle(10,160) t.left(180) t.circle(-10,65) t.circle(10,85) t.fd(120) t.circle(10,95) t.fd(40) t.end_fill() def legs1(): t.seth(-90) t.begin_fill() t.color(\"black\",\"navajo white\") t.circle(100,25) t.left(50) t.circle(50,38) t.left(70) t.circle(-100,23) t.end_fill() t.left(180) t.circle(100,35) t.right(95) t.begin_fill() t.color(\"black\",\"lightgoldenrod\") t.fd(28) t.right(70) t.circle(-100,15) t.left(180) t.circle(100,6) t.right(90) t.circle(10,55) t.circle(-30,40) t.circle(-10,20) t.circle(-100,13) t.circle(5,179) t.fd(78) t.circle(5,120) t.fd(17) t.end_fill() def legs2(): t.seth(-60) t.begin_fill() t.color(\"black\",\"navajo white\") t.circle(-75,30) t.left(75) t.circle(50,43) t.left(70) t.fd(35) t.end_fill() t.left(180) t.fd(35) t.right(5) t.begin_fill() t.color(\"black\",\"lightgoldenrod\") t.fd(20) t.right(20) t.circle(-20,125) t.right(20) t.circle(-25,10) t.fd(20) t.left(180) t.fd(20) t.circle(25,10) t.right(55) t.fd(20) t.circle(5,125) t.fd(76) t.circle(5,180) t.circle(-35,40) t.circle(-15,40) t.circle(5,90) t.end_fill() def snow(size,n): import turtle as t t.pencolor(\"skyblue\") t.pensize(2) if n==1: t.fd(size) t.left(60) t.fd(size) t.right(120) t.fd(size) t.left(60) t.fd(size) else: snow(size\/3,n-1) t.left(60) snow(size\/3,n-1) t.right(120) snow(size\/3,n-1) t.left(60) snow(size\/3,n-1) t.speed(10) t.pensize(5) t.pu() t.goto(-125,-145) t.pd() trou() t.pu() t.goto(-100,-203) t.pd() legs1() t.pu() t.goto(45,-200) t.pd() legs2() t.pu() t.goto(-160,0) t.pd() close() t.pu() t.goto(155,-67) t.pd() hand1() t.pu() t.goto(-185,60) t.pd() hand2() t.pu() t.goto(-100,200) t.pd() head() t.pu() t.goto(-100,200) t.pd() hair() t.pu() t.goto(-98,130) t.pd() braw() t.pu() t.goto(-53,70) t.pd() eyes() t.pu() t.goto(50,72) eyes() t.pu() t.goto(-60,-10) t.pd() mouse() t.speed(10000) for i in range(10): import random as ran x=ran.uniform(-400,-200) y=ran.uniform(-250,250) t.pu() t.goto(x,y) t.pd() for j in range(3): snow(12,3) t.right(120) t.pu() x = ran.uniform(200, 400) y = ran.uniform(-250, 250) t.pu() t.goto(x, y) t.pd() for j in range(3): snow(12, 3) t.right(120) t.pu() # t.pu() # t.goto(200,-200) # t.pd() # t.write(\"***\",font=(18)) t.done() ``` I want to add a pink section between the leg and the shoe to represent a sock. I’m not sure where to place the new fill and how to draw the shape accurately within the current turtle path flow. How should I modify the code to include this pink sock section?",
    "author_id":5701,
    "publication_date":1754294850000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Pi Network Crytocurrency",
    "author_reputation":599.0,
    "tags":"python",
    "text_length":5288,
    "title_length":70,
    "num_tags":1
  },
  {
    "id":6211,
    "title":"Unicode characters outside the ASCII range don&#39;t show up in irb",
    "link":"https:\/\/stackoverflow.com\/questions\/79724543\/unicode-characters-outside-the-ascii-range-dont-show-up-in-irb",
    "text":"Environment: MacOS 12.7.3 LANG set to en_GB.UTF-8 irb from MRI Ruby showing RUBY_VERSION as 2.6.10 When I type into irb a string containing a character which is not in the ASCII range, for instance I type on the keyboard the three characters ``` \"é\" ``` I see it echoed as ``` \"\\U+FFC3\\U+FFA9\" ``` and when I hit the ENTER key, I get as response an empty string ( ``` \"\" ``` ). What could be the reason, and how can I fix it? Note that if I do on the command line a ``` ruby -e \"p 'é'\" ``` I get a ``` \"é\" ``` as response, as one would expect. UPDATE I just noted something: I have two Ruby versions installed: The default which comes with MacOS (2.6.10) and a slightly older one, which I installed myself via RVM (2.4.2). The problem occurs in 2.6.10, but not with the irb that comes with 2.4.2. It seems that the default Ruby shipped with MacOS is broken. UPDATE As @Stefan pointed out in his comment, the reason seems to be a broken readline library, since invoking irb with the option ``` --noreadline ``` makes the problem disappear. Using this option means of course that I can't use the cursor keys anymore for history editing. The solution would be to update readline, but this is not possible either, at least not with homebrew (if someone knows an alternative, drop me a comment please): Homebrew tells me ``` readline is already installed but outdated (so it will be upgraded). Warning: You are using macOS 12. We (and Apple) do not provide support for this old version. ``` Still brew tries to install readline 8.3, but in the end bails out with ``` Error: Your Xcode (11.3.1) at \/Applications\/Xcode.app is too outdated. Please update to Xcode 14.2 (or delete it). Xcode can be updated from the App Store. ``` However, a newer Xcode is also not available for MacOS 12, and I'm reluctant to delete the current version, since I don't know which other applications depend on it. Looks like the main reason is that my OS version is not really supported anymore.",
    "author_id":5700,
    "publication_date":1754295195000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user1934428",
    "author_reputation":22660.0,
    "tags":"ruby, irb",
    "text_length":1969,
    "title_length":67,
    "num_tags":2
  },
  {
    "id":6210,
    "title":"How to replace a model on the same URN?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724545\/how-to-replace-a-model-on-the-same-urn",
    "text":"I'm trying to replace a model on Autodesk Platform Services (APS) while keeping the same URN. To test this, I prepared two different ``` .ifc ``` files. These files have different sizes and numbers of elements. I first uploaded the first IFC file to an OSS bucket and used the Model Derivative API to request translation to SVF2. I received the expected webhook callback , confirming that the model was successfully translated. Then, I uploaded the second IFC file , using: the same file name as the first file the same ``` objectId ``` during the upload and in the Model Derivative translation request However: I did not receive any webhook notification for the second file When I open the same URN in the APS Viewer, it still shows the original model , not the updated one I used the Model Derivative API both times, expecting that using the same object key and file name would: Overwrite the previous source file in the OSS bucket Trigger a new SVF2 translation Replace the existing viewable under the same URN I expected: A webhook callback for the second translation request The Viewer to reflect the updated model from the second IFC file Instead, the second translation didn’t seem to run, and the Viewer still shows the first model.",
    "author_id":4359,
    "publication_date":1754295270000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"astromelon",
    "author_reputation":3.0,
    "tags":"autodesk-forge, autodesk-viewer, autodesk-model-derivative",
    "text_length":1240,
    "title_length":39,
    "num_tags":3
  },
  {
    "id":6209,
    "title":"Align a shaped svg vector image exactly to the right of a div to make it part of the background",
    "link":"https:\/\/stackoverflow.com\/questions\/79724552\/align-a-shaped-svg-vector-image-exactly-to-the-right-of-a-div-to-make-it-part-of",
    "text":"I’m working on a design where I have an S-shaped SVG curve as part of a background behind a containing text and buttons. You can see in my screenshots what I’m trying to achieve: First image: This is the intended look in the screen design. The SVG shape sits behind the content and partly overlaps to the right Second image: This is what I get now. The SVG either misaligns, gets cut off, or doesn’t respect the flow. But my approach doesn’t work well: the SVG doesn’t scale correctly (with my code the width is at 0, I would need to set a specific width which defeats the scaling purpose) and that way the div's content also cannot expand to fill into the S partway. Also there remains a pixel thin line between the div background and the svg. What’s the best way to achieve this? Any advice about how to position and layer the SVG properly so that it behaves as a background decoration and allows content to appear in front of it? What I’ve tried so far: Adding the SVG as a pseudo-element (::after) on the red div What I want: The S-shaped SVG should be visually part of the div’s background. Ideally the div’s content (text\/buttons) can partly overlap in front of the SVG The shape should align consistently to the right edge of the div and scale with the div’s height. ``` .content-wrapper { width: 50%; background-color: red; border-bottom-left-radius: 16px; position: relative; } .content-wrapper::after { content: \" \"; display: inline-block; height: 100%; width: min-content; position: absolute; left: 100%; top: 0; \/*Sorry, unsure how to embed the proper svg here*\/ background: url(\"https:\/\/disk.sample.cat\/samples\/svg\/hugging-face.svg\") no-repeat; background-size: contain; } ``` ``` <div class=\"content-wrapper\"> <h1>Lorem ipsum dolor, sit amet consectetur adipisicing elit.<\/h1> <p> Lorem ipsum dolor, sit amet consectetur adipisicing elit. Inventore blanditiis nemo doloremque laudantium cupiditate. <\/p> <\/div> svg should show up to the right of the box. ```",
    "author_id":5699,
    "publication_date":1754296359000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"rappluk",
    "author_reputation":11.0,
    "tags":"html, css, svg, responsive-design",
    "text_length":1972,
    "title_length":95,
    "num_tags":4
  },
  {
    "id":6208,
    "title":"Blazor image streaming - Chrome out of memory",
    "link":"https:\/\/stackoverflow.com\/questions\/79724558\/blazor-image-streaming-chrome-out-of-memory",
    "text":"We're developing a Blazor application on .NET 8 that streams single ``` .jpeg ``` images (200kB) to the browser. Those images could have a frequency of ~1000 images\/minute. We set the images as a base64 encode string in the UI and visualize the app in Chrome. It seems to be running fine 24\/7, if we have enough CPU threads available. It hast been tested on a huge variety of systems. It runs fine, if we have (number of Chrome processes + 1) >= available CPU threads. But if we have fewer CPU threads, the memory Chrome is using currently increases until it crashes with an out of memory exception. Even if we reload our page or close it, chrome wouldn't free the memory. If we would have a memory leak, I would suggest that it occurs independent of the available CPU threads or am I wrong? Do someone have an explanation, why we are getting this behaviour on hardware with fewer threads?",
    "author_id":5698,
    "publication_date":1754296786000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"xdefeix27",
    "author_reputation":1.0,
    "tags":"c#, google-chrome, blazor, image, streaming",
    "text_length":889,
    "title_length":45,
    "num_tags":5
  },
  {
    "id":6207,
    "title":"Is it okay to instantiate a new service in each method?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724559\/is-it-okay-to-instantiate-a-new-service-in-each-method",
    "text":"I want to use an Azure Durable function because apparently it's good if my application only runs once a year or very rarely. The logic is like this: takes all SharePoint items sends ``` SPItem ``` to a scanner API sends ``` CheckStatus ``` request to scanner API ``` if ``` status is processing then request again in x minutes ``` else if ``` status is finished then continue logic sends ``` GetScannedItem ``` request, takes item and updates ``` SPItem ``` The issue, since all these operations are ``` await ``` ed the durable function resets the environment variables so local class variables will be set to ``` null ``` again. Is it a bad practice to re-instantiate the SharePoint Graph client on each method or is it okay? (rough draft below) I also wanted to use durable functions because then I could get the \"trickle-down\" effect of when one ``` SPItem ``` is still processing, that it will continue the logic for the other items and not get stuck.",
    "author_id":5697,
    "publication_date":1754296883000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"zhrgci",
    "author_reputation":682.0,
    "tags":"c#, azure, sharepoint, azure-durable-functions",
    "text_length":956,
    "title_length":55,
    "num_tags":4
  },
  {
    "id":6206,
    "title":"How do I collect a list of links on page then iterate through and test each one?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724566\/how-do-i-collect-a-list-of-links-on-page-then-iterate-through-and-test-each-one",
    "text":"I have a modal component that is used on a lot of pages on my site. I want to go through each page on my site, collect a series of links on that page which trigger the modal and test each one to see if the modal appears. Some pages might have one modal link, some pages might have multiple. I have created the following test: ``` describe('I am testing the Modals on ' + modalUrls.length + ' pages.', () => { let triggerLinksObj = {}; modalUrls.forEach(function(page, index) { it(`It accesses ${page}`, () => { cy.visit(page); }); it(`Collects modal trigger links on ${page}`, () => { cy.get('[data-modal-trigger]').each(($triggerLink, index) => { const modalId = $triggerLink.attr('data-modal-trigger'); if (modalId) { triggerLinksObj[index] = modalId; } }) }); Object.entries(triggerLinksObj).forEach(([key, val]) => { it(`Processes modal link with ID: ${val}`, () => { cy.task('log', `Processing modal link with key: ${key}, value: ${val}`); \/\/do the tests with the modal }); }); }); }); ``` But the ``` it ``` block within the ``` forEach ``` simply doesn't run. I can put the ``` forEach ``` within the ``` it ``` block as such: ``` it(`Processes modal links`, () => { Object.entries(triggerLinksObj).forEach(([key, val]) => { cy.task('log', `Processing modal link with key: ${key}, value: ${val}`); \/\/do the tests with the modal }); }); ``` But if I have multiple modals and one fails then all the modal tests fail. How can I run a separate ``` it ``` block for each found instance of ``` [data-modal-trigger] ``` ?",
    "author_id":4763,
    "publication_date":1754297361000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MeltingDog",
    "author_reputation":15668.0,
    "tags":"javascript, node.js, cypress",
    "text_length":1521,
    "title_length":80,
    "num_tags":3
  },
  {
    "id":6205,
    "title":"A callback was made on a garbage collected delegate (error)",
    "link":"https:\/\/stackoverflow.com\/questions\/79724569\/a-callback-was-made-on-a-garbage-collected-delegate-error",
    "text":"There is a .so file imported into my C# project, which has three methods: ``` Start() ``` , ``` RegisterCallBack(Delegate callback) ``` , and ``` End() ``` . First, I invoke ``` Start() ``` to create an unmanaged thread. Then I call ``` RegisterCallBack ``` with a delegate (passing a method). The unmanaged thread binds a socket to a port, and when data is received, it invokes the delegate. After first GC runs, an exception is thrown indicating the delegate was garbage collected. I've tried making the delegate static and using ``` GC.KeepAlive() ``` , but neither worked. Interestingly, adding a 100ms delay makes it work. I can't understand what's happening. this is simple code: ``` [DllImport(\"test.so\", EntryPoint = \"start\", CallingConvention = CallingConvention.Cdecl)] public static extern void Start(); [DllImport(\"test.so\", EntryPoint = \"register_call_back\", CallingConvention = CallingConvention.Cdecl)] public static extern void RegisterCallback(LogPrint callback); [DllImport(\"test.so\", EntryPoint = \"end\", CallingConvention = CallingConvention.Cdecl)] public static extern void End(); public delegate void LogPrint(byte[] value); private static LogPrint _log => Print; public static void Print(byte[] value) { var str = Encoding.Default.GetString(value); } static void Main(string[] args) { Start(); RegisterCallback(_log); Wait(100*1000); End(); } ```",
    "author_id":5696,
    "publication_date":1754297591000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"nuub",
    "author_reputation":49.0,
    "tags":"c#",
    "text_length":1369,
    "title_length":59,
    "num_tags":1
  },
  {
    "id":6204,
    "title":"How do I save the file using Cmd+S in VSCode Diff View on MacOS?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724577\/how-do-i-save-the-file-using-cmds-in-vscode-diff-view-on-macos",
    "text":"After the recent update, I can no longer use Cmd+S to save files in the diff view. Steps to reproduce: Ctrl+Shift+G Click a changed file Edit the file in the editable view of the diff view (right side) Cmd+S => Nothing happens Using File > Save from the menu works. Using Option+Cmd+S (Save All) works too.",
    "author_id":5695,
    "publication_date":1754298063000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Timothy Alexis Vass",
    "author_reputation":2739.0,
    "tags":"visual-studio-code, keyboard-shortcuts, gemini-code-assist",
    "text_length":306,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":6203,
    "title":"Robot test framework for Rust",
    "link":"https:\/\/stackoverflow.com\/questions\/79724591\/robot-test-framework-for-rust",
    "text":"I need to generate unit-test and integration-test reports ( ``` cargo test ``` command) of a Rust project. To do this I am using ``` RobotFramework ``` . When I try to run ``` cargo test ``` command it fails with an error ``` \/usr\/bin\/ld: cannot find -lpython3.12: No such file or directory ``` . lib.rs ``` \/* * Copyright (c) 2023 Markus Neifer. * Licensed under the MIT License. * See LICENSE in the project root for license information. * * Adopted from https:\/\/pyo3.rs\/v0.18.1\/ *\/ \/\/ The following line allows non snake-case name for Robot test library #![allow(non_snake_case)] use std::collections::HashMap; use pyo3::prelude::*; #[pyfunction] fn sum_as_string(a: i32, b: i32) -> PyResult<String> { Ok((a + b).to_string()) } #[pyfunction] fn join_strings(a: Vec<String>) -> PyResult<String> { Ok(a.join(\",\")) } #[pyfunction] fn sum_values(a: HashMap<String, i32>) -> PyResult<i32> { let mut values_sum = 0; for (_key, value) in &a { values_sum += value; } Ok(values_sum) } #[pymodule] fn RustyLibrary(_py: Python, m: &PyModule) -> PyResult<()> { m.add_function(wrap_pyfunction!(sum_as_string, m)?)?; m.add_function(wrap_pyfunction!(join_strings, m)?)?; m.add_function(wrap_pyfunction!(sum_values, m)?)?; Ok(()) } #[cfg(test)] mod tests { use super::*; #[test] fn sum_as_string_test() { assert_eq!(\"25\", sum_as_string(5, 20).unwrap()); } #[test] fn join_strings_test() { let foo = String::from(\"foo\"); let bar = String::from(\"bar\"); let the_strings = vec![foo, bar]; assert_eq!(\"foo, bar\", join_strings(the_strings).unwrap()); } #[test] fn sum_values_test() { let mut values = HashMap::new(); values.insert(String::from(\"abc\"), 6); values.insert(String::from(\"def\"), 15); values.insert(String::from(\"ghi\"), 24); assert_eq!(45, sum_values(values).unwrap()); } } ``` Cargo.toml ``` [package] name = \"RustyLibrary\" version = \"0.1.0\" edition = \"2021\" # See more keys and their definitions at https:\/\/doc.rust-lang.org\/cargo\/reference\/manifest.html [lib] name = \"RustyLibrary\" crate-type = [\"cdylib\"] [dependencies] pyo3 = \"0.18.1\" ``` pyproject.toml ``` [build-system] requires = [\"maturin>=1.9.2\"] build-backend = \"maturin\" [project] name = \"RustyLibrary\" version = \"0.1.0\" requires-python = \">=3.12\" classifiers = [ \"Programming Language :: Rust\", \"Programming Language :: Python :: Implementation :: CPython\", \"Programming Language :: Python :: Implementation :: PyPy\", ] dependencies = [ \"maturin>=1.9.2\", \"robotframework>=7.3.2\", ] [tool.maturin] features = [\"pyo3\/extension-module\"] ``` If I build the Rust library using ``` maturin develop ``` , it builds successfully with the below output ``` 📦 Including license file `LICENSE` 🔗 Found pyo3 bindings 🐍 Found CPython 3.12 at \/home\/harsha\/rust_robot\/robot-rust-test-library\/.venv\/bin\/python 📡 Using build options features from pyproject.toml Audited 2 packages in 14ms Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.02s 📦 Built wheel for CPython 3.12 to \/tmp\/.tmpssq3RX\/RustyLibrary-0.1.0-cp312-cp312-linux_x86_64.whl ✏️ Setting installed package as editable 🛠 Installed RustyLibrary-0.1.0 ```",
    "author_id":5694,
    "publication_date":1754299110000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Harry",
    "author_reputation":3898.0,
    "tags":"rust, robotframework",
    "text_length":3068,
    "title_length":29,
    "num_tags":2
  },
  {
    "id":6202,
    "title":"LLVM generates stack usage on simple RISC-V function where GCC doesn&#39;t",
    "link":"https:\/\/stackoverflow.com\/questions\/79724592\/llvm-generates-stack-usage-on-simple-risc-v-function-where-gcc-doesnt",
    "text":"I have a simple function: ``` extern \"C\" Variant test_bool(bool arg) { return arg; } ``` Built with mostly standard settings (I removed part of the paths): ``` zig c++ -target riscv64-linux-musl -I\/tests -I\/tests\/.zig -O3 -DNDEBUG -mcpu=baseline_rv64+rva22u64 -mabi=lp64d -O3 -std=gnu++23 -fno-stack-protector -fno-threadsafe-statics -MD -MT \/test_basic.cpp.o -MF \/test_basic.cpp.o.d -o \/test_basic.cpp.o -c \/test_basic.cpp ``` I need some help to understand why LLVM generates this: ``` 000000000013a56a <test_bool>: test_bool(): 13a56a: 1141 addi sp,sp,-16 13a56c: e406 sd ra,8(sp) 13a56e: e022 sd s0,0(sp) 13a570: 0800 addi s0,sp,16 13a572: 4605 li a2,1 13a574: c110 sw a2,0(a0) 13a576: 00b50423 sb a1,8(a0) 13a57a: 60a2 ld ra,8(sp) 13a57c: 6402 ld s0,0(sp) 13a57e: 0141 addi sp,sp,16 13a580: 8082 ret ``` When GCC correctly emits this: ``` 0000000000012156 <test_bool>: test_bool(): 12156: 4705 li a4,1 12158: c118 sw a4,0(a0) 1215a: 00b50423 sb a1,8(a0) 1215e: 8082 ret ``` The Variant constructor is constexpr: ``` template <typename T> inline constexpr Variant::Variant(T value) { if constexpr (std::is_same_v<T, bool>) { m_type = BOOL; v.b = value; } else if constexpr (std::is_integral_v<T>) { ... } ... ``` The Variant is a simple union with a type: ``` private: Type m_type = NIL; union { int64_t i; bool b; double f; real_t v4[4]; int32_t v4i[4]; } v; ``` Any ideas on what could be the cause of the additional instructions?",
    "author_id":5693,
    "publication_date":1754299257000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"gonzo",
    "author_reputation":547.0,
    "tags":"c++, llvm, riscv",
    "text_length":1436,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":6201,
    "title":"Different UIBarButtonItem Appearance in iOS 26 Beta 4 Compared to iOS 18 Version – How to Retain iOS 18 Style?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724594\/different-uibarbuttonitem-appearance-in-ios-26-beta-4-compared-to-ios-18-version",
    "text":"I'm encountering an issue where the same code for creating a UIBarButtonItem results in different visual appearances between iOS 18 (or earlier) and iOS 26 Beta 4. I'm testing on simulators and devices using Xcode 26 beta 4. In my app, I'm setting a right bar button item in a view controller's navigation bar with the following code: ``` func initView() { self.view.backgroundColor = .systemGray6 self.title = \"Title\" self.preferredContentSize = CGSize(width: 500, height: 600) let cancelButton = UIBarButtonItem.init(title: \"Close\", style: .plain, target: self, action: #selector(cancelAndBack)) self.navigationItem.leftBarButtonItem = cancelButton if #available(iOS 26.0, *) { let doneButton = UIBarButtonItem.init(title: \"Done\", style: .prominent, target: self, action: #selector(cancelAndBack)) self.navigationItem.rightBarButtonItem = doneButton } else { let doneButton = UIBarButtonItem.init(title: \"Done\", style: .done, target: self, action: #selector(cancelAndBack)) self.navigationItem.rightBarButtonItem = doneButton } } ``` For Cancel Button: No explicit style is set here, so it defaults to .plain . On iOS 18 and below, this appears as plain text without a background or rounded frame, which is the desired look. On iOS 26 Beta 4, it shows with a white background and rounded corners, which I don't want. I need a clear background without any rounding or frame. For Done Button: I noticed that UIBarButtonItem.Style.done is deprecated in iOS 26 (as per documentation ). So, I tried using .prominent explicitly, but the appearance is different ion IOS 26 in compared to iOS 18. it is added system blue color as the background color of the button. Question: Is there a recommended way to achieve consistent appearance across iOS 18 and iOS 26, specifically retaining the iOS 18 style (plain text, no background, no rounding) on iOS 26 devices? Are there any new APIs or configuration options in iOS 18 that I should use to override this default styling? Or is this a beta issue that might be resolved? Fully Code: ``` import UIKit class ViewController: UIViewController { override func viewDidLoad() { super.viewDidLoad() \/\/ Create the button let button = UIButton(type: .system) button.setTitle(\"Click me\", for: .normal) button.translatesAutoresizingMaskIntoConstraints = false button.titleLabel?.font = UIFont.systemFont(ofSize: 18) button.setTitleColor(.white, for: .normal) button.backgroundColor = .systemBlue button.layer.cornerRadius = 8 \/\/ Add target for button tap button.addTarget(self, action: #selector(buttonTapped), for: .touchUpInside) \/\/ Add button to view view.addSubview(button) \/\/ Set up constraints to center the button NSLayoutConstraint.activate([ button.centerXAnchor.constraint(equalTo: view.centerXAnchor), button.centerYAnchor.constraint(equalTo: view.centerYAnchor) ]) } @objc func buttonTapped() { let exportViewController = ExportViewController() let navController = UINavigationController(rootViewController: exportViewController) navController.modalPresentationStyle = .formSheet self.present(navController, animated: true, completion: nil) } } import UIKit class ExportViewController: UIViewController { func initView() { self.view.backgroundColor = .systemGray6 self.title = \"Title\" self.preferredContentSize = CGSize(width: 500, height: 600) let cancelButton = UIBarButtonItem.init(title: \"Close\", style: .plain, target: self, action: #selector(cancelAndBack)) self.navigationItem.leftBarButtonItem = cancelButton if #available(iOS 26.0, *) { let doneButton = UIBarButtonItem.init(title: \"Done\", style: .prominent, target: self, action: #selector(cancelAndBack)) self.navigationItem.rightBarButtonItem = doneButton } else { let doneButton = UIBarButtonItem.init(title: \"Done\", style: .done, target: self, action: #selector(cancelAndBack)) self.navigationItem.rightBarButtonItem = doneButton } } override func viewDidLoad() { super.viewDidLoad() \/\/ Call initView first to set up the basic view initView() \/\/ Create and add the label let helloLabel = UILabel() if #available(iOS 26.0, *) { helloLabel.text = \"Hello iOS 26\" } else { helloLabel.text = \"Hello iOS 18\" } helloLabel.font = UIFont.systemFont(ofSize: 48) helloLabel.textAlignment = .center helloLabel.translatesAutoresizingMaskIntoConstraints = false view.addSubview(helloLabel) \/\/ Center the label in the view NSLayoutConstraint.activate([ helloLabel.centerXAnchor.constraint(equalTo: view.centerXAnchor), helloLabel.centerYAnchor.constraint(equalTo: view.centerYAnchor) ]) } @objc func cancelAndBack(){ self.dismiss(animated: true, completion: nil) } } ``` Please, refer attached screenshot for button appearance .",
    "author_id":5692,
    "publication_date":1754299651000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dileepa Chandrasekara",
    "author_reputation":371.0,
    "tags":"ios, swift, uikit, ios26",
    "text_length":4620,
    "title_length":110,
    "num_tags":4
  },
  {
    "id":6200,
    "title":"Object value from store is different inside array.map",
    "link":"https:\/\/stackoverflow.com\/questions\/79724595\/object-value-from-store-is-different-inside-array-map",
    "text":"I have to following code that uses ``` userInfo ``` to determine whether to display something or not. Problem I'm having is the value of ``` userInfo ``` inside ``` apps.map ``` is different from outside. I could not find any code that modifies ``` userInfo ``` . Even if it's modified, my understanding is that the value of ``` userInfo ``` should still be consistent regardless of the ``` app.map() ``` . ``` ... const { userInfo } = useUserStore(); ... <Flex> {apps.map((app) => ( <MyTooltip key={app.id} label={ userInfo?.team?.canWrite && (app.isOwner || userInfo?.team.role !== 'contributor') ? '' : t('app.To Chat') } ... ```",
    "author_id":5691,
    "publication_date":1754299747000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user2817869",
    "author_reputation":162.0,
    "tags":"next.js, reactjs, typescript",
    "text_length":632,
    "title_length":53,
    "num_tags":3
  },
  {
    "id":6199,
    "title":"How can I refactor a widely-used C# class to support constructor dependency injection without breaking existing usages?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724599\/how-can-i-refactor-a-widely-used-c-class-to-support-constructor-dependency-inje",
    "text":"In my C# ASP.NET Core project, I have a class that is currently instantiated using a parameterless constructor. This class is used in hundreds of places throughout the solution. I now need to refactor the class to support constructor dependency injection by requiring a parameter to be passed in (such as a string or a service). However, due to the large number of existing usages, I can't update all of them immediately. Using optional constructor parameters is not a viable solution in my case — while it compiles, it causes runtime issues, possibly due to how the framework or dependency injection container handles nulls or default values. I'm looking for a safe and maintainable way to introduce the new required constructor dependency without breaking existing code. I'm also trying to follow best practices and move toward dependency injection. So far, I’ve considered options like adding an overloaded constructor, using a base class for shared logic, applying a factory or builder pattern, and marking the old constructor as obsolete to track usage. What approach would you recommend to transition this class to constructor-based dependency injection without breaking the current system?",
    "author_id":5690,
    "publication_date":1754299935000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Nikolai Tikhonov",
    "author_reputation":1.0,
    "tags":"c#, .net-core, asp.net-core, dependency-injection",
    "text_length":1196,
    "title_length":119,
    "num_tags":4
  },
  {
    "id":6198,
    "title":"Typing Graphql using Sorbet",
    "link":"https:\/\/stackoverflow.com\/questions\/79724602\/typing-graphql-using-sorbet",
    "text":"I'm trying to define the best way to work with graphql and sorbet typing. I want to know how you solved the correlation between the graphql and the type checking of sorbet. I didn't found posts on it. Do I need to point to the ``` GraphQL::Schema::InputObject ``` of should I convert it to a more standard struct ( ``` T::Struct ``` \/ ``` Dry::Struct ``` ) that the my app will use while doing logic? If I'm creating new struct for every gql type I have code duplication :( That is that I have currently: I creating ``` app_class ``` that points to the relevant structure for the GQL For Enums ``` class NewEnum < BaseEnum app_class ::App::NewEnum ::AppObjects::NewEnum.each_value do |enum_value| value enum_value.serialize end end class NewEnum < T::Enum enums do Value1 = new('value1') Value2 = new('value2') end end class BaseEnum < GraphQL::Schema::Enum extend T::Sig class << self attr_accessor :__app_class end def self.coerce_input(value, ctx) object = super object = __app_class.deserialize(object) if __app_class && object.present? object end sig { params(klass: T.class_of(T::Enum)).void } def self.app_class(klass) self.__app_class = klass end end ``` For Objects ``` class NewInputType < BaseInputObject graphql_name 'NewInput' app_class ::App::NewInput argument :id, ID, required: true argument :value, NewEnum, required: false end class NewInput < T::Struct include HashAccess prop :id, Integer prop :value, T.nilable(NewEnum) end class BaseInputObject < GraphQL::Schema::InputObject class << self attr_accessor :__app_class end def prepare object = super object = self.class.__app_class.new(**object) if self.class.__app_class object end def self.app_class(klass) self.__app_class = klass end end ``` Maybe a there is an existing lib that doing that?",
    "author_id":5289,
    "publication_date":1754300123000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jonathan",
    "author_reputation":1.0,
    "tags":"ruby, graphql, sorbet",
    "text_length":1765,
    "title_length":27,
    "num_tags":3
  },
  {
    "id":6197,
    "title":"Using a Google Sheet with a List of URLs as a filter in a Looker Studio Blended Data Source with (GSC\/GA4) returns wrong metrics Values",
    "link":"https:\/\/stackoverflow.com\/questions\/79724606\/using-a-google-sheet-with-a-list-of-urls-as-a-filter-in-a-looker-studio-blended",
    "text":"My goal is to use a Google Sheet with a list of URLs in a blended data source in Looker Studio to filter the other data source data by the URLs in the sheet without having to use complex regular expressions and to prevent having to manually update filters in the report as a new URL needs to be added. Basically, by adding a new URL to the spreadsheet, data will be added to the report automatically without dealing with filters. Incidentally, is there any other smarter way to do this without dealing with filters\/regex? (URL list is too long). I've tested blending the data with both Google Search Console and GA4, using Landing Page as the key. What happens is that the report seems to work, but the metrics are slightly off (In the case of GA4), or completely off in the case of GSC. I'm having a hard time understanding where the problem lies, even after reading about multiple similar problems. You can see in the images above: Image 1: GSC data (upper chart) compared with blended source filtering a single URL (lower chart) Image 2: GA4 data (upper chart) compared with blended source filtering a single URL Date range for all charts is the same (July 2025). There is no other column in the google sheets, only the URL as a dimension. I'm 100% positive that the URLs match (E.g: they are lower case without special characters or spaces, and for GA4 I'm not using the default landing page dimension, It's a calculated metric with the domain concatenated (full URL) without query string parameters). While left outer join (With spreadsheet on the left) should do the trick, I've tried other join just in case, shifting the positions around and trying inner join, but nothing. Here's an example of the blended sources configuration for Google Search Console I honestly not sure what to try next. In my head it makes complete sense to me that it should work as intended, but it's not, and I can't figure out why the metric discrepancy, as I can't see the pattern. In GA4 it looks like a few sessions difference, but in GSC the impressions discrepancy is very high. Anyone trying to do the same found a solution or an alternative way that would not require manually tweaking filters when a new URL needs to be added as a filter in another data source? (Again, list of URLs is very long)",
    "author_id":5689,
    "publication_date":1754300436000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"lsgbusiness",
    "author_reputation":61.0,
    "tags":"looker-studio, google-analytics-4, google-sheets, google-search-console",
    "text_length":2289,
    "title_length":135,
    "num_tags":4
  },
  {
    "id":6196,
    "title":"How does R round(1.535, 2) → 1.53?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724610\/how-does-r-round1-535-2-%e2%86%92-1-53",
    "text":"My understanding is that R uses IEC 60559 rounding with \"Round to nearest, ties to even\", but it does not appear to be generating valid results. For decimal part .535 rounded to 2 places, my understanding is that the 5 being rounded off should cause the 3 to be rounded to the closest even, 4, except: ``` > round(1.535, 2) = 1.53 expect 1.54 > round(2.535, 2) = 2.54 > round(3.535, 2) = 3.54 > round(101.535, 2) = 101.53 expect 101.54 > round(102.535, 2) = 102.53 expect 102.54 > round(103.535, 2) = 103.53 expect 103.54 > round(201.535, 2) = 201.54 > round(202.535, 2) = 202.54 > round(203.535, 2) = 203.54 ``` Is this just floating point imprecision in R making .535 look like .53499999999 and the round being a bit imprecise?",
    "author_id":5688,
    "publication_date":1754300857000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"simpleuser",
    "author_reputation":1711.0,
    "tags":"r, floating-accuracy",
    "text_length":729,
    "title_length":34,
    "num_tags":2
  },
  {
    "id":6195,
    "title":"Programatically run next build and next start",
    "link":"https:\/\/stackoverflow.com\/questions\/79724618\/programatically-run-next-build-and-next-start",
    "text":"I have a application configuration page that would save certain parameters for the site in the .env. At this point point the application needs to be rebuilt, but I dont want to have to go the server terminal to run next build manually. How can I triger it after my page updates the .env",
    "author_id":5687,
    "publication_date":1754302019000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Fallen Angel",
    "author_reputation":21.0,
    "tags":"build, next.js15",
    "text_length":286,
    "title_length":45,
    "num_tags":2
  },
  {
    "id":6194,
    "title":"How do you get a ConcentricRectangle to respect the device corners, without having to tell it what these are?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724619\/how-do-you-get-a-concentricrectangle-to-respect-the-device-corners-without-havi",
    "text":"This question is about ``` ConcentricRectangle ``` . First, some background: The SwiftUI ``` Shape ``` ``` ContainerRelativeShape ``` has existed since iOS 14. In iOS 26, it seems to respect the shape of the device screen. So the following code shows a gray background behind the content with nice concentric corners: ``` VStack { Text(\"Content\") .frame(maxWidth: .infinity, maxHeight: .infinity) } .background(.quaternary, in: .containerRelative) .padding(20) ``` Btw, the corners are only rounded when running with iOS 26. When I tried it on an iPhone 16 simulator running iOS 18.5, the corners were square. In iOS 26, the new ``` Shape ``` ``` ConcentricRectangle ``` has been introduced. However, when used as the shape for the background in the code above, the corners are square: ``` VStack { Text(\"Content\") .frame(maxWidth: .infinity, maxHeight: .infinity) } .background(.quaternary, in: .rect(corners: .concentric)) \/\/ 👈 changed .padding(20) ``` The documentation states: To use ConcentricRectangle, first make sure the ancestor view hierarchy provides the container shape. Some container shapes are provided by platform kits, but you could provide your own by writing the containerShape(_:) modifier with a shape. OK, so lets try ``` .containerRelative ``` as the container shape: ``` VStack { Text(\"Content\") .frame(maxWidth: .infinity, maxHeight: .infinity) } .background(.quaternary, in: .rect(corners: .concentric)) .padding(20) .containerShape(.containerRelative) \/\/ 👈 added ``` Unfortunately, this doesn't work either and the corners are still square. If I knew the radius of the device corners then I could specify the container shape as a ``` RoundedRectangle ``` . I could also supply a minimum radius as a fallback. But surely, neither of these workarounds should be necessary - after all, it worked in the first example where the background shape was ``` .containerRelative ``` . How do you get a ConcentricRectangle to respect the device corners, without having to tell it what these are?",
    "author_id":5686,
    "publication_date":1754302102000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Benzy Neez",
    "author_reputation":26503.0,
    "tags":"ios, swiftui, ios26, shapes",
    "text_length":2010,
    "title_length":109,
    "num_tags":4
  },
  {
    "id":6193,
    "title":"Google Maps - Places API: Why can&#39;t I combine ADDRESS and ESTABLISHMENT in setTypesFilter()?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724620\/google-maps-places-api-why-cant-i-combine-address-and-establishment-in-setty",
    "text":"I'm using the Places SDK for Android and trying to get both addresses and business names in autocomplete predictions. Since setTypeFilter() was deprecated, I switched to setTypesFilter() as recommended. I expected that with setTypesFilter() I could pass multiple place types in one request, like this: ``` FindAutocompletePredictionsRequest.builder() .setCountries(\"US\") .setTypesFilter(listOf(PlaceTypes.ADDRESS, PlaceTypes.ESTABLISHMENT)) .setSessionToken(token) .setQuery(query) .build() ``` But when I run this, I get the following error: ``` Failure(com.google.android.gms.common.api.ApiException: 9012: address cannot be mixed with other types.) ``` So even though setTypesFilter() supports a list, it seems there are still restrictions on which types can be combined.. My questions: Is it officially unsupported to combine PlaceTypes.ADDRESS with other types like PlaceTypes.ESTABLISHMENT? If so, what’s the best practice to retrieve both address and business predictions for the same user query?",
    "author_id":5685,
    "publication_date":1754302199000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"mmm808",
    "author_reputation":301.0,
    "tags":"android, google-maps, google-places-api",
    "text_length":1003,
    "title_length":96,
    "num_tags":3
  },
  {
    "id":6192,
    "title":"generateForegroundServiceDidNotStartInTimeException Service Android",
    "link":"https:\/\/stackoverflow.com\/questions\/79724622\/generateforegroundservicedidnotstartintimeexception-service-android",
    "text":"I build an Android application in Kotlin. This application start some service where application is started. Sometimes I receive the following error when I start the application (but this error appair not every time). The error is: ``` Context.startForegroundService() did not then call Service.startForeground(): ServiceRecord{e92fa72 u0 it.silv\\\/.service.O2RingServiceReceiver} android.app.ActivityThread.generateForegroundServiceDidNotStartInTimeException(ActivityThread.java:1966) android.app.ActivityThread.throwRemoteServiceException(ActivityThread.java:1937) android.app.ActivityThread.access$2700(ActivityThread.java:257) android.app.ActivityThread$H.handleMessage(ActivityThread.java:2191) android.os.Handler.dispatchMessage(Handler.java:106)\\n\\tat android.os.Looper.loopOnce(Looper.java:201) android.os.Looper.loop(Looper.java:288)\\n\\tat android.app.ActivityThread.main(ActivityThread.java:7911) java.lang.reflect.Method.invoke(Native Method)\\n\\tat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:548) com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1009) Caused by: android.app.StackTrace: Last startServiceCommon() call for this service was made here android.app.ContextImpl.startServiceCommon(ContextImpl.java:1877) android.app.ContextImpl.startForegroundService(ContextImpl.java:1832) android.content.ContextWrapper.startForegroundService(ContextWrapper.java:781) android.content.ContextWrapper.startForegroundService(ContextWrapper.java:781) it.silv.HeaderFragment.checkPulseConfiguration(HeaderFragment.kt:618) it.silv.HeaderFragment.onCreateView(HeaderFragment.kt:122) androidx.fragment.app.Fragment.performCreateView(Fragment.java:3119) androidx.fragment.app.FragmentStateManager.createView(FragmentStateManager.java:577) androidx.fragment.app.FragmentStateManager.moveToExpectedState(FragmentStateManager.java:286) androidx.fragment.app.FragmentStore.moveToExpectedState(FragmentStore.java:114) androidx.fragment.app.FragmentManager.moveToState(FragmentManager.java:1685) androidx.fragment.app.FragmentManager.dispatchStateChange(FragmentManager.java:3319) androidx.fragment.app.FragmentManager.dispatchActivityCreated(FragmentManager.java:3237) androidx.fragment.app.FragmentController.dispatchActivityCreated(FragmentController.java:263) androidx.fragment.app.FragmentActivity.onStart(FragmentActivity.java:350) androidx.appcompat.app.AppCompatActivity.onStart(AppCompatActivity.java:251) it.silv.HomeActivity.onStart(HomeActivity.java:685) android.app.Instrumentation.callActivityOnStart(Instrumentation.java:1468) android.app.Activity.performStart(Activity.java:8110) android.app.ActivityThread.handleStartActivity(ActivityThread.java:3733) android.app.servertransaction.TransactionExecutor.performLifecycleSequence(TransactionExecutor.java:221) android.app.servertransaction.TransactionExecutor.cycleToPath(TransactionExecutor.java:201) android.app.servertransaction.TransactionExecutor.executeLifecycleState(TransactionExecutor.java:173) android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:97) ``` This is the code that I use to start the service: ``` val sensorService: Intent = Intent(context, O2RingServiceReceiver::class.java) sensorService.putExtra(\"device\", \"MACADDRESS OF DEVICE\") sensorService.putExtra(\"deviceName\", \"DEVICE NAME\") sensorService.putExtra(Const.id_user, 200) if (Build.VERSION.SDK_INT < Build.VERSION_CODES.O) { context?.startService(sensorService) break; }else if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.O && Build.VERSION.SDK_INT < Build.VERSION_CODES.TIRAMISU) { context?.startForegroundService(sensorService) break; } else { context?.startForegroundService(sensorService) break; } ``` This is O2RingServiceReceiver class: ``` package it.silv.service import android.app.ActivityManager import android.content.Intent import android.os.Handler import com.lepu.blepro.ext.oxy.RtParam import it.silv.helpers.Global import it.silv.helpers.utils.ACTION_PULSE_PACKET_RECEIVED import it.silv.job.O2RingPacketSyncJob import it_sensor.iotRequest.O2RingDataRequest import it_sensor.service.O2RingServicekt import java.time.LocalDateTime import java.time.format.DateTimeFormatter class O2RingServiceReceiver: O2RingServicekt() { var isStarted: Boolean = false var fmt: DateTimeFormatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss.SSS\") var list: ArrayList<O2RingDataRequest.DataBean> = ArrayList() private var dataRequest = O2RingDataRequest() override fun onCreate() { super.onCreate() isStarted=true; syncGeneralPacket() } override fun receivedData(data: RtParam){ \/\/TODO } private fun syncGeneralPacket() { \/\/TODO } public fun getIsStarted():Boolean{ return this.isStarted; } } ``` This is O2RingServicekt ``` package it.sensor.service import android.bluetooth.BluetoothAdapter import android.bluetooth.BluetoothDevice import android.content.Intent import android.os.Handler import android.util.Log import android.util.SparseArray import androidx.lifecycle.LifecycleService import com.jeremyliao.liveeventbus.LiveEventBus import com.lepu.blepro.event.InterfaceEvent import com.lepu.blepro.ext.BleServiceHelper import com.lepu.blepro.ext.oxy.DeviceInfo import com.lepu.blepro.ext.oxy.OxyFile import com.lepu.blepro.ext.oxy.RtParam import com.lepu.blepro.ext.oxy.RtPpg import com.lepu.blepro.objs.Bluetooth import com.lepu.blepro.objs.BluetoothController import com.lepu.blepro.observer.BIOL import com.lepu.blepro.observer.BleChangeObserver import it.sensor.util.Const open class O2RingServicekt: LifecycleService(), BleChangeObserver { private var rtHandler = Handler() private var rtTask = RtTask() private var model = Bluetooth.MODEL_O2RING val TAG: String = \"O2RING \" var idUser: Int = 0 lateinit var macAddress: String lateinit var deviceName: String private val models = intArrayOf( Bluetooth.MODEL_O2RING ) override fun onCreate() { super.onCreate() initService() log(\"BackgroundTaskService is ready to conquer!\") } override open fun onStartCommand(intent: Intent?, flags: Int, startId: Int): Int { super.onStartCommand(intent, flags, startId) idUser = intent!!.getStringExtra(Const.id_user)!!.toInt() macAddress = intent!!.getStringExtra(\"device\")!!.toString() deviceName = intent!!.getStringExtra(\"deviceName\")!!.toString() log(\"BackgroundTaskService is performing a task! Don't disturb, please!\") BleServiceHelper.BleServiceHelper.setInterfaces(4) val device: BluetoothDevice = BluetoothAdapter.getDefaultAdapter().getRemoteDevice(macAddress) BleServiceHelper.BleServiceHelper.stopScan() BleServiceHelper.BleServiceHelper.connect(applicationContext, 4, device) BluetoothController.clear() lifecycle.addObserver(BIOL(this, intArrayOf(4))) initEventBus() rtHandler.removeCallbacks(rtTask) rtHandler.post(rtTask) return START_STICKY \/\/ If the service is killed, it will be automatically restarted } inner class RtTask: Runnable { override fun run() { rtHandler.postDelayed(rtTask, 1000) BleServiceHelper.BleServiceHelper.oxyGetRtParam(model) rtHandler.removeCallbacks(rtTask) rtHandler.post(rtTask) } } override fun onDestroy() { super.onDestroy() log(\"BackgroundTaskService says goodbye!\") } fun log(str:String){ \/\/Log.d(\"TAG\", \"log: $str\") } private fun initService() { if (BleServiceHelper.BleServiceHelper.checkService()) { BleServiceHelper.BleServiceHelper.startScan(models) } else { val rawFolders = SparseArray<String>() BleServiceHelper.BleServiceHelper.initRawFolder(rawFolders).initService(application).initLog(true) } } private fun initEventBus() { LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxyInfo) .observe(this) { val data = it.data as DeviceInfo val list = data.fileList.split(\",\") } LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxyReadFileError) .observe(this) { val data = it.data as Boolean Log.d(TAG, \"EventOxyReadFileError $data\") } LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxyReadingFileProgress) .observe(this) { val data = it.data as Int Log.d(TAG, \"进度 $data%\") } LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxyReadFileComplete) .observe(this) { val data = it.data as OxyFile Log.d(TAG, \"$data\") } LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxyRtParamData) .observe(this) { val data = it.data as RtParam Log.d(TAG, data.spo2.toString() + \" \" + data.pr.toString()+ \" \" + data.pi.toString()+ \" \"+\"$data\") receivedData(data) } LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxyFactoryReset) .observe(this) { val data = it.data as Boolean Log.d(TAG, \"EventOxyFactoryReset $data\") } LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxySyncDeviceInfo) .observe(this) { val types = it.data as Array<String> for (type in types) { Log.d(TAG, \"$type success\") } } LiveEventBus.get<InterfaceEvent>(InterfaceEvent.Oxy.EventOxyPpgData) .observe(this) { \/\/ BleServiceHelper.BleServiceHelper.oxyGetPpgRt(model) val data = it.data as RtPpg Log.d(TAG, \"EventOxyFactoryReset $data\") } } override fun onBleStateChanged(model: Int, state: Int) { Log.d(TAG, \"model $model, state: $state\") } open fun receivedData(data: RtParam){} } ```",
    "author_id":5684,
    "publication_date":1754302407000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"bircastri",
    "author_reputation":2185.0,
    "tags":"android, kotlin, service",
    "text_length":9057,
    "title_length":67,
    "num_tags":3
  },
  {
    "id":6191,
    "title":"How to generate aadhaar card data from qr code",
    "link":"https:\/\/stackoverflow.com\/questions\/79724624\/how-to-generate-aadhaar-card-data-from-qr-code",
    "text":"I'm trying to generate user data from aadhaar qr code, i'm trying the 2 methods ``` from aadhaar.secure_qr import extract_data extracted_data = extract_data(int(qrData)) print(extracted_data.text_data) # and from pyaadhaar.decode import AadhaarSecureQr secure_qr = AadhaarSecureQr(int(normal_int)) print(secure_qr.data) ``` Both are almost similar methods, but when I try to exctract data from the qr on the uidai site (3181 chars) and this sample data (3166 chars) the data is succesfully generated, but when I try to scan my own aadhaar qr (3142 chars), the following errors occur ``` Traceback (most recent call last): File \"...\", line 18, in <module> extracted_data = extract_data(int(qrData)) ^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\aadhaar\\secure_qr\\extractor.py\", line 330, in extract_data return data_extractor.extract() ^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\aadhaar\\secure_qr\\extractor.py\", line 318, in extract text_data=self._make_text_data(), ^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\aadhaar\\secure_qr\\extractor.py\", line 212, in _make_text_data reference_id=self._make_reference_id(extracted_text_data[\"reference_id\"]), ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\aadhaar\\secure_qr\\extractor.py\", line 197, in _make_reference_id timestamp=datetime.strptime(extracted_data[4:], \"%Y%m%d%H%M%S%f\"), ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\_strptime.py\", line 554, in _strptime_datetime tt, fraction, gmtoff_fraction = _strptime(data_string, format) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\_strptime.py\", line 333, in _strptime raise ValueError(\"time data %r does not match format %r\" % ValueError: time data '' does not match format '%Y%m%d%H%M%S%f' # and Traceback (most recent call last): File \"...\", line 18, in <module> secure_qr = AadhaarSecureQr(int(normal_int)) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyaadhaar\\decode.py\", line 27, in __init__ self._extract_info_from_decompressed_array() File \"C:\\...\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyaadhaar\\decode.py\", line 52, in _extract_info_from_decompressed_array self.data['aadhaar_last_digit'] = self.data['referenceid'][3] ~~~~~~~~~~~~~~~~~~~~~~~~^^^ IndexError: string index out of range ``` I want to know the solution to the problem or any other methods to do the same. Thank you : )",
    "author_id":5683,
    "publication_date":1754302549000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Shubham Singhvi",
    "author_reputation":432.0,
    "tags":"python, aadhaar",
    "text_length":2716,
    "title_length":46,
    "num_tags":2
  },
  {
    "id":6190,
    "title":"How do I make an insert with TMyQuery?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724627\/how-do-i-make-an-insert-with-tmyquery",
    "text":"I'm trying to make an insert using ``` TMyQuery ``` (the doc here says to use it), but when I call ``` Insert() ``` after having defined my fields and connection, I get an error saying the dataset isn't opened. But when I try to set ``` Active ``` to True, it says there's no SQL statement. So how am I supposed to make an insert here?",
    "author_id":5682,
    "publication_date":1754302627000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Arcus",
    "author_reputation":55.0,
    "tags":"delphi, mydac",
    "text_length":335,
    "title_length":38,
    "num_tags":2
  },
  {
    "id":6189,
    "title":"Tatsu parsing – how to skip until a specific word using a rule?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724630\/tatsu-parsing-how-to-skip-until-a-specific-word-using-a-rule",
    "text":"Is there an elegant way to skip input until a specific word using TatSu , without relying on raw regex? For example, I can do: ``` start=\/(.|\\n)*?John\/ ``` But I'd like to write it using proper grammar rules, something like: ``` start = skip_until('John'). ``` I tried defining rules, but couldn't make it work. I want to match multiple names (e.g., \"John\", \"Anna\", etc.) that may appear after arbitrary text (including newlines). Here is the code I want to improve. Is there a more modular way to do this in TatSu? ``` from tatsu import compile grammar = ''' start = \/(.|\\n)*?John\/ ; ''' model = compile(grammar) text = \"\"\"Some random text on multiple lines before John.\"\"\" ast = model.parse(text) print(ast) ```",
    "author_id":5681,
    "publication_date":1754302979000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"רז יחזקאל",
    "author_reputation":1.0,
    "tags":"python, parsing, tatsu",
    "text_length":713,
    "title_length":63,
    "num_tags":3
  },
  {
    "id":6188,
    "title":"String Extraction using R",
    "link":"https:\/\/stackoverflow.com\/questions\/79724631\/string-extraction-using-r",
    "text":"I have a data frame with columns that contains names of movies ``` df = data.frame (movies = c(\"Toy Story\", \"Toy Story 2\", \"Toy Story 3\", \"The Incredibles\", \"Incredibles 2\", \"Elemental\", \"Monsters Inc.\", \"Monsters University\", \"Luca\")) ``` From this column, I want to extract movies and their sequels. The output should look like this ``` movies 1 Toy Story 2 Toy Story 2 3 Toy Story 3 4 The Incredibles 5 Incredibles 2 6 Monsters Inc. 7 Monsters University ``` Doing the following, for example ``` df[grepl(\"Toy Story\", df$movies, fixed = TRUE),1] ``` extracts ``` \"Toy Story\" \"Toy Story 2\" \"Toy Story 3\" ``` Similarly I would like to extract the rest of the movies as well. Suggestions are welcome.",
    "author_id":5680,
    "publication_date":1754303058000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Swaroopa ",
    "author_reputation":9.0,
    "tags":"string, r, extract, stringr, grepl",
    "text_length":700,
    "title_length":25,
    "num_tags":5
  },
  {
    "id":6187,
    "title":"CManagerApi not recognized in C# project using MetaTrader 5 Manager API",
    "link":"https:\/\/stackoverflow.com\/questions\/79724632\/cmanagerapi-not-recognized-in-c-project-using-metatrader-5-manager-api",
    "text":"I'm trying to use the MetaTrader 5 Manager API in a C# project to monitor real-time order or position updates using the ``` CManagerApi ``` class. I've already added a reference to the DLL from the official MT5 SDK: ``` MetaQuotes.MT5ManagerAPI(64).dll ``` But I'm still getting the following error: The type or namespace name 'CManagerApi' could not be found (are you missing a using directive or an assembly reference? (CS0246) ``` using MetaQuotes.MT5ManagerAPI; class Program { static void Main(string[] args) { var api = new CManagerApi(); \/\/ This is where the error occurs } } ``` Am I missing any other DLL that ``` MetaQuotes.MT5ManagerAPI(64).dll ``` depends on? Is ``` CManagerApi ``` possibly excluded in certain versions of the MT5 API? Is there any step I missed in registering the DLL or making it accessible? Any help or pointers from anyone who's worked with MT5 Manager API and .NET would be really appreciated! Any guidance would be really appreciated — especially since I'm still getting familiar with the C# ecosystem.",
    "author_id":5679,
    "publication_date":1754303081000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Karan Parwani",
    "author_reputation":11.0,
    "tags":"c#, .net, metatrader5",
    "text_length":1038,
    "title_length":71,
    "num_tags":3
  },
  {
    "id":6186,
    "title":"How to use javascript onmouseover event from multiple divs of same class?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724642\/how-to-use-javascript-onmouseover-event-from-multiple-divs-of-same-class",
    "text":"I'm struggling with JS, just started relearning after a 5 year break. I've got four div with the same class, I don't want them to all change at the same time, only the one I'm hovering over. When I use ClassName, nothing happens, when I use Id, the first div transitions, but hovering on the other four changes the first one only as well. JS (with ClassName): ``` function HoverEvent() { var element = document.getElementsByClassName(\"js\"); element.classList.remove(\"character\"); element.classList.add(\"character_hover\"); var element = document.getElementsByClassName(\"js\"); element.classList.remove(\"icon\"); element.classList.add(\"icon_hover\"); } function HoverRemove () { var element = document.getElementsByClassName(\"js\"); element.classList.remove(\"character_hover\"); element.classList.add(\"character\"); var element = document.getElementsByClassName(\"js\"); element.classList.remove(\"icon_hover\"); element.classList.add(\"icon\"); } ``` HTML: ``` <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">Title<\/div> <div>Info here<\/div> <\/div> <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">Title<\/div> <div>Info here<\/div> <\/div> <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">Title<\/div> <div>Info here<\/div> <\/div> <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">Title<\/div> <div>Info here<\/div> <\/div> ``` CSS: ``` .character, .character_hover{ height:250px; width:170px; transition:.2s; } .character_hover{ background-color:#373f28!important; } .icon, .icon_hover{ background-repeat:no-repeat; background-size:cover; background-color:white; margin:auto; border-radius:10px; transition:.2s; } .icon{ width:100px; height:100px; } .icon_hover{ width:120px; height:120px; } ``` Snippet (using getElementById instead of ClassName) ``` function HoverEvent() { var element = document.getElementById(\"character\"); element.classList.remove(\"character\"); element.classList.add(\"character_hover\"); var element = document.getElementById(\"icon\"); element.classList.remove(\"icon\"); element.classList.add(\"icon_hover\"); } function HoverRemove() { var element = document.getElementById(\"character\"); element.classList.remove(\"character_hover\"); element.classList.add(\"character\"); var element = document.getElementById(\"icon\"); element.classList.remove(\"icon_hover\"); element.classList.add(\"icon\"); } ``` ``` .character, .character_hover { background-color: #414833; box-sizing: border-box; border-color: black; border-style: solid; border-width: 3px; margin: 10px 10px 0 10px; padding: 5px 10px 5px 10px; border-radius: 5px; } .character, .character_hover { height: 250px; width: 170px; transition: .2s; } .character_hover { background-color: #373f28 !important; } .icon, .icon_hover { background-color: white; margin: auto; border-radius: 10px; transition: .2s; } .icon { width: 100px; height: 100px; } .icon_hover { width: 120px; height: 120px; } ``` ``` <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">title<\/div> <div>Info here<\/div> <\/div> <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">title<\/div> <div>Info here<\/div> <\/div> <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">title<\/div> <div>Info here<\/div> <\/div> <div id=\"character\" class=\"js character\" onmouseover=\"HoverEvent()\" onmouseleave=\"HoverRemove()\"> <div id=\"icon\" class=\"icon\"><\/div> <div class=\"title\">title<\/div> <div>Info here<\/div> <\/div> ``` I tried using ClassName instead of Id, which stopped all transitions from working. Id only affects the first one (I know how Ids work so that is to be expected.) Can this be done using a class on all elements and some kind of code to 'know' which div I want to affect? Edit: I know IDs must be unique. What I was missing was the (myFunction( This ) & how to optimize this code without having a string for each element. CSS is not a solution as far as I know, because I want the child element to change when I hover over the parent, synchronously.",
    "author_id":5678,
    "publication_date":1754303863000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Charlie Winters",
    "author_reputation":21.0,
    "tags":"javascript, html",
    "text_length":4575,
    "title_length":73,
    "num_tags":2
  },
  {
    "id":6185,
    "title":"Moving salted passwords from SQL Server to MySQL",
    "link":"https:\/\/stackoverflow.com\/questions\/79724649\/moving-salted-passwords-from-sql-server-to-mysql",
    "text":"Thanks in advance for any help here. I'm moving the backend database for my website from SQL Server to MySQL. I'm having trouble working out how to check login passwords against those in the DB. The salts were generated by SQL Server using the UniqueIdentifier field. They have 36 characters, including 4 dashes. e.g. 6914b26f-21d7-439f-99d9-207732a4555e The passwords were created and compared using: ``` HASHBYTES('SHA2_512','myPass' + CAST(Salt AS NVARCHAR(36))) ``` The salted passwords are stored as varbinary(200). If I open a table and view the data in SSMS they just say 'Binary data'. I used the migration wizard in MySQL Workbench to bring the database across. The migration converted the salts to VarChar(64) with character set utf8mb4. They now seem to be uppercase, but otherwise identical. e.g. 6914B26F-21D7-439F-99D9-207732A4555E The salted passwords stayed as varbinary(200). If I open the table in Workbench, the field says 'Blob'. The equivalent SQL in MYSQL should be: ``` SHA2(CONCAT('myPass', CAST(Salt AS NVARCHAR(36))),512) ``` But MySQL can't cast to NVARCHAR. UPDATED QUESTION I think the problem is boiling down to the use of NVARCHAR when creating the hashed passwords. Just testing different ideas in each database, the following produce the same result: SQL Server ``` SELECT CONVERT(VARCHAR(1000), HASHBYTES('SHA2_512', 'myPass' + CAST('Salt' AS varchar(36))), 2) AS Expr1 ``` MYSQL ``` SELECT SHA2(CONCAT('myPass', CAST('Salt' AS Char(36))),512) AS Expr1 ``` Unfortunately, when you use NVARCHAR in SQL server (as I have when creating the hashed passwords) the output is different. I need to find a way of creating the equivalent of NVARCHAR in MYSQL. I tried the below, but it doesn't change the output in MYSQL: ``` SELECT SHA2(CONCAT('myPass', CAST('Salt' AS CHAR(36) CHARACTER SET utf8mb4)),512) AS Expr1 ``` This seems relevant, but doesn't offer a solution for MYSQL. https:\/\/dba.stackexchange.com\/questions\/177590\/using-hashbytes-yields-different-results-for-nvarchar-and-a-variable Any ideas appreciated.",
    "author_id":5677,
    "publication_date":1754304248000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user31207162",
    "author_reputation":21.0,
    "tags":"mysql, passwords, hash, casting, migration",
    "text_length":2044,
    "title_length":48,
    "num_tags":5
  },
  {
    "id":6184,
    "title":"Segmentation Error when Creating Charuco Board with Custom Ids",
    "link":"https:\/\/stackoverflow.com\/questions\/79724652\/segmentation-error-when-creating-charuco-board-with-custom-ids",
    "text":"My intention is to create a charuco board object, which supports custom ids. Here is the code snippet being used. ``` def __init__(self, squaresX=11, squaresY=8, squareLength=0.015, markerLength=0.011, dict=cv2.aruco.DICT_5X5_250, start_id=0): self.squaresX = squaresX self.squaresY = squaresY self.squareLength = squareLength self.markerLength = markerLength self.aruco_dict = cv2.aruco.getPredefinedDictionary(dict) self.start_id = start_id num_markers_x = squaresX - 1 num_markers_y = squaresY - 1 num_markers = num_markers_x * num_markers_y if start_id + num_markers > self.aruco_dict.bytesList.shape[0]: raise ValueError(f\"Not enough markers in dictionary for board (required: {num_markers})\") marker_ids = np.arange(start_id, start_id + num_markers, dtype=np.int32).reshape(num_markers_y, num_markers_x) self.board = cv2.aruco.CharucoBoard( (self.squaresX, self.squaresY), self.squareLength, self.markerLength, self.aruco_dict, marker_ids ) ``` However when I create ``` self.board ``` , there is a segmentation error (keep in mind when I'm creating the object, I'm using the same arguments as the default arguments to the ``` __init__ ``` method. ``` cv2.__version__ ``` returns ``` 4.11.0 ```",
    "author_id":5676,
    "publication_date":1754304356000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tommy Llewellyn",
    "author_reputation":37.0,
    "tags":"python, computer-vision, opencv, camera-calibration",
    "text_length":1200,
    "title_length":62,
    "num_tags":4
  },
  {
    "id":6183,
    "title":"Next.js Fallback &#39;blocking&#39; vs true - Generates a different number of paths?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724654\/next-js-fallback-blocking-vs-true-generates-a-different-number-of-paths",
    "text":"So I'm working on a project that generates static pages for a few hundred products. My problem that I'm currently having is that I need to be able to anticipate slug changes or additions, so I've been trying to set up my ``` getStaticPaths() ``` accordingly. However I've noticed that an additional path is generated if I use ``` fallback: true ``` over ``` fallback: blocking ``` . See the logs of my build below: Fallback Blocking: ``` > next build ▲ Next.js 15.4.5 - Environments: .env.local Linting and checking validity of typ ✓ Linting and checking validity of types Creating an optimized production build ... ✓ Compiled successfully in 0ms ✓ Collecting page data Generating static pages (8\/1395) ``` Fallback: True ``` > next build ▲ Next.js 15.4.5 - Environments: .env.local Linting and checking validity of typ ✓ Linting and checking validity of types Creating an optimized production build ... ✓ Compiled successfully in 0ms ✓ Collecting page data Generating static pages (14\/1396) ``` Why does this happen? I want to be able to use ``` fallback: true ``` as it's a bit better for my use case, but because it generates an extra path - which appears to be completely invalid causing my site to fail build, I can't use it. Any help would be greatly appreciated.",
    "author_id":5675,
    "publication_date":1754304390000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tony Ingall",
    "author_reputation":37.0,
    "tags":"next.js",
    "text_length":1269,
    "title_length":84,
    "num_tags":1
  },
  {
    "id":6182,
    "title":"CSS equations for calculating thumbnail widths",
    "link":"https:\/\/stackoverflow.com\/questions\/79724660\/css-equations-for-calculating-thumbnail-widths",
    "text":"I want a section of a page to show three 16x9 video thumbnails, arranged with a large thumbnail on the left and two smaller thumbnails on the right. I want the \"column\" containing the two small thumbnails to match the height of the large thumbnail, so I need to calculate the correct thumbnail widths. This diagram illustrates what I want to achieve. The known factors are the total width ( w ) of the layout and the desired gap ( z ) between thumbnails. The unknown factors, which I need to calculate, are the width of the large image ( x ) and the width of the smaller images ( y ). Here is an implementation which approximates what I want to do: ``` body { margin: 0; text-align: justify; } main { margin: 1em max(1em, 10vw); container-type: inline-size; } .videos { --gap: 20px; --x: 64cqw; --y: 30cqw; display: flex; gap: var(--gap); align-items: start; .right-column { display: flex; flex-direction: column; gap: var(--gap); } } #v1, #v2, #v3 { aspect-ratio: 16\/9; background: green; } #v1 { width: var(--x); flex-shrink: 0; } #v2, #v3 { width: var(--y); } ``` ``` <main> <p> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas tempor nunc mauris, sit amet placerat tortor lobortis dapibus. Nam lectus eros, maximus ac magna vel, congue consequat eros. Fusce id pretium diam. Cras sit amet pharetra ante. Sed quis commodo quam, vel facilisis ipsum. Vestibulum sodales iaculis arcu, et fringilla nisi ullamcorper sed. Donec interdum sit amet est non accumsan. Donec non augue feugiat, fermentum nunc non, convallis est. Cras vel ligula nec odio faucibus ultricies. Sed vulputate tortor eget pretium convallis. Cras interdum elit eget mi porta suscipit. Morbi ut velit diam. Etiam finibus eros et efficitur rutrum. Quisque viverra metus ac eleifend imperdiet. Quisque pretium ut purus vitae tempus. Duis varius risus congue velit faucibus, sed interdum purus consectetur. <\/p> <p> Cras volutpat velit non mi sagittis condimentum. Cras tempor aliquet turpis sed pretium. Nunc aliquet sodales turpis quis ultrices. Duis auctor accumsan enim, quis maximus ex malesuada a. Donec a felis ut erat tempus euismod non vel neque. Proin lectus massa, sagittis at imperdiet nec, consequat ut neque. Sed vel placerat neque, vel varius urna. Vivamus interdum euismod urna a accumsan. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. <\/p> <section class=\"videos\"> <div id=\"v1\"><\/div> <div class=\"right-column\"> <div id=\"v2\"><\/div> <div id=\"v3\"><\/div> <\/div> <\/section> <p> Nulla rhoncus aliquam mauris, eu pretium dolor auctor in. Maecenas a sollicitudin dolor, eget commodo quam. Proin et dui sed ligula vulputate egestas. Quisque eget augue vitae purus placerat pharetra. Aliquam rhoncus convallis lorem, sed facilisis odio blandit scelerisque. Vivamus viverra urna ac nulla interdum, eget ullamcorper leo maximus. Mauris nec feugiat enim. Nam congue, dui sit amet vestibulum posuere, leo mauris fermentum lorem, eget bibendum velit nunc quis leo. <\/p> <p> Curabitur eget ullamcorper justo, sit amet dictum neque. Fusce vitae ligula et felis auctor vulputate vel suscipit nibh. Integer a felis varius purus vestibulum viverra. Morbi venenatis placerat augue sit amet commodo. Sed dapibus molestie eros, vitae ultrices nunc commodo aliquam. Vivamus tempus mollis massa vel egestas. Donec ut ante quis eros commodo volutpat. Proin sem nisi, viverra ac sem tristique, consectetur laoreet sapien. Vivamus suscipit orci vel euismod scelerisque. Nullam sed pulvinar tellus. Nullam pulvinar arcu eget nibh rutrum, eget faucibus ligula ullamcorper. <\/p> <\/main> ``` In this implementation, I am setting ``` x ``` and ``` y ``` to simple percentages of the container width. This does roughly what I want, but I want it to be pixel perfect, and I want to be able to change the gap ( ``` z ``` ) without breaking the layout. To complete my layout, how do I develop the precise CSS ``` calc() ``` declarations for determining ``` x ``` and ``` y ``` ?",
    "author_id":5674,
    "publication_date":1754304859000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Brett Donald",
    "author_reputation":15339.0,
    "tags":"css",
    "text_length":3983,
    "title_length":46,
    "num_tags":1
  },
  {
    "id":6181,
    "title":"How to show an alert dialog based on a flag from backend",
    "link":"https:\/\/stackoverflow.com\/questions\/79724671\/how-to-show-an-alert-dialog-based-on-a-flag-from-backend",
    "text":"I want to show an alert dialog based on backend flag if user payment is failed\/cancelled his subscription I have this layout which is under src\/app\/(routes)\/(user) and all routes use it ``` import { ReactNode } from 'react'; import { TooltipProvider } from '@\/components\/ui\/tooltip'; import FetchPanelLayout from '@\/components\/layout\/admin-panel\/fetch-panel-layout'; import '@\/shared\/lib\/initClientLogger'; const UserLayout = ({ children }: { children: ReactNode }) => ( <TooltipProvider> <FetchPanelLayout>{ children }<\/FetchPanelLayout> <\/TooltipProvider> ); export default UserLayout; the fetchpanel layout: import { getCurrentCoach } from '@\/shared\/lib\/api'; import AdminPanelLayout from '.\/admin-panel-layout'; import { redirect } from 'next\/navigation'; import { headers } from 'next\/headers'; const FetchPanelLayout = async ({ children }: { children: React.ReactNode }) => { const headersList = headers(); const referer = headersList.get('referer') || ''; const pathname = referer ? new URL(referer).pathname : '\/'; const coach = await getCurrentCoach(); if (!coach) { redirect('\/signin'); } if (coach.signupStep === 1) { redirect('\/signup?step=2'); } if (coach.signupStep === 2) { redirect('\/signup?step=3'); } if (!coach.verified) { redirect('\/unverified'); } if (coach.signupStep === 3) { redirect('\/signup?step=4'); } if (coach.signupStep === 4 && coach.traineesLimit === 0 && !pathname.includes('settings')) { redirect('\/settings\/change-plan'); } return <AdminPanelLayout coach ={coach}>{ children }<\/AdminPanelLayout>; }; export default FetchPanelLayout; ``` Admin panel layout: ``` 'use client'; import { cn } from '@\/shared\/lib\/utils'; import Sidebar from '.\/sidebar'; import { useSidebar } from '.\/use-sidebar'; import { useStore } from '.\/use-store'; import { Coach } from '@\/shared\/helpers\/types'; const AdminPanelLayout = ({ children , coach , }: { children: React.ReactNode; coach: Coach; }) => { const sidebar = useStore(useSidebar, ( x ) => x ); if (!sidebar) return null; const { getOpenState, settings } = sidebar; return ( <> <Sidebar coach ={ coach } \/> <main className ={cn( 'min-h-screen bg-zinc-50 transition-[margin-right] duration-300 ease-in-out dark:bg-zinc-900', !settings.disabled && (!getOpenState() ? 'lg:mr-[90px]' : 'lg:mr-64'), )} > { children } <\/main> <\/> ); }; export default AdminPanelLayout; ``` Inside the page itself i also use the content-layout ``` import { redirect } from 'next\/navigation'; import { getCurrentCoach } from '@\/shared\/lib\/api'; import Navbar from '.\/navbar'; import { cn } from '@\/shared\/lib\/utils'; import { hebrewTranslations } from '@\/shared\/lib\/constants'; interface ContentLayoutProps { title: string; description?: string; children: React.ReactNode; className?: string; } const ContentLayout = async ({ title , className , children , description , }: ContentLayoutProps) => { const coach = await getCurrentCoach(); const isWhatsAppPage = title === 'וואטסאפ' || title === 'WhatsApp'; return ( <div> <Navbar title ={ title } description ={ description } coach ={coach} \/> <div className ={cn( !isWhatsAppPage && title !== hebrewTranslations['calendar'] && 'container px-4 pb-8 pt-8 sm:px-8 lg:pt-12', className , )} > { children } <\/div> <\/div> ); }; export default ContentLayout; ``` But, if I have traineesLimit.0 i get an error ``` \"page tried to redirect you too many times\" and site crashes ``` . I want to just have an alert dialog that opens and says user has traineesLimit 0, and he must change plan\/update card payments and two link buttons, but when i tried adding this and opening the alertdialog on all routes (disabling the redirect), it just didnt open the alert dialog So my questions are first how to make the alert dialog open all routes when flag traineeslimit is 0, and not on the settings page, and second, why do I get page tried to redirect you too many times?",
    "author_id":5673,
    "publication_date":1754305219000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"steve123",
    "author_reputation":425.0,
    "tags":"next.js, next.js13, nextjs14",
    "text_length":3852,
    "title_length":56,
    "num_tags":3
  },
  {
    "id":6180,
    "title":"Generate Python wheels with conan",
    "link":"https:\/\/stackoverflow.com\/questions\/79724676\/generate-python-wheels-with-conan",
    "text":"I want to generate Python wheels for my project (the last step before to put them on PyPI). I use conan for that. CMake found system dependencies but not conan dependencies for Ceres, Eigen and pybind11, and also MSYS2 includes. Here is ``` conanfile.txt ``` : ``` [requires] ceres-solver\/2.1.0 eigen\/3.4.0 zlib\/1.2.13 libcurl\/8.6.0 pybind11\/2.12.0 boost\/1.84.0 [generators] CMakeToolchain CMakeDeps [options] ceres-solver\/2.1.0:shared=False ceres-solver\/2.1.0:use_glog=False libcurl\/8.6.0:with_ssl=openssl libcurl\/8.6.0:with_zlib=True libcurl\/8.6.0:shared=False boost\/1.84.0:shared=False ``` The command output is: ``` d:\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg>cd build_conan d:\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg\\build_conan>conan install .. --build=missing ======== Input profiles ======== Profile host: [settings] arch=x86_64 build_type=Release compiler=msvc compiler.cppstd=14 compiler.runtime=dynamic compiler.runtime_type=Release compiler.version=194 os=Windows Profile build: [settings] arch=x86_64 build_type=Release compiler=msvc compiler.cppstd=14 compiler.runtime=dynamic compiler.runtime_type=Release compiler.version=194 os=Windows ======== Computing dependency graph ======== Graph root conanfile.txt: d:\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg\\conanfile.txt Requirements ceres-solver\/2.1.0#61809d7baf47aeb1a0a82472ba2283d0 - Cache eigen\/3.4.0#2e192482a8acff96fe34766adca2b24c - Cache libcurl\/8.6.0#1daf20281d5c28d4999ecde631e15790 - Cache openssl\/3.5.1#7884fb47cae4130ac03814e53e3c7167 - Cache pybind11\/2.12.0#ea1de6fefd9c0c745e2b30e60dd23bfa - Cache zlib\/1.2.13#9df41c65e2c2b6ef47633dc32e0b699a - Cache Build requirements nasm\/2.16.01#31e26f2ee3c4346ecd347911bd126904 - Cache strawberryperl\/5.32.1.1#8d114504d172cfea8ea1662d09b6333e - Cache Resolved version ranges openssl\/[>=1.1 <4]: openssl\/3.5.1 ======== Computing necessary packages ======== Connecting to remote 'conancenter' anonymously zlib\/1.2.13: Main binary package '0d6dd492a7d31822b2f2686ec67bbaef586416a3' missing zlib\/1.2.13: Checking 1 compatible configurations zlib\/1.2.13: Found compatible package '7bfde258ff4f62f75668d0896dbddedaa7480a0f': compiler.version=193 ceres-solver\/2.1.0: Main binary package '44419d077b4887d6d4f6924e3ecd8c57e9cffe84' missing ceres-solver\/2.1.0: Checking 7 compatible configurations ceres-solver\/2.1.0: Found compatible package 'cab6b52ecb1cf2dcf7189e655879e8c802561369': compiler.version=193 Requirements ceres-solver\/2.1.0#61809d7baf47aeb1a0a82472ba2283d0:cab6b52ecb1cf2dcf7189e655879e8c802561369#421d2a33e32bd11f2ee89892cb68e8ba - Cache eigen\/3.4.0#2e192482a8acff96fe34766adca2b24c:da39a3ee5e6b4b0d3255bfef95601890afd80709#b2e7c2d86c5d1dbefc534889aa72e12c - Cache libcurl\/8.6.0#1daf20281d5c28d4999ecde631e15790:64f76b680a223ce0061500e564b2b35f8d9884bf#1531fc7604889978430f77ec01b4bd6f - Cache openssl\/3.5.1#7884fb47cae4130ac03814e53e3c7167:8a4e0dce57bcc1cce0e82875de79e2efc69d4cb8#d28ab3add0e5a1b78eafd74ede2fd2e3 - Cache pybind11\/2.12.0#ea1de6fefd9c0c745e2b30e60dd23bfa:da39a3ee5e6b4b0d3255bfef95601890afd80709#8b620527f1fc5738a168e12326b606fb - Cache zlib\/1.2.13#9df41c65e2c2b6ef47633dc32e0b699a:7bfde258ff4f62f75668d0896dbddedaa7480a0f#a4a942c3c35bb1282dfc546b6ac1de88 - Cache Build requirements Skipped binaries nasm\/2.16.01, strawberryperl\/5.32.1.1 ======== Installing packages ======== eigen\/3.4.0: Already installed! (1 of 6) pybind11\/2.12.0: Already installed! (2 of 6) zlib\/1.2.13: Already installed! (3 of 6) ceres-solver\/2.1.0: Already installed! (4 of 6) openssl\/3.5.1: Already installed! (5 of 6) libcurl\/8.6.0: Already installed! (6 of 6) WARN: deprecated: Usage of deprecated Conan 1.X features that will be removed in Conan 2.X: WARN: deprecated: 'cpp_info.names' used in: eigen\/3.4.0, ceres-solver\/2.1.0, zlib\/1.2.13 WARN: deprecated: 'cpp_info.build_modules' used in: ceres-solver\/2.1.0 ======== Finalizing install (deploy, generators) ======== conanfile.txt: Writing generators to d:\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg conanfile.txt: Generator 'CMakeToolchain' calling 'generate()' conanfile.txt: CMakeToolchain generated: conan_toolchain.cmake conanfile.txt: CMakeToolchain: Preset 'conan-default' added to CMakePresets.json. (cmake>=3.23) cmake --preset conan-default (cmake<3.23) cmake <path> -G \"Visual Studio 17 2022\" -DCMAKE_TOOLCHAIN_FILE=conan_toolchain.cmake -DCMAKE_POLICY_DEFAULT_CMP0091=NEW conanfile.txt: CMakeToolchain generated: d:\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg\\CMakePresets.json conanfile.txt: Generator 'CMakeDeps' calling 'generate()' conanfile.txt: CMakeDeps necessary find_package() and targets for your CMakeLists.txt find_package(Ceres) find_package(Eigen3) find_package(CURL) find_package(ZLIB) find_package(pybind11) target_link_libraries(... Ceres::ceres Eigen3::Eigen CURL::libcurl ZLIB::ZLIB pybind11_all_do_not_use) conanfile.txt: Generating aggregated env files conanfile.txt: Generated aggregated env files: ['conanbuild.bat', 'conanrun.bat'] Install finished successfully d:\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg\\build_conan>cmake .. -DCMAKE_TOOLCHAIN_FILE=conan_toolchain.cmake -- Using Conan toolchain: D:\/Programmes\/msys64\/home\/Julien\/symreg_ws\/SymReg\/conan_toolchain.cmake -- Conan toolchain: CMAKE_GENERATOR_TOOLSET=v143 -- Conan toolchain: Setting CMAKE_MSVC_RUNTIME_LIBRARY=$<$<CONFIG:Release>:MultiThreadedDLL> -- Conan toolchain: C++ Standard 14 with extensions OFF -- Selecting Windows SDK version 10.0.26100.0 to target Windows 10.0.19045. -- Found required Ceres dependency: absl version 20250127 in C:\/Program Files (x86)\/Ceres\/lib\/cmake\/absl -- Found required Ceres dependency: Eigen version 3.4.90 in D:\/Programmes\/eigen-master\/build -- Found Ceres version: 2.3.0 installed in: C:\/Program Files (x86)\/Ceres with components: [EigenSparse, SparseLinearAlgebraLibrary, SchurSpecializations] -- Found pybind11: D:\/Programmes\/msys64\/ucrt64\/include (found version \"2.13.6\") -- Warning: Standard CMAKE_CXX_STANDARD value defined in conan_toolchain.cmake to 14 has been modified to 23 by D:\/Programmes\/msys64\/home\/Julien\/symreg_ws\/SymReg\/CMakeLists.txt -- Found Python3: D:\/Programmes\/Python\/Python312\/python.exe (found version \"3.12.4\") found components: Interpreter -- Found required Ceres dependency: absl version 20250127 in C:\/Program Files (x86)\/Ceres\/lib\/cmake\/absl -- Found required Ceres dependency: Eigen version 3.4.90 in D:\/Programmes\/eigen-master\/build -- Found Ceres version: 2.3.0 installed in: C:\/Program Files (x86)\/Ceres with components: [EigenSparse, SparseLinearAlgebraLibrary, SchurSpecializations] -- Conan: Target declared 'ZLIB::ZLIB' -- Could NOT find WrapVulkanHeaders (missing: Vulkan_INCLUDE_DIR) -- Could NOT find WrapVulkanHeaders (missing: Vulkan_INCLUDE_DIR) -- Found Python3: D:\/Programmes\/Python\/Python312\/python.exe (found version \"3.12.4\") found components: Interpreter Development Development.Module Development.Embed -- Configuring done (4.4s) -- Generating done (0.6s) -- Build files have been written to: D:\/Programmes\/msys64\/home\/Julien\/symreg_ws\/SymReg\/build_conan d:\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg\\build_conan>cmake --build . --config Release Version MSBuild 17.12.12+1cce77968 pour .NET Framework 1>Checking Build System Automatic MOC for target symreg Building Custom Rule D:\/Programmes\/msys64\/home\/Julien\/symreg_ws\/SymReg\/CMakeLists.txt python_bindings.cpp mocs_compilation_Release.cpp D:\\Programmes\\msys64\\ucrt64\\include\\stdio.h(184,64): error C2144: erreur de syntaxe : 'int' doit être précédé de ';' [D :\\Programmes\\msys64\\home\\Julien\\symreg_ws\\SymReg\\build_conan\\symreg.vcxproj] (compiler le fichier source '..\/src\/python_bindings.cpp') ... ``` Here is ``` CMakeLists.txt ``` : ``` cmake_minimum_required(VERSION 3.10) cmake_policy(SET CMP0091 NEW) project(SymReg) find_package(Ceres REQUIRED CONFIG) set(PYBIND11_FINDPYTHON ON) find_package(pybind11 REQUIRED CONFIG) set(CMAKE_BUILD_TYPE Release) #set(CMAKE_BUILD_TYPE Debug) if (CMAKE_CXX_COMPILER_ID MATCHES \"GNU|Clang\") add_compile_options( -march=native # -O3 -ffast-math -DNDEBUG # -fsanitize=address,undefined -g -O1 ) endif() set(CMAKE_CXX_STANDARD 23) set(CMAKE_CXX_STANDARD_REQUIRED ON) include(FetchContent) FetchContent_Declare( googletest GIT_REPOSITORY https:\/\/github.com\/google\/googletest.git GIT_TAG v1.14.0 ) FetchContent_MakeAvailable(googletest) find_package(Ceres REQUIRED) find_package(CURL REQUIRED) find_package(Qt6 REQUIRED COMPONENTS Widgets Charts Svg) find_package(Python3 COMPONENTS Interpreter Development REQUIRED) find_package(pybind11 REQUIRED) find_package(Sym REQUIRED) find_package(Boost REQUIRED) include_directories(include) add_compile_definitions(EIGEN_NO_DEBUG) set(CMAKE_AUTOMOC ON) add_definitions(-DPYBIND11_DETAILED_ERROR_MESSAGES) pybind11_add_module(symreg src\/python_bindings.cpp) if (WIN32) target_link_libraries(symreg PRIVATE Ceres::ceres Sym::Sym Ws2_32 Boost::headers) else() target_link_libraries(symreg PRIVATE Ceres::ceres Sym::Sym Boost::headers) endif() add_executable(primes_demo src\/primes_demo.cpp) if (WIN32) target_link_libraries(primes_demo ${MLPACK_LIBRARIES} # asan ubsan pybind11::embed Python3::Python Ceres::ceres Qt6::Widgets Qt6::Charts Qt6::Svg gvc cgraph Sym::Sym Ws2_32 Boost::headers) else() target_link_libraries(primes_demo ${MLPACK_LIBRARIES} # asan ubsan pybind11::embed Python3::Python Ceres::ceres Qt6::Widgets Qt6::Charts Qt6::Svg gvc cgraph Sym::Sym Boost::headers) endif() add_executable(benchmark src\/benchmark.cpp) if (WIN32) target_link_libraries(benchmark # asan ubsan pybind11::embed Python3::Python Ceres::ceres CURL::libcurl Sym::Sym Ws2_32 Boost::headers) else() target_link_libraries(benchmark # asan ubsan pybind11::embed Python3::Python Ceres::ceres CURL::libcurl Sym::Sym Boost::headers) endif() enable_testing() add_executable(test_symreg tests\/test_symreg.cpp) if (WIN32) target_link_libraries(test_symreg ${MLPACK_LIBRARIES} # asan ubsan pybind11::embed Python3::Python Ceres::ceres CURL::libcurl gtest gtest_main Sym::Sym Ws2_32 Boost::headers) else() target_link_libraries(test_symreg ${MLPACK_LIBRARIES} # asan ubsan pybind11::embed Python3::Python Ceres::ceres CURL::libcurl gtest gtest_main Sym::Sym Boost::headers) endif() math(EXPR TIMEOUT \"3 * 60 * 60\") add_test(NAME TestSymReg COMMAND test_symreg) set_tests_properties(TestSymReg PROPERTIES TIMEOUT ${TIMEOUT}) if (WIN32) else() include(GoogleTest) gtest_discover_tests(test_symreg) endif() install(DIRECTORY include\/SymReg DESTINATION include) add_library(SymReg INTERFACE) target_include_directories(SymReg INTERFACE $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}\/include> $<INSTALL_INTERFACE:include> ) include(CMakePackageConfigHelpers) configure_package_config_file( ${CMAKE_CURRENT_SOURCE_DIR}\/cmake\/SymRegConfig.cmake.in ${CMAKE_CURRENT_BINARY_DIR}\/SymRegConfig.cmake INSTALL_DESTINATION lib\/cmake\/SymReg ) write_basic_package_version_file( ${CMAKE_CURRENT_BINARY_DIR}\/SymRegConfigVersion.cmake VERSION 1.0.0 COMPATIBILITY SameMajorVersion ) install(FILES ${CMAKE_CURRENT_BINARY_DIR}\/SymRegConfig.cmake ${CMAKE_CURRENT_BINARY_DIR}\/SymRegConfigVersion.cmake DESTINATION lib\/cmake\/SymReg ) install(TARGETS SymReg EXPORT SymRegTargets) install(EXPORT SymRegTargets NAMESPACE SymReg:: DESTINATION lib\/cmake\/SymReg ) ``` Doing ``` cmake .. -DCMAKE_TOOLCHAIN_FILE=conan_toolchain.cmake -DCMAKE_PREFIX_PATH=.. ``` make CMake using good dependencies but MSYS2 includes are always used and compilation failed. I removed bin folder of MSYS2 in PATH and after cleaning, compilation continued and succeeded (for information, the project is here on GitHub: https:\/\/github.com\/Julien-Livet\/SymReg ). I tried cibuildwheel on GitHub Actions but it failed because Boost is not found and I don't know how to solve it (conan installation is ok just before that). ``` Building wheel... + rm -rf \/tmp\/cibuildwheel\/built_wheel + mkdir -p \/tmp\/cibuildwheel\/built_wheel + python -m pip wheel \/project --wheel-dir=\/tmp\/cibuildwheel\/built_wheel --no-deps Processing \/project Installing build dependencies: started Installing build dependencies: finished with status 'done' Getting requirements to build wheel: started Getting requirements to build wheel: finished with status 'done' Installing backend dependencies: started Installing backend dependencies: finished with status 'done' Preparing metadata (pyproject.toml): started Preparing metadata (pyproject.toml): finished with status 'done' Building wheels for collected packages: symregpy Building wheel for symregpy (pyproject.toml): started Building wheel for symregpy (pyproject.toml): finished with status 'error' error: subprocess-exited-with-error × Building wheel for symregpy (pyproject.toml) did not run successfully. │ exit code: 1 ╰─> [32 lines of output] WARNING: Use cmake.version instead of cmake.minimum-version with scikit-build-core >= 0.8 WARNING: Use build.verbose instead of cmake.verbose for scikit-build-core >= 0.10 *** scikit-build-core 0.11.5 using CMake 3.27.6 (wheel) *** Configuring CMake... loading initial cache file build\/CMakeInit.txt -- The C compiler identification is GNU 10.2.1 -- The CXX compiler identification is GNU 10.2.1 -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Check for working C compiler: \/opt\/rh\/devtoolset-10\/root\/usr\/bin\/gcc - skipped -- Detecting C compile features -- Detecting C compile features - done -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Check for working CXX compiler: \/opt\/rh\/devtoolset-10\/root\/usr\/bin\/g++ - skipped -- Detecting CXX compile features -- Detecting CXX compile features - done -- Found Python3: \/opt\/python\/cp38-cp38\/bin\/python (found version \"3.8.18\") found components: Interpreter -- Performing Test CMAKE_HAVE_LIBC_PTHREAD -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success -- Found Threads: TRUE CMake Error at \/opt\/_internal\/pipx\/venvs\/cmake\/lib\/python3.10\/site-packages\/cmake\/data\/share\/cmake-3.27\/Modules\/FindPackageHandleStandardArgs.cmake:230 (message): Could NOT find Boost (missing: Boost_INCLUDE_DIR) Call Stack (most recent call first): \/opt\/_internal\/pipx\/venvs\/cmake\/lib\/python3.10\/site-packages\/cmake\/data\/share\/cmake-3.27\/Modules\/FindPackageHandleStandardArgs.cmake:600 (_FPHSA_FAILURE_MESSAGE) \/opt\/_internal\/pipx\/venvs\/cmake\/lib\/python3.10\/site-packages\/cmake\/data\/share\/cmake-3.27\/Modules\/FindBoost.cmake:2392 (find_package_handle_standard_args) build\/_deps\/sym-src\/CMakeLists.txt:18 (find_package) -- Configuring incomplete, errors occurred! *** CMake configuration failed [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for symregpy Failed to build symregpy ERROR: Failed to build one or more wheels ``` How can I do to fix the right dependencies? I hope that I put all necessaries files and information.",
    "author_id":5672,
    "publication_date":1754305404000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"P&#39;tit Ju",
    "author_reputation":143.0,
    "tags":"python, c++, windows, cmake, conan",
    "text_length":14804,
    "title_length":33,
    "num_tags":5
  },
  {
    "id":6179,
    "title":"Net Income year-over-year",
    "link":"https:\/\/stackoverflow.com\/questions\/79724681\/net-income-year-over-year",
    "text":"TradingView has a Net Income indicator: I'd like to show \"Net Income year-over-year percent change\". Here's a pine script program that appears to implement this: ``` \/\/@version=6 indicator(\"NET_INCOME YoY\", overlay = false, timeframe = '3M') import TradingView\/ta\/10 item = request.financial(symbol = syminfo.tickerid, financial_id = 'NET_INCOME', period = 'FQ') chg_pct = (item[0] - item[4]) \/ math.abs(item[4]) * 100 plot(chg_pct, style = plot.style_stepline_diamond) ``` Here's what it looks like: While it is showing the correct values, there are are a couple of issues: The data points do not line up with Net Income indicator. (It's shifted to the right in the case of ``` $GME ``` ). The status only shows the value of the cursor is right above the data point. Otherwise, it shows zero. Is there a way to implement this that would resolve the above issues?",
    "author_id":5671,
    "publication_date":1754305639000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dharmatech",
    "author_reputation":9645.0,
    "tags":"pine-script",
    "text_length":863,
    "title_length":25,
    "num_tags":1
  },
  {
    "id":6178,
    "title":"No such file or directory @ rb_sysopen - CodeDeploy Gives Error Even The Path Exists",
    "link":"https:\/\/stackoverflow.com\/questions\/79724684\/no-such-file-or-directory-rb-sysopen-codedeploy-gives-error-even-the-path-ex",
    "text":"CodeDeploy giving error ``` No such file or directory @ rb_sysopen - C:\\ProgramData\/Amazon\/CodeDeploy\/a5e44c96-0925-4071-806d-72adee9955c7\/d-KTEDSL9LD\/deployment-archive\/dev-glf-orbis-store-build\\packages\\ ``` When i RDP into the server and hit that path C:\\ProgramData\/Amazon\/CodeDeploy\/a5e44c96-0925-4071-80xxxxe9955c7\/d-KTEDSLxxD\/deployment-archive\/dev-glf-orbis-store-build\\packages\\ it exists there. Here is my codedeploy config file ``` version: 0.0 os: windows files: - source: \\ destination: D:\\wwwroot\\nopCommerce overwrite: true file_exists_behavior: OVERWRITE ``` My build directory structure is like this \\dev-glf-orbis-store-build\\ (all build files and directories). I even see the different types of slashes (some forward slashes, some backward slashes which is also very concerning. Why codedeploy picking up those slashes by default. I just want to have another pair of eyes to check what i am doing wrong. Thanks.",
    "author_id":5670,
    "publication_date":1754305854000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Rehan CH",
    "author_reputation":171.0,
    "tags":"windows, cicd, msbuild, aws-code-deploy",
    "text_length":930,
    "title_length":84,
    "num_tags":4
  },
  {
    "id":6177,
    "title":"Calculate standard deviation using if condition but ignoring blank cells",
    "link":"https:\/\/stackoverflow.com\/questions\/79724687\/calculate-standard-deviation-using-if-condition-but-ignoring-blank-cells",
    "text":"I need to calculated the standard deviation in a similar way we calculate the average using the function \"averageif\". I used the function described below to do it (as proposed by the users Scott Craner and @nbayly in https:\/\/stackoverflow.com\/a\/41174620\/31207248 ) ``` =STDEV.S(IF((C:C=\"alpha\")*(D:D=\"S14\"),B:B)) ``` But this function considers blank cells as zero. Is there a way to make it ignore blank cells? I tried to use if(isblank;\"\") and isnumber but it did not work. It says i have entered too many arguments",
    "author_id":5669,
    "publication_date":1754305913000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jeanini Jiusti",
    "author_reputation":23.0,
    "tags":"excel, excel-formula",
    "text_length":517,
    "title_length":72,
    "num_tags":2
  },
  {
    "id":6176,
    "title":"A specialized path planning and collision avoidance problem",
    "link":"https:\/\/stackoverflow.com\/questions\/79724691\/a-specialized-path-planning-and-collision-avoidance-problem",
    "text":"I'm currently stuck on a specialized multi-robot path planning and obstacle avoidance algorithm and would appreciate your suggestions. In my project scenario, multiple robots move within a unidirectional cyclic track-like network (similar to railways). I need to compute paths for them to travel from their respective starting points to destinations without colliding with other robots (their radar systems are quite poor—only detecting peers ahead when moving in the same direction). Originally, I considered borrowing methods from transportation flow equilibrium problems and solving this with a user equilibrium model. However, I realized this requires extensive experimental data as a foundation, whereas my project is still in the conceptual phase. Moreover, it isn't a pure transportation network problem. I ran one calculation with simulated data, but the results were unsatisfactory and unusable. I then explored simulation-based approaches, hoping to gain insights. I've built a basic simulation map using AnyLogic, but I'm unsure about the next steps. ​​Should I persist with transportation-domain methods, or explore methodologies from other fields?​​ If the latter, could you suggest directions with well-established case studies for reference?",
    "author_id":5668,
    "publication_date":1754306096000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"CangWangu",
    "author_reputation":197.0,
    "tags":"python, anylogic, mathematical-optimization, robotics",
    "text_length":1256,
    "title_length":59,
    "num_tags":4
  },
  {
    "id":6175,
    "title":"How to step into C++ code from Python using pybind11 in VSCode when using Python C++ Debugger?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724694\/how-to-step-into-c-code-from-python-using-pybind11-in-vscode-when-using-python",
    "text":"Question: I am trying to debug a Python script that calls a C++ function through a .pyd module built with pybind11 (compiled using MSVC). I want to set a breakpoint in Python, and when I press Step Into (F11,step into) in VSCode, I want it to step into the C++ implementation automatically. Code Example: Python (debug.py) ``` import sys import os # Add debug directory to Python path debug_path = os.path.join(os.path.dirname(__file__), 'build', 'Debug') sys.path.insert(0, debug_path) import myadder result = myadder.add(5, 3) # BREAKPOINT here for i in range(3): result = myadder.add(i, i + 1) print(f\"add({i}, {i+1}) = {result}\") ``` C++ (add.cpp) ``` \/\/ add.cpp #include <cstdio> int add(int a, int b) { printf(\" C++ add() called with a=%d, b=%d\\n\", a, b); int result = a + b; \/\/ Set C++ breakpoint here printf(\" C++ add() returning: %d\\n\", result); return result; } ``` C++ (bindings.cpp) ``` \/\/ bindings.cpp #include <pybind11\/pybind11.h> int add(int a, int b); namespace py = pybind11; PYBIND11_MODULE(myadder, m) { m.def(\"add\", &add, \"A function that adds two numbers\"); } ``` CMakeLists.txt ``` cmake_minimum_required(VERSION 3.14) project(myadder) set(CMAKE_CXX_STANDARD 17) set(CMAKE_BUILD_TYPE Debug) # Use Python to find pybind11 automatically execute_process( COMMAND python -c \"import pybind11; print(pybind11.get_cmake_dir())\" OUTPUT_VARIABLE pybind11_DIR OUTPUT_STRIP_TRAILING_WHITESPACE ) # find pybind11 package find_package(pybind11 REQUIRED) pybind11_add_module(myadder bindings.cpp add.cpp) ``` Build ``` mkdir build cd build cmake .. -G \"Visual Studio 16 2019\" -A x64 cmake --build . --config Debug ``` ✅Build Info: Compiler: MSVC Built as: myadder.pyd with pybind11 Python version: 3.12 Platform: Windows 11 Debugger: cppvsdbg VSCode Plugin: Python C++ Debugger launch.json Config ``` { \/\/ Use IntelliSense to learn about possible attributes. \/\/ Hover to view descriptions of existing attributes. \/\/ For more information, visit: https:\/\/go.microsoft.com\/fwlink\/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Python C++ Debugger\", \"type\": \"pythoncpp\", \"request\": \"launch\", \"pythonLaunchName\": \"Python: Current File\", \"cppAttachName\": \"(Windows) Attach\" }, { \"name\": \"(Windows) Attach\", \"type\": \"cppvsdbg\", \"request\": \"attach\", \"processId\": \"${command:pickProcess}\", \"symbolSearchPath\": \"${workspaceFolder}\/build\/Debug\", \/\/ .pdb }, \/\/ Python { \"name\": \"Python: Current File\", \"type\": \"debugpy\", \"request\": \"launch\", \"program\": \"${workspaceFolder}\/debug.py\", \"console\": \"integratedTerminal\", \"justMyCode\": false, \"stopOnEntry\": true, } ] } ``` Current Behavior: When I run the Python C++ debugger and set a breakpoint at myadder.add(5, 3), it stops in Python, but Step Into (F11) does not go into the C++ function. However, if I first hit the Python breakpoint and then manually click \"Windows(Attach)\", then I can then hit breakpoints in C++. But this is a manual. ✅Desired Behavior: I want to be able to: Run Python C++ Debugger. Stop at the breakpoint in Python code. Press F11 (or clicking Step Into) can make VSCode step into the C++ implementation without clicking (Windows Attach). Is it possible to step into C++ pybind11 code directly from Python using the cppvsdbg debugger in VSCode without manually clicking (Windows) Attach ? If so, what is the correct launch.json configuration or steps to make it work? Any advice? Thanks.",
    "author_id":5667,
    "publication_date":1754306180000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ringo",
    "author_reputation":1317.0,
    "tags":"python, c++, debugging, pybind11",
    "text_length":3378,
    "title_length":94,
    "num_tags":4
  },
  {
    "id":6174,
    "title":"How to reactively respond to form.success updates from +page.server.ts actions in +pages.svelte",
    "link":"https:\/\/stackoverflow.com\/questions\/79724696\/how-to-reactively-respond-to-form-success-updates-from-page-server-ts-actions-i",
    "text":"I have a SvelteKit setup where I perform a fetch request inside an action in +page.server.ts. Specifically, I define an addSender action that handles a POST request to an external API and returns an object with a success flag indicating whether the addition was successful: ``` \/\/ +page.server.ts import type { Actions } from '.\/$types'; export const actions: Actions = { addSender: async ({ request }) => { try { const formData = await request.formData(); const senderData = { name: formData.get('name'), }; const response = await fetch('https:\/\/api.example.com\/senders', { method: 'POST', headers: { 'Content-Type': 'application\/json' }, body: JSON.stringify(senderData), }); if (response.ok) { \/\/ when this happens, the form should be closed again return { success: true }; } } catch (error) { return { success: false, message: 'Error adding sender' }; } }, }; ``` In +page.svelte I want a form to close then the request is succesful. In other words, when ``` form?.success === true ``` : ``` \/\/ +page.svelte <script lang=\"ts\"> let showAddSenderForm = false; function toggleSenderForm() { showAddSenderForm = !showAddSenderForm; } <\/script> {#if form?.success === true} <p>this shows fine :)<\/p> {\/if} <button on:click={toggleSenderForm}> {#if showAddSenderForm} <!-- this form has the logic to do the request, which all works fine --> <FormAddSender \/> {\/if} ``` My question is: How can I set up my Svelte component to reactively detect and respond to changes in form.success returned from the server action? Particularly, I want the UI to update immediately after the action completes with success or failure, reacting appropriately to form.success changes. I am aware the returned data from actions is accessible on the page, but I am unsure how to properly watch or respond to the success state update in the client component lifecycle or reactive statements. I've seen the runes $effect and $derived, but you should not set state in these runes right? \"Generally speaking, you should not update state inside effects, as it will make code more convoluted and will often lead to never-ending update cycles.\" - source: https:\/\/svelte.dev\/docs\/svelte\/$effect ``` \/\/ discouraged? $effect(() => { if (form?.success) { showAddSenderForm = false; } }); ``` Any advice, best practices, or code patterns on how to handle this kind of form state reactive update from server actions in SvelteKit would be much appreciated.",
    "author_id":5666,
    "publication_date":1754306237000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Remi",
    "author_reputation":5457.0,
    "tags":"svelte, sveltekit",
    "text_length":2418,
    "title_length":95,
    "num_tags":2
  },
  {
    "id":6173,
    "title":"ESLint configs error Parsing error: Invalid ecmaVersion",
    "link":"https:\/\/stackoverflow.com\/questions\/79724704\/eslint-configs-error-parsing-error-invalid-ecmaversion",
    "text":"I'm writing a shared ESLint configuration for my own project. One of the configurations for the JS project is: ``` import js from '@eslint\/js'; import { defineConfig } from 'eslint\/config'; import stylistic from '@stylistic\/eslint-plugin'; export default defineConfig([ js.configs.recommended, { plugins: { '@stylistic': stylistic }, rules: {}, }, ]); ``` About the configuration of the Vue project (based on JS configuration improvements): ``` import pluginVue from 'eslint-plugin-vue'; import jsConfig from '.\/js.js'; import { defineConfig } from 'eslint\/config'. export default defineConfig([ jsConfig, ...pluginVue.configs['flat\/recommended'], ]); ``` When I use this shared configuration in another Vue project, I get a Parsing error: Invalid ecmaVersion. error, here is the ESLint configuration when the error occurs: ``` import { defineConfig } from 'eslint\/config'; import configs from '@lai9fox\/eslint-config'. export default defineConfig([ { files: ['**\/*. {js,mjs,jsx,vue}'], extends: [configs.jsVueConfig], }, ]); ``` Strangely enough, when I use JS's shared configuration directly in the Vue project and manually add the ESLint configuration for Vue, it works fine, and the following (2 ways work correctly) is the configuration that works fine: ``` import { defineConfig } from 'eslint\/config'; import configs from '@lai9fox\/eslint-config'; import pluginVue from 'eslint-plugin-vue'; } \/\/ export default defineConfig([ \/\/ { \/\/ files: ['**\/*. {js,mjs,jsx,vue}'], \/\/ extends: [configs.jsConfig, ...pluginVue.configs['flat\/recommended']], \/\/ }, \/\/ ]); const vueConfigs = defineConfig([ configs.jsConfig, ...pluginVue.configs['flat\/recommended'], ]); export default defineConfig([ { files: ['**\/*. {js,mjs,jsx,vue}'], extends: [vueConfigs], }, ]); ``` Why is this happening? How can I improve my shared ESLint configuration?",
    "author_id":5665,
    "publication_date":1754306662000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"lai9fox",
    "author_reputation":69.0,
    "tags":"vue.js, eslint, eslint-plugin-vue",
    "text_length":1834,
    "title_length":55,
    "num_tags":3
  },
  {
    "id":6172,
    "title":"MAUI Community Toolkit Popup V2 Issue with Data Communication",
    "link":"https:\/\/stackoverflow.com\/questions\/79724710\/maui-community-toolkit-popup-v2-issue-with-data-communication",
    "text":"We're using PRISM for navigation in our .NET MAUI app (targeting .NET 9). Initially, we used Mopups for displaying popups, but later switched to the .NET MAUI Community Toolkit Popup because of its cleaner API and better integration. However, with the recent release of Popup V2, we've run into several breaking changes that are causing significant issues: Data exchange between the popup and the view model now seems to require Shell, which PRISM doesn't support. The new API includes multiple overloads for opening and closing popups, but the ones that allow passing parameters to the view model require Shell.Current. Since PRISM does not support Shell, we're unable to use these methods. We've tried manually setting Shell.Current, but it doesn't behave as expected. The PRISM documentation clearly states that Shell is not supported: https:\/\/docs.prismlibrary.com\/docs\/platforms\/maui\/navigation\/index.html At this point, we're considering reverting to Mopups or exploring other alternatives. It feels like the new version of the Community Toolkit Popup is tightly coupled with Shell, making it difficult to use in non-Shell apps. Has anyone else faced this issue? Is there a workaround to use the Community Toolkit Popup V2 with PRISM, or without Shell?",
    "author_id":5664,
    "publication_date":1754307068000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dharmendra Kumar",
    "author_reputation":408.0,
    "tags":"maui, popup, maui-community-toolkit, prism",
    "text_length":1258,
    "title_length":61,
    "num_tags":4
  },
  {
    "id":6171,
    "title":"Set Amazon S3 Storage Class",
    "link":"https:\/\/stackoverflow.com\/questions\/79724712\/set-amazon-s3-storage-class",
    "text":"I'm using the PHP AWS API v.3 and I would like to set the Storage Class when uploading an object to a bucket. I'm using the MultipartUploader. I already tried this: ``` $uploader = new MultipartUploader($s3, $fileToUpload, [ 'bucket' => $bucket, 'key' => $key, 'storageClass' => 'STANDARD_IA' ]); ``` but it didn't work. Is there a way to do that? I looked up the docs and I didn't find any option. Thank you!",
    "author_id":4442,
    "publication_date":1754307226000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Greg",
    "author_reputation":430.0,
    "tags":"php, amazon-web-services, amazon-s3, storage-class",
    "text_length":409,
    "title_length":27,
    "num_tags":4
  },
  {
    "id":6170,
    "title":"install postgres-odbc in RHEL9 minimal version",
    "link":"https:\/\/stackoverflow.com\/questions\/79724713\/install-postgres-odbc-in-rhel9-minimal-version",
    "text":"I want to create a docker image which has a minimal redhat linux enterprise server version.Below is my dockerfile ``` FROM docker.io\/redhat:9.202508.5794770-minimal RUN microdnf makecache RUN microdnf --assumeyes update RUN microdnf install --assumeyes unixODBC postgresql-odbc ``` The postgresql-odbc install is failing. Please help me with how I can have postgresql-odbc installed. Thanks in advance",
    "author_id":5663,
    "publication_date":1754307278000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"RituV",
    "author_reputation":65.0,
    "tags":"postgresql, docker, odbc, containers",
    "text_length":401,
    "title_length":46,
    "num_tags":4
  },
  {
    "id":6169,
    "title":"Unity build failed; CommandInvokationFailure: Gradle build failed",
    "link":"https:\/\/stackoverflow.com\/questions\/79724720\/unity-build-failed-commandinvokationfailure-gradle-build-failed",
    "text":"Unity: 2023.2.18f1 Selected Platform: Android Firebase SDK: 12.10.1 External Dependency Manager for Unity: 1.2.186 Facing ``` CommandInvokationFailure: Gradle build failed. ``` issue. I have tried all the suggested solutions, like updating all the plugins in the Package Manager, updated the Android SDK to the latest 36 ``` deleted resolved libraries ``` , and ``` Force resolve ``` again Deleted ``` Logs ``` folder after closing Unity, and regenerated the ``` Logs ``` folder Any other suggestions to solve this issue Pictures are easy to read, so I added screenshots of the issue",
    "author_id":5662,
    "publication_date":1754307595000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"eagle",
    "author_reputation":931.0,
    "tags":"c#, android, unity-game-engine, firebase, unity-editor",
    "text_length":583,
    "title_length":65,
    "num_tags":5
  },
  {
    "id":6168,
    "title":"Setting default value in USelectMenu",
    "link":"https:\/\/stackoverflow.com\/questions\/79724722\/setting-default-value-in-uselectmenu",
    "text":"I am trying to populate a USelectMenu and set a default value. However the control shows the id and not the label: This is my code: ``` <template> <div class=\"p-8\"> <h1 class=\"text-2xl font-bold mb-4\"> Test Page <\/h1> <p class=\"mb-2\"> This is a minimal Vue page. <\/p> <div> <div>{{ companyOptionsTest }}<\/div> <USelectMenu v-model=\"selectedCompanyTest\" :items=\"companyOptionsTest\" placeholder=\"Select a company\" \/> <\/div> <\/div> <\/template> <script setup lang=\"ts\"> import { ref } from 'vue'; let testCompanies = [ { \"id\": \"458d7c38-7d9c-4641-9d18-9409b1d04377\", \"name\": \"Test1\", \"type\": \"\", \"country\": \"\", \"contacts\": [ { \"id\": \"5ddb52dc-47cd-49d6-9953-d4ca88319a5a\", \"firstName\": \"Eric\", \"lastName\": \"Yang\", \"salutation\": \"Mr\", \"email\": \"\", \"phoneNumber\": \"\" }, { \"id\": \"0750316f-3428-417e-a226-15886b1731ce\", \"firstName\": \"Erik\", \"lastName\": \"Lee\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" }, { \"id\": \"cabcd68d-0a6e-41d8-bcf4-a5772d7c5b57\", \"firstName\": \"mmkk\", \"lastName\": \"k\", \"salutation\": \"\", \"email\": \"k\", \"phoneNumber\": \"k\" } ] }, { \"id\": \"5ef5cabf-ac5c-4639-a04b-617ac17e8906\", \"name\": \"Test2\", \"type\": \"\", \"country\": \"\", \"contacts\": [ { \"id\": \"8afd7173-046a-426e-824c-f2c1bacf3778\", \"firstName\": \"Roberto\", \"lastName\": \"Tester\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" }, { \"id\": \"090f5fed-2466-41fc-abae-ef935bf6f96e\", \"firstName\": \"Tester\", \"lastName\": \"Kjo\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" } ] }, { \"id\": \"a7403568-1dcf-40d5-b611-58defaba75fc\", \"name\": \"Bernard Test3\", \"type\": \"\", \"country\": \"\", \"contacts\": [ { \"id\": \"216b37fc-ee90-41a8-8edb-9430d9389fdd\", \"firstName\": \"James\", \"lastName\": \"\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" }, { \"id\": \"c3dc77fc-ad81-4d27-8df1-d13a557d3974\", \"firstName\": \"Q\", \"lastName\": \"LOL\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" }, { \"id\": \"5a35be30-396a-4f98-8792-fcaa92352960\", \"firstName\": \"dfsfsd\", \"lastName\": \"asdasdas\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" } ] }, { \"id\": \"2ba1ce4c-00e5-4476-a245-25e0cfd3fdbc\", \"name\": \"Besiktas\", \"type\": \"\", \"country\": \"\", \"contacts\": [ { \"id\": \"ac728279-59fc-4ab3-a001-45cd652b9723\", \"firstName\": \"A Adam\", \"lastName\": \"Bro\", \"salutation\": \"dssdffsf11\", \"email\": \"sadasdas11\", \"phoneNumber\": \"virker2\" }, { \"id\": \"551e4bca-ad31-471a-a940-d7eb417cd487\", \"firstName\": \"Omer\", \"lastName\": \"\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" } ] }, { \"id\": \"3a406785-9f2b-41f0-99a1-cb6f8605f504\", \"name\": \"Bihar\", \"type\": \"\", \"country\": \"\", \"contacts\": [ { \"id\": \"4b72120f-70c4-445c-98d7-d6dcfe440635\", \"firstName\": \"Stella\", \"lastName\": \"TEST\", \"salutation\": \"\", \"email\": \"\", \"phoneNumber\": \"\" } ] } ]; const companyOptionsTest = testCompanies.map(company => ({ label: company.name, value: company.id })); \/\/ Set default to Bihar's id const selectedCompanyTest = ref( testCompanies.find(c => c.name === 'Bihar')?.id || null ); <\/script> <style scoped> \/* You can add page-specific styles here *\/ <\/style> ```",
    "author_id":4543,
    "publication_date":1754307655000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Thomas Segato",
    "author_reputation":5323.0,
    "tags":"vue.js, nuxt.js, nuxt3.js",
    "text_length":2965,
    "title_length":36,
    "num_tags":3
  },
  {
    "id":6167,
    "title":"Flutter: Memory Usage Issue",
    "link":"https:\/\/stackoverflow.com\/questions\/79724724\/flutter-memory-usage-issue",
    "text":"I’m building a Flutter app, and some users are receiving the following device notification: “This app uses a lot of memory, so your phone is slow. Switch this feature to ultra power saving so your phone can work smoothly.” Upon inspection, I found that the app consumes 2.9GB of memory, whereas YouTube—despite playing video—only uses around 456MB. I used the Android Studio Memory Profiler and confirmed that my app consistently uses around 1GB, even after navigating through all screens. There was no significant spike in memory usage on any particular screen. Even when I build a minimal version of the app with just Text(\"hi\"), the memory usage sits around 650MB. Interestingly, the EmojiCompat class always appears at the top of the retained size list and even it appears when I build a project with just Text(\"hi\"). I’ve verified that all controllers are properly disposed of. One peculiar aspect of my app is that it uses a large number of SVG images, and there are many images in the assets\/ folder. However, I’ve read that having many assets doesn’t directly impact memory usage unless they’re loaded. Is it acceptable if the app constantly uses around 1GB of memory, even without leaks? Do you see any issues from the Heap Dump screenshot that I might be missing? Could the use of many SVGs contribute significantly to this memory usage?",
    "author_id":4386,
    "publication_date":1754307715000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Hyejung",
    "author_reputation":1302.0,
    "tags":"android, flutter, memory",
    "text_length":1347,
    "title_length":27,
    "num_tags":3
  },
  {
    "id":6166,
    "title":"Azure Function TimerTrigger issue with Microsoft.Identity.Client",
    "link":"https:\/\/stackoverflow.com\/questions\/79724725\/azure-function-timertrigger-issue-with-microsoft-identity-client",
    "text":"I created an Azure Functions project with a timer trigger, using the following configuration. Language: C# Target Framework: .NET 8.0 Package Dependencies: ``` Microsoft.Identity.Client ``` (v4.74.1) and ``` Microsoft.NET.Sdk.Functions ``` (v4.6.0). This is the timer-triggered function: ``` [FunctionName(\"TimerTriggerFunc\")] public static async Task Run([TimerTrigger(\"0 *\/1 * * * *\")]TimerInfo myTimer, ILogger log) { log.LogInformation($\"Sync function {Version} started at: {DateTime.Now}\"); var tokenInfo = await AuthAPIAccess.RetrieveAccessTokenFromGraphMSALAsync(log); } ``` When I execute the method shown here, it throws an exception: ``` var app = ConfidentialClientApplicationBuilder.Create(ClientID) .WithAuthority(AzureCloudInstance.AzurePublic, TenantID) .WithClientSecret(ClientSecret) .Build(); ``` Exception System.IO.FileNotFoundException HResult=0x80070002 Message=Could not load file or assembly 'System.Security.Cryptography, Version=8.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a'. The system cannot find the file specified. Source=Microsoft.Identity.Client StackTrace: at Microsoft.Identity.Client.ConfidentialClientApplication..ctor(ApplicationConfiguration configuration) at Microsoft.Identity.Client.ConfidentialClientApplicationBuilder.Build() at teMonitorStage.BusinessLogic.AuthAPIAccess.d__3.MoveNext() in This exception was originally thrown at this call stack: [External Code] Do I need to perform any additional configuration?",
    "author_id":5661,
    "publication_date":1754307761000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"brendan davton",
    "author_reputation":25.0,
    "tags":"azure-devops, azure-functions",
    "text_length":1470,
    "title_length":64,
    "num_tags":2
  },
  {
    "id":6165,
    "title":"Unable to see sign tab over the anchor tab for Docusign digital signature",
    "link":"https:\/\/stackoverflow.com\/questions\/79724727\/unable-to-see-sign-tab-over-the-anchor-tab-for-docusign-digital-signature",
    "text":"I am using Node SDK for Docusign digital signature. I have specified the anchor string but when I send the document I amunable to see the sign tab over the anchor. ``` const envelopeDefinition = { emailSubject: 'Please sign this document', recipients: { signers: [ { email: vendorEmail, name: vendorName, recipientId: '1', routingOrder: '1', tabs: { signHereTabs: [ { anchorString: '**signature_here**', anchorYOffset: '10', anchorUnits: 'pixels', anchorXOffset: '20' } ] } } ] }, documents: [ { documentBase64: base64Pdf, name: contractType, fileExtension: 'pdf', documentId: '1' } ], status: 'sent', customFields: { textCustomFields: [ { name: 'contractId', value: contractId, show: false \/\/ Set true if you want it to appear in the DocuSign UI }, { name: 'signatureType', value: 'vendor', show: false } ] }, e ``` I checked that the anchor in the document and in the API are the same.",
    "author_id":5660,
    "publication_date":1754307882000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Pratyush Sinha ",
    "author_reputation":1.0,
    "tags":"node.js, docusignapi",
    "text_length":887,
    "title_length":73,
    "num_tags":2
  },
  {
    "id":6164,
    "title":"neutralino proper way to connect to socket via extension?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724734\/neutralino-proper-way-to-connect-to-socket-via-extension",
    "text":"What is the proper way to connect to websocket to Neutralino using a cpp extension? I keep on disconnecting with error 400, what am I doing wrong? This is a stripped down cpp file to test the socket. ``` #include <libwebsockets.h> #include <json-c\/json.h> #include <iostream> #include <fstream> #include <ctime> #include <string> static struct lws_context *context = nullptr; static struct lws *wsi_global = nullptr; static void log_message(const std::string &message) { std::time_t now = std::time(nullptr); char timestamp[32]; std::strftime(timestamp, sizeof(timestamp), \"%Y-%m-%d %H:%M:%S\", std::localtime(&now)); std::string entry = \"[\" + std::string(timestamp) + \"] \" + message; std::ofstream log(\"socket-test.log\", std::ios::app); if (log.is_open()) log << entry << std::endl; std::cout << entry << std::endl; } static void send_response(struct lws *wsi, struct json_object *response) { if (!wsi) { log_message(\"No WebSocket instance to send\"); json_object_put(response); return; } const char *resp_str = json_object_to_json_string(response); size_t len = strlen(resp_str); unsigned char *buf = new unsigned char[LWS_PRE + len]; memcpy(&buf[LWS_PRE], resp_str, len); int n = lws_write(wsi, &buf[LWS_PRE], len, LWS_WRITE_TEXT); log_message(std::string(\"Sent: \") + resp_str + \", bytes: \" + std::to_string(n)); delete[] buf; json_object_put(response); } static int callback(struct lws *wsi, enum lws_callback_reasons reason, void *user, void *in, size_t len) { log_message(\"Callback reason: \" + std::to_string(reason)); switch (reason) { case LWS_CALLBACK_CLIENT_ESTABLISHED: { log_message(\"Connected!\"); wsi_global = wsi; struct json_object *hello = json_object_new_object(); json_object_object_add(hello, \"event\", json_object_new_string(\"extClientConnect\")); json_object_object_add(hello, \"data\", json_object_new_string(\"mongodb-extension\")); send_response(wsi, hello); break; } case LWS_CALLBACK_CLIENT_WRITEABLE: { log_message(\"Sending heartbeat\"); struct json_object *ping = json_object_new_object(); json_object_object_add(ping, \"event\", json_object_new_string(\"extensionHeartbeat\")); json_object_object_add(ping, \"timestamp\", json_object_new_int64(std::time(nullptr))); send_response(wsi, ping); break; } case LWS_CALLBACK_CLIENT_RECEIVE: { log_message(\"Received: \" + std::string((const char *)in, len)); break; } case LWS_CALLBACK_CLIENT_CONNECTION_ERROR: { log_message(\"Connection error: \" + std::string((const char *)in ? (const char *)in : \"unknown\")); break; } case LWS_CALLBACK_CLOSED: { log_message(\"WebSocket closed\"); break; } case LWS_CALLBACK_CLIENT_APPEND_HANDSHAKE_HEADER: { unsigned char **p = (unsigned char **)in; unsigned char *end = (*p) + len; if (lws_add_http_header_by_name(wsi, (const unsigned char *)\"Sec-WebSocket-Protocol\", (const unsigned char *)\"neutralinojs\", strlen(\"neutralinojs\"), p, end) < 0) { log_message(\"Failed to append protocol header\"); return -1; } else { log_message(\"Added Sec-WebSocket-Protocol header\"); } break; } default: break; } return 0; } static struct lws_protocols protocols[] = { { \"neutralino-extension\", callback, 0, 1024 }, { nullptr, nullptr, 0, 0 } }; int main() { log_message(\"Starting socket test\"); std::ifstream auth_file(\".tmp\/auth_info.json\"); if (!auth_file.is_open()) { log_message(\"Failed to open .tmp\/auth_info.json\"); return 1; } std::string auth_json((std::istreambuf_iterator<char>(auth_file)), std::istreambuf_iterator<char>()); struct json_object *config = json_tokener_parse(auth_json.c_str()); std::string port, extensionId, token; struct json_object *port_obj, *ext_obj, *token_obj; if (json_object_object_get_ex(config, \"nlPort\", &port_obj)) port = json_object_get_string(port_obj); if (json_object_object_get_ex(config, \"nlExtensionId\", &ext_obj)) extensionId = json_object_get_string(ext_obj); if (json_object_object_get_ex(config, \"nlConnectToken\", &token_obj)) token = json_object_get_string(token_obj); json_object_put(config); std::string path = \"\/?extensionId=\" + extensionId + \"&connectToken=\" + token; lws_set_log_level( LLL_ERR | LLL_WARN | LLL_NOTICE | LLL_DEBUG | LLL_INFO | LLL_PARSER | LLL_HEADER | LLL_EXT | LLL_CLIENT, [](int level, const char *line) { std::string msg(line ? line : \"\"); if (!msg.empty() && msg.back() == '\\n') msg.pop_back(); log_message(\"libwebsockets: \" + msg); } ); struct lws_context_creation_info info = {}; info.port = CONTEXT_PORT_NO_LISTEN; info.protocols = protocols; context = lws_create_context(&info); if (!context) { log_message(\"Failed to create context\"); return 1; } static std::string path_holder = path; \/\/ persist buffer struct lws_client_connect_info ccinfo = {}; ccinfo.context = context; ccinfo.address = \"localhost\"; ccinfo.port = std::stoi(port); ccinfo.path = path_holder.c_str(); ccinfo.host = \"localhost\"; ccinfo.origin = \"localhost\"; ccinfo.protocol = \"neutralinojs\"; if (!lws_client_connect_via_info(&ccinfo)) { log_message(\"Failed to connect\"); lws_context_destroy(context); return 1; } std::time_t last_ping = std::time(nullptr); while (true) { lws_service(context, 1000); std::time_t now = std::time(nullptr); if (wsi_global && now - last_ping >= 5) { lws_callback_on_writable(wsi_global); last_ping = now; } } lws_context_destroy(context); log_message(\"Test complete\"); return 0; } ``` Error log ``` [2025-08-04 11:48:29] Callback reason: 34 [2025-08-04 11:48:29] Callback reason: 36 [2025-08-04 11:48:29] Callback reason: 44 [2025-08-04 11:48:29] libwebsockets: [2025\/08\/04 11:48:29:5596] W: [wsicli|0|WS\/h1\/default\/localhost]: lws_client_ws_upgrade: got bad HTTP response '400' [2025-08-04 11:48:29] Callback reason: 1 [2025-08-04 11:48:29] Connection error: HS: ws upgrade response not 101 ```",
    "author_id":4795,
    "publication_date":1754308378000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Eric",
    "author_reputation":43.0,
    "tags":"c++, neutralinojs",
    "text_length":5650,
    "title_length":57,
    "num_tags":2
  },
  {
    "id":6163,
    "title":"Does Upgrading CPLEX Improve Solver Speed and Scalability",
    "link":"https:\/\/stackoverflow.com\/questions\/79724735\/does-upgrading-cplex-improve-solver-speed-and-scalability",
    "text":"Does the newer version of CPLEX provide faster solution times compared to older versions? Additionally, can the newer version handle problems with a larger number of decision variables more efficiently? I am currently using CPLEX 12.10.0 for solving a mixed-integer linear programming (MILP) problem. I am currently using the academic version of the CPLEX software. Does the licensed (commercial) version offer faster solution times and better performance for large-scale problems?",
    "author_id":5659,
    "publication_date":1754308378000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vandana Kumari",
    "author_reputation":1.0,
    "tags":"cplex, opl",
    "text_length":481,
    "title_length":57,
    "num_tags":2
  },
  {
    "id":6162,
    "title":"How to make DatePicker to get focused on submitting if the field is empty or giving errors?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724737\/how-to-make-datepicker-to-get-focused-on-submitting-if-the-field-is-empty-or-giv",
    "text":"I'm trying to use ``` @mui\/x-date-pickers ``` : \"^8.3.1 DatePicker component in a form with ``` react-hook-form ``` : \"^7.62.0\" using some workarounds I managed it to give errors onBlur and on submit, but I can't get it to focus on submitting a form. It only focuses for a couple milliseconds(like it's flickering) and goes back to unfocused look. here's my code: ``` const { register, formState:{ errors, }, setFocus, handleSubmit, control } = useForm( { mode: \"onBlur\" } ) useEffect(()=>{ setFocus(\"date\") },[errors.date]) <form noValidate onSubmit={handleSubmit(submitForm)}> <HCenteredLayout> <Controller render={({ field: { onChange, onBlur, value, name, ref }, }) =>(<DatePicker value={value} onChange={onChange} inputRef={ref} disablePast label='Date' className='Form-Field muidatepicker' sx={{width: '310px'}} slotProps={{textField: {required: true, error: !!errors?.date, onBlur: onBlur }, openPickerButton:{ onBlur: onBlur } }} \/>)} name='date' control={control} rules={{required:true}} \/> <TextField {...register(\"title\")} required error={!!errors?.title} label='Title' className='Form-Field' margin='normal' size='medium' fullWidth sx={{width: '310px'}} \/> <TextField {...register(\"description\")} required error={!!errors?.description} label='Description...' className='Form-Field' margin='dense' multiline rows={5} fullWidth sx={{width: '310px'}} \/> <TimeSlotsSection timeSlots={timeSlotsList} variant='createSlots'\/> <Button text='Create event' size='big' color='primary'\/> <\/HCenteredLayout> <\/form> ``` I'm pretty new to using react hook form and ``` MUI ``` so any advice would be appreciated, I feel like just don't have some core understanding of how ``` MUI ``` works. Also I would like to get some advices on ``` MUI ``` DatePicker alternatives because it gives me too many problems that I need to workaround, so I'm thinking on maybe just replacing it with something else. I tried adding required rule in a Controller which didn't work. I tried adding required rule to the slotProps textField, which made it to add error to errors object and now I can use error state when it's empty or has invalid value. I added onBlur to textField and openPickerButton so that it can give errors onBlur But I can't get it to focus on errors as I described in the question. EDIT: I forgot that I also tried to use setFocus method inside an useEffect hook. It causes DatePicker to focuse on page load but when I click submit button it only flickers P.S. the code is updated with useForm hook and useEffect hook that i use",
    "author_id":5658,
    "publication_date":1754308515000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ZEN",
    "author_reputation":1.0,
    "tags":"reactjs, material-ui, mui-x-date-picker, react-hook-form",
    "text_length":2527,
    "title_length":91,
    "num_tags":4
  },
  {
    "id":6161,
    "title":"How to Portably Use std::atomic Inside a Union Across Platforms (MSVC\/Clang on Windows\/macOS\/Linux)?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724741\/how-to-portably-use-stdatomic-inside-a-union-across-platforms-msvc-clang-on-w",
    "text":"I'm working on a cross-platform data structure and trying to define a compact union-based layout that allows atomic access to a 64-bit word, while also optionally accessing the lower 32-bit fields. I want to support: MSVC x64 and ARM64 Clang x64 and ARM64 On Windows, macOS, and Linux What I want: I would like to define a structure like this: ``` struct tTWUnion { union { std::atomic<uint64_t> uValue; struct { std::atomic<uint32_t> uID; std::atomic<uint32_t> uSwitchPointID; }; }; }; ``` Then use it like: ``` tTWUnion tun; tun.uValue.store(100); \/\/ or tun.uID.store(42); tun.uSwitchPointID.store(58); ``` But this fails to compile because std::atomic is non-trivial, and placing multiple std::atomic inside a union causes the compiler to delete the default constructor (which I want to avoid manually overriding for portability reasons). If I instead do: ``` struct tTWUnion { union { std::atomic<uint64_t> uValue; struct { uint32_t uID; uint32_t uSwitchPointID; }; }; }; ``` Then I can compile it, but now only uValue is atomic — and accessing uID or uSwitchPointID breaks atomicity guarantees, and could lead to tearing or data races. What I don't want: I don’t want to use placement new and manual destructors (which makes code harder to maintain across platforms). I don’t want to rely on undefined behavior or non-portable hacks, but I also want to avoid duplicating data if possible. My question: Is there a standard and portable way to define such a union layout where both full 64-bit atomic access and partial 32-bit atomic fields can coexist? Why exactly does placing multiple std::atomic inside a union lead to constructor issues? Are there any known methods for MSVC\/Clang that can allow this safely? Edit: As an alternative idea, I’m wondering if something like the following structure could work safely and portably: ``` struct tTWUnion { union { uint64_t uValue; struct { uint32_t uID; uint32_t uSwitchPointID; }; }; }; ``` And then use it like: ``` std::atomic<tTWUnion> atomicUnion; tTWUnion value = {}; value.uValue = 100; atomicUnion.store(value); tTWUnion value2 = {}; value2.uID = 42; value2.uSwitchPointID = 99; atomicUnion.store(value2); ``` Is using std::atomic like this valid if sizeof(tTWUnion) == 8?",
    "author_id":5657,
    "publication_date":1754308751000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Abhishek",
    "author_reputation":201.0,
    "tags":"c++, clang, atomic, union, stdatomic",
    "text_length":2231,
    "title_length":100,
    "num_tags":5
  },
  {
    "id":6160,
    "title":"Hikvision SDK - Callback for Motion Detection Events Not Triggered",
    "link":"https:\/\/stackoverflow.com\/questions\/79724757\/hikvision-sdk-callback-for-motion-detection-events-not-triggered",
    "text":"I'm trying to listen to all motion detection events from a Hikvision camera using their SDK. So far, I have successfully logged into the camera and set up the listening functionality along with a callback function. However, the callback function is never triggered. Interestingly, when I access the following API via browser: GET \/ISAPI\/Event\/notification\/alertStream it downloads a file containing all the camera events. The camera clearly detects movement and sends data to that file, so the events do seem to be working. Here’s the snippet of my implementation that works fine but there is no data : ``` public void StartListening(string ipAddress, int port, string username, string password, string listenIP, ushort listenPort) { if (!NET_DVR_Init()) { Console.WriteLine(\"SDK Init failed\"); return; } NET_DVR_DEVICEINFO_V30 deviceInfo = new NET_DVR_DEVICEINFO_V30(); int userID = NET_DVR_Login_V30(ipAddress, port, username, password, ref deviceInfo); if (userID < 0) { Console.WriteLine(\"Login failed, error: \" + NET_DVR_GetLastError()); NET_DVR_Cleanup(); return; } \/\/ start listining to the incoming events var callbackfunc = new ALARM_CALLBACK(OnAlarmReceivedCallBack); int iListenHandle = NET_DVR_StartListen_V30(listenIP, listenPort, callbackfunc, IntPtr.Zero); if (iListenHandle < 0) { Console.WriteLine(\"Listen setup failed, error: \" + NET_DVR_GetLastError()); NET_DVR_Logout(userID); NET_DVR_Cleanup(); return; } else { Console.WriteLine(\"Listen Setup success\"); } Console.WriteLine(\"Listening for motion detection events...\"); Console.WriteLine(\"Press any key to close connection\"); Console.ReadLine(); \/\/ Keep app running NET_DVR_Logout(userID); NET_DVR_Cleanup(); } ``` What I've tried: Verified login credentials and connection Confirmed that the camera is detecting motion Monitored the alert stream API to check for event activity Question: Why isn’t my callback function being called? Is there an additional configuration step needed to receive motion events via SDK? and here is my Camera Settings Configuration -> Event -> Motion Detection Enable Motion Detection : true Enable Dynamic Analysis for Motion : true Arming Schedule : All Times Linkage Actions-> Notify Surveillance Center : true Configuration-> Network -> Advanced Settings Set Alarm Host IP : 192.x.x.x Set Alarm Host Port : 5055",
    "author_id":5656,
    "publication_date":1754309772000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mohamed Hanti",
    "author_reputation":1.0,
    "tags":"c#, sdk, callback, motion-detection, hikvision",
    "text_length":2316,
    "title_length":66,
    "num_tags":5
  },
  {
    "id":6159,
    "title":"What difference between brk() and syscall(SYS_brk,)",
    "link":"https:\/\/stackoverflow.com\/questions\/79724758\/what-difference-between-brk-and-syscallsys-brk",
    "text":"``` man 2 brk ``` says: ``` int brk(void *addr); ``` brk() sets the end of the data segment to the value specified by addr ... On success, brk() returns zero. On error, -1 is returned, and errno is set to ENOMEM. But in MUSL brk call used as ``` uintptr_t brk = __brk(0); ``` which is very similar to calling sbrk() ``` void *sbrk(intptr_t increment); ``` sbrk() increments the program's data space by increment bytes. Calling sbrk() with an increment of 0 can be used to find the current location of the program break. On success, sbrk() returns the previous program break. ... On error, (void *) -1 is returned, and errno is set to ENOMEM. Where can I find documentation for Linux's ``` syscall(SYS_brk,) ``` if it differs from ``` man 2 brk ``` ?",
    "author_id":5655,
    "publication_date":1754309829000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Филя Усков",
    "author_reputation":353.0,
    "tags":"linux, system-calls, sbrk, brk",
    "text_length":749,
    "title_length":51,
    "num_tags":4
  },
  {
    "id":6158,
    "title":"Issue after upgrading MongoDB from 6.x to 8.0.12 on MongoDB Atlas",
    "link":"https:\/\/stackoverflow.com\/questions\/79724765\/issue-after-upgrading-mongodb-from-6-x-to-8-0-12-on-mongodb-atlas",
    "text":"We recently attempted to upgrade our MongoDB Atlas cluster from version 6.x to 8.0.12 (on M10 tier). For migration, we: Took a backup from the MongoDB 6 cluster using ``` mongodump ``` Restored the data into a fresh MongoDB 8.0.12 cluster using ``` mongorestore ``` After the upgrade, we observed significant performance degradation in several areas, including: Aggregation pipelines involving ``` $lookup ``` , ``` $unwind ``` , filters, and sorting Even basic ``` find() ``` queries with Mongoose ``` populate() ``` were noticeably slower, especially on large datasets Ran ``` explain() ``` on affected queries and found higher document examination and less efficient index usage Checked query execution plans, which seemed suboptimal compared to MongoDB 6.x Current Status: To confirm the issue, we downgraded to MongoDB 7.x, restored the same backup, and: Performance is back to normal Queries (including aggregations and populate()) run efficiently again Questions: Are there any known changes in MongoDB 8.0.12 that could affect query planner behavior, especially for ``` $lookup ``` or populated queries? Could ``` mongorestore ``` have missed optimizer stats or internal metadata impacting performance on MongoDB 8? Are there recommended post-restore or post-upgrade steps (like re-indexing or running analyze commands) specifically for version 8.x? Has anyone else experienced degraded query performance after moving to MongoDB 8.0.12, especially on Atlas?",
    "author_id":5654,
    "publication_date":1754310183000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Prabhakaran",
    "author_reputation":4029.0,
    "tags":"mongodb, aggregation-framework, mongodb-atlas",
    "text_length":1465,
    "title_length":65,
    "num_tags":3
  },
  {
    "id":6157,
    "title":"Not getting affected field values in zoho notification through api https:\/\/www.zohoapis.com\/crm\/v8\/actions\/watch",
    "link":"https:\/\/stackoverflow.com\/questions\/79724766\/not-getting-affected-field-values-in-zoho-notification-through-api-https-www-z",
    "text":"I am not getting affected fields values in zoho when any field is updated in zoho.i am getting hit in api but getting null in affected fields value. heres api https:\/\/www.zohoapis.com\/crm\/v8\/actions\/watch here's body ``` { \"watch\": [ { \"channel_id\": \"102001\", \"events\": [ \"Deals.all\" ], \"notification_condition\": [ { \"type\": \"field_selection\", \"module\": { \"api_name\": \"Deals\" }, \"field_selection\": { \"group_operator\": \"or\", \"group\": [ { \"field\": { \"api_name\": \"Last_Name\" } }, { \"field\": { \"api_name\": \"Email\" } } ] } } ], \"channel_expiry\": \"2025-12-31T09:58:09+05:30\", \"token\": \"deals.lastname.notif\", \"return_affected_field_values\": true, \"notify_url\": \"https:\/\/5b54c10abb31.ngrok-free.app\/zoho\/notify\" } ] } ``` here's my post api to receive notification ``` [HttpPost] [Route(\"zoho\/notify\")] public async Task<IActionResult> ReceiveZohoNotification([FromBody]ZohoNotification payload) { \/\/var token = payload[\"token\"]?.ToString(); \/\/var eventType = payload[\"event\"]?.ToString(); \/\/var ids = payload[\"ids\"]?.ToObject<List<string>>(); \/\/\/\/ Verify token \/\/if (token != \"yourSecretToken123\") \/\/ return Unauthorized(); \/\/ Process the notification (e.g., log, fetch updated lead data, etc.) return Ok(); } ``` i have tried everything but getting null in affected value here's sample response i am getting from zoho notification ``` { \"server_time\": 1698890700000, \"query_params\": { \"param1\": \"value1\" }, \"module\": \"Deals\", \"resource_uri\": \"\/crm\/v2\/Deals\", \"ids\": [\"1234567890\"], \"affected_fields\": null, \"operation\": \"update\", \"channel_id\": \"100000000001\", \"token\": \"my-channel-token\" } ```",
    "author_id":5653,
    "publication_date":1754310183000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dhruv",
    "author_reputation":11.0,
    "tags":"zoho",
    "text_length":1588,
    "title_length":112,
    "num_tags":1
  },
  {
    "id":6156,
    "title":"Accessing mysql data from docker over the network",
    "link":"https:\/\/stackoverflow.com\/questions\/79724769\/accessing-mysql-data-from-docker-over-the-network",
    "text":"I have created website that uses wordpress (using docker) for backend and react for frontend. when checking my site on my computer everything works fine - i can access data from db and everything works as it should. if i want to check my site on mobile phone i can see only static parts of website but nothing that is stored in db. is there something i need to set up so the mobile phone can access db data as well? here is my docker-compose file: ``` services: # Database db: image: mysql:5.7 volumes: - db_data:\/var\/lib\/mysql restart: always environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress networks: - wppoe # phpmyadmin phpmyadmin: depends_on: - db image: phpmyadmin restart: always ports: - '8081:80' environment: PMA_HOST: db MYSQL_ROOT_PASSWORD: password networks: - wppoe # Wordpress wordpress: depends_on: - db image: wordpress:latest ports: - '8001:80' restart: always volumes: [ '.\/:\/var\/www\/html' ] environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress networks: - wppoe networks: wppoe: volumes: db_data: ```",
    "author_id":5652,
    "publication_date":1754310415000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Timmy",
    "author_reputation":63.0,
    "tags":"docker",
    "text_length":1137,
    "title_length":49,
    "num_tags":1
  },
  {
    "id":6155,
    "title":"Cannot run Sonar analysis on Kotlin source code using Gradle plugin in Jenkins",
    "link":"https:\/\/stackoverflow.com\/questions\/79724770\/cannot-run-sonar-analysis-on-kotlin-source-code-using-gradle-plugin-in-jenkins",
    "text":"Using Sonarqube Gradle plugin version 6.2.0.5505 (latest: https:\/\/plugins.gradle.org\/plugin\/org.sonarqube\/6.2.0.5505 ) , the analysis of Kotlin source code fails when running on my Jenkins instance, with following stack trace: ``` 10:20:44 [stderr] Exception in thread \"main\" java.awt.AWTError: Can't connect to X11 window server using ':99.0' as the value of the DISPLAY variable. 10:20:44 [stderr] at java.desktop\/sun.awt.X11GraphicsEnvironment.initDisplay(Native Method) 10:20:44 [stderr] at java.desktop\/sun.awt.X11GraphicsEnvironment$1.run(Unknown Source) 10:20:44 [stderr] at java.base\/java.security.AccessController.doPrivileged(Unknown Source) 10:20:44 [stderr] at java.desktop\/sun.awt.X11GraphicsEnvironment.initStatic(Unknown Source) 10:20:44 [stderr] at java.desktop\/sun.awt.X11GraphicsEnvironment.<clinit>(Unknown Source) 10:20:44 [stderr] at java.desktop\/sun.awt.PlatformGraphicsInfo.createGE(Unknown Source) 10:20:44 [stderr] at java.desktop\/java.awt.GraphicsEnvironment$LocalGE.createGE(Unknown Source) 10:20:44 [stderr] at java.desktop\/java.awt.GraphicsEnvironment$LocalGE.<clinit>(Unknown Source) 10:20:44 [stderr] at java.desktop\/java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(Unknown Source) 10:20:44 [stderr] at java.desktop\/sun.awt.X11.XToolkit.<clinit>(Unknown Source) 10:20:44 [stderr] at java.desktop\/sun.awt.PlatformGraphicsInfo.createToolkit(Unknown Source) 10:20:44 [stderr] at java.desktop\/java.awt.Toolkit.getDefaultToolkit(Unknown Source) 10:20:44 [stderr] at java.desktop\/java.awt.Toolkit.getEventQueue(Unknown Source) 10:20:44 [stderr] at java.desktop\/java.awt.EventQueue.isDispatchThread(Unknown Source) 10:20:44 [stderr] at java.desktop\/javax.swing.SwingUtilities.isEventDispatchThread(Unknown Source) 10:20:44 [stderr] at com.intellij.mock.MockApplication.isDispatchThread(MockApplication.java:91) 10:20:44 [stderr] at org.jetbrains.kotlin.analysis.api.impl.base.permissions.KaBaseAnalysisPermissionChecker.isProhibitedEdtAnalysis(KaBaseAnalysisPermissionChecker.kt:63) 10:20:44 [stderr] at org.jetbrains.kotlin.analysis.api.impl.base.permissions.KaBaseAnalysisPermissionChecker.isAnalysisAllowed(KaBaseAnalysisPermissionChecker.kt:37) 10:20:44 [stderr] at org.jetbrains.kotlin.analysis.api.impl.base.sessions.KaBaseSessionProvider.beforeEnteringAnalysis(KaBaseSessionProvider.kt:56) 10:20:44 [stderr] at org.jetbrains.kotlin.analysis.api.impl.base.sessions.KaBaseSessionProvider.beforeEnteringAnalysis(KaBaseSessionProvider.kt:48) 10:20:44 [stderr] at org.sonarsource.kotlin.api.visiting.KotlinFileVisitor.scan(KotlinFileVisitor.kt:73) 10:20:44 [stderr] at org.sonarsource.kotlin.api.sensors.AbstractKotlinSensorExecuteContext.analyzeFile$lambda$8$lambda$7(AbstractKotlinSensorExecuteContext.kt:120) 10:20:44 [stderr] at org.sonarsource.kotlin.api.common.MeasureDurationKt.measureDuration(MeasureDuration.kt:23) 10:20:44 [stderr] at org.sonarsource.kotlin.api.sensors.AbstractKotlinSensorExecuteContext.analyzeFile(AbstractKotlinSensorExecuteContext.kt:119) 10:20:44 [stderr] at org.sonarsource.kotlin.api.sensors.AbstractKotlinSensorExecuteContext.analyzeFiles$lambda$6$lambda$5(AbstractKotlinSensorExecuteContext.kt:102) 10:20:44 [stderr] at org.sonarsource.kotlin.api.common.MeasureDurationKt.measureDuration(MeasureDuration.kt:23) 10:20:44 [stderr] at org.sonarsource.kotlin.api.sensors.AbstractKotlinSensorExecuteContext.analyzeFiles(AbstractKotlinSensorExecuteContext.kt:101) 10:20:44 [stderr] at org.sonarsource.kotlin.api.sensors.AbstractKotlinSensor.execute(AbstractKotlinSensor.kt:65) 10:20:44 [stderr] at org.sonar.scanner.sensor.AbstractSensorWrapper.analyse(AbstractSensorWrapper.java:64) 1 ``` The build is run on a Jenkins docker agent based on debian image. I had similar issue with older version of Sonarqube plugin , like ``` https:\/\/plugins.gradle.org\/plugin\/org.sonarqube\/4.4.1.3373 ``` : it was possible to workaround this issue by forcing Gradle process to start in Headless Mode, using ``` -Djava.awt.headless=true ``` With latest version 6.x of sonarqube plugin, this workaround is no longer working. Note: running the sonar analysis locally on my Windows dev environment is working fine. Any suggestion ?",
    "author_id":5651,
    "publication_date":1754310436000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"M.Ricciuti",
    "author_reputation":12304.0,
    "tags":"jetbrains-ide, kotlin, gradle, sonarqube",
    "text_length":4175,
    "title_length":78,
    "num_tags":4
  },
  {
    "id":6154,
    "title":"AWS Step Functions triggering the same Lambda multiple times due to parallel cleanup executions how to avoid race conditions?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724771\/aws-step-functions-triggering-the-same-lambda-multiple-times-due-to-parallel-cle",
    "text":"I'm working with multiple AWS Step Functions, and each one starts with a cleanup step that triggers the same Lambda function to clean up specific S3 prefixes. Here’s what I’ve observed: We added the same cleanup logic at the beginning of each state machine, including a newly added one for our flow. When two or more Step Functions are started around the same time, the initial cleanup steps overlap, and all of them end up invoking the Lambda concurrently. This results in the Lambda being triggered 3 times in total, even though the intent was to run it once per execution. By logging the Lambda event input, I confirmed that it's being called in parallel by different state machines. When I added a Wait state of 10 minutes before the cleanup step in one of the state machines, the issue disappeared each Lambda execution occurred in isolation and worked as expected. This leads me to believe that the concurrency model of Step Functions isn't aware of external Lambda executions that are shared across different workflows, and that this can create race conditions or unwanted parallel invocations. What I want to understand: Is this behavior expected in AWS Step Functions when multiple workflows use the same Lambda in their initial states? Is there a recommended pattern to serialize or queue such cleanup operations across multiple state machines?",
    "author_id":4713,
    "publication_date":1754310450000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Yasin T&#252;rk",
    "author_reputation":13.0,
    "tags":"aws-lambda, amazon-web-services, state-machine",
    "text_length":1354,
    "title_length":125,
    "num_tags":3
  },
  {
    "id":6153,
    "title":"How to set starting condition for airflow task group",
    "link":"https:\/\/stackoverflow.com\/questions\/79724772\/how-to-set-starting-condition-for-airflow-task-group",
    "text":"I have the following dag structure: ``` from datetime import datetime, timedelta from airflow import DAG from airflow.utils.task_group import TaskGroup from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator with DAG( 'irb_test', description='irb_test', tags=[\"irb_test\"], schedule_interval=None, start_date=datetime(2025, 7, 1), default_args={ 'retries': 0, 'retry_delay': timedelta(minutes=1), 'conn_id': 'sgk_gp' } ) as dag: with TaskGroup('test_1') as test_1: for i in range(15): task_1_i = SQLExecuteQueryOperator( task_id=f'task_1_{i}', sql=f\"\"\" select pg_sleep({i}) \"\"\" ) with TaskGroup('test_2') as test_2: for i in range(15): task_2_i = SQLExecuteQueryOperator( task_id=f'task_2_{i}', sql=f\"\"\" select pg_sleep({i}) \"\"\" ) for i in range(15): test_1.get_child_by_label(f'task_1_{i}')\\ >> test_2.get_child_by_label(f'task_2_{i}') ``` What I would like to do is to start task group test_2 only after 10 of the tasks in the first task group have succeeded. I know that I can rewrite the execution order manually, but is there maybe a more elegant way of doing it?",
    "author_id":5650,
    "publication_date":1754310454000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Serge Kashlik",
    "author_reputation":423.0,
    "tags":"python, airflow",
    "text_length":1090,
    "title_length":52,
    "num_tags":2
  },
  {
    "id":6152,
    "title":"How to download protected PDF (ViewDocument) using Selenium or requests?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724774\/how-to-download-protected-pdf-viewdocument-using-selenium-or-requests",
    "text":"I'm trying to download a protected PDF from the New York State Courts NYSCEF website using Python. The URL looks like this: ``` https:\/\/iapps.courts.state.ny.us\/nyscef\/ViewDocument?docIndex=cdHe_PLUS_DaUdFKcTLzBtSo6zw== ``` When I try to use ``` requests.get() ``` or even navigate to the page with Selenium, I either get: A ``` 403 Forbidden ``` response (via ``` requests ``` ) Or a blank page with no ``` <embed> ``` tag (via Selenium) Here’s what I’ve tried: Using requests: ``` import requests url = \"https:\/\/iapps.courts.state.ny.us\/nyscef\/ViewDocument?docIndex=...\" headers = { \"User-Agent\": \"Mozilla\/5.0\", \"Referer\": \"https:\/\/iapps.courts.state.ny.us\/nyscef\/\" } response = requests.get(url, headers=headers) print(response.status_code) # Always 403 ``` And using SeleniumBase: ``` from seleniumbase import SB with SB(headless=False) as sb: sb.open(url) sb.wait(5) try: embed = sb.find_element(\"embed\") print(embed.get_attribute(\"src\")) except Exception as e: print(\"❌ No embed tag found\", e) ``` Nothing works. Full code for reference: ``` from seleniumbase import SB import requests import os import time def download_pdf_with_selenium_and_requests(): # Target document URL doc_url = \"https:\/\/iapps.courts.state.ny.us\/nyscef\/ViewDocument?docIndex=cdHe_PLUS_DaUdFKcTLzBtSo6zw==\" # Setup download directory download_dir = os.path.join(os.getcwd(), \"downloads\") os.makedirs(download_dir, exist_ok=True) filename = os.path.join(download_dir, \"NYSCEF_Document.pdf\") with SB(headless=True) as sb: # Step 1: Navigate to the document page (using browser session) sb.open(doc_url) time.sleep(5) # Wait for any redirects\/cookies to be set # Step 2: Grab the actual PDF <embed src> try: embed = sb.find_element(\"embed\") pdf_url = embed.get_attribute(\"src\") print(f\"Found PDF URL: {pdf_url}\") except Exception as e: print(f\"No <embed> tag found: {e}\") return # Step 3: Extract cookies from Selenium session selenium_cookies = sb.driver.get_cookies() session = requests.Session() for cookie in selenium_cookies: session.cookies.set(cookie['name'], cookie['value']) # Step 4: Download PDF using requests with cookies headers = { \"User-Agent\": \"Mozilla\/5.0\", \"Referer\": doc_url } response = session.get(pdf_url, headers=headers) if response.status_code == 200 and \"application\/pdf\" in response.headers.get(\"Content-Type\", \"\"): with open(filename, \"wb\") as f: f.write(response.content) print(f\"PDF saved as: {filename}\") else: print(f\"PDF download failed. Status: {response.status_code}\") print(f\"Content-Type: {response.headers.get('Content-Type')}\") print(f\"Final URL: {response.url}\") if __name__ == \"__main__\": download_pdf_with_selenium_and_requests() ``` Response: ``` No <embed> tag found: Message: Element {embed} was not present after 10 seconds! ```",
    "author_id":5649,
    "publication_date":1754310491000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Daremitsu",
    "author_reputation":663.0,
    "tags":"selenium-webdriver, python, web-scraping, pdf, python-requests",
    "text_length":2752,
    "title_length":72,
    "num_tags":5
  },
  {
    "id":6151,
    "title":"Java web-socket ssl certificate error while cert is valid",
    "link":"https:\/\/stackoverflow.com\/questions\/79724775\/java-web-socket-ssl-certificate-error-while-cert-is-valid",
    "text":"Hello I've been facing a weird problem the past few days. I send out a demo of my app to a few people. however for around 75% of the people the demo didn't work, while for the other 25% it works great. (even for myself) so I investigated and found the issue. the mayority is getting the following error when connecting to the web-socket: ``` Caused by: java.security.cert.CertificateException: No subject alternative names matching IP address 45.116.104.89 found ``` Full stack trace: ``` [21:48:31] [pool-4-thread-1\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager:lambda$tryStartConnection$0:48]: Attempting to connect to WebSocket... [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:77]: WebSocket error: [21:48:32] [WebSocketWriteThread-107\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:77]: WebSocket error: [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative names matching IP address 45.116.104.89 found [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:302) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:296) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1497) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:212) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.Handshaker.processLoop(Handshaker.java:979) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.Handshaker.process_record(Handshaker.java:914) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1062) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:928) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at java.io.InputStream.read(InputStream.java:101) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at com.heckvision.shadowed.java_websocket.client.WebSocketClient.run(WebSocketClient.java:543) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: [com.heckvision.bingosplash.web.WebSocketManager$1:onError:78]: at java.lang.Thread.run(Thread.java:745) [21:48:32] [WebSocketConnectReadThread-106\/INFO]: ``` after researching I found out that this error means that the SSL certificate is not valid. this is weird since I have a valid 'lets encrypt' cert and domain. and the other 25% can connect proving the cert is valid. the error shows I'm connecting directly to a raw IP but I'm not. im using wss:\/\/testserver.heckvision.com this domain is linked to a freshly setup test server for this demo. running nginx (for upgrading to wss) and docker (to host the web-socket) the project is on JAVA 8. I've tried multiple versions of JAVA 8 and sorts, but nothing seems to help. even weirder is that when I used the same class in a JAVA 21 project it works great and for 100% of the people. I use the following java class to connect to the web-socket: ``` package com.heckvision.bingosplash.web; import org.java_websocket.client.WebSocketClient; import org.java_websocket.enums.ReadyState; import org.java_websocket.handshake.ServerHandshake; import javax.net.ssl.SSLContext; import java.net.URI; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class WebSocketManager { private final URI serverUri; private volatile WebSocketClient client; \/\/ Made volatile for thread safety private final ScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(); private volatile boolean shouldConnect = false; \/\/ Made volatile for thread safety private volatile boolean connecting = false; \/\/ Made volatile for thread safety private MessageListener messageListener; public void setMessageListener(MessageListener listener) { this.messageListener = listener; } public WebSocketManager(String serverUrl) { this.serverUri = URI.create(serverUrl); } public void setShouldConnect(boolean shouldConnect) { this.shouldConnect = shouldConnect; if (shouldConnect) { tryStartConnection(); } else { disconnect(); } } private void tryStartConnection() { if (client != null && client.getReadyState() == ReadyState.OPEN) return; if (connecting) return; connecting = true; executor.execute(() -> { try { System.out.println(\"Attempting to connect to WebSocket...\"); WebSocketClient socket = new WebSocketClient(serverUri) { @Override public void onOpen(ServerHandshake handshakedata) { System.out.println(\"WebSocket connected.\"); connecting = false; \/\/ Connection successful, clear connecting flag } @Override public void onMessage(String message) { System.out.println(\"Received: \" + message); if (messageListener != null) { messageListener.onMessage(message); } } @Override public void onClose(int code, String reason, boolean remote) { System.out.println(\"WebSocket closed: \" + reason); client = null; connecting = false; if (shouldConnect) { scheduleReconnect(); } } @Override public void onError(Exception ex) { System.err.println(\"WebSocket error:\"); ex.printStackTrace(); client = null; connecting = false; if (shouldConnect) { scheduleReconnect(); } } }; SSLContext sslContext = SSLContext.getDefault(); socket.setSocketFactory(sslContext.getSocketFactory()); client = socket; socket.connect(); \/\/ Non-blocking } catch (Exception e) { System.err.println(\"WebSocket connect exception:\"); e.printStackTrace(); connecting = false; \/\/ FIX: Only reconnect if we should still be connecting if (shouldConnect) { scheduleReconnect(); } } }); } private void scheduleReconnect() { executor.schedule(this::tryStartConnection, 5, TimeUnit.SECONDS); } private void disconnect() { if (client != null && (client.getReadyState() == ReadyState.OPEN || client.getReadyState() == ReadyState.NOT_YET_CONNECTED)) { try { client.close(); System.out.println(\"WebSocket disconnected by request.\"); } catch (Exception e) { System.err.println(\"WebSocket disconnect exception:\"); e.printStackTrace(); } } client = null; } public void shutdown() { shouldConnect = false; disconnect(); executor.shutdownNow(); } } ``` multiple versions and sorts of java - did not help multiple pc and operating systems - did not help adding custom ssl handshake checker - did not work and eventually came back wit the same error hosting websocket on another server - did not work same error",
    "author_id":5648,
    "publication_date":1754310545000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Christian2B",
    "author_reputation":21.0,
    "tags":"java, ssl, websocket, java-8, java-websocket",
    "text_length":8061,
    "title_length":57,
    "num_tags":5
  },
  {
    "id":6150,
    "title":"Confusion about post-decrement and assignment execution order in C",
    "link":"https:\/\/stackoverflow.com\/questions\/79724776\/confusion-about-post-decrement-and-assignment-execution-order-in-c",
    "text":"I'm confused about the behavior of the post-decrement (i--) and pre-decrement (--i) operators in C, especially when used in the context of array indexing and assignment. I'm trying to understand the execution order between the decrement and the assignment operation. Here is my code: ``` #include <stdio.h> int main() { int arr1[] = {11, 12, 13, 14, 15, 16}; int arr2[] = {21, 22, 23, 24, 25, 26}; int i = 5; arr1[i--] = arr2[i]; \/\/1st assignment int arr3[] = {11, 12, 13, 14, 15, 16}; int arr4[] = {21, 22, 23, 24, 25, 26}; int j = 5; arr3[j] = arr4[j--]; \/\/2nd assignment return 0; } ``` What I expected: For the first assignment ( ``` arr1[i--] = arr2[i]; ``` ): Since ``` i ``` is initially 5, I expected ``` arr1[5] = arr2[4] ``` , which should make ``` arr1[5] = 25 ``` . So, ``` arr1 ``` should be ``` {11, 12, 13, 14, 15, 25} ``` . For the second assignment ( ``` arr3[j] = arr4[j--]; ``` ): Since ``` j ``` is also 5, I expected ``` arr3[5] = arr4[5] ``` , which is ``` arr3[5] = 26 ``` , and then ``` j ``` becomes 4. So, ``` arr3 ``` should be ``` {11, 12, 13, 14, 15, 26} ``` . What actually happened: The first assignment behaved as expected: ``` arr1 = {11, 12, 13, 14, 15, 25} ``` But the second assignment resulted in: ``` arr3 = {11, 12, 13, 14, 26, 16} ``` Question: Why does the second assignment ( ``` arr3[j] = arr4[j--]; ``` ) result in modifying ``` arr3[4] ``` instead of ``` arr3[5] ``` ? I thought the post-decrement should use the current value before decrementing, so both indices should be 5 in this case. Could this be due to sequence point rules or undefined behavior?",
    "author_id":5647,
    "publication_date":1754310552000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"DaiKaVinhBao",
    "author_reputation":13.0,
    "tags":"c, arrays, variable-assignment",
    "text_length":1599,
    "title_length":66,
    "num_tags":3
  },
  {
    "id":6149,
    "title":"How to use both fixed colors and colorbar with range in matplotlib\/seaborn heatmap",
    "link":"https:\/\/stackoverflow.com\/questions\/79724779\/how-to-use-both-fixed-colors-and-colorbar-with-range-in-matplotlib-seaborn-heatm",
    "text":"I saw JohanC's answer here: How to set fixed color ranges with Seaborn heatmap? When I made my plot, I had two types of cells that I needed to highlight: those with fixed values (e.g.,0 and 1), and those that varied within a range (e.g., 9-200). For the first type, fixed colors look nice. For the second type, a standard colorbar looks nice. Is there a way to have both, i.e., specify a set of cells to conditionally use fixed colors and if they go above a certain value, use a colorbar to color them? Thanks, Michael Regular colorbar example ``` sns.heatmap(np.clip(np.random.rand(21, 12) - 0.1, 0, 1)*100) plt.show() ```",
    "author_id":5243,
    "publication_date":1754310623000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Cats",
    "author_reputation":11.0,
    "tags":"matplotlib, seaborn",
    "text_length":623,
    "title_length":82,
    "num_tags":2
  },
  {
    "id":6148,
    "title":"Clicking outside in the AsyncPaginate while having the Custom Value Container is not closing the menu list",
    "link":"https:\/\/stackoverflow.com\/questions\/79724780\/clicking-outside-in-the-asyncpaginate-while-having-the-custom-value-container-is",
    "text":"Problem Description I’ve created a custom ``` Select ``` component using ``` react-select ``` and ``` AsyncPaginate ``` . I use a custom ``` ValueContainer ``` to display the number of selected items instead of rendering their labels when ``` isShowCount ``` is ``` true ``` . However, when ``` isShowCount ``` is ``` true ``` , the dropdown menu does not close when clicking outside of the select component after making a selection : Video demonstration : https:\/\/streamable.com\/cphx9q ``` import React from \"react\"; import Select, { components } from \"react-select\"; import { useController, Control, FieldValues, Path } from \"react-hook-form\"; import CustomCheckbox from \".\/CustomCheckbox\"; import { AsyncPaginate } from \"react-select-async-paginate\"; interface CheckBoxSelectProps<T extends FieldValues> { name: Path<T>; control: Control<T>; label?: string; \/\/ options: { value: string; label: string }[]; placeholder?: string; isRequired?: boolean; disabled?: boolean; isShowCount?: boolean; isClearable?: boolean; isMulti?: boolean; onValueChange?: (value: any) => void; onSearchTextChange?: (text: string) => void; loadOptions: (page: number) => Promise<{ value: string; label: string }[]>; className?: string; maxMenuHeight?: string; } const CheckBoxSelect = <T extends FieldValues>({ name, control, label, \/\/ options, placeholder = \"Select...\", isRequired = false, disabled = false, isShowCount = true, isClearable = true, isMulti = false, onValueChange, onSearchTextChange, loadOptions, className, maxMenuHeight = \"15rem\", }: CheckBoxSelectProps<T>) => { const { field: { onChange, onBlur, value, ref }, fieldState: { error }, } = useController({ name, control, rules: { required: isRequired ? \"Это поле не может быть пустым\" : false, }, }); const labelParts = label ? label.split(\"*\") : []; const isLabelRequired = labelParts.length > 1 && labelParts[0].trim() !== \"\"; const [options, setOptions] = React.useState< { value: string; label: string; profile_img: string }[] >([]); const [page, setPage] = React.useState(1); const [hasMore, setHasMore] = React.useState(true); const [isLoading, setIsLoading] = React.useState(false); React.useEffect(() => { const loadMoreOptions = async () => { if (!hasMore || isLoading) return; setIsLoading(true); try { const newOptions = await loadOptions(page); setOptions((prev) => { const uniqueOptions = Array.from( new Map( [...prev, ...newOptions].map((opt) => [opt.value, opt]) ).values() ); return uniqueOptions; }); if (newOptions.length === 0) { setHasMore(false); } } catch (error) { toast.error(\"Не удалось загрузить параметры.\", error); } finally { setIsLoading(false); } }; loadMoreOptions(); }, [page]); const formatValue = (value: any) => { if (isMulti) { return Array.isArray(value) ? value : []; } return value || null; }; \/\/ Handle infinite scroll const handleMenuScrollToBottom = () => { if (hasMore && !isLoading) { setPage((prev) => prev + 1); } }; const handleChange = (selectedOption: any) => { if (isMulti) { onChange(selectedOption); if (onValueChange) onValueChange(selectedOption); } else { onChange(selectedOption ? selectedOption : null); if (onValueChange) onValueChange(selectedOption ? selectedOption : null); } }; \/\/ Custom option with checkbox const CustomOption = (props: any) => { const { data, isSelected, innerRef, innerProps } = props; return ( <div ref={innerRef} {...innerProps} className=\"flex items-center px-3 py-2 cursor-pointer\" > <CustomCheckbox label={data.label} isChecked={isSelected} handleCheckboxChange={() => props.selectOption(data)} className=\"mr-2 text-sm\" \/> <\/div> ); }; \/\/ Custom value container to show selected count const CustomValueContainer = ({ children, ...props }: any) => { const selectedValues = props.getValue(); if (selectedValues.length === 0) { return ( <components.ValueContainer {...props}> {children} <\/components.ValueContainer> ); } return ( <components.ValueContainer {...props}> {selectedValues.length} выбрано {children[0]} {children[1]} <\/components.ValueContainer> ); }; const asyncLoadOptions = async ( inputValue: string, loadedOptions: any[], { page }: any ) => { const newOptions = await loadOptions(page || 1); return { options: newOptions, hasMore: newOptions.length > 0, additional: { page: (page || 1) + 1 }, }; }; return ( <div className={`mb-4 ${!label ? \"\" : \"\"} ${className} w-full flex flex-col`} > <label className=\"text-sm font-medium text-[#344054]\"> {isRequired ? labelParts[0] : label} {isLabelRequired && <span className=\"ml-1 text-red-600\">*<\/span>} <\/label> <AsyncPaginate isMulti={isMulti} inputRef={ref} onChange={handleChange} onBlur={onBlur} value={formatValue(value)} isDisabled={disabled} options={options} placeholder={placeholder} classNamePrefix=\"custom-select\" isSearchable={false} closeMenuOnSelect={!isMulti} blurInputOnSelect={!isMulti} loadOptions={asyncLoadOptions} hideSelectedOptions={!isMulti} isClearable={isClearable} components={{ Option: CustomOption, ...(isShowCount ? { ValueContainer: CustomValueContainer } : {}), }} styles={{ control: (base, state) => ({ ...base, borderRadius: \"8px\", borderColor: error ? \"red\" : base.borderColor, boxShadow: state?.isFocused ? \"0 0 0 3px #CCE1FF\" : \"none\", \"&:hover\": { borderColor: error ? \"red\" : base.borderColor, }, }), menu: (base) => ({ ...base, }), menuPortal: (base) => ({ ...base, zIndex: 9999, }), multiValue: (base) => ({ ...base, backgroundColor: \"transparent\", border: \"1px solid #d1d5db\", borderRadius: \"4px\", }), multiValueLabel: (base) => ({ ...base, color: \"#1a202c\", }), multiValueRemove: (base) => ({ ...base, color: \"#e53e3e\", \":hover\": { backgroundColor: \"#fed7d7\", }, }), valueContainer: (base) => ({ ...base, maxHeight: \"2.5rem\", overflowY: \"auto\", }), menuList: (base) => ({ ...base, padding: \"0.37rem\", maxHeight: maxMenuHeight, }), option: (base, state) => ({ ...base, borderRadius: \"6px\", color: state.isSelected ? \"white\" : \"#000\", backgroundColor: state.isSelected ? \"#0069F9\" : \"white\", \"&:hover\": { backgroundColor: state.isSelected ? \"#0069F9\" : \"rgb(222, 235, 255)\", color: state.isSelected ? \"white\" : \"#000\", }, }), }} \/\/ menuPortalTarget={document.body} menuPosition=\"absolute\" additional={{ page: 1 }} \/\/ onMenuScrollToBottom={handleMenuScrollToBottom} \/> {error && <p className=\"text-red-500 text-xs mt-1\">{error.message}<\/p>} <\/div> ); }; export default CheckBoxSelect; ``` What I’ve Tried Rendering ``` children ``` inside the custom ``` ValueContainer ``` (as recommended in various GitHub issues and StackOverflow answers) so blur and focus behavior is preserved. Clicking on the dropdown chevron helps , after doing that, clicking outside works. But this is not intuitive and results in poor UX. Manually handling open\/close state of the dropdown, but this didn't solve the issue either. What Works The default ``` ValueContainer ``` works as expected. The menu closes on blur when ``` isShowCount ``` is false (using default value rendering). Clicking outside works after toggling the menu manually. Expected Behavior When I: Select one or more options, And ``` isShowCount ``` is ``` true ``` , I should be able to click anywhere outside the dropdown and have it close automatically . How can I make the dropdown menu close automatically when clicking outside the ``` Select ``` component, even when a custom ``` ValueContainer ``` is used? Is there a workaround or a proper way to handle this when overriding the ``` ValueContainer ``` ?",
    "author_id":5646,
    "publication_date":1754310655000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sokhibov Farkhod",
    "author_reputation":13.0,
    "tags":"next.js, reactjs, react-select",
    "text_length":7395,
    "title_length":106,
    "num_tags":3
  },
  {
    "id":6147,
    "title":"Problem with modifying RAM module to improve Hardware Accelerator",
    "link":"https:\/\/stackoverflow.com\/questions\/79724782\/problem-with-modifying-ram-module-to-improve-hardware-accelerator",
    "text":"A colleague of mine made a Q-learning Hardware Accelerator that is integrated to the RISC-V Core. The hardware accelerator is controlled by custom instructions. The system is based on RVfpgaEL2 System which is based on the VeeRwolf SoC ( https:\/\/github.com\/chipsalliance\/VeeRwolf ) which, in turn, uses open-source RISC-V VeeR EL2 Core ( https:\/\/github.com\/chipsalliance\/Cores-VeeR-EL2 ). in the file exu_div_ctl, he added a hardware accelerator module, which uses floating point adder, multiplier, and a RAM module. He uses RAM Module to store and load values in the Q-Table. One of the process in the hardware accelerator is comparing a value registered to an address to the values in the next three address. ``` if(i_action != 4) begin case(learn_state) get_max_candidate: begin ram_read_enable_reg <= 1; learn_state <= compare_max_candidate; end compare_max_candidate: begin if(ram_read_done) begin if(~not_first_time_read_max) begin max_q <= ram_data_out; max_q_index <= ram_address; not_first_time_read_max <= 1; end else begin \/* Identical to floating-point comparison when: - Both numbers are positive, positive-zero, or positive-infinity. - One positive and one negative number, and you are using a signed integer comparison. Inverse of floating-point comparison when: - Both numbers are negative, negative-zero, or negative-infinity. *\/ \/\/ both positive (S = 0) if(!ram_data_out[31] & !max_q[31]) begin if(ram_data_out[30:0] > max_q[30:0]) begin max_q <= ram_data_out; max_q_index <= ram_address; end \/\/ one negative and one positive end else if(ram_data_out[31] ^ max_q[31]) begin if(max_q[31]) begin max_q <= ram_data_out; max_q_index <= ram_address; end \/\/ both negative end else if(ram_data_out[31] & max_q[31]) begin if(ram_data_out[30:0] < max_q[30:0]) begin max_q <= ram_data_out; max_q_index <= ram_address; end end learn_state <= get_max_candidate; i_action <= i_action + 1; ram_read_enable_reg <= 0; ram_address_reg <= ram_address_reg + 1; end end end endcase end ``` The RAM module the HW module used is as follows: ``` module ram( input clk, input write_enable, input [31:0]address, input [31:0]data_in, input [31:0]read_enable, output reg read_done, output reg [31:0]data_out ); reg [31:0]ram_block[0:1023]; always @(posedge clk) begin if(read_done) read_done <= 0; if(write_enable) ram_block[address] <= data_in; if(read_enable) begin data_out <= ram_block[address]; read_done <= 1; end end endmodule ``` I wanted to add another port to this module so that the RAM module can read 2 addresses at the same time, reducing cycle needed. ``` module ram( input clk, input write_enable, input [31:0]address, input [31:0]address2, input [31:0]data_in, input [31:0]read_enable, input [31:0]read_enable2, output reg read_done, output reg read_done2, output reg [31:0]data_out, output reg [31:0]data_out2 ); reg [31:0]ram_block[0:1023]; always @(posedge clk) begin if(read_done) read_done <= 0; if(read_done2) read_done2 <= 0; if(write_enable) ram_block[address] <= data_in; if(read_enable) begin data_out <= ram_block[address]; read_done <= 1; end if(read_enable2) begin data_out2 <= ram_block[address2]; read_done2 <= 1; end end endmodule ``` I successfully generated the bitstream with vivado after making modifications on RAM and HW module, but when I program my FPGA with the bitstream and execute the Q-Learning program in Catapult SDK, it results in error, and I am unable to halt the execution process. I have changed nothing else besides the RAM module and the hardware accelerator module in the el2_exu_div_ctl file, so I figured the problem is within that part only. I also already simulated the process in modelsim and confirm it works during max seeking. I used Nexys A7- 100T for my FPGA. The main question I want to ask is if whether adding another port and searching at different at different addresses at the same time could lead to error? I also tried expanding the output so an address contains 4 values to be compared, but it results in the same error.",
    "author_id":5645,
    "publication_date":1754310699000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"257_ Raihan Elfazri",
    "author_reputation":9.0,
    "tags":"verilog, system-verilog, ram, hardware-acceleration, q-learning",
    "text_length":3986,
    "title_length":65,
    "num_tags":5
  },
  {
    "id":6146,
    "title":"how do I optimise my semantic similarity check to detect bot like responses to an open ended question?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724786\/how-do-i-optimise-my-semantic-similarity-check-to-detect-bot-like-responses-to-a",
    "text":"Currently I'm using tf-idf vectoriser to check for semantic similarity, but this is not working too well because responses like 'sssss' 'hkfjhwekhwke' are not being detected as bot responses. ``` # 🧬 Semantic Similarity Check try: tfidf = TfidfVectorizer().fit([text, question_text]) vecs = tfidf.transform([text, question_text]) similarity = cosine_similarity(vecs[0], vecs[1])[0][0] if similarity < 0.1: result[\"similarity_flag\"] = 1 except: pass return result ```",
    "author_id":5644,
    "publication_date":1754310813000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Arpita Laxmisha",
    "author_reputation":1.0,
    "tags":"python, bots, semantics",
    "text_length":466,
    "title_length":102,
    "num_tags":3
  },
  {
    "id":6145,
    "title":"InvalidAuthenticationToken in script but working fine in Postman",
    "link":"https:\/\/stackoverflow.com\/questions\/79724790\/invalidauthenticationtoken-in-script-but-working-fine-in-postman",
    "text":"I am executing the below code from the CI-CD pipeline, then I am getting InvalidAuthenticationToken: EvolvedSecurityTokenService access token is invalid. But after logging and using the value of ``` $restAPi ``` and ``` $token ``` in Postman, I am getting the proper value. ``` $baseUrl = \"https:\/\/management.azure.com\" $token = (Get-AzAccessToken -ResourceUrl $baseUrl).Token $RId = (Get-AzResource -ResourceGroupName $resourceGroupName -Name $queryPackName).ResourceId $restAPi = \"$baseUrl$RId\/savedSearches?api-version=2025-12-01\" $response = Invoke-RestMethod -Uri $restAPi -Method Get -Headers @{Authorization = \"Bearer $token\"} ``` Any idea why?",
    "author_id":5643,
    "publication_date":1754311128000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"NutsAndBolts",
    "author_reputation":425.0,
    "tags":"powershell, azure, bearer-token",
    "text_length":651,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":6144,
    "title":"PHP calling model function",
    "link":"https:\/\/stackoverflow.com\/questions\/79724793\/php-calling-model-function",
    "text":"I'm trying to learn constructors and statics and I'm not sure what I'm doing wrong here controller ``` <?php namespace App\\Http\\Controllers\\Api; use App\\Models\\SampleModel; class SampleController { private static $sm; public function __construct() { self::$sm = new SampleModel; } \/\/ called via ($class)::{$function}($params); public static function create($file_id) { self::$sm->createRecord($file_id); \/\/ error occurs when this line is triggered } } ``` model ``` <?php class SampleModel { public function createRecord($file_id) { return true; } } ``` error ``` local.ERROR: Call to a member function createRecord() on null ```",
    "author_id":5642,
    "publication_date":1754311163000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"rhemmuuu",
    "author_reputation":1209.0,
    "tags":"php, constructor",
    "text_length":629,
    "title_length":26,
    "num_tags":2
  },
  {
    "id":6143,
    "title":"Why doesn&#39;t qemu manage to use my customized DRAM?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724796\/why-doesnt-qemu-manage-to-use-my-customized-dram",
    "text":"I need to implement a custom QEMU DRAM Device to simulate row activations and precharge cycles and enable high-frequency row access to induce emulated bit flips. But when I run qemu on buildroot this is the problem I get: ``` `.\/buildroot\/output\/images\/start-qemu.sh qemu-system-aarch64: Standard RAM disabled - Using custom DRAM only qemu-system-aarch64: ..\/system\/memory.c:2647: memory_region_add_subregion_common: Assertion `!subregion->container' failed. Aborted ``` What could be the problem? Here my modification to function ``` machvirt_init ``` into ``` qemu\/arm\/virt.c ``` . ``` static void machvirt_init(MachineState *machine) { VirtMachineState *vms = VIRT_MACHINE(machine); VirtMachineClass *vmc = VIRT_MACHINE_GET_CLASS(machine); MachineClass *mc = MACHINE_GET_CLASS(machine); const CPUArchIdList *possible_cpus; MemoryRegion *sysmem = get_system_memory(); if (machine->ram) { error_report(\"Standard RAM disabled - Using custom DRAM only\"); machine->ram = NULL; } custom_dram_device_init(machine); MemoryRegion *secure_sysmem = NULL; MemoryRegion *tag_sysmem = NULL; MemoryRegion *secure_tag_sysmem = NULL; int n, virt_max_cpus; bool firmware_loaded; bool aarch64 = true; bool has_ged = !vmc->no_ged; unsigned int smp_cpus = machine->smp.cpus; unsigned int max_cpus = machine->smp.max_cpus; possible_cpus = mc->possible_cpu_arch_ids(machine); \/* * In accelerated mode, the memory map is computed earlier in kvm_type() * for Linux, or hvf_get_physical_address_range() for macOS to create a * VM with the right number of IPA bits. *\/ if (!vms->memmap) { Object *cpuobj; ARMCPU *armcpu; int pa_bits; \/* * Instantiate a temporary CPU object to find out about what * we are about to deal with. Once this is done, get rid of * the object. *\/ cpuobj = object_new(possible_cpus->cpus[0].type); armcpu = ARM_CPU(cpuobj); pa_bits = arm_pamax(armcpu); object_unref(cpuobj); virt_set_memmap(vms, pa_bits); } \/* We can probe only here because during property set * KVM is not available yet *\/ finalize_gic_version(vms); if (vms->secure) { \/* * The Secure view of the world is the same as the NonSecure, * but with a few extra devices. Create it as a container region * containing the system memory at low priority; any secure-only * devices go in at higher priority and take precedence. *\/ secure_sysmem = g_new(MemoryRegion, 1); memory_region_init(secure_sysmem, OBJECT(machine), \"secure-memory\", UINT64_MAX); memory_region_add_subregion_overlap(secure_sysmem, 0, sysmem, -1); } firmware_loaded = virt_firmware_init(vms, sysmem, secure_sysmem ?: sysmem); \/* If we have an EL3 boot ROM then the assumption is that it will * implement PSCI itself, so disable QEMU's internal implementation * so it doesn't get in the way. Instead of starting secondary * CPUs in PSCI powerdown state we will start them all running and * let the boot ROM sort them out. * The usual case is that we do use QEMU's PSCI implementation; * if the guest has EL2 then we will use SMC as the conduit, * and otherwise we will use HVC (for backwards compatibility and * because if we're using KVM then we must use HVC). *\/ if (vms->secure && firmware_loaded) { vms->psci_conduit = QEMU_PSCI_CONDUIT_DISABLED; } else if (vms->virt) { vms->psci_conduit = QEMU_PSCI_CONDUIT_SMC; } else { vms->psci_conduit = QEMU_PSCI_CONDUIT_HVC; } \/* * The maximum number of CPUs depends on the GIC version, or on how * many redistributors we can fit into the memory map (which in turn * depends on whether this is a GICv3 or v4). *\/ if (vms->gic_version == VIRT_GIC_VERSION_2) { virt_max_cpus = GIC_NCPU; } else { virt_max_cpus = virt_redist_capacity(vms, VIRT_GIC_REDIST); if (vms->highmem_redists) { virt_max_cpus += virt_redist_capacity(vms, VIRT_HIGH_GIC_REDIST2); } } if (max_cpus > virt_max_cpus) { error_report(\"Number of SMP CPUs requested (%d) exceeds max CPUs \" \"supported by machine 'mach-virt' (%d)\", max_cpus, virt_max_cpus); if (vms->gic_version != VIRT_GIC_VERSION_2 && !vms->highmem_redists) { error_printf(\"Try 'highmem-redists=on' for more CPUs\\n\"); } exit(1); } if (vms->secure && !tcg_enabled() && !qtest_enabled()) { error_report(\"mach-virt: %s does not support providing \" \"Security extensions (TrustZone) to the guest CPU\", current_accel_name()); exit(1); } if (vms->virt && kvm_enabled() && !kvm_arm_el2_supported()) { error_report(\"mach-virt: host kernel KVM does not support providing \" \"Virtualization extensions to the guest CPU\"); exit(1); } if (vms->virt && !kvm_enabled() && !tcg_enabled() && !qtest_enabled()) { error_report(\"mach-virt: %s does not support providing \" \"Virtualization extensions to the guest CPU\", current_accel_name()); exit(1); } if (vms->mte && hvf_enabled()) { error_report(\"mach-virt: %s does not support providing \" \"MTE to the guest CPU\", current_accel_name()); exit(1); } create_fdt(vms); assert(possible_cpus->len == max_cpus); for (n = 0; n < possible_cpus->len; n++) { Object *cpuobj; CPUState *cs; if (n >= smp_cpus) { break; } cpuobj = object_new(possible_cpus->cpus[n].type); object_property_set_int(cpuobj, \"mp-affinity\", possible_cpus->cpus[n].arch_id, NULL); cs = CPU(cpuobj); cs->cpu_index = n; numa_cpu_pre_plug(&possible_cpus->cpus[cs->cpu_index], DEVICE(cpuobj), &error_fatal); aarch64 &= object_property_get_bool(cpuobj, \"aarch64\", NULL); if (!vms->secure) { object_property_set_bool(cpuobj, \"has_el3\", false, NULL); } if (!vms->virt && object_property_find(cpuobj, \"has_el2\")) { object_property_set_bool(cpuobj, \"has_el2\", false, NULL); } if (vmc->kvm_no_adjvtime && object_property_find(cpuobj, \"kvm-no-adjvtime\")) { object_property_set_bool(cpuobj, \"kvm-no-adjvtime\", true, NULL); } if (vmc->no_kvm_steal_time && object_property_find(cpuobj, \"kvm-steal-time\")) { object_property_set_bool(cpuobj, \"kvm-steal-time\", false, NULL); } if (vmc->no_tcg_lpa2 && object_property_find(cpuobj, \"lpa2\")) { object_property_set_bool(cpuobj, \"lpa2\", false, NULL); } if (object_property_find(cpuobj, \"reset-cbar\")) { object_property_set_int(cpuobj, \"reset-cbar\", vms->memmap[VIRT_CPUPERIPHS].base, &error_abort); } object_property_set_link(cpuobj, \"memory\", OBJECT(sysmem), &error_abort); if (vms->secure) { object_property_set_link(cpuobj, \"secure-memory\", OBJECT(secure_sysmem), &error_abort); } if (vms->mte) { if (tcg_enabled()) { \/* Create the memory region only once, but link to all cpus. *\/ if (!tag_sysmem) { \/* * The property exists only if MemTag is supported. * If it is, we must allocate the ram to back that up. *\/ if (!object_property_find(cpuobj, \"tag-memory\")) { error_report(\"MTE requested, but not supported \" \"by the guest CPU\"); exit(1); } tag_sysmem = g_new(MemoryRegion, 1); memory_region_init(tag_sysmem, OBJECT(machine), \"tag-memory\", UINT64_MAX \/ 32); if (vms->secure) { secure_tag_sysmem = g_new(MemoryRegion, 1); memory_region_init(secure_tag_sysmem, OBJECT(machine), \"secure-tag-memory\", UINT64_MAX \/ 32); \/* As with ram, secure-tag takes precedence over tag. *\/ memory_region_add_subregion_overlap(secure_tag_sysmem, 0, tag_sysmem, -1); } } object_property_set_link(cpuobj, \"tag-memory\", OBJECT(tag_sysmem), &error_abort); if (vms->secure) { object_property_set_link(cpuobj, \"secure-tag-memory\", OBJECT(secure_tag_sysmem), &error_abort); } } else if (kvm_enabled()) { if (!kvm_arm_mte_supported()) { error_report(\"MTE requested, but not supported by KVM\"); exit(1); } kvm_arm_enable_mte(cpuobj, &error_abort); } else { error_report(\"MTE requested, but not supported \"); exit(1); } } qdev_realize(DEVICE(cpuobj), NULL, &error_fatal); object_unref(cpuobj); } \/* Now we've created the CPUs we can see if they have the hypvirt timer *\/ vms->ns_el2_virt_timer_irq = ns_el2_virt_timer_present() && !vmc->no_ns_el2_virt_timer_irq; fdt_add_timer_nodes(vms); fdt_add_cpu_nodes(vms); memory_region_add_subregion(sysmem, vms->memmap[VIRT_MEM].base, machine->ram); ``` This is how I implemented my \"init\" function in my \"custom_dram.c\": ``` void custom_dram_device_init(MachineState *machine) { \/\/ Verify and normalize RAM size uint64_t ram_size = machine->ram_size; if (ram_size < DEFAULT_DRAM_SIZE) { ram_size = DEFAULT_DRAM_SIZE; machine->ram_size = ram_size; } ram_size = QEMU_ALIGN_UP(ram_size, GiB); if (ram_size >= (256 * GiB)) { ram_size = (256 * GiB) - (1 * GiB); machine->ram_size = ram_size; qemu_log(\"RAM size adjusted to 255 GiB\\n\"); } \/\/ Configure memory slots machine->maxram_size = ram_size + (8 * MiB); machine->ram_slots = 1; CustomDRAMDevice *dram = g_new0(CustomDRAMDevice, 1); dram->memory = g_malloc0(ram_size); if (!dram->memory) { qemu_log(\"Failed to allocate DRAM memory\\n\"); g_free(dram); return; } \/\/ Initialize DRAM state dram->active_row = 0; dram->precharge_delay = 50; dram->bit_flip_rate = 10; memset(dram->access_count, 0, sizeof(dram->access_count)); memset(dram->is_row_active, 0, sizeof(dram->is_row_active)); \/\/ Initialize main memory region memory_region_init(&dram->mr, NULL, \"custom-dram-container\", ram_size); \/\/ Initialize IO region for custom handling memory_region_init_io(&dram->io_region, NULL, &custom_dram_ops, dram, \"custom-dram-io\", ram_size); \/\/ Map the IO region into the container memory_region_add_subregion(&dram->mr, 0, &dram->io_region); \/\/ Remove standard RAM if it exists if (machine->ram) { memory_region_del_subregion(get_system_memory(), machine->ram); machine->ram = NULL; } \/\/ Add our custom DRAM to system memory memory_region_add_subregion(get_system_memory(), 0, &dram->mr); machine->ram = &dram->mr; qemu_log(\"[CUSTOM_DRAM] Initialized at 0x0 (Size: %\" PRIu64 \" MB)\\n\", ram_size \/ MiB); } ```",
    "author_id":5641,
    "publication_date":1754311342000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ifquit",
    "author_reputation":19.0,
    "tags":"c, qemu",
    "text_length":9490,
    "title_length":54,
    "num_tags":2
  },
  {
    "id":6142,
    "title":"TypeScript solution for managing filesystem path structure",
    "link":"https:\/\/stackoverflow.com\/questions\/79724797\/typescript-solution-for-managing-filesystem-path-structure",
    "text":"This question is all about a good TypeScript solution for managing all your paths of your filesystem structure. When I came across web-based local applications or website backend pages, to manage the filesystem structure's paths, I would create a Version 1: Nested objects used to categorize paths, doesn't affect extra resolving ``` \/\/ the program's data dir on local machine const appDataDir = \"\/path\/to\/program\/data\" export const paths = { data: { settings: \"settings.json\", translations: \"translations\/\", translationList: \"translations.msgpack\", }, } initPaths(paths) \/\/ concat \"settings.json\" etc. with a base dir path function initPaths(p: typeof paths) { for (const key in p.data) { p.data[key] = resolve(appDataDir, p.data[key]) } } ``` The above is simple, just do ``` paths.data.settings ``` and you get the path. Later on, I discovered that most of the files and directories are nested in a structured form (that's what folders do, right?), so I developed it to join paths in nested objects. Version 2: Represents directory structure with nested objects by their prop name ( ``` paths -> content -> pages ``` ) This is ``` \/\/ example paths that I'm trying to get \/\/ \".\/\" \/\/ \".\/content\" \/\/ \".\/content\/pages\/Hello World!\/attachments\" \/\/ \".\/auth.json\" const self = \"\" export const paths = { self, content: { self, pages: { self, }, }, auth: \"auth.json\", } \/\/ concat the paths of the nested objects recursively... function resolvePath( paths: Record<string, string | object>, base: string = \"\/path\/to\/root\/dir\", ) { for (const key in paths) { const t = typeof paths[key] if (t === \"object\") { let dirPath: string if (key === \"any\") { dirPath = \".\" } else { dirPath = join(base, key) if (!existsSync(dirPath)) { mkdirSync(dirPath) } } resolvePath(paths[key] as Record<string, string | object>, dirPath) } else if (t === \"string\") { paths[key] = join(base, paths[key] as string) } } } resolvePath(paths) ``` Problem starts Consider that I wanted to represent a wildcard or some pattern to constrain some path pieces. How about adding a special name ``` \"any\" ``` , and treat it specially in the ``` resolvePaths ``` ? ``` \/\/ I want paths like \".\/content\/pages\/Category#1\/Article#2\/files\" \/\/ ~~~~~~~~~~~~~~~~~~~~ = \"any\" const self = \"\" export const paths = { self, content: { self, pages: { self, any: { self, attachments: { self }, files: { self }, }, }, }, auth: \"auth.json\", } ``` Man, that's literally \"any\", maybe I should implement a concat function which don't accept path pieces that don't exist. ``` const concatHelper: <T extends PathPieces[]>(...path: T) concatHelper(\"content\") \/\/ Good concatHelper(\"hello\") \/\/ Error! concatHelper(\"auth.json\") \/\/ Good concatHelper(\"content\", \"hi\") \/\/ Error! concatHelper(\"content\", \"pages\") \/\/ Good concatHelper(\"content\", \"pages\", \"Article#1\") \/\/ Good ``` That's the problem, representing that \"any\" is difficult (just look at the A glimpse of what I've tried and you'll know). A static path structure is simple, but it becomes complicated when matching patterns. The complexity shows up especially when writing a concat function that constrains path pieces with types (oh string literal types). So I've tried the version 1, just static paths, and version 2 the nested objects. It's not a function not working, or an algorithm is having bugs. It's about finding a solution to manage paths of a filesystem structure, specifically in this concat function solution, to construct a TypeScript type (That's hard, I tried in Option 2 ). A glimpse of what I've tried Option 1: totally types-based ``` type Dir<Name extends string, Content extends string = \"\"> = `${Name}\/${\"\" | Content}` \/\/ prettier-ignore export type Paths = Dir<\".\", | Dir<\"content\", | Dir<\"pages\", | Dir<string, | \"index.md\" | Dir<\"attachments\"> | Dir<\"files\"> > > > | \"auth.json\" > ``` Option 2: such a mess, it doesn't work at all Do not try to understand if you can't, just take all these Meta Programming for fun ``` import { join } from \"node:path\" \/\/ \"ts-arithmetic\" package is really cool, arithmetic with number types! import type { Add, Lt, Subtract } from \"ts-arithmetic\" type File = string interface Dir { name: string children: Item[] } type Item = Dir | File function dir<T extends string, U extends Item[]>(name: T, ...children: U) { return { name, children, } as const } const wildCard: string = \"*\" const pathss = dir( \"\", dir(\"b\", dir(\"haha\", dir(\"wildCard\", dir(\"attachments\", dir(\"files\")), dir(\"files\")))), dir(\"content\", dir(\"pages\", dir(wildCard, dir(\"attachments\"), dir(\"files\")))), dir(\"a\", dir(\"pagesaa\", dir(wildCard, dir(\"attachments\"), dir(\"files\")))), ) type Paths = typeof pathss type Children<T extends Dir> = T[\"children\"][number] type GetName<T extends Item> = T extends Dir ? T[\"name\"] : T type GetDir<T extends Item> = T extends Dir ? T : never type SubPaths<T extends Dir> = GetName<Children<T>> type SubDirs<T extends Dir> = GetDir<Children<T>> type Indices<T extends unknown[]> = { [K in keyof T]: K }[number] type IndexOf<T extends string, U extends Item[]> = { [K in Indices<U>]: GetName<U[K]> extends T ? K : never }[Indices<U>] type Range<A extends number, B extends number> = Lt<A, B> extends 1 ? A | Range<Add<A, 1>, B> : never type a = Indices<Paths[\"children\"]> type t = Range<1, 10> type s = IndexOf<\"content\", Paths[\"children\"]> type Slices<T extends Dir, Length extends number> = Length extends 0 ? [] : [SubDirs<T>[\"name\"], ...Slices<SubDirs<T>, Subtract<Length, 1>>] \/\/ I gotta try another way... const test: Slices<Paths, 3> = [\"b\", \"pages\", \"asdfasdfasdfasdf\"] type As<T, U> = T extends U ? T : never type Slices2<T extends Dir, Length extends number> = { length: Length } & { [K in Range<0, Length>]: K extends 0 ? SubDirs<T>[\"name\"] : Slices2<T, Length>[As<Subtract<K, 1>, Range<0, Length>>] } \/\/ Why is this not erroring?! const test2: Slices<Paths, 3> = [\"content\", \"haha\", \"b\"] function joinHelper< U extends [...string[]] & Slices2<Base, Length>, Length extends number = U[\"length\"], Base extends Dir = Paths, >(base: Base, [slice, ...slices]: [...U]): string { return join(slice, joinHelper(base, slices)) } ```",
    "author_id":5640,
    "publication_date":1754311390000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"CoderJo",
    "author_reputation":9.0,
    "tags":"typescript, javascript, typescript-generics, path, filesystems",
    "text_length":6086,
    "title_length":58,
    "num_tags":5
  },
  {
    "id":6141,
    "title":"Using P5 createRadio() in a class and having two objects of that class leads to radio buttons not functioning independently",
    "link":"https:\/\/stackoverflow.com\/questions\/79724803\/using-p5-createradio-in-a-class-and-having-two-objects-of-that-class-leads-to",
    "text":"When I use the ``` createRadio() ``` function from the p5js library in a function constructor and create multiple objects of that class, the radio buttons act differently, when I select an option in a radio button of a class it affects both objects. How can i fix this? ``` let myClass1 let myClass2 function myClass(x, y) { this.myRadio = createRadio(\"theme\") this.myRadio.size(50) this.myRadio.position(x, y) this.myRadio.option(\"red\") this.myRadio.option(\"blue\") this.myRadio.selected(\"blue\") } function setup() { createCanvas(800, 800) myClass1 = new myClass(100, 100) myClass2 = new myClass(100, 300) } function draw() { fill(myClass1.myRadio.value()) ellipse(500, 100, 20) fill(myClass1.myRadio.value()) ellipse(500, 300, 20) } ``` ``` <script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/p5.js\/1.11.0\/p5.js\"><\/script> ```",
    "author_id":5639,
    "publication_date":1754311473000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"hamed rahafrouz",
    "author_reputation":21.0,
    "tags":"javascript, dom, p5.js",
    "text_length":827,
    "title_length":123,
    "num_tags":3
  },
  {
    "id":6140,
    "title":"Getting the Error with UPS in the magento2.4.1",
    "link":"https:\/\/stackoverflow.com\/questions\/79724807\/getting-the-error-with-ups-in-the-magento2-4-1",
    "text":"I am using Magento 2.4.1 with the UPS shipping method. Shipping methods load correctly on the cart page, but when I select a method, the shipping rate recalculates to $0 in the subtotal. Then, when I proceed to the checkout page, no shipping methods are available, or I get an error like: \"No shipping method with given code: 02\" We also contacted UPS support, and their response was: \"Have you reached out to Magento? We don't control or activate the necessary fields to apply the OAuth credentials. If the fields are not displaying options for the OAuth credentials and information, you would need to reach out to Magento.\" We have tested the same setup on Magento 2.4.7, but the issue still persists. Can anyone please help with this?",
    "author_id":5638,
    "publication_date":1754311651000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vikram",
    "author_reputation":11.0,
    "tags":"oauth-2.0, magento2, ups, license-key",
    "text_length":737,
    "title_length":46,
    "num_tags":4
  },
  {
    "id":6139,
    "title":"How to use the &quot;is_in&quot; function correctly?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724809\/how-to-use-the-is-in-function-correctly",
    "text":"In Polars 0.46.0 it works normally: ``` let df = df!( \"id\" => [0, 1, 2, 3, 4], \"col_1\" => [1, 2, 3, 4, 5], \"col_2\" => [3, 4, 5, 6, 7], ) .unwrap(); dbg!(&df); let s = df.column(\"col_2\").unwrap().as_materialized_series(); let combo = df .clone() .lazy() .filter(col(\"id\").is_in(lit(s.clone()), false)) .collect() .unwrap(); dbg!(&combo); ``` The same code in Polars 0.50.0 is deprecated: Deprecation: ``` is_in ``` with a collection of the same datatype is ambiguous and deprecated. Please use ``` implode ``` to return to previous behavior. See https:\/\/github.com\/pola-rs\/polars\/issues\/22149 for more information. How should I write it in Polars 0.50.0, to not get a deprecation warning?",
    "author_id":5637,
    "publication_date":1754311784000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Alex Avin",
    "author_reputation":43.0,
    "tags":"rust, polars, rust-polars",
    "text_length":687,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":6138,
    "title":"Remove `firstName` and `lastName` fields from Keycloak user profile using Terraform",
    "link":"https:\/\/stackoverflow.com\/questions\/79724814\/remove-firstname-and-lastname-fields-from-keycloak-user-profile-using-terraf",
    "text":"I am using the latest version of Keycloak (26) and the official Keycloak Terraform provider. I am attempting to remove the (default) fields ``` firstName ``` and ``` lastName ``` from the user profile. In the admin interface, one can achieve this by going to Realm settings -> User profile and delete the two fields from the list. However, I would like to do this non-interactively using the Terraform provider. Does anyone know if it is possible? And if so, how to achieve it? The question was clear as asked: Is there any way to use Terraform to delete the default fields? The answer I have since learned is unfortunately, no. This is of course a huge feature gap which makes managing a Keycloak instance using Code as Configuration nigh impossible. I am currently writing a guide with manual steps to perform after running Terraform to fill that gap. Anyone with deep knowledge of the Keycloak Terraform provider could have known this and answered it. But without that deep knowledge it is indeed a difficult question to answer. It took me two days to find the answer. That does not however make the question unclear...",
    "author_id":5636,
    "publication_date":1754311920000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Stijn de Witt",
    "author_reputation":42440.0,
    "tags":"terraform, keycloak",
    "text_length":1122,
    "title_length":83,
    "num_tags":2
  },
  {
    "id":6137,
    "title":"Angular template conditional rendering and signal inputs",
    "link":"https:\/\/stackoverflow.com\/questions\/79724816\/angular-template-conditional-rendering-and-signal-inputs",
    "text":"I have this input: ``` customHTMLContent = input<string>() ``` In my HTML template calling this works: ``` <p>{{ customHTMLContent }}<\/p> ``` And this doesn't: ``` <p>{{ customHTMLContent() }}<\/p> ``` Also when using ``` @if ``` this is always truthy (I know it's because the signal itself exists): ``` @if (customHTMLContent) { <div [innerHTML]=\"customHTMLContent\"><\/div> } ``` But when trying to access it like this it returns that ``` customHTMLContent ``` is not a function: ``` @if (customHTMLContent()) { <div [innerHTML]=\"customHTMLContent\"><\/div> } ``` So what am I getting wrong with how signal inputs should be called in the template? And how do I check if ``` customHTMLContent ``` exists?",
    "author_id":5635,
    "publication_date":1754311970000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"AirKlisa",
    "author_reputation":153.0,
    "tags":"typescript, html, angular, angular-signals, angular-binding",
    "text_length":700,
    "title_length":56,
    "num_tags":5
  },
  {
    "id":6136,
    "title":"Rendering Video Frames on a Custom Surface Instead of SurfaceViewRenderer",
    "link":"https:\/\/stackoverflow.com\/questions\/79724817\/rendering-video-frames-on-a-custom-surface-instead-of-surfaceviewrenderer",
    "text":"I am trying to render a WebRTC video on a ``` Surface ``` instead of using ``` SurfaceViewRenderer ``` . On some devices this works fine, but on others, it shows a black screen. If I use ``` SurfaceViewRenderer ``` , it works properly on all devices. All frames are being received. Logs: ``` EglRenderer: SurfaceVideoSinkDuration: 4001 ms. Frames received: 25. Dropped: 0. Rendered: 25. Render fps: 6.2. Average render time: 418 us. Average swapBuffer time: 7856 us. ``` ``` MainActivity.kt ``` : ``` @Preview(showBackground = true, showSystemUi = true) @Composable fun MainActivityPreview(roomId: String = \"10086\") { val context = LocalContext.current val host = \"http:\/\/192.168.2.112:4000\" var mSurfaceView by remember { mutableStateOf<Surface?>(null) } Column { Text(\"Remote Preview\") var surfaceView by remember { mutableStateOf<SurfaceView?>(null) } AndroidView( factory = { context -> surfaceView = SurfaceView(context) val holder = surfaceView!!.holder holder.addCallback(object : SurfaceHolder.Callback { override fun surfaceCreated(holder: SurfaceHolder) { ThreadUtils.checkIsOnMainThread() mSurfaceView = holder.surface } override fun surfaceChanged(holder: SurfaceHolder, format: Int, width: Int, height: Int) {} override fun surfaceDestroyed(holder: SurfaceHolder) {} }) surfaceView!! }, modifier = Modifier.weight(1f) ) Button(onClick = { GlobalConstants.renderSurface = mSurfaceView GlobalConstants.requireWebRTC(this@MainActivity) }) { Text(\"Connect\") } Button(onClick = { GlobalConstants.disconnectWebRTC() }) { Text(\"Disconnect\") } } } ``` ``` GlobalConstants.kt ``` : ``` var renderSurface: Surface? = null var renderSink: VideoSink? = null val eglBase = EglBase.create() fun disconnectWebRTC() { Log.d(\"PeerConnectionClient\", \"Require to disconnect from remote\") peerConnectionClient?.let { it.onDestroy() Log.d(\"PeerConnectionClient\", \"WebRTC disconnected\") peerConnectionClient = null } } fun requireWebRTC(applicationContext: Context) { disconnectWebRTC() peerConnectionClient = PeerConnectionClient( applicationContext, \"10086\", SimpleRtcListener( fxOnAddRemoteStream = { val mainHandler = Handler(Looper.getMainLooper()) mainHandler.post { renderSink?.let { sink -> it.videoTracks.first().addSink(sink) } if (renderSurface != null) { Log.d(\"PeerConnectionClient\", \"Target Surface: $renderSurface\") it.videoTracks.first().addSink( SurfaceVideoSink( renderSurface!!, eglBase.eglBaseContext ) ) } else { Log.w(\"PeerConnectionClient\", \"Target Surface not found\") } } } ), \"http:\/\/192.168.2.112:4000\", EglBase.create() ) } ``` ``` SurfaceVideoSink.kt ``` : ``` class SurfaceVideoSink( surface: Surface, eglBaseContext: EglBase.Context ) : VideoSink { private val eglRenderer: EglRenderer = EglRenderer(\"SurfaceVideoSink\") init { Handler(Looper.getMainLooper()).post { ThreadUtils.checkIsOnMainThread() eglRenderer.init( eglBaseContext, EglBase.CONFIG_PLAIN, GlRectDrawer() ) eglRenderer.createEglSurface(surface) } eglRenderer.setMirror(false) eglRenderer.disableFpsReduction() } override fun onFrame(frame: VideoFrame) { try { eglRenderer.onFrame(frame) } catch (e: Exception) { Log.e(\"SurfaceVideoSink\", \"Render error\", e) } } } ``` Why does rendering to a custom ``` Surface ``` work only on some devices, while ``` SurfaceViewRenderer ``` works everywhere? Is there something wrong in my EGL surface creation or rendering lifecycle?",
    "author_id":5634,
    "publication_date":1754312046000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Andelic47 Aaron",
    "author_reputation":9.0,
    "tags":"android, webrtc, egl, webrtc-android",
    "text_length":3355,
    "title_length":73,
    "num_tags":4
  },
  {
    "id":6135,
    "title":"Django does not automatically create test database",
    "link":"https:\/\/stackoverflow.com\/questions\/79724820\/django-does-not-automatically-create-test-database",
    "text":"I have a ``` Django ``` project, version - ``` 3.2.25 ``` . The problem I'm facing is that I'm unable to create a test database, when I'm executing my tests using ``` VScode ``` test runner. If matters here are the ``` pytest ``` versions: ``` name : pytest version : 8.2.2 description : pytest: simple powerful testing with Python required by - pytest-django requires >=5.4.0 - pytest-dotenv requires >=5.0.0 ``` Basically, I have a test case, which uses my local database, I want to start using a virtual database, which is dynamically created when executing a test, instead of a local one. To do that, I've tried to set a ``` Test ``` property inside my ``` settings.py ``` database configuration: ``` DATABASES = { 'default': { \"NAME\": \"localDatabase\", \"ENGINE\": \"django.contrib.gis.db.backends.postgis\", \"USER\": \"test\", \"PASSWORD\": \"test\", \"HOST\": \"127.0.0.1\", \"PORT\": \"5432\", \"TEST\": { \"NAME\": \"test_local\" }, }, } ``` So inside my test case: ``` class BillAPITests(APITestCase): def test_create_bill(self): from django.db import connection print(connection.settings_dict[\"NAME\"] response = self.client.post(self.base_url, self.base_payload, format=\"json\") self.assertEqual(response.status_code, status.HTTP_405_METHOD_NOT_ALLOWED) ``` the print gives me: localDatabase instead of \"test_local\". As second attempt, I have tried settings up a ``` pytest.fixture ``` inside my ``` conftest.py ``` : ``` @pytest.fixture(scope=\"session\") def django_db_setup(): load_dotenv() settings.DATABASES[\"default\"] = { \"NAME\": \"test_local\", \"ENGINE\": \"django.contrib.gis.db.backends.postgis\", \"USER\": \"test\", \"PASSWORD\": \"test\", \"HOST\": \"127.0.0.1\", \"PORT\": \"5432\", } Still the same output... I have also tried closing all connections in the `django_db_setup` before swapping my default databases using: ```py from django import db db.connections.close_all() ``` No result... The only thing that gave me different behaviour is settings the database name directly in the ``` django_db_setup ``` , like so: ``` settings.DATABASES[\"default\"][\"NAME\"] = \"test_local\" ``` However, that way django does not automatically create me \"test_local\" database and it fails connecting to it. Any help appreciated, thanks",
    "author_id":5633,
    "publication_date":1754312226000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Geo",
    "author_reputation":306.0,
    "tags":"python, django, pytest, postgis, pytest-django",
    "text_length":2196,
    "title_length":50,
    "num_tags":5
  },
  {
    "id":6134,
    "title":"Tensor Flow FULLY CONNECTED and android app",
    "link":"https:\/\/stackoverflow.com\/questions\/79724822\/tensor-flow-fully-connected-and-android-app",
    "text":"I am having a very persistent issue. I am trying to use a TF model to categorise some audio that I would like to have in an Android app. I can train the model and create the tflite file in Colab and then download it and import into Android Studio. However a persistent fully connected error means that the model will not run in the emulator. Has anyone encoutered anything like this and know of a workaround? Very frustrating.",
    "author_id":5632,
    "publication_date":1754312253000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vincent Ryan",
    "author_reputation":113.0,
    "tags":"android, android-studio, tensorflow, google-colaboratory",
    "text_length":426,
    "title_length":43,
    "num_tags":4
  },
  {
    "id":6133,
    "title":"&quot;Timeout was reached&quot;, &quot;Operation too slow&quot; with `httr2` even after setting high `httr2::req_timeout()`",
    "link":"https:\/\/stackoverflow.com\/questions\/79724824\/timeout-was-reached-operation-too-slow-with-httr2-even-after-setting-high",
    "text":"I'm trying to perform a local request with ``` httr2 ``` . I don't seem to be able to keep the request waiting for more than 10 minutes. I appreciate this is a very long time, but in my case, it is occasionally expected. Even if I set a very high timeout of many hours with: ``` req |> httr2::req_timeout(seconds = 333333) |> httr2::req_perform() ``` I still get the following error after 10 minutes: ``` Error in `httr2::req_perform()`: ! Failed to perform HTTP request. Caused by error in `curl::curl_fetch_memory()`: ! Timeout was reached [localhost]: Operation too slow. Less than 1 bytes\/sec transferred the last 600 seconds Run `rlang::last_trace()` to see where the error occurred. ``` If I print the request object, I see it records the lengthy timeout: ``` Options: * timeout_ms : 333333000 * connecttimeout: 0 ``` This is with the most recent version available on CRAN for key packages, in particular ``` httr2 ``` version 1.2.1, and ``` rcurl ``` version 1.98-1.17 (latest R 4.5.1 on Fedora Linux). Any hint for increasing this timeout above 600 seconds or further troubleshooting is warmly welcome.",
    "author_id":5631,
    "publication_date":1754312505000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"giocomai",
    "author_reputation":3617.0,
    "tags":"r, connection-timeout, rcurl, httr2",
    "text_length":1110,
    "title_length":123,
    "num_tags":4
  },
  {
    "id":6132,
    "title":"Some .so files not supported 16 kb",
    "link":"https:\/\/stackoverflow.com\/questions\/79724825\/some-so-files-not-supported-16-kb",
    "text":"Below libraries not supported 16 kb App does not support 16 KB ``` Libraries that do not support 16 KB: base\/lib\/arm64-v8a\/libmodft2.so base\/lib\/arm64-v8a\/libmodpdfium.so base\/lib\/arm64-v8a\/libmodpng.so base\/lib\/arm64-v8a\/libyuv_to_rgb_jni.so base\/lib\/x86_64\/libc++_shared.so base\/lib\/x86_64\/libjniPdfium.so base\/lib\/x86_64\/libmodft2.so base\/lib\/x86_64\/libmodpdfium.so base\/lib\/x86_64\/libmodpng.so base\/lib\/x86_64\/libopentok.so base\/lib\/x86_64\/libyuv_to_rgb_jni.so ``` I have updated my project as android 16 and found above warning. How could I solve above warning issue?",
    "author_id":5630,
    "publication_date":1754312614000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vishal",
    "author_reputation":31.0,
    "tags":"android, android-jetpack-compose, kotlin, android-16",
    "text_length":572,
    "title_length":34,
    "num_tags":4
  },
  {
    "id":6131,
    "title":"How can I dynamically forward custom tracking IDs (GA4, Meta, TikTok) per event organizer using GTM Server Container?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724826\/how-can-i-dynamically-forward-custom-tracking-ids-ga4-meta-tiktok-per-event",
    "text":"I'm building Next.js (Node.js) web application where different users (e.g. clients or account owners) can configure their own tracking pixels — such as Google Analytics 4, Meta Pixel, or TikTok Pixel — to track activity related to their content. Each piece of content (e.g. a page, product, or campaign) can be associated with one or more tracking IDs I want to implement server-side tracking using a Google Tag Manager Server container, and dynamically forward events (like page_view, purchase, etc.) to the appropriate tracking platforms based on these IDs — without manually creating separate tags for each user. I’ve already set up a GTM Server container to handle server-side tracking, and it works correctly for my platform’s own tracking IDs. Each user must receive tracking data only for their own content, not others. So the payload or request should be correctly filtered, probably using the page route (i guess?) Any ideas how can i do it via tag manager server container, without need to do in my backend? The server container receives incoming events successfully. Tags are currently set up only for static (platform-owned) tracking IDs — they’re hardcoded. I want to move to a dynamic setup where the container reads the incoming tracking ID(s) from the event payload and dispatches accordingly.",
    "author_id":5629,
    "publication_date":1754312634000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"niceguyeddie01",
    "author_reputation":9.0,
    "tags":"next.js, google-analytics-4, google-tag-manager, facebook-pixel",
    "text_length":1309,
    "title_length":117,
    "num_tags":4
  },
  {
    "id":6130,
    "title":"React Toast Notification not closing on &quot;✕&quot; button click",
    "link":"https:\/\/stackoverflow.com\/questions\/79724827\/react-toast-notification-not-closing-on-button-click",
    "text":"I'm building a toast notification system in React that appears when a custom event is dispatched. The toast is supposed to: Show a message when triggered Auto-dismiss after 3 seconds (this works) Dismiss immediately when the user clicks the ✕ button (this does not work — the toast stays visible until the timeout finishes) I’ve confirmed that setShowToast(false) is called in the onClick handler, and I've tried logging and clearing the timeout as well. Still, the toast doesn't disappear until the timeout ends. ``` \/\/ App.jsx import React, { useState, useEffect, useRef } from 'react'; function App() { const [showToast, setShowToast] = useState(false); const [toastMessage, setToastMessage] = useState(''); const toastTimeoutRef = useRef(null); useEffect(() => { const handleShowToast = (event) => { setToastMessage(event.detail); setShowToast(true); \/\/ Clear existing timeout if (toastTimeoutRef.current) { clearTimeout(toastTimeoutRef.current); } \/\/ Auto-dismiss after 3 seconds toastTimeoutRef.current = setTimeout(() => { setShowToast(false); toastTimeoutRef.current = null; }, 3000); }; window.addEventListener('show-toast', handleShowToast); return () => { window.removeEventListener('show-toast', handleShowToast); if (toastTimeoutRef.current) { clearTimeout(toastTimeoutRef.current); } }; }, []); const triggerToast = () => { window.dispatchEvent( new CustomEvent('show-toast', { detail: 'This is a toast message!', }) ); }; const hideToast = () => { if (toastTimeoutRef.current) { clearTimeout(toastTimeoutRef.current); toastTimeoutRef.current = null; } setShowToast(false); }; return ( <div style={{ padding: '2rem' }}> <button onClick={triggerToast}>Trigger Toast<\/button> {showToast && ( <div style={{ position: 'fixed', bottom: '1rem', left: '1rem', background: '#E6F8F1', color: '#1C4731', padding: '1rem', borderRadius: '6px', display: 'flex', alignItems: 'center', gap: '1rem', boxShadow: '0 2px 6px rgba(0, 0, 0, 0.2)', }} > <span>{toastMessage}<\/span> <button onClick={hideToast} style={{ background: 'transparent', border: 'none', fontWeight: 'bold', fontSize: '1rem', cursor: 'pointer', color: '#1C4731', }} aria-label=\"Close toast\" > ✕ <\/button> <\/div> )} <\/div> ); } export default App; ``` What I've Tried Clearing the timeout manually in the hideToast function Logging to verify hideToast runs on click (it does) Confirmed setShowToast(false) is executed Clicking the ✕ should hide the toast immediately, regardless of the timeout. Currently, it only disappears after the 3-second timeout finishes. Any ideas why the toast won’t close immediately on click?",
    "author_id":5628,
    "publication_date":1754312706000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ikram Wani",
    "author_reputation":1.0,
    "tags":"reactjs, javascript, toast",
    "text_length":2584,
    "title_length":66,
    "num_tags":3
  },
  {
    "id":6129,
    "title":"Compile-time error referencing project in multiple solutions",
    "link":"https:\/\/stackoverflow.com\/questions\/79724832\/compile-time-error-referencing-project-in-multiple-solutions",
    "text":"I have a solution \"A\" with a project \"PA\", and a solution \"B\", where I added \"PA\". In solution \"B\" I have added a new project \"PB\". \"B\" |-> \"PA\" |-> \"PB\" \"PB\" references the dll of \"PA\", but it does not recognize namespaces or classes. (Same happens with project reference) I get the compile-time error saying a namespace couldn't be found, yet the build succeeds. I tried creating a new project in a new solution and I can reference the dll of \"PA\" without a problem. It appears it only happens when both projects coexist in the same solution . Couldn't find this problem anywhere. I'm using .NET 8 and VS 2022 17.14.10 Can anyone explain the problem?",
    "author_id":5578,
    "publication_date":1754313004000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Daniel Patr&#237;cio",
    "author_reputation":11.0,
    "tags":".net-8.0",
    "text_length":652,
    "title_length":60,
    "num_tags":1
  },
  {
    "id":6128,
    "title":"Interstitial Ad getting closed unexpectedly with gesture navigation (AdMob, Android)",
    "link":"https:\/\/stackoverflow.com\/questions\/79724841\/interstitial-ad-getting-closed-unexpectedly-with-gesture-navigation-admob-andr",
    "text":"I’m facing an issue with AdMob app open and interstitial ads on Android when using gesture navigation with Predictive Back (Android 13+). When an ad is displayed using a real AdMob ad unit ID, users can swipe back with the gesture before the close button appears, and the ad is dismissed prematurely. This doesn’t happen with traditional 3-button navigation, where the ad stays on screen until the close button shows up as expected. As a workaround, I found that removing ``` android:enableOnBackInvokedCallback=\"true\" ``` from the manifest prevents the ad from being dismissed by gestures, but this disables Predictive Back entirely, which I’d like to avoid since it’s a key part of the modern Android UX. Ideally, the ad should not be dismissible via gestures until the close button is visible, while still keeping Predictive Back enabled for the rest of the app. Has anyone else experienced this or found a way to prevent AdMob ads from closing early with Predictive Back enabled?",
    "author_id":5627,
    "publication_date":1754313470000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Nithin.P Molethu",
    "author_reputation":109.0,
    "tags":"android, admob, interstitial",
    "text_length":983,
    "title_length":84,
    "num_tags":3
  },
  {
    "id":6127,
    "title":"webdatarocks custom element hides then pivot on fullscreen mode",
    "link":"https:\/\/stackoverflow.com\/questions\/79724843\/webdatarocks-custom-element-hides-then-pivot-on-fullscreen-mode",
    "text":"I use nuxt 3, and webdatarocks. For toolbar i created new btn that calls \"dialog\" with select and options. ``` function customizePivotToolbar(toolbar) { const tabs = toolbar.getTabs(); tabs.unshift({ id: \"custom-select\", title: \"Select View\", icon: toolbar.icons.options, handler: () => { console.log(\"Custom select clicked\"); const items = [ { id: 1, name: \"name1\" }, { id: 2, name: \"name2\" }, { id: 3, name: \"name3\" }, ]; \/\/ Create <select> element const select = document.createElement(\"select\"); select.style.padding = \"8px 12px\"; select.style.fontSize = \"14px\"; select.style.marginBottom = \"12px\"; select.style.width = \"100%\"; \/\/ Add default option const defaultOption = document.createElement(\"option\"); defaultOption.text = \"Select an item\"; defaultOption.disabled = true; defaultOption.selected = true; select.appendChild(defaultOption); \/\/ Add options items.forEach((item) => { const option = document.createElement(\"option\"); option.value = item.id; option.textContent = item.name; select.appendChild(option); }); \/\/ Handle selection select.addEventListener(\"change\", () => { const selected = items.find((i) => i.id == select.value); console.log(\"Selected item:\", selected); \/\/ You can trigger WebDataRocks logic here }); \/\/ Create modal const modal = document.createElement(\"div\"); modal.style.position = \"fixed\"; modal.style.top = \"50%\"; modal.style.left = \"50%\"; modal.style.transform = \"translate(-50%, -50%)\"; modal.style.backgroundColor = \"white\"; modal.style.padding = \"15px\"; modal.style.zIndex = \"10000\"; modal.style.boxShadow = \"0 0 10px rgba(0, 0, 0, 0.5)\"; modal.appendChild(select); \/\/ Close button const closeButton = document.createElement(\"button\"); closeButton.textContent = \"Close\"; closeButton.style.marginTop = \"12px\"; closeButton.onclick = () => { document.body.removeChild(modal); }; modal.appendChild(closeButton); document.body.appendChild(modal); }, }); toolbar.getTabs = () => tabs; return toolbar; } ``` Then start component we see new custome btn. After press it we see in console msg \"Custom select clicked\". And opens \"dialog\" with selects. Start component open in fullscreen mode using \"fullscreen\" btn from toolbar. After press custom btn we see in console msg \"Custom select clicked\". \"Dialog\" opened, but under pivot. Then we close fullscreen mode, we see opened \"dialog\" I dont understand how to fix it. Mb it's css z-posiziton problem...",
    "author_id":5626,
    "publication_date":1754313662000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Julian Sinicyn",
    "author_reputation":1.0,
    "tags":"nuxt3.js, webdatarocks",
    "text_length":2383,
    "title_length":63,
    "num_tags":2
  },
  {
    "id":6126,
    "title":"Convert Decimal values to float64 when creating a Pandas DataFrame",
    "link":"https:\/\/stackoverflow.com\/questions\/79724845\/convert-decimal-values-to-float64-when-creating-a-pandas-dataframe",
    "text":"I'm working with a dictionary that contains a list of ``` decimal.Decimal ``` values as one of its fields: ``` import pandas as pd from decimal import Decimal data = { 'Item': ['Apple', 'Banana', 'Orange'], 'Price': [Decimal('1.25'), Decimal('0.75'), Decimal('2.00')], 'Quantity': [10, 20, 15] } ``` When I convert this dictionary into a Pandas DataFrame, the Price column (which contains Decimal objects) is inferred as object: ``` df = pd.DataFrame(data) print(df.dtypes) # Output: # Item object # Price object # Quantity int64 # dtype: object ``` I would like the Price column to be of type float64 instead. I tried using ``` pd.DataFrame.from_records(data, coerce_float=True) ``` , but it didn’t change the type of the Decimal values. I’m aware I can convert the column using .astype(float), but in my actual use case, I may not know the column name in advance. What’s the best way to ensure that all Decimal values are automatically converted to floats when creating the DataFrame, ideally without hardcoding column names?",
    "author_id":5625,
    "publication_date":1754313793000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Gino",
    "author_reputation":871.0,
    "tags":"python, dataframe, pandas, type-conversion",
    "text_length":1027,
    "title_length":66,
    "num_tags":4
  },
  {
    "id":6125,
    "title":"BitmapFactory.decodeStream returns null when printing image from Android PrintService",
    "link":"https:\/\/stackoverflow.com\/questions\/79724846\/bitmapfactory-decodestream-returns-null-when-printing-image-from-android-printse",
    "text":"I'm developing a custom PrintService for an Android app that connects to a Bluetooth thermal printer. My service successfully handles PDF documents by converting each page to a bitmap and printing it. However, I'm running into an issue when trying to print image files (PrintDocumentInfo.CONTENT_TYPE_PHOTO). The core of the problem seems to be that BitmapFactory.decodeStream() returns null , indicating that it's unable to decode the image data. I'm taking the ParcelFileDescriptor from the PrintJob , writing its contents to a temporary file, and then trying to decode that file. What I'm trying to do: A user selects an image to print. My MyPrintService receives the print job. In onPrintJobQueued , I call a function handleImageDocument to process the job. Inside handleImageDocument, I copy the data from the ParcelFileDescriptor into a temporary file on the device's cache directory. I then attempt to decode this temporary file into a Bitmap using BitmapFactory.decodeStream(). The goal is to resize and print this bitmap to the connected Bluetooth printer. The Problem: The line val bitmap = BitmapFactory.decodeStream(inputStream) consistently returns null . This happens even though the temporary file is created and has a non-zero size, suggesting that the data is being copied. Crucially, I have inspected the temporary file in the device's cache folder, and it appears to be corrupted . When I try to open it with an image viewer, it fails to display the original image. The file size is correct, but the data seems to be invalid or incomplete. My Code Snippet: ``` \/\/ ... (imports and class definition) override fun onPrintJobQueued(printJob: PrintJob) { CoroutineScope(Dispatchers.Main).launch { printJob.start() } val fileDescriptor = printJob.document.getData() val contentType = printJob.document.info.contentType if (fileDescriptor == null) { CoroutineScope(Dispatchers.Main).launch { printJob.fail(\"No document data found.\") } return } CoroutineScope(Dispatchers.IO).launch { val outputStream = connectToPrinter() if (outputStream == null) { withContext(Dispatchers.Main) { printJob.fail(\"Could not connect to printer.\") } appLogWrite(this@MyPrintService, \"Could not connect to printer.\") return@launch } try { when (contentType) { PrintDocumentInfo.CONTENT_TYPE_DOCUMENT -> handlePdfDocument( fileDescriptor, outputStream, printJob ) PrintDocumentInfo.CONTENT_TYPE_PHOTO -> handleImageDocument( fileDescriptor, outputStream, printJob ) else -> withContext(Dispatchers.Main) { printJob.fail(\"Unsupported document type.\") } } } finally { closeBluetoothConnection() } } } private suspend fun handleImageDocument( fileDescriptor: ParcelFileDescriptor, outputStream: OutputStream, printJob: PrintJob ) { var tempFile: File? = null var inputStream: FileInputStream? = null try { \/\/ Step 1: Copy to a temp file tempFile = File.createTempFile(\"temp_image\", \".png\", cacheDir) ParcelFileDescriptor.AutoCloseInputStream(fileDescriptor).use { input -> FileOutputStream(tempFile).use { output -> input.copyTo(output) } } \/\/ Step 2: Decode the bitmap from the temp file \/\/ The issue is here: this line returns null inputStream = withContext(Dispatchers.IO) { FileInputStream(tempFile) } val bitmap = BitmapFactory.decodeStream(inputStream) if (bitmap == null) { withContext(Dispatchers.Main) { printJob.fail(\"Failed to decode image from the temporary file.\") } \/\/ Log message confirms this path is being taken appLogWrite(this, \"Failed to decode image. Bitmap is null.\") return } \/\/ ... (rest of the printing logic, which is not reached) } catch (e: Exception) { \/\/ ... (error handling) } finally { \/\/ Cleanup try { inputStream?.close() \/\/ tempFile?.delete() } catch (e: Exception) { appLogWrite(this, \"Failed to cleanup temp image file: ${e.message}\") } } } ```",
    "author_id":5624,
    "publication_date":1754313833000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mihir Sharma",
    "author_reputation":71.0,
    "tags":"android, kotlin, printing",
    "text_length":3770,
    "title_length":85,
    "num_tags":3
  },
  {
    "id":6124,
    "title":"Athena is appending UTC to an iceberg timestamp results, why? how to fix it?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724849\/athena-is-appending-utc-to-an-iceberg-timestamp-results-why-how-to-fix-it",
    "text":"I am storing a simple datetime value ``` (e.g 2025-01-24 13:58:14.000) ``` from SQL to an iceberg table using glue catelog. I don't want anything with timezones. We only work in EST so all our datetimes don't carry timezone information. Let's say the table name is - iceberg_timestamp When queried above table from athena - It appends UTC in the results to each record. i.e. ``` (2025-01-24 13:58:14.000000 UTC) ``` Now, if I simply create an external table in athena(without iceberg). e.g. athena_timestamp . and dump the same value for datetime. It gives me expected results i.e. ``` 2025-01-24 13:58:14.000 ``` On further analysis I found that athena treats its own timestamp as naive and iceberg as always with timezone. Thus, it explicitly call it out as UTC in results. This is due to iceberg connectors present in athena. What I want to know is that is there any way I can get the same datetime I am storing in the output without appending UTC. I want the fix on server side. i.e. I don't want to go through approaches like cast, views etc. Apart from these options, is there any way? Below things I already tried but it did not work - I added ``` .config(\"spark.sql.session.timeZone\", \"UTC\") \/\/ treat EST as literal .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"false\") ``` in the spark code so that my datetimes will get stored as local\/naive i.e. without timezone. But it didn't work out.",
    "author_id":5623,
    "publication_date":1754314093000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Chaitanya Kulkarni",
    "author_reputation":23.0,
    "tags":"apache-spark, apache-iceberg, aws-glue, amazon-athena",
    "text_length":1416,
    "title_length":76,
    "num_tags":4
  },
  {
    "id":6123,
    "title":"Failing to start SW-Emulation at Vitis due to TCF agent not working",
    "link":"https:\/\/stackoverflow.com\/questions\/79724853\/failing-to-start-sw-emulation-at-vitis-due-to-tcf-agent-not-working",
    "text":"I face a weird error during executing SW-Emulation of my project. I'm trying to run an entry-level HLS project for vector addition. After inputting the C++ files necessary and building entire project (seemingly with no warnings) I'm trying to run project's SW emulation (main_project -> Run As -> Launch SW Emulation) (I also can provide C++ files used for defining kernel and host cores if necessary) Then I face a progress bar saying \"waiting for the TCF agent to start\" which never ends. I also see QEMU Process emulation console with following output: Once I create an Application project, add kernel and host files (can provide those if necessary) I've tried to investigate the issue by myself, however didn't succeed yet. I'm not entirely sure what this TCF agent is used for and on which side it is missing (desktop Linux or PetaLinux I use for board definition). It might be related to a version incompatibility between Vitis and PetaLinux(?). Would appreciate any suggestions. My setup: * Ubuntu 22.04.5 LTS * Xilinx Vitis IDE v2022.1.0 (64-bit) * Ultra96V2 platform definition files: https:\/\/avnet.me\/ZedSupport -> 2022.1\/Vitis_Platform\/u96v2_sbc_base.tar.gz QEMU Process emulation console log: ``` Current working dir \/home\/call_me_utka\/Documents\/Projects\/aes-ultra96-v2-playground\/hls_vector_addition\/vector_addition_application_system\/Emulation-SW\/package Required emulation files like qemu_args exists qemu-system-aarch64: -chardev socket,path=.\/qemu-rport-_pmu@0,server=on,id=pmu-apu-rp: info: QEMU waiting for connection on: disconnected:unix:.\/qemu-rport-_pmu@0,server=on qemu-system-aarch64: -chardev socket,id=pl-rp,host=127.0.0.1,port=7045,server=on: info: QEMU waiting for connection on: disconnected:tcp:127.0.0.1:7045,server=on qemu-system-aarch64: warning: hub 0 is not connected to host network CRITICAL_WARNING: [LAUNCH_EMULATOR] DEPRECATED !! Using the old flow which uses launch_emulator.tcl. Please use v++ -p to generate the script to launch new launch_emulator.py INFO: [LAUNCH_EMULATOR] Killing process in file \/home\/call_me_utka\/Documents\/Projects\/aes-ultra96-v2-playground\/hls_vector_addition\/vector_addition_application_system\/Emulation-SW\/emulation.pid qemu-system-aarch64: terminating on signal 15 from pid 359998 () qemu-system-microblazeel: \/pmu@0: Disconnected clk=87402423072 ns Successfully killed launch_emulator process ``` A little description of what I did already check: ChatGPT suggested to look for launch_sw_emu.sh and check if there is port forwarding defined (-forward-port 1440 1534) - no such line found there I also checked Run Configurations -> Target Setup tab for any relevant parameters. Surprisingly this tab is totally empty (just saying \"None\") There was a similar topic on this post: https:\/\/adaptivesupport.amd.com\/s\/article\/69606 but seems like Target Connection Details window does not change the project settings (port used for connecting to TCF)",
    "author_id":5622,
    "publication_date":1754314337000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Call_me_Utka",
    "author_reputation":21.0,
    "tags":"vitis, fpga",
    "text_length":2913,
    "title_length":67,
    "num_tags":2
  },
  {
    "id":6122,
    "title":"Configure www redirect to non-www in DigitalOcean App Platform app spec",
    "link":"https:\/\/stackoverflow.com\/questions\/79724858\/configure-www-redirect-to-non-www-in-digitalocean-app-platform-app-spec",
    "text":"I'm trying to deploy an app with the app spec below, but the ingress rule for a www to non-www redirect is rejected with the error: ``` error validating app spec field \"ingress.rules.rule.match.type\": App Ingress routing match type is missing ``` If I add a \"path\" entry, the error instead is: ``` error validating app spec field \"ingress.rules.rule.match\": rule matching path prefix \"\/\" already in use by rule with component: \"website\" ``` ``` alerts: - rule: DEPLOYMENT_FAILED - rule: DOMAIN_FAILED domains: - domain: domain.bla type: PRIMARY zone: domain.bla - domain: www.domain.bla type: ALIAS zone: domain.bla features: - buildpack-stack=ubuntu-22 ingress: rules: - component: name: website match: path: prefix: \/ authority: exact: \"domain.bla\" - redirect: authority: \"domain.bla\" match: authority: exact: \"www.domain.bla\" path: prefix: \/ name: domain-bla region: lon services: - http_port: 80 image: # snip instance_count: 1 instance_size_slug: basic-xxs name: website ```",
    "author_id":5621,
    "publication_date":1754314687000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jasper van den Bosch",
    "author_reputation":3217.0,
    "tags":"digital-ocean-apps",
    "text_length":979,
    "title_length":71,
    "num_tags":1
  },
  {
    "id":6121,
    "title":"Setting the WPF ComboBoxItem Background Color when an ObjectDataProvider binding is used to populate ComboBox",
    "link":"https:\/\/stackoverflow.com\/questions\/79724861\/setting-the-wpf-comboboxitem-background-color-when-an-objectdataprovider-binding",
    "text":"I have a ComboBox with an ObjectDataProvider binding to an enum of colors. I have the enum descriptions displaying. I would like to have the ComboBoxItem background color match the designated color. I've created a converter that takes the enum value and returns the Brushes.Color. I'm trying to create a style, but I'm getting \"incompatible type\". Here's my XAML: ``` xmlns:converter=\"clr-namespace:FuelAnalysisTool.Converters\" <Window.Resources> <ObjectDataProvider x:Key=\"dataFromEnum\" ObjectType=\"{x:Type sys:Enum}\" MethodName=\"GetValues\"> <ObjectDataProvider.MethodParameters> <x:Type TypeName=\"enum:MyColorEnum\"\/> <\/ObjectDataProvider.MethodParameters> <\/ObjectDataProvider> <converter:EnumValueColorConverter x:Key=\"ItemColorConverter\" \/> <Style x:Key=\"ColoredComboBoxItemStyle\" TargetType=\"ComboBoxItem\"> <Setter Property=\"Background\" Value=\"{Binding RelativeSource={RelativeSource Mode=Self}, Path=Content, Converter={StaticResource ItemColorConverter}}\"\/> <\/Style> <\/Window.Resources> <ComboBox Style=\"{Binding ColoredComboBoxItemStyle}\" ItemsSource=\"{Binding Source={StaticResource dataFromEnum}}\" SelectedValue=\"{Binding ColorSelected}\" \/> ```",
    "author_id":5620,
    "publication_date":1754314953000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Unanamous",
    "author_reputation":31.0,
    "tags":"combobox, data-binding, styles, wpf-controls",
    "text_length":1154,
    "title_length":109,
    "num_tags":4
  },
  {
    "id":6120,
    "title":"I am getting error always when i try to run npm command",
    "link":"https:\/\/stackoverflow.com\/questions\/79724863\/i-am-getting-error-always-when-i-try-to-run-npm-command",
    "text":"this is packag.json file I am installing react and when i try to run \"npm install\" it gives me this error.. refer to above linked image This is the issue when i try to run \"npm install\" on termux app i tried so many methods available on internet but it didn't worked",
    "author_id":5619,
    "publication_date":1754314996000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vipul Gehlot",
    "author_reputation":7.0,
    "tags":"npm",
    "text_length":266,
    "title_length":55,
    "num_tags":1
  },
  {
    "id":6119,
    "title":"Am I correctly generating a list of randomly assigned pairs with exclusions in python?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724874\/am-i-correctly-generating-a-list-of-randomly-assigned-pairs-with-exclusions-in-p",
    "text":"I have an array of names and roles of people within a company: Example array: ``` names_and_titles = [ (\"Samantha Reyes\", \"Innovation\", \"Product Owner\"), (\"Ethan McAllister\", \"Data Scientist\"), (\"Priya Deshmukh\", \"Data Architect\", \"SMT\"), (\"Marcus Liu\", \"Stream 3\"), (\"Elena Petrova\", \"SMT\", \"Stream 3\"), ] ``` I also have a csv file with all of the previous pairs that have been generated. I want to create a list of paired individuals that have not been paired before (unique pair) which also do not share the same role e.g. \"Marcus Liu\" and \"Elena Petrova\" cannot be paired together as they share the same role \"Stream 3\". This is my code for generating unique pairs and saving the generated pairs back into the CSV file. ``` import random import pandas as pd # An array of all names and titles names_and_titles = [ (\"Samantha Reyes\", \"Innovation\", \"Product Owner\"), (\"Ethan McAllister\", \"Data Scientist\"), (\"Priya Deshmukh\", \"Data Architect\", \"SMT\"), (\"Marcus Liu\", \"Stream 3\"), (\"Elena Petrova\", \"SMT\", \"Stream 3\"), ] # Load previously generated pairs from the CSV file def load_seen_pairs_from_csv(prev_pairs2): df = pd.read_csv(prev_pairs2) seen_pairs = set() for _, row in df.iterrows(): pair = (row['name1'], row['name2']) reverse_pair = (row['name2'], row['name1']) seen_pairs.add(pair) seen_pairs.add(reverse_pair) return seen_pairs # Save new pairs to the CSV file def save_pairs_to_csv(prev_pairs2, pairs): df = pd.DataFrame(pairs, columns=['name1', 'name2']) df.to_csv(prev_pairs2, index=False, mode='a', header=False) # Path to the CSV file containing previously generated pairs csv_file_path = 'prev_pairs2.csv' # Initialize the seen_pairs set with previously created pairs seen_pairs = load_seen_pairs_from_csv(csv_file_path) # Excluded pairs for pairing logic excluded_pairs = [] def create_unique_pairs_with_debugging_and_fallback(names_and_titles, seen_pairs, excluded_pairs): excluded_set = set(excluded_pairs) | set((pair[1], pair[0]) for pair in excluded_pairs) # Include reverse pairs max_retries = 10000 # Increase the number of retries to improve pairing chances retries = 0 while retries < max_retries: random.shuffle(names_and_titles) pairs = [] used_names = set() # Track names already paired in this run skipped = [] # Track skipped individuals for debugging for i in range(0, len(names_and_titles) - 1, 2): person1, person2 = names_and_titles[i], names_and_titles[i + 1] roles1 = set(person1[2:]) if len(person1) > 2 else set() # Handle blank roles roles2 = set(person2[2:]) if len(person2) > 2 else set() # Handle blank roles pair = (person1[0], person2[0]) reverse_pair = (person2[0], person1[0]) # Debugging: Log why pairs are skipped if pair in excluded_set or reverse_pair in excluded_set: print(f\"Skipping pair due to exclusion: {person1[0]} - {person2[0]}\") continue if pair in seen_pairs or reverse_pair in seen_pairs: print(f\"Skipping pair due to seen pair: {person1[0]} - {person2[0]}\") continue if roles1 & roles2: # Skip if roles overlap print(f\"Skipping pair due to role overlap: {person1[0]} ({roles1}) - {person2[0]} ({roles2})\") continue if person1[0] in used_names or person2[0] in used_names: # Avoid duplicate pairings in this run print(f\"Skipping pair due to duplicate usage: {person1[0]} - {person2[0]}\") continue pairs.append((person1, person2)) used_names.update([person1[0], person2[0]]) # Mark names as used # Add skipped individuals to the next round skipped = [person for person in names_and_titles if person[0] not in used_names] if not skipped: # If no one is skipped, pairing is complete pairs_set = set((pair[0][0], pair[1][0]) for pair in pairs) if not any((pair in seen_pairs or (pair[1], pair[0]) in seen_pairs) for pair in pairs_set): seen_pairs.update(pairs_set) return pairs retries += 1 # If retries are exhausted, raise an error with debugging information print(\"Unable to generate unique pairs with the given restrictions.\") print(f\"Skipped individuals: {[person[0] for person in skipped]}\") raise ValueError(\"Unable to generate unique pairs with the given restrictions.\") # Generate unique pairs with debugging and fallback logic unique_pairs = create_unique_pairs_with_debugging_and_fallback(names_and_titles, seen_pairs, excluded_pairs) # Print the unique pairs with names only for pair in unique_pairs: print(f\"{pair[0][0]} - {pair[1][0]}\") print(\"-----\") # Print the total number of pairs print(f\"Total pairs: {len(unique_pairs)}\") # Save the unique pairs to the CSV file save_pairs_to_csv(csv_file_path, [(pair[0][0], pair[1][0]) for pair in unique_pairs]) ``` I was expecting to be given a list of pairs who are unique and do not share the same role. However I am generating a list of pairs where a few pairings share the same role. For those wondering this is the format of the prev_pairs2.csv file: ``` name1,name2 Ava Thompson,Noah Bennett Liam Carter,Zara Mahmood Maya Chen,Oliver Grant Nina Kowalski,Tomás Rivera Rajiv Mehta,Grace O'Connor Daniel Okoro,Isla McKenzie ```",
    "author_id":5618,
    "publication_date":1754315424000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Imam",
    "author_reputation":27.0,
    "tags":"python, pandas, csv",
    "text_length":4953,
    "title_length":86,
    "num_tags":3
  },
  {
    "id":6118,
    "title":"Azure DevOps Pipeline Settings Error - &quot;This setting cannot be changed as it is enforced at the organization level&quot;",
    "link":"https:\/\/stackoverflow.com\/questions\/79724875\/azure-devops-pipeline-settings-error-this-setting-cannot-be-changed-as-it-is",
    "text":"I'm trying to update pipeline-related settings under ``` Project Settings > Pipelines > Settings ``` in Azure DevOps, but some of the options (e.g., \"Disable creation of classic build pipelines\", \"Disable creation of classic release pipelines\") are grayed out and show a tooltip saying: \"This setting cannot be changed as it is enforced at the organization level.\" I am a Project Administrator but still cannot change this setting. What I’ve Tried: Tried to change the settings under project-level settings (shown in screenshot). Looked into organization-level settings but I do not have access. Searched Azure DevOps documentation but couldn't find a clear explanation about how to override or request changes for organization-enforced settings. Expected Behavior: I expected to be able to toggle these settings as a project admin. Actual Behavior: The settings are locked with a tooltip saying they are enforced by the organization, and cannot be edited at the project level. Question: Where exactly in Azure DevOps organization settings can these restrictions be updated? Do I need to contact an organization admin to change this? Is there any way to temporarily override this at the project level (e.g., for testing)?",
    "author_id":5617,
    "publication_date":1754315433000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mahammad Azimzada",
    "author_reputation":149.0,
    "tags":"azure, azure-devops",
    "text_length":1221,
    "title_length":125,
    "num_tags":2
  },
  {
    "id":6117,
    "title":"polars implementation for creating objects selecting specific attributes",
    "link":"https:\/\/stackoverflow.com\/questions\/79724877\/polars-implementation-for-creating-objects-selecting-specific-attributes",
    "text":"The ``` stanza ``` annotation pipeline processes a text and it creates ``` Sentence ``` s which in turn comprise of ``` Word ``` s. These are objects created by Stanza. I want to select specific attributes of the ``` Word ``` objects that Stanza creates and create my own objects in a list of lists (the outer list is the whole text and the inner lists are the sentences). With a ``` pandas ``` ``` DataFrame ``` having each text annotation in a ``` DataFrame ``` cell, I would create a function with a double for loop to achieve that. I want to use the ``` polars ``` library. Can I do that using the ``` polars ``` API, or I do that like the ``` pandas ``` implementation? Edit for including minimal code example. ``` from typing import NamedTuple nlp = stanza.Pipeline('en') class Word(NamedTuple): id: int head_id: int text: str span: list[int] def get_doc_words(doc: stanza.Document) -> list[list[Word]]: doc_words = [] for sentence in doc.sentences: sentence_words = [] for sent_word in sentence.words: word = Word( id=sent_word.id, head_id=sent_word.head, text=sent_word.text, span=[sent_word.start_char, sent_word.end_char], ) sentence_words.append(word) doc_words.append(sentence_words) return doc_words df=pd.DataFrame( { 'text': [ 'This is some sample text. A second sentence.', 'And a second sample. Having a second sentence as well' ] } ) df['stanza_annotation'] = df['text'].apply(nlp) df['stanza_words'] = df['stanza_annotation'].apply(get_doc_words) ``` And this is the output I expect for each piece of text. ``` [[Word(id=1, head_id=5, text='This', span=[0, 4]), Word(id=2, head_id=5, text='is', span=[5, 7]), Word(id=3, head_id=5, text='some', span=[8, 12]), Word(id=4, head_id=5, text='sample', span=[13, 19]), Word(id=5, head_id=0, text='text', span=[20, 24]), Word(id=6, head_id=5, text='.', span=[24, 25]], [Word(id=1, head_id=3, text='A', span=[26, 27]), Word(id=2, head_id=3, text='second', span=[28, 34]), Word(id=3, head_id=0, text='sentence', span=[35, 43]), Word(id=4, head_id=3, text='.', span=[43, 44])]] ```",
    "author_id":5616,
    "publication_date":1754315479000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"exch_cmmnt_memb",
    "author_reputation":561.0,
    "tags":"python, python-polars, stanford-nlp",
    "text_length":2039,
    "title_length":72,
    "num_tags":3
  },
  {
    "id":6116,
    "title":"CSP is set but I still get Content Security Policy directive violation",
    "link":"https:\/\/stackoverflow.com\/questions\/79724882\/csp-is-set-but-i-still-get-content-security-policy-directive-violation",
    "text":"I'm using VITE, and I'm trying to connect my app to Matomo. I used VITE_APP_CSP_POLICY with no issue in the past, but right now I'm encountering the problem I can't resolve: container_XXX.js:85 Refused to connect to 'https:\/\/myapp.matomo.cloud\/matomo.php?action_name=SOME_MATOMO_THINGS' because it violates the following Content Security Policy directive: \"connect-src 'self' blob: somesite.eu blob: https:\/\/cdn.matomo.cloud\/myapp.matomo.cloud\/container_XXX.js blob: https:\/\/myapp.matomo.cloud\/ * https:\/\/*.amazonaws.com\". As you can see, the connection that breaks the CSP is https:\/\/myapp.matomo.cloud\/matomo.php?action_name=SOME_MATOMO_THINGS , but afaik it should fall under ``` blob: https:\/\/myapp.matomo.cloud\/* ``` . My whole CSP is set like this: ``` VITE_APP_CSP_POLICY=\"default-src 'self'; img-src 'self' data:; manifest-src 'self'; style-src 'self' fonts.googleapis.com 'unsafe-inline'; font-src 'self' data: fonts.gstatic.com; script-src 'self' 'unsafe-eval' https:\/\/www.data-dog-url.com https:\/\/cdn.matomo.cloud\/myapp.matomo.cloud\/container_XXX.js https:\/\/myapp.matomo.cloud\/*; connect-src 'self' blob: somesite.eu blob: https:\/\/cdn.matomo.cloud\/myapp.matomo.cloud\/container_XXX.js blob: https:\/\/myapp.matomo.cloud\/* https:\/\/*.amazonaws.com;\" ``` Is there something obvious I'm missing? I already tried to define it without ``` blob: ``` but it didn't work as well (I got the same error).",
    "author_id":5615,
    "publication_date":1754315841000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Whichmann",
    "author_reputation":197.0,
    "tags":"vite, content-security-policy, matomo",
    "text_length":1401,
    "title_length":70,
    "num_tags":3
  },
  {
    "id":6115,
    "title":"Combine std module with concurrent\/ppl",
    "link":"https:\/\/stackoverflow.com\/questions\/79724883\/combine-std-module-with-concurrent-ppl",
    "text":"I am trying to convert a header file into a module by compiling the following: ``` \/\/ def.ixx export module def; #include <concurrent_unordered_map.h> import std; export using dictionary = std::unordered_map<std::string, std::variant<double, std::string, bool, std::vector<double>, int, std::size_t>>; export using results = std::unordered_map<std::string, std::vector<double>>; export using pool_results = std::unordered_map<std::string, std::vector<std::vector<std::vector<std::vector<double>>>>>; export using cache = concurrency::concurrent_unordered_map<std::size_t, results>; export using pool_cache = std::unordered_map<std::size_t, pool_results>; ``` I have previously compiled the ``` std ``` module. I am compiling with ``` cl \/std:c++latest \/EHsc \/nologo \/reference \"std=std.ifc\" def.ixx ``` and get the following error: ``` def.ixx def.ixx(5): warning C5244: '#include <concurrent_unordered_map.h>' in the purview of module 'def' appears erroneous. Consider moving that directive before the module declaration, or replace the textual inclusion with 'import <concurrent_unordered_map.h>;'. def.ixx(3): note: see module 'def' declaration LINK : fatal error LNK1561: entry point must be defined ``` I have tried as it suggests, however, a new error appear. Is it possible to use ``` std ``` as a module and also use ``` concurrency ``` \/ ``` concurrent_unordered_map ``` ?",
    "author_id":5468,
    "publication_date":1754315934000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dogAwakeCat",
    "author_reputation":384.0,
    "tags":"c++, c++-modules, ppl",
    "text_length":1381,
    "title_length":38,
    "num_tags":3
  },
  {
    "id":6114,
    "title":"Unable to write JSON:Java 8 date\/time type java.time.LocalDateTime when using Hateos Json API",
    "link":"https:\/\/stackoverflow.com\/questions\/79724891\/unable-to-write-jsonjava-8-date-time-type-java-time-localdatetime-when-using-ha",
    "text":"I am working on Spring Hateos Json APi . I have a Get request ``` import static com.toedter.spring.hateoas.jsonapi.MediaTypes.JSON_API_VALUE; import org.springframework.hateoas.EntityModel; import java.time.LocalDateTime; @GetMapping(\"\/getMovie\",produces = JSON_API_VALUE) public Movie getMovie(){ LocalDateTime localDateTime=LocalDateTime.now(); return new Movie(1,\"Avengers\",localDateTime); } ``` and the Movie class has ``` Public class Movie{ private Integer id; private String name; private LocalDateTime availableTime; \/\/Getters setters and Constructor } } ``` I have added the following configuration for Object Mapper ``` @Bean @Primary fun objectMapper(): ObjectMapper = JsonMapper.builder() .addModule(JavaTimeModule()) .build() ``` I am using following dependencies of Hateos and jsonapi ``` implementation 'org.springframework.boot:spring-boot-starter' implementation 'com.toedter:spring-hateoas-jsonapi:2.1.3' implementation 'org.springframework.boot:spring-boot-starter-hateoas' compile 'com.fasterxml.jackson.datatype:jackson-datatype-jsr310:2.7.3' ``` I get the following error ``` org.springframework.http.converter.HttpMessageNotWritableException:Could not write JSON:Java 8 date\/time type java.time.LocalDateTime not supported by default: add Module \"com.fasterxml.jackson.datatype:jackson-datatype-jsr310\" ``` If I add ``` @JsonSerialize(using = LocalDateTimeSerializer.class) @JsonDeserialize(using = LocalDateTimeDeserializer.class) private LocalDateTime localdaetTime; ``` When I use the above code on the getter,It works but I cannot use this because the model class is present in the library and I cannot change it. Also If I remove , ``` produces = JSON_API_VALUE ``` from the controller , the code seems to work. Looks like it is taking the ObjectMapper configurations from the hateos Json api and not from the configurations I have provided",
    "author_id":5614,
    "publication_date":1754316399000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Thejas",
    "author_reputation":517.0,
    "tags":"java, java-time, jackson-databind, spring-hateoas",
    "text_length":1868,
    "title_length":93,
    "num_tags":4
  },
  {
    "id":6113,
    "title":"How to remove system bottom navigation transparency for android 15 and upper?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724892\/how-to-remove-system-bottom-navigation-transparency-for-android-15-and-upper",
    "text":"I want to add background color for system navigation bar. For Samsung Galaxy s22 ultra android 15 color is applying with transparent navigation bar. I want translucent background and want that under the system navigation bar my app content should not visible. For Most of the Tab with android version 15 color does not applied. This is my Code: protected void onCreate(Bundle savedInstanceState) { ``` super.onCreate(null); Window window = getWindow(); if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) { window.setNavigationBarColor(Color.parseColor(\"#00FF00\")); } if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.R) { window.setDecorFitsSystemWindows(true); } else { window.getDecorView().setSystemUiVisibility(View.SYSTEM_UI_FLAG_VISIBLE); } ``` }",
    "author_id":5613,
    "publication_date":1754316421000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Habib Rayhan",
    "author_reputation":11.0,
    "tags":"android, react-native, android-studio",
    "text_length":759,
    "title_length":77,
    "num_tags":3
  },
  {
    "id":6112,
    "title":"Drupal checkbox and form field rendering nested labels",
    "link":"https:\/\/stackoverflow.com\/questions\/79724893\/drupal-checkbox-and-form-field-rendering-nested-labels",
    "text":"In my drupal project I have a form field: ``` $form['field_consent_abcd']['widget']['value']['#title'] = t('Test text <a href=\"@url\" target=\"_blank\">target url text<\/a>.', ['@url' => '\/aomething']); ``` This field is set as a checkbox field in drupal UI. Problem: Drupal is rendering label and the form field is also rendering a label. so nested label prevents checkbox click from working. However, upon clicking the link text the box gets checked. HTML structure rendered: ``` <div class=\"usa-checkbox...\"> <input data-drupal-selector=\"edit-field-abcd-value\" type=\"checkbox\" id=\"edit-field-abcd-value\" name=\"field_abcd[value]\" value=\"1\" class=\"form-checkbox required form-item form-item__input--form-boolean form-item--type-checkbox usa-checkbox__input\" required=\"required\" aria-required=\"true\"> <label #theme=\"form_element_label\" #title=\"fdfsfsfsf\" #title_display=\"after\" #id=\"edit-field-abcd-value\"...> ::before --labels nested here--- <label for=\"edit-field-abcd-value\" class=\"form-item form-item__label option\"> this is a test <a href=\"\/something>url text<\/a> ::after <\/label> <\/label> <\/div> ``` To fix this in my form alter i tried: ``` $form['field_consent_abcd']['widget']['value']['#title'] = ''; $form['field_consent_abcd']['widget']['value']['#title_display'] = 'none'; $form['field_consent_abcd']['widget']['value']['#attributes']['id'] = 'field-consent-abcd'; $form['field_consent_abcd']['#suffix'] = ' <label for=\"field-consent-abcd\" class=\"usa-checkbox__label\"> This is a test text <a href=\"\/something\" target=\"_blank\">for url<\/a>. <\/label> '; ``` The above fix renders the below html structure: ``` <div.. <div... <input... <label... -- No label associated with a form field ::before <\/label <\/div> <\/div \"link text\" <a href... ``` Problem: My fix is rendering an input and a label but on the label i see \"No label associated with a form field\" error Checkbox click doesn't do anything originally drupal renders label while my form field is also rendering the label. Any help on how this can be fixed?",
    "author_id":5612,
    "publication_date":1754316474000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Rick",
    "author_reputation":1579.0,
    "tags":"drupal, drupal-10",
    "text_length":2018,
    "title_length":54,
    "num_tags":2
  },
  {
    "id":6111,
    "title":"What command line options can you pass to an installer built with Wise 9?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724896\/what-command-line-options-can-you-pass-to-an-installer-built-with-wise-9",
    "text":"I maintain an installer which has to run several secondary sub-installers. Those secondary ones need to run with minimal user interaction, ideally \"silent\". I have to add one which was compiled using Wise 9. What are the standard command line options that I can pass to it (assuming there are any at all)? Wise 9 is ~20 years out of date but we have no ability to rebuild this particular installer.",
    "author_id":5611,
    "publication_date":1754316878000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"StayOnTarget",
    "author_reputation":13288.0,
    "tags":"wise",
    "text_length":398,
    "title_length":73,
    "num_tags":1
  },
  {
    "id":6110,
    "title":"Running Visual Studio Code Python debugger in Pixi shell",
    "link":"https:\/\/stackoverflow.com\/questions\/79724899\/running-visual-studio-code-python-debugger-in-pixi-shell",
    "text":"I'm using Pixi to set up a Python environment that a python project needs to run in. So I first do ``` pixi shell ``` , which sets several environment variables defined in the ``` pyproject.toml ``` file and also runs a ``` .bat ``` script. After this I run ``` python.exe .\/script.py ``` to run the application. I'm using Visual Studio Code as IDE, so I'm running this in its Terminal view. But when I try to use the debugger in Visual Studio Code, I run it from the IDE instead of launching ``` python.exe ``` from the terminal view. But for this Visual Studio Code creates a new Terminal session \"Python Debug Console\", for which ``` pixi shell ``` has not been run, and so it will not work correctly. When launching the debugger from the Visual Studio Code IDE, it runs ``` & 'python.exe' 'c:\\...\\ms-python.debugpy-2025.10.0-win32-x64\\bundled\\libs\\debugpy\\launcher' '49494' '--' 'c:\\...\\script.py ``` in that session \"Python Debug Console\". Running this manually in the \"pixi\" shell also does not work, because then the Python debugger will not connect to the IDE because the IDE is not waiting for a connection from a Python debugger. Is there a way to run a Python debugging session with the Visual Studio Code IDE, such that it is launched in a Pixi environment? One solution would be if I could set up the IDE to wait for a connection from the debugger, and then manually run ``` ms-python.debugpy-2025.10.0-win32-x64\\bundled\\libs\\debugpy\\launcher ... ``` in the Pixi shell, but this does not seem possible. Another way would be if it could be set to run ``` pixi shell ``` . But this seems also problematic, because ``` pixi shell ``` does not only set environment variables, but also launches a new ``` cmd ``` shell as subprocess. (So it is not a command that can be run before running the debugger, but the debugger needs to be run inside the shell spawned by ``` pixi shell ``` ).",
    "author_id":5610,
    "publication_date":1754316963000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"tmlen",
    "author_reputation":9180.0,
    "tags":"python, visual-studio-code, vscode-debugger, pixi-package-manager",
    "text_length":1893,
    "title_length":56,
    "num_tags":4
  },
  {
    "id":6109,
    "title":"Is there a way to reduce polling woth django dramtiq",
    "link":"https:\/\/stackoverflow.com\/questions\/79724903\/is-there-a-way-to-reduce-polling-woth-django-dramtiq",
    "text":"I have a webapp using django hosted on render cloud platform which allows users to make api call to deepseek. These api calls sometimes take 5 minutes to respond, and doing so with async may cause issues if user connection drops. So I planned to use a Dramatiq background worker hosted on a render service too seperate to the web app. I set up redis with upstash and connected to dramtiq and it turned out it was polling redis hundreds to thousand per minute which on upstash equates to commands which equates to a high cost. So my goal is to reduce the polling to once per minute to reduce my redis commands number to reduce costs. I have tried setting up gevent too but now sure how to reduce redis polling with dramtiq and also use gevent.",
    "author_id":5609,
    "publication_date":1754317377000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Bilal Waseem",
    "author_reputation":13.0,
    "tags":"django, web-applications, gevent, dramatiq",
    "text_length":742,
    "title_length":52,
    "num_tags":4
  },
  {
    "id":6108,
    "title":"Activating conda venv in wsl via Windows .bat file",
    "link":"https:\/\/stackoverflow.com\/questions\/79724907\/activating-conda-venv-in-wsl-via-windows-bat-file",
    "text":"I am trying to use a .bat file that would automatically open the console, and change into a certain conda venv in WSL \/ Ubuntu. The following .bat file does open Ubuntu \/ wsl, but it stays in (base) instead of activating the condavenv. ``` wsl -d Ubuntu --cd \"\/mnt\/d\/dev\/projects\/<MYPROJECTNAME>\" bash -c \"source \/home\/<USERNAME>\/miniconda3\/etc\/profile.d\/conda.sh && conda activate \/mnt\/d\/dev\/projects\/<MYPROJECTNAME>\/condavenv; exec bash\" pause ``` I have noticed that once I am in WSL, the following would take me to the conda venv: . .\/activator.sh activator.sh: ``` #!\/bin\/bash source $(conda info --base)\/etc\/profile.d\/conda.sh conda activate \/mnt\/d\/dev\/Projects\/<PROJECTNAME>\/condavenv ``` I am looking for a way to put this functionality into my .bat file. I don't see why it doesn't have the desired effect currently. Thank you!",
    "author_id":5608,
    "publication_date":1754317799000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"tmighty",
    "author_reputation":11451.0,
    "tags":"windows, conda, ubuntu, windows-subsystem-for-linux, venv",
    "text_length":836,
    "title_length":50,
    "num_tags":5
  },
  {
    "id":6107,
    "title":"Use Powershell to list mapped drives via the info found in HKCU",
    "link":"https:\/\/stackoverflow.com\/questions\/79724914\/use-powershell-to-list-mapped-drives-via-the-info-found-in-hkcu",
    "text":"Typically, I'd just use something like this to get the info I need: ``` Get-PSDrive -PSProvider FileSystem | Where-Object {$_.DisplayRoot -match \"^\\\\\\\\\"} | Select-Object -Property Root,DisplayRoot ``` However, we're working with an RMM that runs in the NT Authority\\System context so the results are always blank. I've had to do something similar before - where I find what user is logged in and find their SID and then figure out the path to the HKCU from there. But I cannot sort out how to get the drive letter & path for each mapped drive. Below is what I've got so far. (If there's a better way, by all means, tell me.) ``` $Username = (Get-WMIObject -ClassName Win32_ComputerSystem).Username New-PSDrive -PSProvider Registry -Name HKU -Root HKEY_USERS $User = New-Object System.Security.Principal.NTAccount($Username) $SID = $User.Translate([System.Security.Principal.SecurityIdentifier]) $RegistryPath = \"HKU:$($sid.Value)\\Network\" ``` With that code, I can typically sort the absolute path of HKCU for the user. At that point, I run something like this to list out all their mapped drives and their paths. ``` Get-ChildItem $RegistryPath ``` The output I get is what you see below: ``` Name Property ---- -------- g RemotePath : \\\\FS1\\Company UserName : ProviderName : Microsoft Windows Network ProviderType : 131072 ConnectionType : 1 ConnectFlags : 0 DeferFlags : 4 UseOptions : {68, 101, 102, 67...} M RemotePath : \\\\FS1\\ManagementTeam UserName : 0 ProviderName : Microsoft Windows Network ProviderType : 131072 ConnectionType : 1 ConnectFlags : 0 DeferFlags : 4 UseOptions : {68, 101, 102, 67...} o RemotePath : \\\\FS1\\Operations UserName : ProviderName : Microsoft Windows Network ProviderType : 131072 ConnectionType : 1 ConnectFlags : 0 DeferFlags : 4 UseOptions : {68, 101, 102, 67...} ``` I tried piping the above command to a ``` Select-Object ``` and just grabbing the Name and RemotePath but I only get back the Name ; the RemotePath is blank. For that matter, the Name has some extra fluff I don't need but I can live with it if need be. Sample output: ``` Name RemotePath ---- ---------- HKEY_USERS\\S-1-5-21-645643383-2545892271-1688843945-1108\\Network\\g HKEY_USERS\\S-1-5-21-645643383-2545892271-1688843945-1108\\Network\\M HKEY_USERS\\S-1-5-21-645643383-2545892271-1688843945-1108\\Network\\o ``` All I'm trying to do is get the drive letter and its path. Something like this would be ideal: ``` Name RemotePath ---- ---------- G \\\\FS1\\Company M \\\\FS1\\ManagementTeam O \\\\FS1\\Operations ```",
    "author_id":5607,
    "publication_date":1754318016000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"RKillcrazy",
    "author_reputation":183.0,
    "tags":"powershell, registry, mapped-drive",
    "text_length":2506,
    "title_length":63,
    "num_tags":3
  },
  {
    "id":6106,
    "title":"TS2307: Cannot find module for imported SVG in React component library when used in another MFE",
    "link":"https:\/\/stackoverflow.com\/questions\/79724916\/ts2307-cannot-find-module-for-imported-svg-in-react-component-library-when-used",
    "text":"I'm working on a React component library where I'm importing SVG files as React components like this: ``` import React, { FC } from \"react\"; import { ReactComponent as DashedDangerSvgIconBase } from \".\/svgs\/DashedDangerSvgIcon.svg\"; import { ReactComponent as DashedWarningSvgIconBase } from \".\/svgs\/DashedWarningSvgIcon.svg\"; import { ReactComponent as SuccessSvgIconBase } from \".\/svgs\/SuccessSvgIcon.svg\"; import { IconProps } from \".\/types\"; ``` This works fine locally and builds successfully. However, when I consume this library in another Micro Frontend (MFE) and raise a PR, the build fails with these errors: ``` ERROR in D:\\BuildAgent\\_work\\27\\s\\Document\\node_modules\\@ln\\create-react-components\\src\\foundations\\icons\\svgIcons\\ln-svg-icons.tsx 4:59-92 ERROR in D:\\BuildAgent\\_work\\27\\s\\Document\\node_modules\\@ln\\create-react-components\\src\\foundations\\icons\\svgIcons\\ln-svg-icons.tsx(4,60) TS2307: Cannot find module '.\/svgs\/DashedWarningSvgIcon.svg' or its corresponding type declarations. ERROR in D:\\BuildAgent\\_work\\27\\s\\Document\\node_modules\\@ln\\create-react-components\\src\\foundations\\icons\\svgIcons\\ln-svg-icons.tsx 5:53-80 ERROR in D:\\BuildAgent\\_work\\27\\s\\Document\\node_modules\\@ln\\create-react-components\\src\\foundations\\icons\\svgIcons\\ln-svg-icons.tsx(5,54) TS2307: Cannot find module '.\/svgs\/SuccessSvgIcon.svg' or its corresponding type declarations. ``` Why does this error occur only when used in another MFE build pipeline? Is this a type declaration issue for SVG imports? How can I make sure SVGs are correctly bundled\/exported from the library and consumed without this TS error? How can I fix this issue? Any help or pointers would be greatly appreciated!",
    "author_id":5606,
    "publication_date":1754318129000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sumisha Sankar",
    "author_reputation":3.0,
    "tags":"reactjs, svg, import, tsx, mfe",
    "text_length":1686,
    "title_length":95,
    "num_tags":5
  },
  {
    "id":6105,
    "title":"react NavLink - How to use the link multiple times",
    "link":"https:\/\/stackoverflow.com\/questions\/79724919\/react-navlink-how-to-use-the-link-multiple-times",
    "text":"I have a link for my notifications. But when I'm on the target page and click the link again, the pages doesn't reload. I use the \"reloadDocument\" Option, but no effect. ``` <NavLink to={link} reloadDocument>name<\/NavLink> ``` The problem comes on \"useEffect\". Because all the dependencies are the same. Yes, I could implement something to have different dependencies. But I don't like that solution, because I would have to do it on every page that is a notification link target. And if I'm on the page and click a link to a different sub page (both have the same useEffect) it does the stuff from the previous target before it does it for the current. So, is there no way to say react to handle the link as if it would be the first call?",
    "author_id":5605,
    "publication_date":1754318381000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user18513066",
    "author_reputation":null,
    "tags":"reactjs",
    "text_length":739,
    "title_length":50,
    "num_tags":1
  },
  {
    "id":6104,
    "title":"What is the most efficient way to check if a Polars LazyFrame has duplicates?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724922\/what-is-the-most-efficient-way-to-check-if-a-polars-lazyframe-has-duplicates",
    "text":"With the help of claude sonnet 4, I cooked up this function, which I hope does what I asked it to do. ``` def has_duplicates_early_exit(df: pl.LazyFrame, subset: list[str]) -> bool: \"\"\"Can exit early when first duplicate is found\"\"\" return df.select( pl.struct(subset).is_duplicated().any() ).collect().item() ``` Is this the most efficient you can do?",
    "author_id":5494,
    "publication_date":1754318727000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Nicol&#242; Cavalleri",
    "author_reputation":203.0,
    "tags":"python, python-polars, polars",
    "text_length":352,
    "title_length":77,
    "num_tags":3
  },
  {
    "id":6103,
    "title":"Volume mappings from Windows to WSL are causing problems when using \/mnt\/C\/",
    "link":"https:\/\/stackoverflow.com\/questions\/79724928\/volume-mappings-from-windows-to-wsl-are-causing-problems-when-using-mnt-c",
    "text":"We are developing an ASP.NET app on windows, which runs in docker. When using docker desktop, it \"all just works\". But a few of us are used to develop in Linux and prefer to work with WSL instead (I understand that docker desktop also relies on WSL, but we just prefer to be able to do \"everything\" through CLI). It all works fine, except the volume mapping are causing problems. When we have this (simplified) docker compose as example ``` timescale-db: container_name: timescale image: timescale\/timescaledb:latest-pg16 environment: # some env vars volumes: - .\/dbdata:\/var\/lib\/postgresql\/data ``` The folder \"dbdata\" will be created on the windows host in the current directory. docker compose is ran inside WSL, and it will execute in my case in \/mnt\/c\/Users\/bpr\/dev\/iai.productionmaster.central\/ In WSL, the path \/mnt\/c\/Users\/bpr\/dev\/iai.productionmaster.central\/dbdata is then mapped to the docker container in \/var\/lib\/postgresql\/data. So far so good. But, when the postgress instance then starts, it wants to change permissions on its \/var\/lib\/postgresql\/data folder, which fails initdb: error: could not change permissions of directory \"\/var\/lib\/postgresql\/data\": Operation not permitted 2025-08-04T12:29:08.635400362Z fixing permissions on existing directory \/var\/lib\/postgresql\/data ... 2025-08-04T12:29:21.768621211Z chmod: \/var\/lib\/postgresql\/data: Operation not permitted 2025-08-04T12:29:21.778706750Z According to several posts (e.g. this one ) this has everything to do with NTFS permissions getting in the way. So when the \"host\" part of the volume mapping is inside the NTFS mount, I can run into this kind of problems. When I would change the volume mapping to something like \"\/home\/test:\/var\/lib\/postgresql\/data\", it also works just fine. But then I would need to navigate to \\wsl$\\home\\test to see the contents. Again, not a big issue, but we are trying to get as close as possible to what docker desktop does. The obvious solution ``` sudo chmod -R 777 \/mnt\/c\/Users ``` does not solve anything. Does anybody recognize this problem and do you have a solution to this problem?",
    "author_id":5604,
    "publication_date":1754319198000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"bas",
    "author_reputation":15150.0,
    "tags":"docker-compose, postgresql, docker, windows-subsystem-for-linux",
    "text_length":2097,
    "title_length":75,
    "num_tags":4
  },
  {
    "id":6102,
    "title":"&quot;The path is not of a legal form&quot; after Visual Studio update",
    "link":"https:\/\/stackoverflow.com\/questions\/79724929\/the-path-is-not-of-a-legal-form-after-visual-studio-update",
    "text":"enter image description here After updating Visual Studio Pro, ILRepack.Lib.MSBuild.Task started throwing an ArgumentException with the message \"The path is not of a legal form\" during the assembly loading process. The error occurs specifically during the LoadPriorityObject method execution. Whole issue described here. https:\/\/github.com\/ravibpatel\/ILRepack.Lib.MSBuild.Task\/issues\/72",
    "author_id":5603,
    "publication_date":1754319317000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mirco Bianchini",
    "author_reputation":11.0,
    "tags":"visual-studio, ilrepack",
    "text_length":386,
    "title_length":70,
    "num_tags":2
  },
  {
    "id":6101,
    "title":"How can a long double be that big in C++?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724935\/how-can-a-long-double-be-that-big-in-c",
    "text":"The ``` sizeof(long double) ``` is 8, which means that if I use all the bits for the integer part of an unsigned number, I can maximum store ``` 2^64-1=18446744073709551615 ``` . However, ``` std::numeric_limits<long double>::max() ``` is ``` 1.79769e+308 ``` , which is ``` 1.79769 * 10^308 ``` , which is way larger than ``` 2^64 ``` . How can I then store a number that big in just 8 bytes?",
    "author_id":5591,
    "publication_date":1754319523000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"alekscooper",
    "author_reputation":859.0,
    "tags":"c++, floating-point",
    "text_length":393,
    "title_length":41,
    "num_tags":2
  },
  {
    "id":6100,
    "title":"Sum of integer data and numeric data is unexpected numeric value, R",
    "link":"https:\/\/stackoverflow.com\/questions\/79724941\/sum-of-integer-data-and-numeric-data-is-unexpected-numeric-value-r",
    "text":"Why is the sum of the ``` iaea_mass_dat_trunc$AM_WN ``` ( ``` integer ``` ) and ``` iaea_mass_dat_trunc$AM_frac ``` ( ``` numeric ``` ) ``` 499496.6 ``` for all results? How do I get the appropriate result (for example, the sum of the values in ``` AM_WN ``` and ``` AM_frac ``` in the first row should be: 1+0.0086649159 = 1.00086649159 ``` Atomic_Mass ``` should be the sum of ``` AM_WN ``` and ``` AM_frac ``` . ``` iaea_mass_dat_trunc$Atomic_Mass <- sum(iaea_mass_dat_trunc$AM_WN, iaea_mass_dat_trunc$AM_frac) ``` Data ``` iaea_mass_dat_trunc = structure(list(CC = c(\"0\", \" \", \"0\", \"0\", \" \", \" \", \"0\", \" \", \" \", \"0\", \" \", \" \", \" \", \"0\", \" \", \" \", \" \", \" \", \"0\"), ` 1-NZ` = c(1L, -1L, 0L, 1L, -1L, -3L, 2L, 0L, -2L, 3L, 1L, -1L, -3L, 4L, 2L, 0L, -2L, -4L, 5L), N = c(1L, 0L, 1L, 2L, 1L, 0L, 3L, 2L, 1L, 4L, 3L, 2L, 1L, 5L, 4L, 3L, 2L, 1L, 6L), Z = c(0L, 1L, 1L, 1L, 2L, 3L, 1L, 2L, 3L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 5L, 1L), A = c(1L, 1L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L), El = c(\" n \", \"H \", \"H \", \"H \", \"He \", \"Li \", \"H \", \"He \", \"Li \", \"H \", \"He \", \"Li \", \"Be \", \"H \", \"He \", \"Li \", \"Be \", \"B \", \"H \"), O = c(\" \", \" \", \" \", \" \", \" \", \" -pp\", \" -n\", \" \", \" -p\", \" -nn\", \" -n\", \" -p\", \" x\", \" -3n\", \" \", \" \", \" -\", \" x\", \" -nn\"), Mass_Excess_keV = c(0.00807131806, 0.007288971064, 0.013135722895, 0.0149498109, 0.01493121888, 0.028667, 0.024621129, 0.00242491587, 0.02532319, 0.032892447, 0.011231234, 0.011678887, 0.037139, 0.041875725, 0.017592095, 0.01408688044, 0.018375034, 0.04732, 0.049135), Mass_Excess_Unc = c(4.4e-10, 1.3e-11, 1.5e-11, 8e-11, 6e-11, 0.002, 1e-04, 1.5e-10, 0.000212132, 8.9443e-05, 2e-05, 5e-05, 0.002003, 0.000254127, 5.3e-08, 1.44e-09, 5.448e-06, 0.002003, 0.001004), `Binding_Energy\/A_keV` = c(0, 0, 0.011122831, 0.028272654, 0.0257268044, -0.02267, 0.017204491, 0.070739156, 0.011537603, 0.013363592, 0.055121325, 0.052661325, 0.00018, 0.009616395, 0.048785199, 0.053323312, 0.044872478, -0.00467, 0.0094), Binding_Energy_unc = c(0, 0, 2e-09, 3e-09, 1.5e-09, 0.00667, 0.00025, 2e-09, 0.00053033, 0.000178885, 4e-05, 1e-04, 0.00401, 0.000423545, 8.9e-08, 3e-09, 9.08e-06, 0.00334, 0.00143), particle = c(\"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\", \"B-\"), BD_energy_keV = c(0.00782347, NA, NA, 0.0001859202, -0.13736, NA, 0.221962131, -0.22898274, NA, 0.216612131, -0.004476529, -0.2546, NA, 0.242836294, 0.035052147, -0.042881534, -0.28945, NA, 0.23062), BD_energy_unc = c(4e-09, NA, NA, 6e-10, 0.02, NA, 0.001, 0.00212132, NA, 0.000916515, 0.000538516, 0.02003, NA, 0.002541268, 5.32e-07, 5.4478e-05, 0.02003, NA, 0.01004), AM_WN = c(1L, 1L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L), AM_frac = c(0.0086649159, 0.007825031898, 0.014101777844, 0.01604928132, 0.01602932197, 0.030775, 0.026431867, 0.00260325413, 0.027185561, 0.035311492, 0.012057224, 0.0125378, 0.03987, 0.044955437, 0.018885889, 0.01512288742, 0.019726409, 0.0508, 0.052749), AM_unc = c(4.7e-10, 1.4e-11, 1.5e-11, 8e-11, 6e-11, 0.002147, 0.000107354, 1.6e-10, 0.000227733, 9.602e-05, 2.147e-05, 5.3677e-05, 0.00215, 0.000272816, 5.7e-08, 1.55e-09, 5.848e-06, 0.00215, 0.001078), Atomic_Mass = c(499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906, 499496.622850906)), row.names = c(NA, 19L ), class = \"data.frame\") ```",
    "author_id":5520,
    "publication_date":1754319954000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Brian",
    "author_reputation":117.0,
    "tags":"r, numeric, sum, integer",
    "text_length":3617,
    "title_length":67,
    "num_tags":4
  },
  {
    "id":6099,
    "title":"Transparent border around marker in apexcharts",
    "link":"https:\/\/stackoverflow.com\/questions\/79724943\/transparent-border-around-marker-in-apexcharts",
    "text":"This is how my graph looks like now: And this is what I would like to have: These are the values for my graph: ``` chartData.value = [ { date: '2020-08-04', value: 800 }, \/\/ 5Y ago { date: '2021-08-04', value: 950 }, \/\/ 4Y ago { date: '2022-08-04', value: 1100 }, \/\/ 3Y ago { date: '2023-08-04', value: 1300 }, \/\/ 2Y ago { date: '2024-02-04', value: 1500 }, \/\/ 6M ago { date: '2025-07-04', value: 1800 }, \/\/ 1M ago { date: '2025-07-28', value: 2000 }, \/\/ 1W ago { date: '2025-07-29', value: 2100 }, { date: '2025-07-30', value: 2200 }, { date: '2025-07-31', value: 2400 }, { date: '2025-08-01', value: 2800 }, { date: '2025-08-02', value: 3200 }, { date: '2025-08-03', value: 3800 }, { date: '2025-08-04', value: 4500 } \/\/ Today ] ``` And this is the CSS style: ``` export const useBalanceChartOptionsComposable = (chartData: ChartDataItem[]) => { const { getAverageValue } = useBalanceChartSeriesComposable(chartData) const formatLabelValue = () => { if (!chartData.length) return '' const lastValue = chartData[chartData.length - 1] return formattedValue(lastValue.value) } \/\/ Find the highest value and its index const getHighestValue = () => { if (!chartData.length) return { value: 0, index: 0 } let maxValue = chartData[0].value let maxIndex = 0 chartData.forEach((item, index) => { if (item.value > maxValue) { maxValue = item.value maxIndex = index } }) return { value: maxValue, index: maxIndex } } const { value: highestValue, index: highestIndex } = getHighestValue() const chartOptions = computed<ApexOptions>(() => ({ chart: { type: 'line', background: 'transparent', toolbar: { show: false }, zoom: { enabled: false }, width: '100%', parentHeightOffset: 0, offsetX: 0, offsetY: 0, }, stroke: { curve: 'smooth', width: 3, colors: ['var(--ion-color-secondary-contrast)'], }, grid: { show: false, padding: { left: 16, right: 80, top: 0, bottom: 0, }, }, xaxis: { labels: { show: false }, axisBorder: { show: false }, axisTicks: { show: false }, lines: { show: false }, max: chartData.length + 1, range: undefined, }, yaxis: [{ show: false, labels: { formatter: () => formatLabelValue(), }, }], annotations: { yaxis: [ { y: getAverageValue.value, borderWidth: 1, strokeDashArray: 10, borderColor: 'var(--ion-color-secondary-contrast)', label: { borderColor: 'transparent', style: { fontSize: '12px', background: 'none', color: 'var(--ion-color-secondary-contrast)', }, text: formattedValue(getAverageValue.value), position: 'right', offsetX: 14, offsetY: 6, textAnchor: 'start', }, }, { y: highestValue, borderWidth: 0, borderColor: 'transparent', label: { borderColor: 'transparent', style: { fontSize: '12px', background: 'none', color: 'var(--ion-color-secondary-contrast)', }, text: formattedValue(highestValue), position: 'right', offsetX: 14, offsetY: 6, textAnchor: 'start', }, }, ], }, dataLabels: { enabled: false }, markers: { size: 0, discrete: [{ seriesIndex: 0, dataPointIndex: chartData.length - 1, fillColor: 'var(--ion-color-secondary-contrast)', strokeColor: 'transparent', strokeWidth: 5, size: 5, shape: 'circle', offsetY: -5 }] }, legend: { show: false }, tooltip: { enabled: false }, })) return { chartOptions, } } ``` I already tried to make transparent border around the marker to cut out the line so that I would have the space there but that did not work is there a way to do what I want ?",
    "author_id":5602,
    "publication_date":1754320100000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Peter Plevko",
    "author_reputation":193.0,
    "tags":"html, css, vue.js, charts, apexcharts",
    "text_length":3324,
    "title_length":46,
    "num_tags":5
  },
  {
    "id":6098,
    "title":"Looking for simple garbage collector load test",
    "link":"https:\/\/stackoverflow.com\/questions\/79724945\/looking-for-simple-garbage-collector-load-test",
    "text":"I'm looking for some code or some benchmark to roughly asses the pause times or cpu load caused by some GC in order to get some rough estimate how efficient it is. I just want to see whether some GC has really large pause times or not. That would be sufficient. Some language like Java or C# allow for lots of logging information to be written to the console which can then be analysed. But for some open source languages without commercial backing from some large company this is often not the case. So in order just to get some rough estimate I would run some code that does some arbitrary computation like running the number crunching code of some benchmark. Then I would load the heap in parallel with a lot of data like creating large arrays of strings, because strings consume a lot of memory and the GC will somewhen be forced to free memory and then I can have a look how long the execution time is taking with causing pressure on memory and without. My question is whether someone has some idea for a better approach. Maybe some GC benchmark code exists that I can adapt to the respective language.",
    "author_id":5601,
    "publication_date":1754320106000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"OlliP",
    "author_reputation":1607.0,
    "tags":"garbage-collection, load-testing, benchmarking",
    "text_length":1107,
    "title_length":46,
    "num_tags":3
  },
  {
    "id":6097,
    "title":"How to securely connect Flutter to a Node.js server with TLS or RSA encryption?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724948\/how-to-securely-connect-flutter-to-a-node-js-server-with-tls-or-rsa-encryption",
    "text":"I'm building a Flutter app that needs to send user data (like name and street) to a Node.js backend server. Currently, the data is sent via URL like this: http:\/\/192.168.1.126:8080\/save?name=toto&street=somestreet I want to make this communication secure. Option 1: TLS\/HTTPS I followed this tutorial: https:\/\/www.geeksforgeeks.org\/node-js\/how-to-use-ssl-tls-with-node-js\/ I managed to start a TLS server, but I'm not sure how to connect to it from Flutter. Question: How can I make an HTTPS request from Flutter to my local Node.js server using a self-signed certificate? Option 2: RSA encryption Alternatively, I thought about encrypting the user data (name, street, etc.) with an RSA public key in Flutter, and decrypting it in Node.js using the private key. But I'm stuck on both sides: I don’t know how to properly encode\/encrypt a string in Flutter with a public key. And I’m not sure how to decrypt it in Node.js. Any help or working examples for either method would be appreciated!",
    "author_id":5600,
    "publication_date":1754320316000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"totoskiller",
    "author_reputation":15.0,
    "tags":"flutter, encryption, ssl, node.js, rsa",
    "text_length":989,
    "title_length":79,
    "num_tags":5
  },
  {
    "id":6096,
    "title":"How to solve undo_retention error while migration data via Debezium from table without primary key?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724953\/how-to-solve-undo-retention-error-while-migration-data-via-debezium-from-table-w",
    "text":"We are using Debezium to process migration from Oracle database to PostgreSQL . Oracle contains a large table with hundreds of millions of rows without a primary key. After 2-3 days of collecting data into Kafka connector falls with an error \"ORA-01555\" (which means that UNDO_RETENTION space is \"full\"). We tried to increase this parameter and came to undo space of 2 days. And it still drops this error - after some time started to show Failed reading SCN timestamp from database but it seems that's it also connected with undo space. The question is: does anybody know how to solve such problems? How could we transfer table data to Kafka? We can't add a PK, that means no incremental snapshot mode can be used.",
    "author_id":5599,
    "publication_date":1754320580000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"HighQuality",
    "author_reputation":29.0,
    "tags":"oracle-database, debezium, data-migration, confluent-platform, undo-redo",
    "text_length":714,
    "title_length":99,
    "num_tags":5
  },
  {
    "id":6095,
    "title":"CUDA 12.9 uninstallation",
    "link":"https:\/\/stackoverflow.com\/questions\/79724958\/cuda-12-9-uninstallation",
    "text":"I have concerned questions related to Pytorch installation. Firstly, when I did my project recent months ago, I install pytorch by using ``` pip install torch torchvision torchaudio --index-url https:\/\/download.pytorch.org\/whl\/cu118 ``` . My CUDA version at that time is 11.8. However, recently, I found out when trying to install Tensorflow-gpu that there is a installation guide in which I have to install CUDA and CuDNN seperately before installing Pytorch. Before installing CUDA 12.9, I have checked my GPU by ``` nvidia-smi ``` , it already has CUDA 12.9. Then I installed CUDA 12.9 and CuDNN 8.9. My question is: How can I completely remove all the files, folders of CUDA 12.9 and CuDNN 8.9 that I installed? Thank you.",
    "author_id":5598,
    "publication_date":1754321100000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dũng Nguyễn Minh",
    "author_reputation":1.0,
    "tags":"installation, cuda, pytorch, uninstallation",
    "text_length":726,
    "title_length":24,
    "num_tags":4
  },
  {
    "id":6094,
    "title":"Why can I not create a vertex array with ModernGL?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724961\/why-can-i-not-create-a-vertex-array-with-moderngl",
    "text":"I'm trying to make a game with pygame, and I'm using moderngl to use shaders. However, I'm getting constant issues trying to create moderngl objects. Currently, I cannot create multiple moderngl Vertex Arrays. It's weird, I'm able to create one but when I go to create the second or third the code returns a moderngl Error. Here's my shader manager that manages the loading and post_processing of shaders: ``` class ShaderManager: def __init__(self, ctx: moderngl.Context, initial_size: tuple[int,int]): self.ctx = ctx # 1 quad VBO (x,y) + (u,v) verts = np.array([ -1, 1, 0.0, 0.0, -1, -1, 0.0, 1.0, 1, 1, 1.0, 0.0, 1, -1, 1.0, 1.0, ], dtype='f4') self.quad_vbo = ctx.buffer(verts.tobytes()) self.programs: dict[str, moderngl.Program] = {} self.vaos: dict[str, moderngl.VertexArray] = {} self.fbo_cache: dict[tuple[int,int], tuple] = {} # pre-allocate initial FBO textures self._ensure_fbo(initial_size) def load(self, name: str, vert_source: str | Path, frag_source: str | Path): \"\"\"Compile and register a shader program from file paths or raw source strings.\"\"\" if isinstance(vert_source, Path) or isinstance(vert_source, str) and Path(vert_source).exists(): vs = Path(vert_source).read_text(encoding='utf-8') else: vs = vert_source # treat as raw GLSL string if isinstance(frag_source, Path) or isinstance(frag_source, str) and Path(frag_source).exists(): fs = Path(frag_source).read_text(encoding='utf-8') else: fs = frag_source # treat as raw GLSL string prog = self.ctx.program(vertex_shader=vs, fragment_shader=fs) self.programs[name] = prog def _ensure_fbo(self, size: tuple[int,int]): \"\"\"Make textures+FBO for this resolution if missing.\"\"\" if size in self.fbo_cache: return w,h = size in_tex = self.ctx.texture((w,h), 4, alignment=1) out_tex = self.ctx.texture((w,h), 4, alignment=1) for t in (in_tex, out_tex): t.filter = (moderngl.NEAREST, moderngl.NEAREST) t.swizzle = 'BGRA' fbo = self.ctx.framebuffer(color_attachments=[out_tex]) self.fbo_cache[size] = (in_tex, out_tex, fbo) def _get_vao(self, name: str): \"\"\"Lazily create a VAO that feeds quad_vbo into in_vert\/in_uv.\"\"\" if name not in self.vaos: prog = self.programs[name] # Explicitly tell moderngl how the VBO is structured: 2 floats (pos), 2 floats (uv) vao = self.ctx.vertex_array( prog, [(self.quad_vbo, '2f 2f', 'in_vert', 'in_uv')] ) self.vaos[name] = vao return self.vaos[name] def post_process(self, passes: list[str], scene: pygame.Surface) -> pygame.Surface: \"\"\" Run each named pass in order over `scene`, returning the final. Internally uses two ping-pong FBOs sized to scene.get_size(). \"\"\" size = scene.get_size() self._ensure_fbo(size) in_tex, out_tex, fbo = self.fbo_cache[size] # upload initial data = pygame.image.tostring(scene, 'RGBA', False) in_tex.write(data) in_tex.use(location=0) current_tex = in_tex for name in passes: prog = self.programs[name] # bind prog['tex'].value = 0 if 'time' in prog: prog['time'].value = time.time() % 1000 # render into out_tex fbo.use() self.ctx.clear(0.0,0.0,0.0,1.0) vao = self._get_vao(name) vao.render(mode=moderngl.TRIANGLE_STRIP) # swap current_tex, out_tex = out_tex, current_tex fbo = self.ctx.framebuffer(color_attachments=[out_tex]) out_tex.use(location=0) # readback final final_data = current_tex.read() return pygame.image.frombuffer(final_data, size, 'RGBA') ``` However, it's returning the following error: ``` PS C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain> python main.py pygame 2.6.1 (SDL 2.28.4, Python 3.11.9) Hello from the pygame community. https:\/\/www.pygame.org\/contribute.html Traceback (most recent call last): File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\main.py\", line 39, in <module> run_game() File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\main.py\", line 35, in run_game instanceManager(screen, settingsManager(), ctx) File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\utility\\instanceManager.py\", line 39, in __init__ main_menu(screen, self) File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\screens\\main_menu.py\", line 184, in main_menu base.run() File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\utility\\screen_utility\\baseScreen.py\", line 271, in run self.draw(virtual_mouse) File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\utility\\screen_utility\\baseScreen.py\", line 246, in draw surf = sm.post_process([\"invert\", \"blur\"], surf) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\utility\\shaderManager.py\", line 93, in post_process vao = self._get_vao(name) ^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\infamoose\\Documents\\coding\\spellsmithAgain\\utility\\shaderManager.py\", line 61, in _get_vao vao = self.ctx.vertex_array( ^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\infamoose\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\moderngl\\__init__.py\", line 1901, in vertex_array return self._vertex_array(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\infamoose\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\moderngl\\__init__.py\", line 1922, in _vertex_array res.mglo, res._glo = self.mglo.vertex_array( ^^^^^^^^^^^^^^^^^^^^^^^ _moderngl.Error: cannot create vertex array ```",
    "author_id":5597,
    "publication_date":1754321164000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"infamoose",
    "author_reputation":5.0,
    "tags":"python, opengl, vertex-buffer, vertex-array, python-moderngl",
    "text_length":5306,
    "title_length":50,
    "num_tags":5
  },
  {
    "id":6093,
    "title":"messages gone after migration from opensearch to datanode",
    "link":"https:\/\/stackoverflow.com\/questions\/79724964\/messages-gone-after-migration-from-opensearch-to-datanode",
    "text":"Environnement : Docker compose (Graylog 6.3 \/ Mongo 6.0.14) migration from Opensearch 2.15 to Datanode 6.1 ( documentation ) update from 6.1 to 6.3 I followed the documentation to migrate from OpenSearch to Data Node. I encountered some connection errors during the process, but the migration completed successfully. Before the migration, everything was working fine, and messages were being stored without any issues. After the migration, everything still works, and new messages are being stored and are accessible. However, I can’t access the old messages — they appear to be gone. But physically, the data is still there (I can see the 2.8 TB in ``` nodes\/0\/indices ``` ). After some research, I found that the old indices were not in the same location as the new ones. I moved the old indices to the new location (with all Docker containers stopped), but they are still not being recognized. I tried using Maintenance\/Recalculate index ranges and Maintenance\/Rotate active write index , but neither had any effect. Interestingly, in the System\/Overview window logs, I still see logs from before the migration. Just before the migration, there is a message: ``` Running retention action [org.graylog2.indexer.retention.strategies.ClosingRetentionStrategy] for indices <graylog_242> ``` which seems to point to the last index. Immediately after the migration, I see: ``` Cycled index alias <graylog_deflector> from <none> to <graylog_0>. ``` My guess is that Graylog doesn’t show the old data because the old indices are numbered higher than the new ones. Any help to recover the old messages would be appreciated.",
    "author_id":5596,
    "publication_date":1754321350000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"DSX",
    "author_reputation":151.0,
    "tags":"graylog, docker, opensearch",
    "text_length":1617,
    "title_length":57,
    "num_tags":3
  },
  {
    "id":6092,
    "title":"Very confusing error with erase method on vector",
    "link":"https:\/\/stackoverflow.com\/questions\/79724965\/very-confusing-error-with-erase-method-on-vector",
    "text":"Somehow when removing the first element of a vector it is actually removing the last element. ``` std::cout << \"Before:\\n\"; for (const Move &_move : moves) { std::cout << _move << std::endl; } moves.erase(moves.begin()); std::cout << \"\\n\"; std::cout << \"After:\\n\"; for (const Move &_move : moves) { std::cout << _move << std::endl; } std::cout << \"\\n\"; ``` Somehow this creates the output: ``` Before: Attacking: (0, 2) None: (0, 4) SingleMove: (0, 3) After: Attacking: (0, 2) None: (0, 4) ``` Showing that it removed the last element ``` SingleMove: (0, 3) ``` instead of the first ``` Attacking: (0, 2) ``` I am programming this in CLion and its debugger also showed the same result from what the vectors where storing The before debugger info The after debugger info This is the full code ``` void AICalculator::replacePromotionMove(Piece *pawn) { std::vector<Move> moves = pawn->getMoves(); for (const Move &move : pawn->getMoves()) { if (const PieceColour &pawnColour = pawn->getPieceColour(); Move::moveTypeInVector(Piece::POSSIBLE_MOVETYPES, move.type) && (pawnColour == PieceColour::White && move.x == 0 || pawnColour == PieceColour::Black && move.x == 7)) { if (const int moveIndex = Move::moveIndexInVector(moves, move); moveIndex != -1) { std::cout << \"Before:\\n\"; for (const Move &_move : moves) { std::cout << _move << std::endl; } moves.erase(moves.begin()); std::cout << \"\\n\"; std::cout << \"After:\\n\"; for (const Move &_move : moves) { std::cout << _move << std::endl; } std::cout << \"\\n\"; moves.emplace_back(move, MoveType::PromotionQueen); moves.emplace_back(move, MoveType::PromotionKnight); moves.emplace_back(move, MoveType::PromotionRook); moves.emplace_back(move, MoveType::PromotionBishop); } } } pawn->setMoves(moves); } ``` Move Header file ``` #ifndef MOVE_H #define MOVE_H #include \"Position.hpp\" #include <array> #include <string> enum class MoveType { Empty, Attacking , Friendly, Enpassant, SingleMove, DoubleMove, KingMove, Castling, PromotionQueen, PromotionKnight, PromotionRook, PromotionBishop, Pin, None }; class Move { static const std::string NOTATION_LETTERS; static const std::string NOTATION_NUMBERS; public: const int x; const int y; MoveType type; explicit Move(const std::array<int, 2>& position); Move(const std::array<int, 2>& position, MoveType type); Move(const Move& position, MoveType type); Move& operator=(const Move& other); static Move copy(const Move& other); static bool moveTypeInVector(const std::vector<MoveType> &moveTypes, const MoveType &moveTypeToCheck); \/* returns -1 if not found *\/ static int moveIndexInVector(const std::vector<Move> &moves, const Move &moveToCheck); static MoveType getPromotionType(int promotionID); bool isPromotion() const; friend std::ostream &operator<<(std::ostream &os, const Move &move); std::string str() const; \/** Doesn't compare Move type *\/ bool coordEqualTo(const Move& otherMove) const; Position toPosition(float squareSize, bool correctCentre = false) const; std::string getNotation() const; static Move notationToPosition(const std::string &notation); }; inline std::ostream &operator<<(std::ostream &os, const MoveType &pieceType) { switch (pieceType) { case MoveType::Empty: os << \"Empty\"; break; case MoveType::Attacking: os << \"Attacking\"; break; case MoveType::Friendly: os << \"Friendly\"; break; case MoveType::Enpassant: os << \"Enpassant\"; break; case MoveType::SingleMove: os << \"SingleMove\"; break; case MoveType::DoubleMove: os << \"DoubleMove\"; break; case MoveType::KingMove: os << \"KingMove\"; break; case MoveType::Castling: os << \"Castling\"; break; case MoveType::Pin: os << \"Pin\"; break; case MoveType::None: os << \"None\"; break; case MoveType::PromotionQueen: os << \"PromotionQueen\"; break; case MoveType::PromotionKnight: os << \"PromotionKnight\"; break; case MoveType::PromotionRook: os << \"PromotionRook\"; break; case MoveType::PromotionBishop: os << \"PromotionBishop\"; break; } return os; } #endif \/\/MOVE_H ``` Move cpp file ``` #include \"Move.hpp\" #include <sstream> #include <cmath> #include <iostream> const std::string Move::NOTATION_LETTERS = \"abcdefgh\"; const std::string Move::NOTATION_NUMBERS = \"87654321\"; Move::Move(const std::array<int, 2> &position) : x(position[0]), y(position[1]), type(MoveType::None) {} Move::Move(const std::array<int, 2> &position, const MoveType type) : x(position[0]), y(position[1]), type(type) {} Move::Move(const Move &position, const MoveType type) : x(position.x), y(position.y), type(type){} Move Move::copy(const Move &other) { return Move({other.x, other.y}, other.type); } bool Move::moveTypeInVector(const std::vector<MoveType> &moveTypes, const MoveType &moveTypeToCheck) { for (const MoveType &moveType : moveTypes) { if (moveType == moveTypeToCheck) return true; } return false; } int Move::moveIndexInVector(const std::vector<Move> &moves, const Move &moveToCheck) { for (int i = 0; i < moves.size(); ++i) { if (const Move &move = moves.at(i); move.coordEqualTo(moveToCheck) && move.type == moveToCheck.type) return i; } return -1; } MoveType Move::getPromotionType(const int promotionID) { switch (promotionID) { case 1: return MoveType::PromotionKnight; case 3: return MoveType::PromotionBishop; case 2: return MoveType::PromotionRook; case 0: return MoveType::PromotionQueen; default: throw std::invalid_argument(\"Error passed promotionID does not exist: \\n\"); } } bool Move::isPromotion() const { if (type == MoveType::PromotionQueen || type == MoveType::PromotionKnight || type == MoveType::PromotionRook || type == MoveType::PromotionBishop) return true; return false; } Move &Move::operator=(const Move &other) { return *this; } std::ostream &operator<<(std::ostream &os, const Move &move) { return os << move.type << \": \" << \"(\" << move.x << \", \" << move.y << \")\"; } std::string Move::str() const { std::stringstream ss; ss << \"(\" << x << \", \" << y << \")\"; return ss.str(); } bool Move::coordEqualTo(const Move& otherMove) const { if (x == otherMove.x && y == otherMove.y) return true; return false; } Position Move::toPosition(const float squareSize, const bool correctCentre) const { const int squareSizeInt = static_cast<int>(squareSize); const int halfSquareSize = correctCentre ? static_cast<int>(std::round(0.5*squareSize)) : 0; return Position(sf::Vector2i{y * squareSizeInt + halfSquareSize, x * squareSizeInt + halfSquareSize}); } std::string Move::getNotation() const { return NOTATION_LETTERS[y] + std::to_string(x); } Move Move::notationToPosition(const std::string &notation) { const int letterPosition = NOTATION_LETTERS.find(notation[0]); const int numberPosition = NOTATION_NUMBERS.find(notation[1]); return Move({numberPosition, letterPosition}); } ``` Minimal thing ``` int main() { std::vector moves{ Move({1, 2}, MoveType::Castling), Move({3, 4}, MoveType::PromotionBishop), Move({5, 6}, MoveType::Attacking), }; std::cout << \"Before:\\n\"; for (const Move &move : moves) { std::cout << move << std::endl; } moves.erase(moves.begin()); std::cout << \"\\n\"; std::cout << \"After:\\n\"; for (const Move &move : moves) { std::cout << move << std::endl; } std::cout << \"\\n\"; return 0; } ```",
    "author_id":5595,
    "publication_date":1754321495000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"oliver B",
    "author_reputation":9.0,
    "tags":"c++, iterator, stdvector, erase",
    "text_length":7077,
    "title_length":48,
    "num_tags":4
  },
  {
    "id":6091,
    "title":"Inbuilt scrolling support for custom control",
    "link":"https:\/\/stackoverflow.com\/questions\/79724966\/inbuilt-scrolling-support-for-custom-control",
    "text":"There is a known behaviour with ``` cs grid ``` where if the content of the cell is larger than the computed layout cell size, the cell expands. Meaning your grid is only a fixed size if the content is less than or equal to the cell size. There are several workarounds for this that involve you changing the style for the grid and cells themselves. However, I have noticed that these workarounds are not necessary when I use a 3rd party control (e.g ``` MUI List ``` ), where all you have to do is set ``` overflow: 'auto' ``` on the actual control itself. You don't have to mess with the parent ``` grid ``` . So my question is. How do I go about making a control that will switch to a scrollviewer if its content is too large to be displayed inside a grid cell. i.e I don't want to mess with the parent grid every time I want to use my control in a new place. Below I have included a fiddle that shows the issue and the workaround. But I would to know how to fix it without messing with grid cells, as per the case when using 3rd party controls https:\/\/jsfiddle.net\/8b074ung\/11\/ ``` .grid { display: grid; grid-template-columns: repeat(2, 1fr); width: 800px; height: 300px; } .cell { background: red; margin: 10px; \/* min-width: 0; min-height: 0; display: flex; flex-direction: row; *\/ } .scrollable { background: orange; margin: 10px; overflow: auto; } .row { background: purple; display: grid; grid-auto-flow: column; width: max-content; } .item { width: 100px; height: 100px; margin: 5px; background: green; } ``` ``` <div class=\"grid\"> <div class=\"cell\"> <div class=\"scrollable\"> <div class='row'> <div class='item'><\/div> <div class='item'><\/div> <div class='item'><\/div> <div class='item'><\/div> <div class='item'><\/div> <\/div> <\/div> <\/div> <div class=\"cell\"> <\/div> <\/div> ```",
    "author_id":5594,
    "publication_date":1754321589000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"wforl",
    "author_reputation":35.0,
    "tags":"html, css, css-grid",
    "text_length":1786,
    "title_length":44,
    "num_tags":3
  },
  {
    "id":6090,
    "title":"Binding a ComboBox to an object&#39;s property",
    "link":"https:\/\/stackoverflow.com\/questions\/79724968\/binding-a-combobox-to-an-objects-property",
    "text":"I'm sure the question was asked thousands of times, but nevertheless: I want to bind a ComboBox to a ObservableCollection of objects, and have a particular property displayed in a ComboBox list. I do: ``` <ComboBox x:Name=\"daemonsCombo\" MinWidth=\"200\" ItemsSource=\"{Binding Daemons}\"><\/ComboBox> ``` with code behind: ``` public class Daemon { public Daemon(string n, string aet, string i, int p) { Name = n; AETitle = aet; IP = i; Port = p; } public string Name; public string AETitle; public string IP; public int Port; } public partial class MainWindow : Window { public ObservableCollection<Daemon> Daemons { get; set; } public MainWindow() { Daemons = new ObservableCollection<Daemon>(); Daemons.Add(new Daemon(\"TBox\", \"MyDaemon\", \"10.96.130.167\", 51402)); Daemons.Add(new Daemon(\"Clinical\", \"MyDeamon\", \"10.96.130.91\", 51402)); ``` When I compile that I get a list of objects in a ComboBox, a la \"NamespaceName.Daemon\". Fine. But how can I have the property Name displayed instead of this? If I do ``` <ComboBox x:Name=\"daemonsCombo\" MinWidth=\"200\" ItemsSource=\"{Binding Daemons}\" DisplayMemberPath=\"Name\"><\/ComboBox> ``` or ``` <ComboBox x:Name=\"daemonsCombo\" MinWidth=\"200\" ItemsSource=\"{Binding Daemons}\"> <ComboBox.ItemTemplate> <DataTemplate> <TextBlock Margin=\"5,0,5,0\" Text=\"{Binding Name}\"\/> <\/DataTemplate> <\/ComboBox.ItemTemplate> <\/ComboBox> ``` I just get an empty list..",
    "author_id":5593,
    "publication_date":1754321693000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"kosholu",
    "author_reputation":7.0,
    "tags":"binding, wpf, combobox",
    "text_length":1389,
    "title_length":46,
    "num_tags":3
  },
  {
    "id":6089,
    "title":"Why does my assembly written delay function hangs?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724970\/why-does-my-assembly-written-delay-function-hangs",
    "text":"I need a delay function written in assembly for Cortex-M0 CPU and keil uVision 5.38. I did the following code: ``` static __INLINE __ASM void _asm_delay10us(unsigned int num) { \/* R0 contains \"num\"*\/ PUSH {R4, LR} \/* R4 will contain number of loops for a single 10us delay (158 loops) *\/ MOVS R4, #158 \/* Calculate total number of loops as R4 = 158*value *\/ MULS R4, R0, R4 \/\/ loop SUBS R4, #1 BEQ exit B loop exit POP {R4, pc} } ``` Then I call it from C code like this: ``` void delay1ms(unsigned int u32Cnt) { _asm_delay10us(100 * u32Cnt); } ``` This function runs a few times. And then it hangs, I don't understand why. Update1 : I noticed it goes to HardFault at the second call of the function Update2 : Here is a snippet from disassembly created by the compiler: ``` __asm___5_hal_c_0731ac3b___asm_delay10us 0x08000154: b510 .. PUSH {r4,lr} 0x08000156: 249e .$ MOVS r4,#0x9e 0x08000158: 4344 DC MULS r4,r0,r4 0x0800015a: 3c01 .< SUBS r4,#1 0x0800015c: d000 .. BEQ 0x8000160 ; __asm___5_hal_c_0731ac3b___asm_delay10us + 12 0x0800015e: e7fc .. B 0x800015a ; __asm___5_hal_c_0731ac3b___asm_delay10us + 6 0x08000160: bd10 .. POP {r4,pc} 0x08000162: 0000 .. MOVS r0,r0 ``` ... ``` .text delay1ms 0x080026bc: b510 .. PUSH {r4,lr} 0x080026be: 4604 .F MOV r4,r0 0x080026c0: 2164 d! MOVS r1,#0x64 0x080026c2: 4361 aC MULS r1,r4,r1 0x080026c4: 4608 .F MOV r0,r1 0x080026c6: f7fdfd45 ..E. BL __asm___5_hal_c_0731ac3b___asm_delay10us ; 0x8000154 0x080026ca: bd10 .. POP {r4,pc} hal_init 0x080026cc: b570 p. PUSH {r4-r6,lr} 0x080026ce: f7fdfe57 ..W. BL SystemCoreClockUpdate ; 0x8000380 0x080026d2: 48a2 .H LDR r0,[pc,#648] ; [0x800295c] = 0x9c4 0x080026d4: f7fffff2 .... BL delay1ms ; 0x80026bc 0x080026d8: 2164 d! MOVS r1,#0x64 ```",
    "author_id":5592,
    "publication_date":1754321858000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vladimir T",
    "author_reputation":876.0,
    "tags":"assembly, cortex-m, keil",
    "text_length":1727,
    "title_length":50,
    "num_tags":3
  },
  {
    "id":6088,
    "title":"How to use import inside service worker",
    "link":"https:\/\/stackoverflow.com\/questions\/79724971\/how-to-use-import-inside-service-worker",
    "text":"I'm getting the error message below for my service worker which starts with import statements. The registation of the service worker is as a module, so it should work as far as I know? Service Worker ``` firebase-messaging-sw.js ``` : ``` import firebaseConfig from '.\/@api\/$.js'; import { initializeApp } from 'https:\/\/www.gstatic.com\/firebasejs\/12.0.0\/firebase-app.js'; import { getMessaging, onBackgroundMessage } from 'https:\/\/www.gstatic.com\/firebasejs\/12.0.0\/firebase-messaging-sw.js'; ... ``` Registration: ``` if('serviceWorker' in navigator) navigator.serviceWorker.register('\/firebase-messaging-sw.js', { type: 'module' }); ``` Uncaught SyntaxError: Cannot use import statement outside a module (at firebase-messaging-sw.js:1:1)",
    "author_id":4722,
    "publication_date":1754321882000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Simon Ferndriger",
    "author_reputation":5098.0,
    "tags":"service-worker, es6-modules",
    "text_length":738,
    "title_length":39,
    "num_tags":2
  },
  {
    "id":6087,
    "title":"CLion can&#39;t find cmath.h",
    "link":"https:\/\/stackoverflow.com\/questions\/79724972\/clion-cant-find-cmath-h",
    "text":"I've tried to include ``` math.h ``` in CLion, but it says that it's deprecated and suggests ``` cmath.h ``` . When I include ``` cmath.h ``` , it can't find it and suggests a fix - installing something (honestly, I don't understand what exactly). However, when it does install this thing, it doesn't help and CLion still can't find the ``` cmath.h ``` . What do I do to make it see it? I'm using M4 Pro Mac OS X 15.5.",
    "author_id":5591,
    "publication_date":1754321934000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"alekscooper",
    "author_reputation":859.0,
    "tags":"c++, macos, clion",
    "text_length":418,
    "title_length":28,
    "num_tags":3
  },
  {
    "id":6086,
    "title":"How to unify ScrollViewer Scroll across two columns?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724973\/how-to-unify-scrollviewer-scroll-across-two-columns",
    "text":"I'm building a two-column layout in WinUI 3 using a Grid, where each column can have a different amount of content (for example, the left column might be taller than the right one, or vice versa). I want the entire page to scroll vertically using just one single scrollbar, attached to a ScrollViewer at the page level — not on each column individually. When scrolling, both columns should scroll together. If one column has less content than the other, it should scroll only until its content is fully visible, and then stop moving, while the longer column continues scrolling as expected. Also, when scrolling back to the top, the shorter column should start scrolling again only after the scroll reaches the point where its content naturally begins (i.e., its actual height position). I have tried below approach but the UI is not smooth and I don't think using viewchanged event for this is the best. Any suggestions or workarounds would be helpful. Thanks in Advance Attaching the code below. ``` <Grid> <ScrollViewer x:Name=\"MainScrollViewer\" ViewChanged=\"ScrollViewer_ViewChanged\"> <Grid VerticalAlignment=\"Stretch\"> <Grid.ColumnDefinitions> <ColumnDefinition Width=\"7.25*\"\/> <ColumnDefinition Width=\"2.75*\"\/> <\/Grid.ColumnDefinitions> <Grid x:Name=\"LeftContentContainer\" Grid.Column=\"0\" VerticalAlignment=\"Stretch\" Height=\"auto\" Background=\"{ThemeResource BackgroundBrush}\"> <Grid.RenderTransform> <TranslateTransform x:Name=\"LeftContentTransform\"\/> <\/Grid.RenderTransform> <!--Content Here--> <\/Grid> <Grid x:Name=\"RightContentContainer\" Grid.Column=\"1\" VerticalAlignment=\"Stretch\" Height=\"auto\" Background=\"{ThemeResource BackgroundBrush}\"> <Grid.RenderTransform> <TranslateTransform x:Name=\"RightContentTransform\"\/> <\/Grid.RenderTransform> <!--Content Here--> <\/Grid> <\/Grid> <\/ScrollViewer> <\/Grid> ``` Code behind: ``` private void ScrollViewer_ViewChanged(object sender, ScrollViewerViewChangedEventArgs e) { double offset = MainScrollViewer.VerticalOffset; double maxScroll0 = Math.Max(0, leftColumnHeight - viewportHeight); double maxScroll1 = Math.Max(0, rightColumnHeight - viewportHeight); double trans0 = -Math.Min(offset, maxScroll0); double trans1 = -Math.Min(offset, maxScroll1); LeftContentTransform.Y = trans0; RightContentTransform.Y = trans1; } ```",
    "author_id":5590,
    "publication_date":1754321959000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Bevara Satyasairam",
    "author_reputation":107.0,
    "tags":"winui-3, xaml, winui, windows-app-sdk",
    "text_length":2275,
    "title_length":52,
    "num_tags":4
  },
  {
    "id":6085,
    "title":"can&#39;t create info.plist from info-&gt; URL Types",
    "link":"https:\/\/stackoverflow.com\/questions\/79724978\/cant-create-info-plist-from-info-url-types",
    "text":"In Google Admob get started page they are suggesting to update info.plist file with GADApplicationIdentifier and SKAdNetworkItems keys: Google Admob update info.plist . However, I can't find info.plist in the project folder. On the internet, there are videos suggesting to create info.plist file from pressing + under Targets -> info -> URL Types section. Unfortunately , I can't generate the info.plist using that method. How should I proceed? It is difficult to enter all the keys defined in the Google Admob link there are 40+ keys. Thank you for your help.",
    "author_id":5589,
    "publication_date":1754322178000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user30756127",
    "author_reputation":11.0,
    "tags":"ios, xcode, admob",
    "text_length":560,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":6084,
    "title":"How can I upgrade to Facebook iOS SDK v18.0.0 using Swift Package Manager?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724980\/how-can-i-upgrade-to-facebook-ios-sdk-v18-0-0-using-swift-package-manager",
    "text":"I'm trying to upgrade the Facebook iOS SDK in my Swift-based iOS app using Swift Package Manager (SPM). Currently, my project is locked at version 14.1.0 with the rule set to Up to Next Major Version. I can see from the official repository ( https:\/\/github.com\/facebook\/facebook-ios-sdk ) that version v18.0.0 is already available. But in Xcode’s Swift Packages UI, I can’t select or update to v18. It’s limited to <15.0.0. Here’s what I’ve tried: • Removing and re-adding the package • Cleaning DerivedData • Manually entering the GitHub URL again • Update Packages How can I update the Facebook SDK to version 18 via SPM? Is there something blocking it (e.g., missing tag, metadata issue, incompatible interface)? Any help is appreciated.",
    "author_id":5588,
    "publication_date":1754322264000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Kerem DEMİR",
    "author_reputation":54.0,
    "tags":"ios, swift-package-manager, facebook",
    "text_length":740,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":6083,
    "title":"Is there any advantage to updating a branch that will be squashed with rebase over merge?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724991\/is-there-any-advantage-to-updating-a-branch-that-will-be-squashed-with-rebase-ov",
    "text":"Is there any reason to prefer ``` rebase ``` over ``` merge ``` when updating a branch with new commits from main, if that branch will be squashed when it is merged into main anyways? It seems that most people use ``` rebase ``` to do such updates (and it's what Gitlab suggest in the web interface), but as squashing will guarantee a linear history even with ``` merge ``` , and ``` rebase ``` introducing the risk of breaking intermediate commits, it seems to me that ``` merge ``` is the better option here (merging main into the feature branch to update and then squash merging the feature branch into main). Is there any reason why I should use ``` rebase ``` over ``` merge ``` here?",
    "author_id":5587,
    "publication_date":1754322601000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Doodelusion",
    "author_reputation":110.0,
    "tags":"git",
    "text_length":689,
    "title_length":89,
    "num_tags":1
  },
  {
    "id":6082,
    "title":"How can I keep imported SVG files vector-sharp in a SwiftUI app?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724992\/how-can-i-keep-imported-svg-files-vector-sharp-in-a-swiftui-app",
    "text":"I need to display arbitrary SVG assets in a SwiftUI app (iOS or macOS) while preserving their native vector fidelity at any view size. I’m using ``` Image ``` , but it rasterises the SVG to a fixed-resolution bitmap, causing pixelation on resize. Is there currently a built-in SwiftUI mechanism—without rewriting the SVG as SwiftUI ``` Shape\/Path\/Text ``` or embedding ``` WKWebView ``` —to render these SVGs as continuously scalable vector graphics? After all, the entire point of SVG is scalability, and once the system converts it to a bitmap that principle is destroyed.",
    "author_id":5586,
    "publication_date":1754322634000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mango",
    "author_reputation":99.0,
    "tags":"swiftui, xcode, svg",
    "text_length":574,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":6081,
    "title":"How to include WebRTC VAD in my C project",
    "link":"https:\/\/stackoverflow.com\/questions\/79724995\/how-to-include-webrtc-vad-in-my-c-project",
    "text":"I am trying to include WebRTC VAD into my project, specifically I want the feature for distinguishing audio between voiced and unvoiced but having troubles including it. I am using gcc Built by MinGW-W64 project. I tried its official repo and even its some community made repo as well but I just can't get them to work. I'm getting some unable to find files error then more unreferenced errors. Please help me how to structure my project to include all necessary files and a makefile would be appreciated. This is how I am including it: ``` #include\"webrtc\/common_audio\/vad\/webrtc_vad.c\" ``` I tried making gcc use root directory (which contains webrtc) using I. but getting unreferenced, specifically: ``` gcc -I. main.c -o main C:\\Users\\ALLAHI~1\\AppData\\Local\\Temp\\ccovjJHo.o:main.c:(.text+0x8f5): undefined reference to `WebRtcSpl_Init' C:\\Users\\ALLAHI~1\\AppData\\Local\\Temp\\ccovjJHo.o:main.c:(.text+0x93e): undefined reference to `WebRtcVad_InitCore' C:\\Users\\ALLAHI~1\\AppData\\Local\\Temp\\ccovjJHo.o:main.c:(.text+0x992): undefined reference to `WebRtcVad_set_mode_core' C:\\Users\\ALLAHI~1\\AppData\\Local\\Temp\\ccovjJHo.o:main.c:(.text+0xa39): undefined reference to `WebRtcVad_CalcVad48khz' C:\\Users\\ALLAHI~1\\AppData\\Local\\Temp\\ccovjJHo.o:main.c:(.text+0xa5e): undefined reference to `WebRtcVad_CalcVad32khz' C:\\Users\\ALLAHI~1\\AppData\\Local\\Temp\\ccovjJHo.o:main.c:(.text+0xa83): undefined reference to `WebRtcVad_CalcVad16khz' C:\\Users\\ALLAHI~1\\AppData\\Local\\Temp\\ccovjJHo.o:main.c:(.text+0xaa8): undefined reference to `WebRtcVad_CalcVad8khz' collect2.exe: error: ld returned 1 exit status ```",
    "author_id":5585,
    "publication_date":1754322787000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Muhammad Ali khalid",
    "author_reputation":11.0,
    "tags":"c, include, webrtc",
    "text_length":1594,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":6080,
    "title":"Changing from PHP 7.4 to 8.4 results in an invalid callback error on my WP-site",
    "link":"https:\/\/stackoverflow.com\/questions\/79724997\/changing-from-php-7-4-to-8-4-results-in-an-invalid-callback-error-on-my-wp-site",
    "text":"I am switching from PHP 7.4 to 8.4 but it results in a critical error on my WordPress site. The theme causes the problem, what is to do? This is ``` Debug.log ``` : ``` PHP Fatal error: Uncaught TypeError: call_user_func_array(): Argument #1 ($callback) must be a valid callback, function \"removeperth-headings-fonts\" not found or invalid function name in \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/class-wp-hook.php:324 Stack trace: #0 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/class-wp-hook.php(348): WP_Hook->apply_filters(NULL, Array) #1 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/plugin.php(517): WP_Hook->do_action(Array) #2 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/script-loader.php(2299): do_action('wp_enqueue_scri...') #3 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/class-wp-hook.php(324): wp_enqueue_scripts('') #4 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/class-wp-hook.php(348): WP_Hook->apply_filters(NULL, Array) #5 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/plugin.php(517): WP_Hook->do_action(Array) #6 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/general-template.php(3192): do_action('wp_head') #7 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-content\/themes\/perth\/header.php(28): wp_head() #8 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/template.php(810): require_once('\/hp\/bk\/ac\/cm\/ww...') #9 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/template.php(745): load_template('\/hp\/bk\/ac\/cm\/ww...', true, Array) #10 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/general-template.php(48): locate_template(Array, true, true, Array) #11 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-content\/themes\/perth\/page-templates\/page_front-page.php(6): get_header() #12 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/template-loader.php(106): include('\/hp\/bk\/ac\/cm\/ww...') #13 \/hp\/bk\/ac\/cm\/www\/xxx\/wp-blog-header.php(19): require_once('\/hp\/bk\/ac\/cm\/ww...') #14 \/hp\/bk\/ac\/cm\/www\/xxx\/index.php(17): require('\/hp\/bk\/ac\/cm\/ww...') #15 {main} thrown in \/hp\/bk\/ac\/cm\/www\/xxx\/wp-includes\/class-wp-hook.php on line 324 ``` I have no idea how to solve it. So I searched for ``` removeperth-headings-fonts ``` in my installation and found the term in my ``` functions.php ``` : ``` add_action('wp_print_styles', 'perth_deregister_styles', 100); function perth_deregister_styles() { wp_deregister_style( 'perth-headings-fonts' ); wp_deregister_style( 'perth-body-fonts' ); } add_action('wp_enqueue_scripts', 'removeperth-headings-fonts'); add_action('wp_enqueue_scripts', 'removeperth-body-fonts'); add_action( 'wp_enqueue_scripts', 'perth_scripts' ); ```",
    "author_id":5584,
    "publication_date":1754322906000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tia",
    "author_reputation":55.0,
    "tags":"php, wordpress, php-8",
    "text_length":2373,
    "title_length":79,
    "num_tags":3
  },
  {
    "id":6079,
    "title":"Resource called twice in SSR Application",
    "link":"https:\/\/stackoverflow.com\/questions\/79724998\/resource-called-twice-in-ssr-application",
    "text":"I'm working with a SSR Angular 20 and noticed Resource could be called twice : Here's an example of the code: Service: ``` code = signal<string | undefined>(undefined); authCode = resource< { message: string; tokens: { accessToken: string; refreshToken: string } }, string | undefined >({ params: this.code, loader: async ({ params, }): Promise<{ message: string; tokens: { accessToken: string; refreshToken: string }; }> => { const res = await fetch( `${environment.API_URL}\/auth\/callback?code=${params}` ); const data = await res.json(); if (!res.ok) { throw new Error(data.error); } const parsedData = this.tokensSchema.parse(data); return { message: parsedData.message, tokens: parsedData.tokens, }; }, }); setCode(code: string) { this.code.set(code); } ``` Component: ``` readonly #callBackService = inject(AuthCallbackService); code = input.required<string>(); tokens = this.#callBackService.authCode; ngOnInit(): void { if (this.code()) { this.#callBackService.setCode(this.code()); } } ``` In this scenario, the Resource is called twice. Added this code to check if I'm on the server side: ``` if (typeof window === 'undefined') { return { message: '', tokens: { accessToken: '', refreshToken: '', }, }; } ``` And only one call was triggered.",
    "author_id":5583,
    "publication_date":1754322937000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ncls7523",
    "author_reputation":41.0,
    "tags":"typescript, angular, angular-signals, angular-ssr",
    "text_length":1250,
    "title_length":40,
    "num_tags":4
  },
  {
    "id":6078,
    "title":"How can the repository size increase when using &quot;git maintenance run --task gc&quot;?",
    "link":"https:\/\/stackoverflow.com\/questions\/79724999\/how-can-the-repository-size-increase-when-using-git-maintenance-run-task-gc",
    "text":"A multi-year repository with large files is about 154M. The .git folder is 189M. Cloning the repository requires more than twice the data transfer it actually contains. Following this question and running ``` git maintenance run --task=gc ``` , the .git folder jumped to 193M. Why did the size increase? How can the .git folder be reduced in size correctly? Can the .git folder theoretically be reduced to below 154M? The history is linear and anything older than 2 years can be deleted without worry.",
    "author_id":5582,
    "publication_date":1754322987000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dubious",
    "author_reputation":255.0,
    "tags":"git, git-maintenance",
    "text_length":501,
    "title_length":90,
    "num_tags":2
  },
  {
    "id":6077,
    "title":"How do I replicate Oracle to PostgreSQL in real time with schema evolution support?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725006\/how-do-i-replicate-oracle-to-postgresql-in-real-time-with-schema-evolution-suppo",
    "text":"I want to replicate Oracle to PostgreSQL in real time with schema evolution support. Is there is any open source tool available? I am not using Oracle Goldengate. It is too heavy and time consuming to configure. Also licensing cost is high.",
    "author_id":5581,
    "publication_date":1754323069000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Pokhraj Das",
    "author_reputation":21.0,
    "tags":"replication, database-replication, logical-replication",
    "text_length":240,
    "title_length":83,
    "num_tags":3
  },
  {
    "id":6076,
    "title":"Show full import of symbol in VS Code hover popups",
    "link":"https:\/\/stackoverflow.com\/questions\/79725007\/show-full-import-of-symbol-in-vs-code-hover-popups",
    "text":"``` NavBar ``` comes from an npm package. Right now, the hover popup only shows ``` import Navbar ``` . This only tells me it's imported. I'd like to know where it's imported from without scrolling up to the imports or cmd+clicking to go to types or source. Is this possible? If not, can I add it via an extension? Possible display options: ``` import { Footer, Layout, Navbar, ThemeSwitch } from 'nextra-theme-blog'; ``` ``` import { Navbar } from 'nextra-theme-blog'; ``` ``` nextra-theme-blog > Navbar ``` ``` From nextra-theme-blog ```",
    "author_id":5580,
    "publication_date":1754323228000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Will M",
    "author_reputation":2153.0,
    "tags":"typescript, visual-studio-code",
    "text_length":539,
    "title_length":50,
    "num_tags":2
  },
  {
    "id":6075,
    "title":"Jetpack Compose nested graphs navigation and TopAppBar Up button behavior",
    "link":"https:\/\/stackoverflow.com\/questions\/79725009\/jetpack-compose-nested-graphs-navigation-and-topappbar-up-button-behavior",
    "text":"I'm building a Jetpack Compose app that uses a bottom ``` NavigationBar ``` to switch between destinations. Each destination is implemented as a nested graph , and there's also a shared ``` TopAppBar ``` . I'm following the type-safe navigation approach shown in this official video . Desired navigation schema: ``` ├── AGraph │ ├── A1Screen │ └── A2Screen └── BGraph ├── B1Screen └── B2Screen ``` Demonstration code: ``` @Serializable object AGraph @Serializable object A1 @Serializable object A2 @Serializable object BGraph @Serializable object B1 @Serializable object B2 data class NavBarDestination<T : Any>( val route: T, val icon: ImageVector, val label: String, ) val navBarDestinations = listOf( NavBarDestination( route = AGraph, icon = Icons.Default.Call, label = \"A\", ), NavBarDestination( route = BGraph, icon = Icons.Default.Settings, label = \"B\", ), ) @OptIn(ExperimentalMaterial3Api::class) @Composable fun OctopusApp() { val navController = rememberNavController() val navBackStackEntry by navController.currentBackStackEntryAsState() val currentDestination = navBackStackEntry?.destination Scaffold( topBar = { TopAppBar( title = { Text(stringResource(R.string.app_name)) }, navigationIcon = { val canNavigateUp = navController.previousBackStackEntry != null if (canNavigateUp) { IconButton(onClick = navController::navigateUp) { Icon( imageVector = Icons.AutoMirrored.Filled.ArrowBack, contentDescription = stringResource(R.string.up_button), ) } } }, ) }, bottomBar = { NavigationBar { navBarDestinations.forEach { destination -> NavigationBarItem( selected = currentDestination?.hierarchy?.any { it.hasRoute(destination.route::class) } == true, onClick = { navController.navigate(destination.route) { popUpTo(navController.graph.findStartDestination().id) { saveState = true } launchSingleTop = true restoreState = true } }, icon = { Icon( imageVector = destination.icon, contentDescription = destination.label, ) }, label = { Text(destination.label) }, ) } } }, ) { innerPadding -> NavHost( navController, startDestination = AGraph, modifier = Modifier .fillMaxSize() .padding(innerPadding), ) { navigation<AGraph>(startDestination = A1) { composable<A1> { A1Screen(onClick = { navController.navigate(A2) }) } composable<A2> { A2Screen() } } navigation<BGraph>(startDestination = B1) { composable<B1> { B1Screen(onClick = { navController.navigate(B2) }) } composable<B2> { B2Screen() } } } } } ``` Problem When I tap between items in the bottom navigation bar, the back stack is actually grows by exactly one destination (even with that ``` popUpTo ``` action in ``` NavigationBarItem ``` ``` onClick ``` callback). As a result, the case of ``` navController.previousBackStackEntry != null ``` is fired and the Up button appears in the ``` TopAppBar ``` , which is not the desired behavior for primary destinations. What I want instead: ``` TopAppBar ``` should not show the Up button when switching top level tabs — unless deep inside a section. Is there a canonical way to implement this behavior? Do I really need to implement custom multiple back stacks (then why do we need these nested navigation graphs at all?)?",
    "author_id":5579,
    "publication_date":1754323272000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Eshfield",
    "author_reputation":540.0,
    "tags":"android, android-jetpack-compose, navigation",
    "text_length":3140,
    "title_length":73,
    "num_tags":3
  },
  {
    "id":6074,
    "title":"Disable MSBuild auto-upgrade from &lt;Reference&gt; to a &lt;ProjectReference&gt;",
    "link":"https:\/\/stackoverflow.com\/questions\/79725010\/disable-msbuild-auto-upgrade-from-reference-to-a-projectreference",
    "text":"I have a solution with two projects, where A references B through the dll. From what I understand, MS Build replaces the Reference to a ProjectReference automatically. Is there a way to disable this behaviour? Giving a bit more context, the dll of project B has some classes which are not included in the project, but exist in the dll (a custom ms build ensures that), so ProjectReference wouldn't work. Therefore, I'd like to continue having both projects in the same solution, but referencing the dll.",
    "author_id":5578,
    "publication_date":1754323311000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Daniel Patr&#237;cio",
    "author_reputation":11.0,
    "tags":".net-8.0, msbuild",
    "text_length":503,
    "title_length":81,
    "num_tags":2
  },
  {
    "id":6073,
    "title":"How to implement unique_lock for multiple mutexes",
    "link":"https:\/\/stackoverflow.com\/questions\/79725016\/how-to-implement-unique-lock-for-multiple-mutexes",
    "text":"``` <mutex> ``` contains different RAII wrappers: ``` lock_guard ``` - single mutex, always locked ``` unique_lock ``` - single mutex, can defer or try ``` scoped_lock ``` - many mutexes, always locked, supersedes ``` lock_guard ``` as of C++17 Why is there no ``` unique_lock ``` for multiple mutexes? I have some code which requires the following: ``` std::mutex a, b, c; std::unique_multilock lock{ std::defer_lock, a, b, c }; ... if (lock.try_lock()) { ... } ``` Neither ``` unique_lock ``` nor ``` scoped_lock ``` suits me, since the first does not support multiple locks, while the second does not support deferring and try locking. It does not seem to be a technical limitation, since ``` std::try_lock ``` supports multiple locks, correctly unlocking the ones it locked if any following failed. If I wanted to write ``` unique_multilock ``` myself, what are the dangers I should look out for? Is it recommended to ``` unlock ``` in the same or reverse order as ``` (try_)lock ``` ?",
    "author_id":4883,
    "publication_date":1754323479000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dominik Kaszewski",
    "author_reputation":2668.0,
    "tags":"c++, mutex, raii",
    "text_length":989,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":6072,
    "title":"ttk.Combobox selected text vanishes when focus is lost (clam theme, readonly)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725017\/ttk-combobox-selected-text-vanishes-when-focus-is-lost-clam-theme-readonly",
    "text":"I'm new to python and recently started coding a combobox using the \"clam\" UI theme. However, the selected text vanishes when focus is lost from the dropdown? It seems confined to \"clam\" as other UI themes such as \"alt\" work fine. For example, the initial screen shows Click the dropdown icon Now, re-click the dropdown icon Notice the default value ``` apples ``` has vanished? I was expecting it to be visible. Example Code ``` import tkinter as tk, tkinter.ttk as ttk root = tk.Tk() style = ttk.Style(root) style.theme_use(\"clam\") style.configure(\"TCombobox\", fieldbackground=\"white\", background=\"white\") style.map(\"TCombobox\", fieldbackground=[(\"readonly\", \"white\")], background=[(\"readonly\", \"white\")]) cb = ttk.Combobox(root, state=\"readonly\", values=(\"apples\", \"oranges\", \"bananas\")) cb.set(\"apples\") cb.pack(padx=40, pady=40) root.mainloop() ```",
    "author_id":5577,
    "publication_date":1754323515000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"vengy",
    "author_reputation":2389.0,
    "tags":"python, tkinter, combobox, tk-toolkit",
    "text_length":852,
    "title_length":77,
    "num_tags":4
  },
  {
    "id":6071,
    "title":"Primeng20 Component-level css styles is not getting evaluated",
    "link":"https:\/\/stackoverflow.com\/questions\/79725023\/primeng20-component-level-css-styles-is-not-getting-evaluated",
    "text":"We have Angular hybrid application where we are using PrimeNg20 with Angulra20. Tries with below dependencies: \"primeicons\": \"^7.0.0\", \"@primeuix\/themes\": \"1.2.3\", \"@primeuix\/utils\": \"0.6.1\", \"primeng\": \"^20.0.0\", \"typescript\": \"5.8.3\" With PrimeNG20, @primuix\/themes, @primenuix\/utils it is not applying default css styles properly, and while troubleshooting I have found that it required CSS styles is not getting evaluated properly like below.",
    "author_id":5576,
    "publication_date":1754323781000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Kanti",
    "author_reputation":1092.0,
    "tags":"primeng, angular20",
    "text_length":446,
    "title_length":61,
    "num_tags":2
  },
  {
    "id":6070,
    "title":"TFLite Python API vs C++ API Differences",
    "link":"https:\/\/stackoverflow.com\/questions\/79725027\/tflite-python-api-vs-c-api-differences",
    "text":"I am noticing big discrepancies between Python and C++ TFLite API. Using the same (quantized) model for object detection and the same preprocessing steps I am noticing big difference in the outputs. I do expect minor rounding errors, but nothing to this extent. For example, on one image the detections would be almost identical, while on others completely different. Attaching visualized outputs from python (2 detections, both at 90% acc) and cpp (3 detections). Versions TFLite version: 2.14.0 (compiled tflite lib from source) Ultralytics version: 8.3.171 Python 3.10 I've used a pretrained model yolo8 and converted it to tflite using ultralytics's convert call. So basically just these two lines. ``` from ultralytics import YOLO model = YOLO(\"yolov8n.pt\") # Load a pre-trained YOLOv8 model model.export(format='tflite', int8=True, imgsz=320, nms=True, data='coco8.yaml', fraction=1.0) ``` What I've checked so far : (Raw, pixle-wise) Inputs are almost identical - occasional minor rounding errors. I've checked that I am assigning data according to model's input type I've checked that I am taking the output from the output according to model's output type Quantization parameters and calculations are correct (Raw, element-wise) Outputs have large differences Raw pixel-wise difference between python and cpp input (after quantization): Raw elemet-wise difference between python and cpp output (before de-quantization): CPP Code I am using to test the cpp inference. ``` #include <opencv2\/opencv.hpp> #include <fstream> #include <string> #include <vector> #include <memory> #include <iostream> #include <filesystem> #include \"tensorflow\/lite\/interpreter.h\" #include \"tensorflow\/lite\/kernels\/register.h\" #include \"tensorflow\/lite\/model.h\" int main(){ tflite::ops::builtin::BuiltinOpResolver _resolver; std::unique_ptr<tflite::Interpreter> _interpreter; TfLiteTensor* _input_tensor = nullptr; TfLiteTensor* _output_tensor = nullptr; uint _input_height; uint _input_width; uint _input_channels; uint _input_type; float _input_scale; int _input_zero_point; std::unique_ptr<tflite::FlatBufferModel> _model = tflite::FlatBufferModel::BuildFromFile(\"\/src\/yolov8n_full_integer_quant.tflite\"); tflite::InterpreterBuilder builder(*_model, _resolver); builder(&_interpreter); _interpreter->AllocateTensors(); int input_index = _interpreter->inputs()[0]; int output_index = _interpreter->outputs()[0]; _input_tensor = _interpreter->tensor(input_index); _output_tensor = _interpreter->tensor(output_index); _input_width = _input_tensor->dims->data[2]; _input_height = _input_tensor->dims->data[1]; _input_channels = _input_tensor->dims->data[3]; _input_scale = _input_tensor->params.scale; _input_zero_point = _input_tensor->params.zero_point; float _output_scale = _output_tensor->params.scale; int _output_zero_point = _output_tensor->params.zero_point; uint input_data_type = _input_tensor->type; _input_type = (input_data_type == kTfLiteFloat32) ? CV_32FC3 : CV_8UC3; \/\/ tensor type to cv type if (input_data_type == kTfLiteFloat32) { std::cout << \"Input tensor type: float32\" << std::endl; } else { std::cout << \"Input tensor type: int8\" << std::endl; } std::cout << \"Input tensor type: \" << input_data_type << std::endl; std::vector<std::string> image_paths = { \"\/src\/zidane.jpg\", }; int icount = 0; for (const auto& image_path : image_paths) { std::cout << \"Processing image: \" << image_path << std::endl; cv::Mat input_frame = cv::imread(image_path, cv::IMREAD_COLOR); cv::cvtColor(input_frame, input_frame, cv::COLOR_BGR2RGB); \/\/ convert to RGB int original_height = input_frame.rows; int original_width = input_frame.cols; cv::Size new_shape = cv::Size(320, 320); float scale = std::min(static_cast<float>(new_shape.width) \/ original_width, static_cast<float>(new_shape.height) \/ original_height); int new_width = static_cast<int>(std::round(original_width * scale)); int new_height = static_cast<int>(std::round(original_height * scale)); \/\/ Compute padding float dw = (new_shape.width - new_width) \/ 2.0f; \/\/ width padding float dh = (new_shape.height - new_height) \/ 2.0f; \/\/ height padding cv::Mat resized_img; if (original_width != new_width || original_height != new_height) { cv::resize(input_frame, resized_img, cv::Size(new_width, new_height), 0, 0, cv::INTER_LINEAR); } else { resized_img = input_frame.clone(); } int top = static_cast<int>(std::round(dh - 0.1f)); int bottom = static_cast<int>(std::round(dh + 0.1f)); int left = static_cast<int>(std::round(dw - 0.1f)); int right = static_cast<int>(std::round(dw + 0.1f)); cv::Mat padded_frame; cv::copyMakeBorder(resized_img, padded_frame, top, bottom, left, right, cv::BORDER_CONSTANT, cv::Scalar(114, 114, 114)); TfLiteTensor* input_data = _interpreter->tensor(_interpreter->inputs()[0]); int8_t* image_data = input_data->data.int8; \/\/ Convert to the input type if (_input_type == CV_32FC3) { padded_frame.convertTo(padded_frame, CV_32FC3, 1.0 \/ 255.0); \/\/ normalize to [0, 1] } else { int idx = 0; for (int y = 0; y < padded_frame.rows; ++y) { for (int x = 0; x < padded_frame.cols; ++x) { for (int c = 0; c < 3; ++c) { int8_t pixel = padded_frame.at<cv::Vec3b>(y, x)[c]; float normalized = static_cast<float>(pixel) \/ 255.0f; float quant = normalized \/ _input_scale + _input_zero_point; int8_t q = static_cast<int8_t>(quant); image_data[idx++] = q; } } } } float pad_y_ratio = static_cast<float>(top) \/ padded_frame.rows; float pad_x_ratio = static_cast<float>(left) \/ padded_frame.cols; std::memcpy(image_data, padded_frame.ptr<int8_t>(0), padded_frame.total() * padded_frame.channels() * sizeof(int8_t)); if (_interpreter->Invoke() != kTfLiteOk) { std::cerr << \"Failed to invoke interpreter!\" << std::endl; exit(-1); } std::cout << _output_tensor->dims->size << std::endl; int batch = _output_tensor->dims->data[0]; int detections = _output_tensor->dims->data[1]; int results = _output_tensor->dims->data[2]; int8_t* output_data = _interpreter->tensor(_interpreter->outputs()[0])->data.int8; \/\/std::cout << \"Output data obtained \" << output_data << std::endl; uint output_data_type = _output_tensor->type; for (int i = 0; i < detections; i++) { \/\/ Each detection is a vector of 6 floats: [x1, y1, x2, y2, confidence, class_id] int offset = i * results; \/\/ Get scores and apply confidence threshold float x1 = (static_cast<int>(output_data[offset + 0]) - _output_zero_point) * _output_scale; float y1 = (static_cast<int>(output_data[offset + 1]) - _output_zero_point) * _output_scale; float x2 = (static_cast<int>(output_data[offset + 2]) - _output_zero_point) * _output_scale; float y2 = (static_cast<int>(output_data[offset + 3]) - _output_zero_point) * _output_scale; float score = (static_cast<int>(output_data[offset + 4]) - _output_zero_point) * _output_scale; float class_id_f = (static_cast<int>(output_data[offset + 5]) - _output_zero_point) * _output_scale; int cls_id = static_cast<int>(std::round(class_id_f)); if (score < 0.2) { continue; } \/\/ Adjust for padding x1 -= pad_x_ratio; y1 -= pad_y_ratio; x2 -= pad_x_ratio; y2 -= pad_y_ratio; \/\/ Adjust scaling float scale = static_cast<float>(std::max(input_frame.cols, input_frame.rows)); x1 *= scale; y1 *= scale; x2 *= scale; y2 *= scale; float w = x2 - x1; float h = y2 - y1; \/\/ Create bounding box cv::Rect box(static_cast<int>(x1), static_cast<int>(y1), static_cast<int>(w), static_cast<int>(h)); cv::Scalar color(0,255,0); \/\/ Green color for bounding box cv::rectangle(input_frame, box, color, 2); std::string label = \"Class \" + std::to_string(cls_id) + \": \" + std::to_string(score); int baseline = 0; cv::Size label_size = cv::getTextSize(label, cv::FONT_HERSHEY_SIMPLEX, 0.5, 1, &baseline); cv::Point label_origin(box.x, box.y - label_size.height - 5); cv::rectangle(input_frame, cv::Rect(label_origin, cv::Size(label_size.width, label_size.height + baseline)), color, cv::FILLED); cv::putText(input_frame, label, label_origin + cv::Point(0, label_size.height), cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 0, 0), 1); } cv::cvtColor(input_frame, input_frame, cv::COLOR_RGB2BGR); \/\/ convert back to BGR for saving \/\/ get image name std::string image_name = image_path.substr(image_path.find_last_of(\"\/\\\\\") + 1); icount++; \/\/ Save the output image cv::imwrite(\"\/src\/test_output\/\" + image_name, input_frame); } return 0; } ``` Compilation command: ``` g++ src\/infer_yolo.cpp -o out.so -I\/usr\/local\/tensorflow\/include -L\/usr\/local\/tensorflow\/lib -ltensorflowlite -I\/usr\/local\/include\/opencv4\/ -L\/usr\/local\/lib\/ -lopencv_core -lopencv_imgproc -lopencv_imgcodecs ``` The code is not the prettiest and I don't have better instructions to set up C++ environment than to follow the official tensorflow instructions in a docker container. Has anyone experienced anything similar or can see if I'm doing something wrong in the C++ code? EDIT 1 The comments about all the casts and floats\/doubles. I understand that could cause issues. But those are used just to calculate the necessary paddings for the input. After all the preprocessing and casts, the differences between python's input and c++ input are minor 4 The code that I derived my C++ version from is available at official ultralytics github here",
    "author_id":5575,
    "publication_date":1754323947000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Maja",
    "author_reputation":74.0,
    "tags":"c++, tensorflow, tflite",
    "text_length":9137,
    "title_length":40,
    "num_tags":3
  },
  {
    "id":6069,
    "title":"AWS Aurora MySQL table archive running slow for one table",
    "link":"https:\/\/stackoverflow.com\/questions\/79725028\/aws-aurora-mysql-table-archive-running-slow-for-one-table",
    "text":"I'm working on archiving a bunch of tables in an environment where archiving was never done, with some data going back 10 years. I've written a script to perform the work, which loops through the primary key (an autoincrement ``` bigint ``` ) n rows at a time, calling a procedure to archive the data to a separate table and then deleting that same data from the main table. I'm doing it in small batches to prevent any long term locking of the main tables. It also ``` sleep ``` s in between each loop iteration. Batch size and sleep time are configurable via a config file. On my test system, for this table, I'm using a batch size of 1000 and a sleep time of 0. Instance class is r7g.4xl. Most tables archive at several thousand rows per second, which is acceptable. But I have one table whose archiving is going very slowly; averaging under 550 rows\/sec. There is no other activity in the database (there are other archives running against other DBs in the cluster at the same time, but killing them didn't improve the performance of this one). Here's the table schema (the schema for the archive table is identical): ``` CREATE TABLE `inbox_item` ( `id` bigint NOT NULL AUTO_INCREMENT, `user_id` bigint NOT NULL, `template_id` bigint NOT NULL, `url` varchar(4000) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, `created_at` datetime NOT NULL, `hash` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, `parameters` varchar(4000) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `hash_uidx` (`hash`), KEY `template_id_idx` (`template_id`), KEY `user_id_created_at_idx` (`user_id`,`created_at`) ) ENGINE=InnoDB AUTO_INCREMENT=442872663 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ``` Note that while there is a large ``` varchar ``` , total column width is under 300 bytes. Here's the procedure that's being called: ``` CREATE DEFINER=`root`@`%` PROCEDURE `archive_inbox_item_proc`(IN pkmin bigint, IN pkmax bigint, IN querymax bigint) begin declare exit handler for sqlexception begin get diagnostics condition 1 @err = MYSQL_ERRNO, @msg = MESSAGE_TEXT; select -1; select concat('Error ', cast(@err as char), ': ',@msg) 'Error'; rollback; end; start transaction; insert ignore into `inbox`.`inbox_item_archive` select arctable.* from `inbox`.`inbox_item` as arctable where created_at <= date_sub(now(), interval 2 year) and arctable.`id` >= pkmin and arctable.`id` < querymax and arctable.`id` <= pkmax ; delete arctable from `inbox`.`inbox_item` as arctable where created_at <= date_sub(now(), interval 2 year) and arctable.`id` >= pkmin and arctable.`id` < querymax and arctable.`id` <= pkmax ; select row_count(); commit; end ``` ``` pkmin ``` is always the actual minimum pkey value. There are no foreign keys or triggers referencing the table. Here's the table status: ``` Name: inbox_item Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 340242753 Avg_row_length: 249 Data_length: 84797997056 Max_data_length: 0 Index_length: 128656244736 Data_free: 32300335104 Auto_increment: 442872663 Create_time: 2025-03-28 06:15:36 Update_time: 2025-08-04 14:01:37 Check_time: NULL Collation: utf8mb4_unicode_ci Checksum: NULL Create_options: Comment: ``` Any ideas on what's causing this to run so slow relative to other tables in other databases?",
    "author_id":5574,
    "publication_date":1754324009000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Swechsler",
    "author_reputation":121.0,
    "tags":"amazon-rds, amazon-aurora",
    "text_length":3335,
    "title_length":57,
    "num_tags":2
  },
  {
    "id":6068,
    "title":"keystone ImportError: ERROR: fail to load the dynamic library",
    "link":"https:\/\/stackoverflow.com\/questions\/79725033\/keystone-importerror-error-fail-to-load-the-dynamic-library",
    "text":"I'm trying to install keystone-engine==0.9.2 into a virtual environment on my M4 Mac (Apple Silicon) running macOS Sequoia 15.5. I'm also using capstone==5.0.0.post1. The installation succeeds, but the required dynamic libraries (like libkeystone.dylib) are missing, which causes runtime errors. Any idea why the dynamic libraries aren't being built or installed? How can I force it to compile correctly for Apple Silicon? I tried the following to ensure it's not using a cached or prebuilt binary: pip install keystone-engine==0.9.2 --no-cache-dir --force-reinstall --no-binary keystone-engine I also deleted the wheel from my local cache manually, but the result is the same error message. ``` using latest makefile for target android make: Nothing to be done for `events'. \/Users\/<user_name>\/venvs\/1fd8726b3398a06d29c459e3d723bd39ecce0992907bd320f3bdac07a44da014\/lib\/python3.9\/site-packages\/capstone\/__init__.py:327: UserWarning: pkg_resources is deprecated as an API. See https:\/\/setuptools.pypa.io\/en\/latest\/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81. ... import keystone ... raise ImportError(\"ERROR: fail to load the dynamic library.\") ImportError: ERROR: fail to load the dynamic library. ```",
    "author_id":5573,
    "publication_date":1754324394000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Keren Schneider",
    "author_reputation":1.0,
    "tags":"pip",
    "text_length":1305,
    "title_length":61,
    "num_tags":1
  },
  {
    "id":6067,
    "title":"fvcore multiple arguments forward modules?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725035\/fvcore-multiple-arguments-forward-modules",
    "text":"I have a model that take more than one arguments in forward. Recently I'm trying to query some informations my model by fvcore module in python, but I can't find any document for multiple forward arguments! and edited code tried to have multiple functions: ``` from fvcore.nn import FlopCountAnalysis, parameter_count_table def modelCount(model, input_tensor, *args, **kwargs): def _wrapped_forward(x): return model(x, *args, **kwargs) flops = FlopCountAnalysis(_wrapped_forward, input_tensor).total() params = parameter_count_table(model) return flops, params ``` but it does not help ... I still have error.",
    "author_id":5572,
    "publication_date":1754324434000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"H.M",
    "author_reputation":586.0,
    "tags":"python, torch",
    "text_length":609,
    "title_length":42,
    "num_tags":2
  },
  {
    "id":6066,
    "title":"Why can CrudRepository.findById() return a Hibernate proxy?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725040\/why-can-crudrepository-findbyid-return-a-hibernate-proxy",
    "text":"It seems that ``` findById() ``` and manual queries in Spring repositories can return Hibernate proxies instead of real objects if these objects have been \"loaded\" earlier as lazy fields of other objects. Such a behaviour seems a bit surprising to me. Is it intended? Is there some clarification in the documentation? It works so with ``` org.springframework.boot:spring-boot-starter-parent ``` ``` 3.3.13 ``` (my) and ``` 3.5.4 ``` (the newest at the moment). I reproduced the behaviour using an entity class which contains an identifier and a lazy link to an entity of the same class and a corresponding repository. ``` import jakarta.persistence.Entity; import jakarta.persistence.FetchType; import jakarta.persistence.Id; import jakarta.persistence.ManyToOne; @Entity public class ExampleEntity { @Id private Character id; @ManyToOne(fetch = FetchType.LAZY) private ExampleEntity parent; public ExampleEntity setId(Character id) { this.id = id; return this; } public ExampleEntity setParent(ExampleEntity parent) { this.parent = parent; return this; } } ``` ``` import org.springframework.data.jpa.repository.JpaRepository; public interface ExampleEntityRepository extends JpaRepository<ExampleEntity, Character> { } ``` The behaviour is reproduced in the following tests. Both of the tests create two entities identified A and B. The latter links the former. Test Access the entity A The class of ``` entityA ``` object The first time ``` ExampleEntity ``` (a real object) proxy The entity A has been represented with a proxy for the field ``` parent ``` of the entity B ``` HibernateProxy ``` ``` import org.assertj.core.api.Assertions; import org.hibernate.proxy.HibernateProxy; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.transaction.PlatformTransactionManager; import org.springframework.transaction.support.TransactionTemplate; @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.NONE) class ExampleEntityRepositoryTest { private final TransactionTemplate transactionTemplate; private final ExampleEntityRepository sut; @Autowired ExampleEntityRepositoryTest(PlatformTransactionManager platformTransactionManager, ExampleEntityRepository exampleEntityRepository) { this.transactionTemplate = new TransactionTemplate(platformTransactionManager); this.sut = exampleEntityRepository; } @Test void proxy() { try { transactionTemplate.executeWithoutResult(status -> { sut.save(new ExampleEntity().setId('A')); sut.save(new ExampleEntity().setId('B').setParent(new ExampleEntity().setId('A'))); }); ExampleEntity entityA = transactionTemplate.execute(status -> { sut.findById('B'); \/\/ Load an entity with a lazy link to the target entity. return sut.findById('A').orElse(null); }); Assertions .assertThat(entityA) .isNotNull() .isInstanceOf(HibernateProxy.class) .isInstanceOf(ExampleEntity.class); } finally { transactionTemplate.executeWithoutResult(status -> sut.deleteAll()); } } @Test void object() { try { transactionTemplate.executeWithoutResult(status -> { sut.save(new ExampleEntity().setId('A')); sut.save(new ExampleEntity().setId('B').setParent(new ExampleEntity().setId('A'))); }); ExampleEntity entityA = transactionTemplate.execute(status -> { \/\/ Load the target entity. return sut.findById('A').orElse(null); }); Assertions .assertThat(entityA) .isNotNull() .isInstanceOf(ExampleEntity.class) .isNotInstanceOfAny(HibernateProxy.class); } finally { transactionTemplate.executeWithoutResult(status -> sut.deleteAll()); } } } ```",
    "author_id":5571,
    "publication_date":1754324664000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Yury Shafran",
    "author_reputation":51.0,
    "tags":"java, spring, hibernate, jpa, spring-data-jpa",
    "text_length":3599,
    "title_length":59,
    "num_tags":5
  },
  {
    "id":6065,
    "title":"Why does Android 15 Spinner show when items are loaded in designview but not programmetically?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725042\/why-does-android-15-spinner-show-when-items-are-loaded-in-designview-but-not-pro",
    "text":"I have an android 15 tablet. I want to create a spinner. The spinner is in main activity, but I want to access it via a fragment I had used the following code snippet in the fragments ``` onCreateView ``` method: ``` dd1 = (requireActivity() as MainActivity).spinner ArrayAdapter.createFromResource( requireContext(), R.array.videoEntries, \/\/ <-- this exists android.R.layout.simple_spinner_item ).also { adapter -> \/\/ Specify the layout to use when the list of choices appears. adapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item) \/\/ Apply the adapter to the spinner. dd1.adapter = adapter Log.d(\"Monsoon SPINNER\", \"spinner adapter ready\") } return view ``` Unfortunately, the spinner won't show anything, if the down arrow was clicked. I have seen this question and I applied the fix ``` view.findViewById(R.id.spinner) ``` , but there was no improvement. Querying the height\/width etc of the spinner would return 0 - even though, in the layout xml, the spinner dropdown width is explicitly set to 160dp. I tried to assign the array in ``` fragment.OnAttach ``` method, as well as ``` mainActivity.onCreateview ``` method. Did not work, dropwdown still won't show. Then I tried ``` dd1.post { try { dd1.performClick() \/\/ Now safe } catch (e: Exception) { Log.e(\"SPINNER\", \"Failed to open dropdown: ${e.message}\") } } ``` still without success. Note that If I query the spinner items after setting the adapter like so: ``` if (adapter != null) { val stringBuilder = StringBuilder(\"Spinner Content:\\n\") for (i in 0 until adapter.count) { val item = adapter.getItem(i) stringBuilder.append(\"Index $i: $item\\n\") } \/\/ Print to Logcat println(stringBuilder.toString()) \/\/ or Log.d(\"SpinnerContent\", stringBuilder.toString()) \/\/ Optionally, show as a Toast (for debugging on device) Toast.makeText(this, stringBuilder.toString(), Toast.LENGTH_LONG).show() } ``` I would see the items in all cases. I followed the tutorial here : still no drop down. Note that spinner mode is set as ``` android:spinnerMode=\"dropdown\" ``` in the layout xml. Finally, I solved it, by assigning the array of entries directly in the design view I hopw I have given the relevant parts of the code. Question: Why is it, that assigning the array in design view works, but programmetically it does not? I have looked for ``` kotlin spinner dropdown android 15 ``` , and ``` kotlin spinner dropdown android 10+ ``` in google. I could not find anything to explain this behavior. Note that the app is to control a DJI drone, and the spinner would work, even when set programmatically without any issue, on DJI remote controls that run android 10. On samsung tablet, I had this issue. What am I doing wrong?",
    "author_id":5570,
    "publication_date":1754324788000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sean",
    "author_reputation":881.0,
    "tags":"android, kotlin, android-spinner",
    "text_length":2699,
    "title_length":94,
    "num_tags":3
  },
  {
    "id":6064,
    "title":"Can&#39;t install Sdk From Android studio",
    "link":"https:\/\/stackoverflow.com\/questions\/79725046\/cant-install-sdk-from-android-studio",
    "text":"I once had Android Studio in Windows 11 and I uninstalled it. Now I installed it again It is installing correctly but then I the errors or the problems that are listed in the pictures below: What's the reason behind being unable to download the SDK and how to solve this problem",
    "author_id":5569,
    "publication_date":1754325033000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Peter Tom",
    "author_reputation":63.0,
    "tags":"android, android-studio",
    "text_length":278,
    "title_length":41,
    "num_tags":2
  },
  {
    "id":6063,
    "title":"How to set permissions for AWS automation documents?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725048\/how-to-set-permissions-for-aws-automation-documents",
    "text":"I use AWS automation to stop all my machines. The document for automation is set up with Terraform. I want to restrict the usage of the document to my developer group. But I don't know, how to set the right ARNs in the policy. Some permissions need the ARN of the document, some the ARN of the automation-definition and some the ARN of the execution. How to get all three ARNs? Code ``` terraform { required_providers { aws = { source = \"hashicorp\/aws\" version = \"~> 5.72\" } } } provider \"aws\" { default_tags { tags = { project = \"myproject\" } } } resource \"aws_ssm_document\" \"stop\" { name = \"stop\" document_type = \"Automation\" document_format = \"YAML\" content = <<DOC schemaVersion: '0.3' description: 'Stop machine.' mainSteps: - description: 'Stop machine.' name: stop action: aws:changeInstanceState isEnd: true inputs: DesiredState: stopped InstanceIds: - ${aws_instance.mymachine.id} DOC } resource \"aws_iam_group\" \"developers\" { name = \"developers\" } resource \"aws_iam_group_policy\" \"developer\" { name = \"developer\" group = aws_iam_group.developers.name policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Action = [ \"ec2:StopInstances\" ] Effect = \"Allow\" Resource = [ aws_instance.mymachine.arn ] }, { Action = [ \"ssm:ListDocuments\", ] Effect = \"Allow\" Resource = [ \"*\" ] }, { Action = [ # automation-definition \"ssm:StartAutomationExecution\", # document \"ssm:ListDocumentVersions\", \"ssm:DescribeDocument\", \"ssm:GetDocument\", # execution \"ssm:GetAutomationExecution\", \"ssm:DescribeAutomationStepExecutions\", \"ssm:DescribeAutomationExecutions\" ] Effect = \"Allow\" Resource = [ \"${aws_ssm_document.stop.arn}\" ] } ] }) } ``` Research Before version 5.72.0 the ``` aws_ssm_document ``` resource returned the ARN of the document. With version 5.72.0 it is returning the ARN of the automation-definition, see [Bug]: aws_ssm_document constructs document ARN regardless of type . But there is no way to get both ARNs. I read ResourceTag condition key , but automation executions have no automatically added tags. I read Using wildcards in resource ARNs , but the ARN of an execution doesn't contain the name of the document, so I can't use a wildcard for the ID part. I use default tags of Terraform AWS provider , so I ran ``` aws ssm list-tags-for-resource --resource-type \"Automation\" --resource-id \"dd43ea60-5995-4d34-9603-caf839aae102\" ``` but the result was an empty array. Question How to set permissions for AWS automation documents with Terraform?",
    "author_id":5568,
    "publication_date":1754325132000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dur",
    "author_reputation":17256.0,
    "tags":"terraform, amazon-web-services, terraform-provider-aws, aws-ssm",
    "text_length":2464,
    "title_length":52,
    "num_tags":4
  },
  {
    "id":6062,
    "title":"Ansible script is not generating a text document",
    "link":"https:\/\/stackoverflow.com\/questions\/79725049\/ansible-script-is-not-generating-a-text-document",
    "text":"I wrote a script to try and create a .txt document with a list of local user accounts across the enterprise. I'm not getting any errors, but it doesn't seem to create the document at the end with the information. The script is below: ``` --- - name: Gather local user information and create a report hosts: all # Target all hosts in your inventory gather_facts: no tasks: - name: Get user information ansible.builtin.shell: | getent passwd | awk -F: '{print $1\",\"$3\",\"$4\",\"$6\",\"$7}' register: user_data - name: Construct per-host user report ansible.builtin.set_fact: host_user_report: | ### Users on {{ ansible_hostname }} ({{ inventory_hostname }}) ### Username,UID,GID,Home Directory,Shell {{ user_data.stdout }} - name: Append per-host report to a single file on the control node ansible.builtin.copy: content: \"{{ host_user_report }}\\n\\n\" dest: \"\/tmp\/ansible_user_report.txt\" mode: '0644' delegate_to: localhost run_once: true ``` I've changed the directory a few times, and even pre-made the document, and I keep getting fails and it doesn't generate the document.",
    "author_id":5567,
    "publication_date":1754325227000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Will Burnside",
    "author_reputation":47.0,
    "tags":"automation, linux, ansible",
    "text_length":1070,
    "title_length":48,
    "num_tags":3
  },
  {
    "id":6061,
    "title":"RuntimeError in torch.cat during VACE-Wan2.1 inference: mask and video tensor shape mismatch",
    "link":"https:\/\/stackoverflow.com\/questions\/79725054\/runtimeerror-in-torch-cat-during-vace-wan2-1-inference-mask-and-video-tensor-sh",
    "text":"I'm using the Wan2.1-VACE video generation model, and during inference I encountered a ``` RuntimeError ``` related to mismatched tensor shapes in a ``` torch.cat ``` operation inside the ``` vace_latent() ``` function. From the debug output: ``` src_video tensor size before generation: torch.Size([3, 1, 848, 464]) src_mask tensor size before generation: torch.Size([1, 1, 848, 464]) ``` This causes the following error when the code tries to concatenate tensors from video and mask: ``` RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 3 but got size 2 for tensor number 1 in the list. ``` The error comes from this part of the code: ``` return [torch.cat([zz, mm], dim=0) for zz, mm in zip(z, m)] ``` It seems that the batch\/frame count along dim=0 doesn't match between video and mask, but I'm not sure how to fix this properly or what the expected input shape should be. To fix the shape mismatch, I tried manually expanding the mask tensor’s channel dimension to match the video input. Here’s what I did: ``` if src_mask is not None and src_mask[0].shape[1] == 1: src_mask[0] = src_mask[0].expand(-1, 3, -1, -1) print(f\"DEBUG: src_mask channel expanded to 3: {src_mask[0].shape}\") ``` I expected this would make the mask and video inputs have matching channel sizes (both with shape ``` [3, 1, H, W] ``` ), so the model could concatenate them without error. However, I still got the same ``` RuntimeError ``` during ``` torch.cat ``` , complaining about size mismatch along dimension 0. This is how I’m running the inference: ``` python vace\/vace_wan_inference.py \\ --src_video assets\/videos\/src_video_rgb.mp4 \\ --src_mask assets\/videos\/src_mask_rgb.mp4 \\ --src_ref_images \"assets\/ref\/hamer_61.png,assets\/ref\/pose_61.png\" \\ --prompt \"重建一隻符合骨架姿勢與結構的真實人手，寫實風格，聚焦手部細節\" ``` It seems like either the number of frames or some dimension is still misaligned, but I’m unsure whether I should be expanding on batch\/frame dimension or just channel-wise.",
    "author_id":5566,
    "publication_date":1754325409000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"范姜伯軒",
    "author_reputation":59.0,
    "tags":"python, machine-learning, pytorch, computer-vision, deep-learning",
    "text_length":1979,
    "title_length":92,
    "num_tags":5
  },
  {
    "id":6060,
    "title":"Reduce file size of multiple ggplotly graphs",
    "link":"https:\/\/stackoverflow.com\/questions\/79725055\/reduce-file-size-of-multiple-ggplotly-graphs",
    "text":"I'm creating a document with a lot of ggplotly objects. This returns in a big file size. Apparently we could use the ``` partial_bundle ``` function to reduce the file size of plotly object. Unfortunately when I want to use this function it returns an error: ``` Error in curl::curl_download(paste0(\"https:\/\/cdn.plot.ly\/\", bundle_script), : HTTP response code said error [cdn.plot.ly]: The requested URL returned error: 403 ``` This probably happens because I'm working on a VM. Luckily we have this wonder answer , but I'm stuck at the part where we want to download the files: ``` library(plotly) library(ggplot2) p <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() plt <- ggplotly(p) %>% partial_bundle(local = FALSE) plt$dependencies[[5]] plt$dependencies[[5]]$src$href link <- paste0(plt$dependencies[[5]]$src$href, \"\/\", plt$dependencies[[5]]$script) download.file(url = link, destfile = paste0(\"~\/\", \"file.txt\")) ``` Output: ``` Error in download.file(url = link, destfile = paste0(\"~\/\", \"file.txt\")) : cannot open URL 'https:\/\/cdn.plot.ly\/plotly-basic-2.11.1.min.js' In addition: Warning message: In download.file(url = link, destfile = paste0(\"~\/\", \"file.txt\")) : URL 'https:\/\/cdn.plot.ly\/plotly-basic-2.11.1.min.js': status was 'HTTP response code said error' ``` This probably happens because I'm working on a VM without internet connection. So I was wondering if anyone knows how to reduce the file sizes of ggplotly object without using internet?",
    "author_id":5565,
    "publication_date":1754325440000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Quinten",
    "author_reputation":42500.0,
    "tags":"r, plotly, ggplotly",
    "text_length":1479,
    "title_length":44,
    "num_tags":3
  },
  {
    "id":6059,
    "title":"External members of MS Outlook &quot;Modern Groups&quot; not receiving emails",
    "link":"https:\/\/stackoverflow.com\/questions\/79725060\/external-members-of-ms-outlook-modern-groups-not-receiving-emails",
    "text":"I have a \"Modern Group\" set up in MS Outlook for my company, and we have internal and external members. Whenever an internal member sends an email to the group, it forwards to all group members, internal and external. But when an external member tries to email the group, it forwards to all internal group members, but not to the external group members. We've tried multiple settings to fix this, but can't figure it out. Followed advice in google searches and ChatGPT and still no luck. Any ideas?",
    "author_id":5564,
    "publication_date":1754325535000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"doxguy",
    "author_reputation":193.0,
    "tags":"outlook",
    "text_length":498,
    "title_length":77,
    "num_tags":1
  },
  {
    "id":6058,
    "title":"How to can declare custom group-open variant for hamburger menu?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725062\/how-to-can-declare-custom-group-open-variant-for-hamburger-menu",
    "text":"I am trying to create a hamburger menu that upon clicking the top and bottom lines rotate to indicate open but it wont work for some reason. index.html ``` <div id=\"mobile-menu-button\" class=\"group\"> <div class=\"bg-zinc-200 rounded-full h-1 w-8 group-open:rotate-45 group-open:top-2\"><\/div> <div class=\"bg-zinc-200 rounded-full h-1 w-8 my-1 opacity-100 group-open:opacity-0\"><\/div> <div class=\"transition-all bg-zinc-200 rounded-full h-1 w-8 group-open:-rotate-45 group-open:bottom-2\"><\/div> <\/div> ``` ``` @import \"tailwindcss\"; @theme { --font-quicksand: \"Quicksand\", sans-serif; } @layer base { body { font-family: var(--font-quicksand); } } @layer components { .menu-item { @apply flex relative h-full items-center p-4 cursor-pointer text-pink-200 hover:text-zinc-200 hover:bg-white\/10 transition-colors ease-in-out; } } @variant group-open { &: is(: where(.group): is(.open, [open], : popover-open, : open) *); @slot; } ```",
    "author_id":5563,
    "publication_date":1754325647000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"F Adam",
    "author_reputation":21.0,
    "tags":"tailwind-css, tailwind-css-4",
    "text_length":928,
    "title_length":64,
    "num_tags":2
  },
  {
    "id":6057,
    "title":"`ERROR: [VRFC 10-2063] Module &lt;my_task&gt; not found while processing module instance &lt;&#39;Undefined&#39;&gt;` when using verilog task",
    "link":"https:\/\/stackoverflow.com\/questions\/79725063\/error-vrfc-10-2063-module-my-task-not-found-while-processing-module-instan",
    "text":"``` `timescale 1ns\/1ps module m_top ( input GCLK, input [7:0] i_in1, output o_out1, output reg o_out2, inout io_data, output reg[7:0]yy ); assign o_out1=5; initial o_out2=6; task my_task; input a,b; output c; assign c=a+b; endtask my_task(o_out1,o_out2,yy); endmodule ``` I got error when running verilog script: ``` ERROR: [VRFC 10-2063] Module <my_task> not found while processing module instance <'Undefined'> ERROR: [XSIM 43-3322] Static elaboration of top level Verilog design unit(s) in library work failed. ``` Where is the problem? Thanks in advance.",
    "author_id":4545,
    "publication_date":1754325678000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"kittygirl",
    "author_reputation":2519.0,
    "tags":"verilog, vivado",
    "text_length":558,
    "title_length":141,
    "num_tags":2
  },
  {
    "id":6056,
    "title":"SQL update query where table contains NVARCHAR(MAX) held in LOB",
    "link":"https:\/\/stackoverflow.com\/questions\/79725064\/sql-update-query-where-table-contains-nvarcharmax-held-in-lob",
    "text":"I have a SQL script that updates data in various tables. It used to run quickly but is now slowing down. The tables being updated have ``` NVARCHAR(MAX) ``` columns and these hold big-ish data that's off-row and held in the LOB. I've been advised to do this in batches and have the following example SQL: ``` WHILE 1 = 1 BEGIN ;WITH cte AS ( SELECT TOP 5000 * FROM [dbo].[MyTable] WITH (ROWLOCK, READPAST) WHERE [Created] < @cutoff AND [IsExpired] = 0 ORDER BY [Created] ) UPDATE cte SET [MyNvarchMaxField] = NULL, [IsExpired] = 1 SET @batchCount = @@RowCount; SET @rowsAffected = @rowsAffected + @batchCount; IF @batchCount = 0 BREAK; WAITFOR DELAY '00:00:00.200'; END ``` I asked AI if ``` select * ``` in the ``` cte ``` is going to cause performance issues and it said that it will do, so suggested I changed it to: ``` WHILE 1 = 1 BEGIN ;WITH cte AS ( SELECT TOP 5000 [Id] FROM [dbo].[MyTable] WITH (ROWLOCK, READPAST) WHERE [Created] < @cutoff AND [IsExpired] = 0 ORDER BY [Id] ) UPDATE cte SET [MyNvarchMaxField] = NULL, [IsExpired] = 1 FROM [dbo].[MyTable] JOIN cte ON MyTable.ID = cte.ID SET @batchCount = @@RowCount; SET @rowsAffected = @rowsAffected + @batchCount; IF @batchCount = 0 BREAK; WAITFOR DELAY '00:00:00.200'; END ``` And AI then said that doing the JOIN was slowing things down and to change it to ``` select * ``` . I don't have a database that's large enough to study realistic execution plans, so thought I'd ask a knowledgeable human what's the best option. The details from the execution plan (removing rows that are insignificant) are: The non-zero parts are here: And to give an idea of where the resouce bottleneck was, these are some metrics from our Dev-tier DB in Azure:",
    "author_id":5562,
    "publication_date":1754325683000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"DrGriff",
    "author_reputation":4916.0,
    "tags":"sql-server, query-optimization",
    "text_length":1704,
    "title_length":63,
    "num_tags":2
  },
  {
    "id":6055,
    "title":"How to use the default value of a param when import a rule in Snakemake?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725066\/how-to-use-the-default-value-of-a-param-when-import-a-rule-in-snakemake",
    "text":"I am trying to generalize my snakemake workflow, but I see a weird behavior, which I am not sure if I am doing something wrong or if it is by default. I created a file ``` analysis.smk ``` rules and the evariable I tried to make a generic rule, to import it in many of my snakemake workflows, so I have, for instance in my ``` analysis.smk ``` rule: ``` rule analysis: output: test.root input: input_test.root params: variable: \"SvB\" rebin: 10 shell: `python run_analysis.py {params.variable} {params.rebin}` ``` In another Snakefile, I am importing this rule and using it as: ``` module analysis: snakefile: \"analysis.smk\" config: config use rule analysis from analysis as run_analysis with: output: f\"{config['output']}\/hist.root\" input: f\"{config['output']}\/input_hist.root\" params: rebin: 20 ``` And when I run my workflow, I am getting errors like: AttributeError: 'Params' object has no attribute 'variable', when formatting the following: Because I didn't define the ``` variable ``` . Is this expected? I would just like to change the value of some params, and use the default in others. I am using snakemake version 8.27.1",
    "author_id":5561,
    "publication_date":1754325700000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Alejandro",
    "author_reputation":5276.0,
    "tags":"python, snakemake",
    "text_length":1131,
    "title_length":72,
    "num_tags":2
  },
  {
    "id":6054,
    "title":"RMarkdown - flextable paging in pdf",
    "link":"https:\/\/stackoverflow.com\/questions\/79725069\/rmarkdown-flextable-paging-in-pdf",
    "text":"I'm having trouble paginating a flextable table when publishing it in an RMarkdown document. The table has some merged columns. In the reproducible example, this is column 1. In the ``` sekwencja ``` variable, I have numbers assigning individual rows to groups. In the ``` list_of_rows_in_group ``` variable, I have a list of rows in each group in each element (in the reprex, this is the same number of rows, but in the real-world problem, the number of rows in each group is different). The rows are actually merged (I added borders around each table cell for readability). Unfortunately, the table's page division isn't as expected. I want each merged cell to be unbroken; if the next one doesn't fit, it's moved to the next page). ``` --- title: test output: pdf_document: latex_engine: xelatex papersize: a4 --- ```{r include=FALSE} library(tidyverse) library(flextable) ``` ```{r echo=FALSE} test_tb <- tibble(a = rep(LETTERS, each = 10)) %>% mutate(b = row_number()) # create a dummy data sekwencja <- seq_along(LETTERS) %>% rep(each = 10) # vector of groups for merging cells list_of_rows_in_group <- vector(mode = \"list\", length = max(sekwencja)) for (i in seq_len(max(sekwencja))) { list_of_rows_in_group[[i]] <- which(sekwencja == i) } test_ft <- test_tb %>% regulartable() %>% flextable::border_remove() %>% # adding border for visibility flextable::border_outer(part=\"all\", border = officer::fp_border(color=\"black\", width = 1)) %>% flextable::border_inner_h(part=\"all\", border = officer::fp_border(color=\"black\", width = 1)) %>% flextable::border_inner_v(part=\"all\", border = officer::fp_border(color=\"black\", width = 1)) for (i in seq_len(max(sekwencja))) { test_ft <- test_ft %>% # merging some columns merge_at(i = list_of_rows_in_group[[i]], j = 1, part = \"body\") } test_ft2 <- test_ft %>% # paginating doesn't work paginate(init = NULL, hdr_ftr = TRUE, group = sekwencja, group_def = c(\"rle\") ) test_ft2 ``` ``` I have the ``` sekwencja ``` and ``` list_of_rows_in_group ``` variables from earlier calculations. I want to use the ``` flextable ``` package because the table is quite complexly formatted (merged cells, rotated text, specific column width, specific font). The first page of the document with the incorrectly divided table is shown below:",
    "author_id":5560,
    "publication_date":1754325899000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"wacekk",
    "author_reputation":23.0,
    "tags":"r, r-markdown, r-flextable",
    "text_length":2271,
    "title_length":35,
    "num_tags":3
  },
  {
    "id":6053,
    "title":"Enable microprofile TomEE Plume 10.0.1 using tomee.xml",
    "link":"https:\/\/stackoverflow.com\/questions\/79725071\/enable-microprofile-tomee-plume-10-0-1-using-tomee-xml",
    "text":"I have an existing jakartaEE application I'm trying to port to TomEE. I'm getting stuck however since the deployment is failing on injecting ``` MetricRegistry ``` (no default provider) and if I temporarily disable that, ``` ConfigProperty ``` (no default provider). I tried Tomee+Microprofile UnsatisfiedResolutionException and this seemed to work, but I would prefer this solution in the tomee.xml Is this possible? https:\/\/javanexus.com\/blog\/troubleshooting-microprofile-configurations-apache-tomee seems to indicate that it is, but they do not give any (much needed) examples. Can you guys help me out?",
    "author_id":5559,
    "publication_date":1754325972000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"RInverid",
    "author_reputation":13.0,
    "tags":"apache-tomee, microprofile",
    "text_length":606,
    "title_length":54,
    "num_tags":2
  },
  {
    "id":6052,
    "title":"Folder Icons Displaying Question Marks in Android Studio",
    "link":"https:\/\/stackoverflow.com\/questions\/79725072\/folder-icons-displaying-question-marks-in-android-studio",
    "text":"I'm using Android Studio to develop a Flutter app, and I recently came back to it after a long time. Today, I updated Android Studio and created a new Flutter project. However, I noticed that some folder icons are showing a question mark (❓) symbol. I also checked some of my older projects that didn’t have this issue before, and now they’re showing the same folder icon problem. I’ve committed all the files and confirmed that the emulator is working fine, but the folder icons still show question marks. Is there a way to fix this? What does it mean? Thank you.",
    "author_id":5558,
    "publication_date":1754326003000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Christy_Vicky",
    "author_reputation":1.0,
    "tags":"flutter, android-studio",
    "text_length":564,
    "title_length":56,
    "num_tags":2
  },
  {
    "id":6051,
    "title":"BIOS int 0x13 sector read works, but jump to loaded second stage code at 0x0800:0x0000 does not transfer control",
    "link":"https:\/\/stackoverflow.com\/questions\/79725074\/bios-int-0x13-sector-read-works-but-jump-to-loaded-second-stage-code-at-0x0800",
    "text":"I’m writing a simple bootloader that uses BIOS interrupt int 0x13 to read a second-stage loader from disk and then jump to it. ``` ; boot\\boot.asm [bits 16] [org 0x7C00] jmp strict short start nop OEM_LABEL db \"DAYS0.1 \" ; BIOS Parameter Block (BPB) bpb_bytes_per_sector dw 512 ; 512 bytes per sector bpb_sector_per_cluster db 1 ; 1 sector per cluster bpb_reserved_sectors dw 1 ; 1 reserved sector (boot sector) bpb_fat_count db 2 ; 2 FAT tables (1 default, 1 reserved) bpb_root_entries_count dw 512 ; Number of entries in root directory bpb_total_sectors_small dw 32768 ; 16 * 1024*1024 (16MB)\/512 = 32768 ; sectors bpb_media_descriptor dw 0xf8 ; 0xF8 = fixed disk(hard disk) bpb_sectors_per_fat dw 128 ; 1 FAT-table = 128 sectors bpb_sectors_per_track dw 32 ; dummy geometry for virtual disk bpb_head_count dw 16 ; dummy geometry for virtual disk bpb_hidden_sectors dd 0 ; no hidden sectors before this volume bpb_total_sectors_large dd 0 ; 0 = use bpb_total_sectors_small ; Extended Boot Record (FAT16) ebr_drive_number db 0 ; BIOS drive number: 0x80 = hard disk ; Set by bootloader using DL register ebr_reserved db 0 ; Reserved byte, must be zero ebr_boot_signature db 0x29 ; Extended boot signature (must be 0x29) ; Indicates that the following fields are ; valid: ; ebr_volume_id, ebr_volume_label, ; ebr_filesystem_type ebr_volume_id dd 0x7E73B1BE ; Volume serial number ; (any 32-bit value, usually random) ; Used by the OS ebr_volume_label db \"DAYS_OS \" ; Volume label (must be 11 bytes, ; padded with spaces) ebr_filesystem_type db \"FAT16 \" ; File system type label (8 bytes, padded ; with spaces) ; Calculated constants from BPB bpb_root_dir_sectors equ (512 * 32) \/ 512 bpb_first_root_sector equ 1 + (2 * 128) bpb_first_data_sector equ bpb_first_root_sector + bpb_root_dir_sectors ; Bootloader start start: cmp dl, 0x80 ; Compare BIOS-loaded boot drive number (in DL) with 0x80 je main ; Jump to main code if dl = 0x80 (correct drive device) ; Error: Booted from unexpected device mov si, wrong_drive_error_msg call puts jmp halt main: cli xor ax, ax mov ds, ax mov es, ax mov ss, ax ; Set Stack Segment (SS) to 0x0000 mov sp, 0x7c00 ; Set Stack Pointer (SP) to 0x7c00 mov ax, cs ; Load current Code Segment (CS) into AX mov si, no_error_msg call puts call load_second_stage ; Load second stage boot sector and jump to in load_second_stage: ; CH = cylinder number mov ch, 0x00 ; CL = sector number mov cl, 0x02 ; DH = head number mov dh, 0x09 ; DL already contain the boot drive number (0x80 for HDD), ; so no need to modify it unless you're switching drives. mov ax, 0x0800 ; Set ES:BX as destination memory address for sector mov es, ax ; Physical address = ES * 16 + BX = xor bx, bx ; 0x0800 * 16 + 0x0000 = 0x8000 mov ah, 0x02 ; AH = 0x02 -> BIOS function: Read sector mov al, 0x01 ; AL = number of sectors to read (1 sector) int 0x13 ; BIOS disk interrupt - reads sector into ES:BX ; If Carry Flag (CF) set, jump to .disk_read_error label ; (BIOS sets flag (CF) if reading with \"int 0x13\" failed jc .disk_read_error ; Success: jump to the entry point of the second loading stage jmp 0x0800:0x0000 .disk_read_error: mov si, disk_error_msg call puts jmp halt ; Function puts - prints a null-terminated string puts: push si push ax .loop: lodsb or al, al jz .done mov ah, 0x0e mov bh, 0 int 0x10 jmp .loop .done: pop ax pop si ret halt: hlt jmp halt ; data wrong_drive_error_msg db \"error: Booted from unexpected device\", 0 disk_error_msg db \"error: Disk read error\", 0 no_error_msg db \"Booted coorectly\", 0 new_line db 0x0d, 0x0a, 0 times 510 - ($ - $$) db 0 dw 0xAA55 ``` ``` ; loader\\loader.asm [org 0x0000] [bits 16] start: mov ax, 0x0800 mov ds, ax mov es, ax ; stack mov ax, 0x0900 mov ss, ax mov sp, 0x9200 ; print 'S' mov ah, 0x0E mov al, 'S' int 0x10 .loop jmp .loop times 512 - ($ - $$) db 0 ``` ``` ; Makefile BOOT = build\/boot.bin KERNEL = build\/kernel.bin LOADER = build\/loader.bin IMAGE = os.img NASM = nasm DD = dd MTOOLS = mcopy all: $(IMAGE) # creating build directory if it's not exists build: mkdir -p build # building boot.asm $(BOOT): boot\/boot.asm | build $(NASM) -f bin boot\/boot.asm -o $(BOOT) # building kernel.asm #$(KERNEL): kernel\/kernel.asm | build # $(NASM) -f bin kernel\/kernel.asm -o $(KERNEL) #building loader.asm $(LOADER): loader\/loader.asm | build $(NASM) -f bin loader\/loader.asm -o $(LOADER) # creating FAT16-image with boot-loader and file $(IMAGE): $(BOOT) $(LOADER) # 1. creating empty file with 16MB size $(DD) if=\/dev\/zero of=$(IMAGE) bs=1M count=16 # 2. formating image like FAT16 not touching first sector mkfs.fat -F 16 -n DAYS_OS $(IMAGE) # 3. Loads boot-loader sector $(DD) if=$(BOOT) of=$(IMAGE) bs=512 count=1 conv=notrunc # 4. Copying loader into the image $(DD) if=$(LOADER) of=$(IMAGE) bs=512 seek=289 count=1 conv=notrunc run: qemu-system-x86_64 -drive format=raw,file=$(IMAGE) clean: rm -rf build $(IMAGE) ``` ``` ; hexdump os.img 00000000 eb 3d 90 44 41 59 53 30 2e 31 20 00 02 01 01 00 |.=.DAYS0.1 .....| 00000010 02 00 02 00 80 f8 00 80 00 20 00 10 00 00 00 00 |......... ......| 00000020 00 00 00 00 00 00 00 29 be b1 73 7e 44 41 59 53 |.......)..s~DAYS| 00000030 5f 4f 53 20 20 20 20 46 41 54 31 36 20 20 20 80 |_OS FAT16 .| 00000040 fa 80 74 08 be 9a 7c e8 3b 00 eb 4b fa 31 c0 8e |..t...|.;..K.1..| 00000050 d8 8e c0 8e d0 bc 00 7c 8c c8 be d6 7c e8 25 00 |.......|....|.%.| 00000060 e8 00 00 b5 00 b1 02 b6 09 b8 00 08 8e c0 31 db |..............1.| 00000070 b4 02 b0 01 cd 13 72 05 ea 00 00 00 08 be bf 7c |......r........|| 00000080 e8 02 00 eb 12 56 50 ac 08 c0 74 08 b4 0e b7 00 |.....VP...t.....| 00000090 cd 10 eb f3 58 5e c3 f4 eb fd 65 72 72 6f 72 3a |....X^....error:| 000000a0 20 42 6f 6f 74 65 64 20 66 72 6f 6d 20 75 6e 65 | Booted from une| 000000b0 78 70 65 63 74 65 64 20 64 65 76 69 63 65 00 65 |xpected device.e| 000000c0 72 72 6f 72 3a 20 44 69 73 6b 20 72 65 61 64 20 |rror: Disk read | 000000d0 65 72 72 6f 72 00 42 6f 6f 74 65 64 20 63 6f 6f |error.Booted coo| 000000e0 72 65 63 74 6c 79 00 0d 0a 00 00 00 00 00 00 00 |rectly..........| 000000f0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 000001f0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 55 aa |..............U.| 00000200 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00000800 f8 ff ff ff 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00000810 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00004800 f8 ff ff ff 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00004810 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00008800 44 41 59 53 5f 4f 53 20 20 20 20 08 00 00 ea 82 |DAYS_OS .....| 00008810 04 5b 04 5b 00 00 ea 82 04 5b 00 00 00 00 00 00 |.[.[.....[......| 00008820 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00024200 b4 0e b0 53 cd 10 b8 00 08 8e d8 8e c0 b8 00 09 |...S............| 00024210 8e d0 bc 00 92 b4 0e b0 53 cd 10 fa f4 eb fe 00 |........S.......| 00024220 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 01000000 ``` The first stage bootloader loads successfully from sector 1, and I confirmed it works — reading the sector with int 0x13 succeeds and I can output a character via int 0x10. The second stage loader should be loaded from sector 289 (calculated using disk geometry: 16 heads, 32 sectors per track, cylinder = 0, head = 9, sector = 2). I set segment register ES = 0x0800 and BX = 0, call int 0x13 to read one sector. The Carry Flag (CF) is clear, so no disk read error. I check the first byte loaded at ES:[BX] — it differs from what I expect, but if I read sector 1 instead, the byte matches correctly. After reading the second stage, I do a far jump jmp 0x0800:0x0000 to the loaded code, which was assembled with [org 0x0000]. In the second stage, I properly initialize segment registers (DS, ES). I also placed code at the start of the second stage to output a character using int 0x10 to verify it runs, but no output appears and control does not seem to transfer. I verified that sector 289 in my disk image (disk.img) contains the expected second-stage code at the offset 289 * 512 = 0x24200.",
    "author_id":5557,
    "publication_date":1754326223000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"55kkkk",
    "author_reputation":31.0,
    "tags":"assembly, nasm, operating-system",
    "text_length":8140,
    "title_length":112,
    "num_tags":3
  },
  {
    "id":6050,
    "title":"How to generate a file without timeout for the client",
    "link":"https:\/\/stackoverflow.com\/questions\/79725079\/how-to-generate-a-file-without-timeout-for-the-client",
    "text":"I've got an endpoint created in Spring Boot that downloads a file from another endpoint, updates the file with some custom data and then sends the updated file to the client. The problem is that it might take 30-45 seconds to download the file from the other endpoint, so the client of my endpoint might get a timeout since there is no response. Is there a way to solve this so there is no timeout for the client? So far I haven't found any solution online. I can't download and update the file in advance. Currently the endpoint code looks like: ``` public ResponseEntity<StreamingResponseBody> downloadProject(UUID entityId, UUID versionId) { try { StreamingResponseBody stream = out -> { Path outputFile = null; try { outputFile = createTempFile(); \/\/ Download the file to update and send to the client fileClient.downloadFile(accessToken, fileId, outputFile, new FileDownloadCallback() { @Override public void onDownloadComplete(Path path) { updateFile(path); \/\/ Update the file with additional data try { Files.copy(path, out); out.flush(); } catch (IOException e) { throw new RuntimeException(\"Error streaming file\", e); } } @Override public void onDownloadError(Throwable error) { throw new RuntimeException(\"Failed to download file\", error); } }); } finally { if (outputFile != null) { try { Files.deleteIfExists(outputFile); } catch (IOException e) { log.warn(\"Failed to delete temporary file: {}\", outputFile, e); } } } }; return ResponseEntity.ok() .contentType(MediaType.APPLICATION_OCTET_STREAM) .body(stream); } catch (Exception e) { return ResponseEntity.status(500).build(); } } ```",
    "author_id":5556,
    "publication_date":1754326564000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mackan",
    "author_reputation":1479.0,
    "tags":"spring-boot, rest",
    "text_length":1598,
    "title_length":53,
    "num_tags":2
  },
  {
    "id":6049,
    "title":"Error 5 on Pivot Table Second Time Running ChangePivotCache",
    "link":"https:\/\/stackoverflow.com\/questions\/79725082\/error-5-on-pivot-table-second-time-running-changepivotcache",
    "text":"I have a worksheet with the data that i will be separating by its columns in its respectives worksheets (1 ws per column) My Macro does the following: After updating the ws with the data, it will check if the others ws have a PivotTable, if it dont exist it will create one, else it should just update every PivotTable cache with the new Range of data. The problem is that it only Updates the PivotTable on the first iteration of my For Each Loop, breaking on entering the second time in pt.ChangePivotCache pc ``` Sub tabelasDinamicas(wsDados, wbAnualizacao) Dim pt As PivotTable Dim pc As PivotCache Dim tabelaExiste As Boolean Dim rangeDados As Range Dim ultimaCol As Long, ultimaLinha As Long ultimaCol = wsDados.Cells(1, wsDados.Columns.Count).End(xlToLeft).Column ultimaLinha = wsDados.Cells(wsDados.Rows.Count, 1).End(xlUp).Row Set rangeDados = wsDados.Range(wsDados.Cells(1, 1), wsDados.Cells(ultimaLinha, ultimaCol)) Set pc = wbAnualizacao.PivotCaches.Create( _ SourceType:=xlDatabase, _ SourceData:=rangeDados) For Each ws In wbAnualizacao.Worksheets If ws.Name <> \"Dados\" Then tabelaExiste = False For Each pt In ws.PivotTables tabelaExiste = True pt.ChangePivotCache pc pt.RefreshTable Next pt If Not tabelaExiste Then Dim ptNovo As PivotTable Dim ptRange As Range Set ptRange = ws.Range(\"B5\") Set ptNovo = pc.CreatePivotTable( _ tabledestination:=ptRange, _ TableName:=ws.Name) With ptNovo On Error Resume Next .PivotFields(\"GRUPOS\").Orientation = xlHidden .PivotFields(\"GRUPOS\").Orientation = xlRowField ``` Tried changing the order of my Ws and renaming it but the result still the same. Also cant find anythig on SO or any vba related sites. Any helpers?",
    "author_id":5555,
    "publication_date":1754326823000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"JvSitta",
    "author_reputation":13.0,
    "tags":"vba, excel",
    "text_length":1670,
    "title_length":59,
    "num_tags":2
  },
  {
    "id":6048,
    "title":"EF Core query hint for SQL",
    "link":"https:\/\/stackoverflow.com\/questions\/79725083\/ef-core-query-hint-for-sql",
    "text":"I am writing a user-driven report subsystem using C# and EF Core 8. When it is used there are cases where a query takes too long to be executed. I figured it is due to not optimal plan being used. I started looking further, and realized most of the time the queries are different, so, no need to cache a plan. I tried to switch to raw SQL and use ``` OPTION(OPTIMIZE FOR UNKNOWN) ``` and it seems to perform much better for cases where queries are the same for different users due to the nature of their data being different. My question is: when using linq, how do we add query hints in EF Core? I found a way with interceptors and adding the hint as a text to the end, but it is too generic, and you need a way to tag your query for this. Again, I tried to use a tag that is converted into a comment in the generated SQL, then I looked for the unique string I used for the tag and added the hint to the required queries. Is it the way to go? It seems a bit cumbersome to me: generating an SQL string and then parsing the string for the tag. Edited: I will very likely use ``` OPTION(RECOMPILE) ``` , it is not part of the question which hints to use, I want to know how to add any hint to a Linq query. Also, please ignore database generalisation, I know it is going to be SQL Server for the foreseeable future. There is a possibility suggested by Svyatoslav Danyliv to disable parameter sensitivity ( parameter sniffing problem ), which is what I had. Starting from SQL Server 2016, it is possible to disable Parameter Sniffing via ALTER DATABASE SCOPED CONFIGURATION (Transact-SQL) The command is ALTER DATABASE SCOPED CONFIGURATION SET PARAMETER_SNIFFING = OFF; However, this setting will disable parameter sniffing for ALL queries in the database, not a particular set, and this is not suitable for my setup.",
    "author_id":5554,
    "publication_date":1754326854000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mykola",
    "author_reputation":236.0,
    "tags":"c#, sql-server, entity-framework-core, ef-core-8.0",
    "text_length":1814,
    "title_length":26,
    "num_tags":4
  },
  {
    "id":6047,
    "title":"CollectionView causes crash in Maui App on iOS only",
    "link":"https:\/\/stackoverflow.com\/questions\/79725085\/collectionview-causes-crash-in-maui-app-on-ios-only",
    "text":"The issue Here's my situation : I have a page in my Maui application with a ``` CollectionView ``` bound to an ``` ObservableCollection<string> ``` . I dynamically add at several points in my app items to this Collection through a custom ``` WeakReferenceMessenger ``` . Especially, I have a button in the same page which triggers several message additions to the collection. Here comes my issue : when I press this button on an android environment, everything works just fine, I see my messages added to the collection without any problem. But when I test it on iOS, either on an emulator or on a physical device, the application crashes and sends me the following exception : Exception ``` Objective-C exception thrown. Name: NSInternalInconsistencyException Reason: request for index path for global index 12 when there are only 12 items in the collection view Native stack trace: 0 CoreFoundation 0x0000000197c87228 7821F73C-378B-3A10-BE90-EF526B7DBA93 + 1155624 1 libobjc.A.dylib 0x0000000195121abc objc_exception_throw + 88 2 Foundation 0x0000000196f85670 34DE055D-8683-380A-9198-C3347211D13D + 7988848 3 UIKitCore 0x000000019a790ff8 96636F64-106F-30C8-A780-82DCEBB0F443 + 3362808 4 UIKitCore 0x000000019a9363cc 96636F64-106F-30C8-A780-82DCEBB0F443 + 5088204 5 UIKitCore 0x000000019a9362d4 96636F64-106F-30C8-A780-82DCEBB0F443 + 5087956 6 UIKitCore 0x000000019a9688a0 96636F64-106F-30C8-A780-82DCEBB0F443 + 5294240 7 UIKitCore 0x000000019a9684ec 96636F64-106F-30C8-A780-82DCEBB0F443 + 5293292 8 UIKitCore 0x000000019a4912b4 96636F64-106F-30C8-A780-82DCEBB0F443 + 217780 9 UIKitCore 0x000000019a75eec8 96636F64-106F-30C8-A780-82DCEBB0F443 + 3157704 10 UIKitCore 0x000000019a75d06c 96636F64-106F-30C8-A780-82DCEBB0F443 + 3149932 11 UIKitCore 0x000000019aa1bcec 96636F64-106F-30C8-A780-82DCEBB0F443 + 6028524 12 UIKitCore 0x000000019aa1b9d0 96636F64-106F-30C8-A780-82DCEBB0F443 + 6027728 13 UBScoring 0x0000000104977648 xamarin_dyn_objc_msgSendSuper + 164 14 UBScoring 0x0000000104b332b0 do_icall + 200 15 UBScoring 0x0000000104b318f4 do_icall_wrapper + 348 16 UBScoring 0x0000000104b24e28 mono_interp_exec_method + 2580 17 UBScoring 0x0000000104b221dc interp_entry_from_trampoline + 656 18 UBScoring 0x0000000104949970 native_to_interp_trampoline + 112 19 UBScoring 0x0000000104b8d458 -[__MonoMac_NSAsyncActionDispatcher xamarinApplySelector] + 96 20 Foundation 0x000000019685d574 34DE055D-8683-380A-9198-C3347211D13D + 484724 21 CoreFoundation 0x0000000197b7ca8c 7821F73C-378B-3A10-BE90-EF526B7DBA93 + 64140 22 CoreFoundation 0x0000000197b7c8a4 7821F73C-378B-3A10-BE90-EF526B7DBA93 + 63652 23 CoreFoundation 0x0000000197b7c700 7821F73C-378B-3A10-BE90-EF526B7DBA93 + 63232 24 CoreFoundation 0x0000000197b7d080 7821F73C-378B-3A10-BE90-EF526B7DBA93 + 65664 25 CoreFoundation 0x0000000197b7ec3c CFRunLoopRunSpecific + 572 26 GraphicsServices 0x00000001e4d5d454 GSEventRunModal + 168 27 UIKitCore 0x000000019a591274 96636F64-106F-30C8-A780-82DCEBB0F443 + 1266292 28 UIKitCore 0x000000019a55ca28 UIApplicationMain + 336 29 UBScoring 0x00000001049601f4 xamarin_UIApplicationMain + 60 30 UBScoring 0x0000000104b33324 do_icall + 316 31 UBScoring 0x0000000104b318f4 do_icall_wrapper + 348 32 UBScoring 0x0000000104b24e28 mono_interp_exec_method + 2580 33 UBScoring 0x0000000104b229e0 interp_runtime_invoke + 236 34 UBScoring 0x0000000104af11a8 mono_jit_runtime_invoke + 1244 35 UBScoring 0x0000000104a9891c mono_runtime_invoke_checked + 148 36 UBScoring 0x0000000104a9e820 mono_runtime_exec_main_checked + 116 37 UBScoring 0x0000000104af7be4 mono_jit_exec + 356 38 UBScoring 0x00000001049760ac xamarin_main + 2032 39 UBScoring 0x0000000104b64634 main + 64 40 dyld 0x00000001bea53f08 86D5253D-4FD1-36F3-B4AB-25982C90CBF4 + 257800 ``` Code The CollectionView in the page's architecture ``` <VerticalStackLayout Grid.Row=\"1\" VerticalOptions=\"End\"> <Border Stroke=\"{StaticResource Gray700}\" StrokeThickness=\"2\" BackgroundColor=\"{StaticResource Gray200}\" Padding=\"5\" Margin=\"10\"> <CollectionView x:Name=\"LogsCV\" HeightRequest=\"200\" BackgroundColor=\"Transparent\" ItemsSource=\"{Binding SessionLog}\" Rotation=\"180\" VerticalScrollBarVisibility=\"Always\" HorizontalScrollBarVisibility=\"Never\"> <CollectionView.ItemTemplate> <DataTemplate x:DataType=\"x:String\"> <Label Rotation=\"180\" Text=\"{Binding .}\" TextType=\"Html\" StyleClass=\"Popup\" FontSize=\"12\" LineBreakMode=\"WordWrap\"\/> <\/DataTemplate> <\/CollectionView.ItemTemplate> <\/CollectionView> <\/Border> <\/VerticalStackLayout> ``` The code when the ViewModel is reacting to the message ``` [ObservableProperty] public partial ObservableRangeCollection<string> SessionLog { get; set; } = new(); public void RegisterLogging() { WeakReferenceMessenger.Default.Register<UserLogMessage>(this, (r, m) => MainThread.BeginInvokeOnMainThread(() => { SessionLog.Add($\"<b>{DateTime.Now:HH:mm:ss}<\/b> | {m.Value}\"); })); } ``` ``` public class UserLogMessage(string message) : ValueChangedMessage<string>(message) { public UserLogMessage(string CompetitionName, LoadCompetitionStatus status) : this(StatusToMessage(CompetitionName, status)) { } private static string StatusToMessage(string CompetitionName, LoadCompetitionStatus status) { return $\"<i>{CompetitionName}<\/i> - {status}\"; } } ``` The function called when clicking on the Button causing the crash ``` [RelayCommand] public void ReloadCompetitionInfo() { MainThread.BeginInvokeOnMainThread(async () => { IsBusy = true; IsSyncing = true; count_i = 0; await AppCore.Instance.ReloadCompetitionInfo(); AppData.Instance.WriteLogFile(\"Competition reloaded from server.\"); CanBroadcast = AppData.Instance.AppDataCompetitions.LoggedOnUser.CanBroadcast; Broadcasting = AppData.Instance.AppDataSettings.Broadcast; IsSyncing = false; IsBusy = false; }); } ``` The LoadCompetitionFromApiResponse, called from within ReloadCompetitionInfo ``` private static UBModels.Competition LoadCompetitionFromApiResponse(UBApiModels.CompetitionData CompetitionDataFromUBAPI, bool IsMatchDirector) { UBModels.Competition AppCompetition = new(); WeakReferenceMessenger.Default.Send(new UserLogMessage(CompetitionName: AppCompetition.Name, LoadCompetitionStatus.Init)); \/\/ Every Load[...]FromUBAPI method ends with a new UserLogMessage being sent if (CompetitionDataFromUBAPI.squads.Count != 0) LoadSquadsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); if (CompetitionDataFromUBAPI.teams.Count != 0) LoadTeamsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); if (CompetitionDataFromUBAPI.competitors.Count != 0) LoadCompetitorsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); if (CompetitionDataFromUBAPI.draws.Count != 0) LoadBracketsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); if (CompetitionDataFromUBAPI.rounds.Count != 0) LoadRoundsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); if (CompetitionDataFromUBAPI.targets.Count != 0) LoadTargetsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); if (CompetitionDataFromUBAPI.contest_results.Count != 0) LoadResultsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); if (CompetitionDataFromUBAPI.duels.Count != 0) LoadDuelsFromApiResponse(AppCompetition, CompetitionDataFromUBAPI); WeakReferenceMessenger.Default.Send(new UserLogMessage(AppCompetition.Name, LoadCompetitionStatus.CompetitionLoaded)); return AppCompetition; } ``` Workaround I actually found a potential workaround : every time I send a message through the messenger in ``` ReloadCompetitionInfo ``` , I added a call to a new method ``` WaitIOS ``` before. However it feels like it's not the correct approach, and that I shouldn't need to add a delay to every single message in here. ``` private static void WaitiOS(int delayMs = 1000) { #if IOS Task.Delay(delayMs).Wait(); #endif } ```",
    "author_id":5553,
    "publication_date":1754326952000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"GChapX",
    "author_reputation":524.0,
    "tags":"c#, ios, maui",
    "text_length":7728,
    "title_length":51,
    "num_tags":3
  },
  {
    "id":6046,
    "title":"What does 0 mean in someList.toArray(new AnotherClass[0])?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725087\/what-does-0-mean-in-somelist-toarraynew-anotherclass0",
    "text":"In Java we have toArray() method which will return Object[]. If I want to return another data type I need to pass it to parameters like this ``` SomeClass.SomeList.toArray(new AnotherClass[0]); ``` What does 0 mean in this case?",
    "author_id":5552,
    "publication_date":1754326978000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"JustDreaming",
    "author_reputation":35.0,
    "tags":"java, methods, list, zero",
    "text_length":228,
    "title_length":58,
    "num_tags":4
  },
  {
    "id":6045,
    "title":"access deny C# class in Unity packages from script in sub Folder",
    "link":"https:\/\/stackoverflow.com\/questions\/79725091\/access-deny-c-class-in-unity-packages-from-script-in-sub-folder",
    "text":"I opened the project I was working on yesterday and saw a strange problem today and I was faced with a lot of errors!? For example ``` Assets\\Script\\PlayerIControls.cs(15,19): error CS0234: The type or namespace name 'InputSystem' does not exist in the namespace 'UnityEngine' (are you missing an assembly reference?) ``` When I moved all the C# script code files from ``` Assets\\Scripts\\ ``` to the ``` Assets\\ ``` folder, the problem was solved !!!!! To organize the project, I want the scripts to be in their own folders, but when they are moved to any folder, access to the Unity package classes is cut off !? Friends, does anyone have a solution to this problem ?",
    "author_id":5551,
    "publication_date":1754327380000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"abdol-hamid Hosseiny",
    "author_reputation":142.0,
    "tags":"c#, unity-game-engine, visual-studio",
    "text_length":668,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":6044,
    "title":"Problems handling numbers too great in JavaScript",
    "link":"https:\/\/stackoverflow.com\/questions\/79725094\/problems-handling-numbers-too-great-in-javascript",
    "text":"I am having problems with handling numbers that are too big in JavaScript. I have made a password security code - it calculates the possible combinations, it's very simple, but when it gets to quadrilion combinations, which isn't more than 100 days, so I cannot just put else condition to handle \"too long\". I have tried ``` BigInt() ``` but for some reason it doesn't display at all. Here is my code, I would be thankful for any suggestions. ``` const submit = document.querySelector(\".submit\"); const input = document.querySelector(\".input\"); submit.addEventListener(\"click\", function (event) { var password = input.value; var HasLowercase = false; var HasUppercase = false; var HasNumber = false; var HasSpecial = false; if (password.length == 0) { alert(\"Please input password!\"); } const regex = \/[!@#$%^&*()\\-+={}[\\]:;\"'<>,.?\\\/|\\\\]\/; for (var i = 0; i < password.length; i++) { var char = password[i]; if (char >= \"a\" && char <= \"z\") { HasLowercase = true; } if (char >= \"A\" && char <= \"Z\") { HasUppercase = true; } if (char >= \"0\" && char <= \"9\") { HasNumber = true; } if (regex.test(char)) { \/\/I had to research regex on geeksforgeeks.org\/javascript HasSpecial = true; } } var PossibleCombinationsBasic = 0; if (HasLowercase == true) { PossibleCombinationsBasic = PossibleCombinationsBasic + 26; } if (HasUppercase == true) { PossibleCombinationsBasic = PossibleCombinationsBasic + 26; } if (HasNumber == true) { PossibleCombinationsBasic = PossibleCombinationsBasic + 10; } if (HasSpecial == true) { PossibleCombinationsBasic = PossibleCombinationsBasic + 32; } var PasswordStrenght = PossibleCombinationsBasic ** password.length; var CombinationsPerSecond = 1000000000; var TimeToCrack = PasswordStrenght \/ CombinationsPerSecond; if (TimeToCrack < 60) { Time = Math.round(TimeToCrack) + \" seconds\"; } else if (TimeToCrack < 3600) { Time = Math.round(TimeToCrack \/ 60) + \" minutes\"; } else if (TimeToCrack < 86400) { Time = Math.round(TimeToCrack \/ 3600) + \" hours\"; } else if (TimeToCrack < 31536000) { Time = Math.round(TimeToCrack \/ 86400) + \" days\"; } else { Time = Math.round(timetocrack \/ 31536000) + \" years\"; } if (TimeToCrack < 1) { var message = \"There are \" + PasswordStrenght + \" possible combinations. \" + \"To a regular computer, it would take an instant to crack your password.\"; \/\/Had to find toLocaleString on StackOverflow } else { var message = \"There are \" + PasswordStrenght + \" possible combinations. \" + \"To a regular computer, it would take approximately \" + Time + \" to crack your password.\"; \/\/Had to find toLocaleString on StackOverflow } document.getElementById(\"strength\").innerHTML = message; }); ``` ``` <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>Password Strenght Checker!<\/title> <link rel=\"stylesheet\" href=\"style.css\"> <\/head> <body> <div class=\"titlecontainer\"> <p class=\"title\">Password Security Test<\/p> <p class=\"text subtitle\">How safe is your password?<\/p> <\/div> <div class=\"container\"> <p class=\"text\">Enter your password: <\/p> <input type=\"text\" class=\"input\" placeholder=\"E.g.: 5LoX&M2@\"> <button class=\"submit\">Submit<\/button> <\/div> <div class=\"results\"> <p class=\"resulttext\">Results:<\/p> <p class=\"strength\" id=\"strength\"><\/p> <\/div> <\/body> <\/html> ```",
    "author_id":5550,
    "publication_date":1754327457000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Maximko13",
    "author_reputation":21.0,
    "tags":"javascript, html, bigint",
    "text_length":3308,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":6043,
    "title":"Memoized component with `use-context-selector` still rerenders",
    "link":"https:\/\/stackoverflow.com\/questions\/79725097\/memoized-component-with-use-context-selector-still-rerenders",
    "text":"I have the following component: ``` import { CalendarContext } from '@\/app\/(app)\/account\/lecture\/calendar\/calendar-provider'; import { SlotResizer } from '@\/app\/(app)\/account\/lecture\/calendar\/views\/slots\/slot-resizer'; import { useDragSlot } from '@\/app\/(app)\/account\/lecture\/calendar\/views\/slots\/use-drag-slot'; import { useResizeSlot } from '@\/app\/(app)\/account\/lecture\/calendar\/views\/slots\/use-resize-slot'; import { ScrollArea } from '@\/components\/ui\/scroll-area'; import { useTraceChange } from '@\/hooks\/use-trace-change'; import { formatDate } from '@\/lib\/utils'; import { LectureSlotRepeatMode, Slot } from '@\/types'; import { Clock } from 'lucide-react'; import { memo } from 'react'; import { useContextSelector } from 'use-context-selector'; const Test = memo(({ slot }: { slot: Slot }) => { console.log('Rerender'); const color = CALENDAR_SLOT_ITEMS.find( (el) => el.type === slot.repeatMode )!.color; const mode = useContextSelector(CalendarContext, (ctx) => ctx.mode); return <div>DIV<\/div>; }); const CALENDAR_SLOT_ITEMS = [ { type: LectureSlotRepeatMode.DAY, color: 'green', }, { type: LectureSlotRepeatMode.WEEK, color: 'blue', }, { type: LectureSlotRepeatMode.NO_REPEAT, color: 'violet', }, ]; Test.displayName = 'Test'; export const CalendarSlotEventInner = memo(({ slot }: { slot: Slot }) => { const color = CALENDAR_SLOT_ITEMS.find( (el) => el.type === slot.repeatMode )!.color; const getEventStyles = useContextSelector( CalendarContext, (ctx) => ctx.getEventStyles ); const mode = useContextSelector(CalendarContext, (ctx) => ctx.mode); const { height } = getEventStyles(slot); const { isResizingBottom, isResizingTop, topResizerRef, bottomResizerRef, onMouseDownResizer, } = useResizeSlot(slot); const { dragging, slotRef, onMouseDown, newTop, translateX } = useDragSlot(slot); return ( <div ref={slotRef} className='absolute z-[4] flex w-full flex-col gap-1 overflow-hidden border-x-[3px]' style={{ top: newTop, height, borderColor: `var(--${color}8)`, backgroundColor: `var(--${color}1)`, transform: translateX ? `translateX(${translateX}px)` : undefined, }} > {mode === 'slots' && ( <SlotResizer onMouseDown={(e) => onMouseDownResizer(e, true)} ref={topResizerRef} dir='up' slot={slot} \/> )} <ScrollArea className='flex-1' onMouseDown={onMouseDown} style={{ cursor: isResizingTop || isResizingBottom ? 'ns-resize' : dragging ? 'grabbing' : mode === 'slots' ? 'grab' : 'default', }} > <div className='p-2'> <span className='flex items-center gap-1.5 text-xs'> <Clock className='h-4 w-4 shrink-0' \/> <span className='font-bold'> {formatDate(slot.fromDate, 'HH:mm')} -{' '} {formatDate(slot.toDate, 'HH:mm')} <\/span> <\/span> <span className='flex items-center gap-1.5'> {\/* <Repeat className='h-4 w-4 shrink-0' \/> *\/} {\/* {slot.repeatMode === LectureSlotRepeatMode.NO_REPEAT ? 'Brak' : slot.repeatMode === LectureSlotRepeatMode.WEEK ? '1 W' : '1 BD'} *\/} {\/* <span> { getSelectableRepeatModes().find( (el) => el.value === slot.repeatMode )?.label } <\/span> *\/} <\/span> <\/div> <\/ScrollArea> {mode === 'slots' && ( <SlotResizer ref={bottomResizerRef} slot={slot} dir='down' onMouseDown={(e) => onMouseDownResizer(e, false)} \/> )} <\/div> ); }); CalendarSlotEventInner.displayName = 'CalendarSlotEventInner'; export const CalendarSlotEvent = memo(({ slot }: { slot: Slot }) => { if (slot.id !== '69fc4516-970c-4a00-92f1-3fa073cbe4d8') return <Test slot={slot} \/>; return <CalendarSlotEventInner slot={slot} \/>; \/\/ return ( \/\/ <Popover> \/\/ <PopoverTrigger asChild> \/\/ <\/PopoverTrigger> \/\/ <PopoverContent \/\/ className='p-0' \/\/ onOpenAutoFocus={(e) => e.preventDefault()} \/\/ > \/\/ <SlotCard slot={slot} editable={mode === 'slots'} \/> \/\/ <\/PopoverContent> \/\/ <\/Popover> \/\/ ); }); ``` What is happening here is the following: ``` CalendarSlotEvent ``` is my main component and I render it for every event in my calendar I want to render ``` CalendarSlotEventInner ``` for every slot but there are renderes that I dont understand so I render CalendarSlotEventInner only for single event (to be able to resize it) and for others I render ``` Test ``` component. When I resize any event, I dont want to rerender other event. For that Im using ``` use-context-selector ``` to prevent rerenders when value from context does not change, but Test still rerenders... When I remove ``` const mode = useContextSelector(CalendarContext, (ctx) => ctx.mode); ``` then ``` Test ``` is correctly not rerendered. In my contxt I have: ``` const getEventStyles = useCallback( (event: { fromDate: Date; toDate: Date }) => { const fromDate = event.fromDate; const fromMinutes = fromDate.getHours() * 60 + fromDate.getMinutes() - hourRange[0] * 60; const top = fromMinutes * minuteHeight; const height = differenceInMinutes(event.toDate, event.fromDate) * minuteHeight; return { top, height }; }, [hourRange, minuteHeight] \/\/ zależności, jeśli się zmieniają, to funkcja się zaktualizuje ); ``` where ``` const [mode, setMode] = useState<CalendarMode>( user.role === Role.TEACHER ? 'both' : 'bookings' ); ``` and my context returns ``` <DndProvider backend={HTML5Backend}> <CalendarContext.Provider value={{ bookings: bookings ? bookings.results.filter( (el) => el.status !== BookingStatus.CANCELLED ) : [], slots, setSlots, date, setDate, days, view, setView, dateRange, setDateRange, onDateChange, isFullscreenMode, setIsFullscreenMode, hourHeight, setHourHeight, hourRange, setHourRange, hourScale, showSlots, setShowSlots, showBookings, setShowBookings, getEventStyles, minuteHeight, mode, onModeChange, stepLength, setStepLength, showWeekend, setShowWeekend, }} > {children} <\/CalendarContext.Provider> <\/DndProvider> ``` Any suggestions why the ``` Test ``` component rerenders? Here is the output from ``` react-scan ``` so there are 40 rerenders of ``` Test ``` component when I resize other slots and only last 2 rerenders comes from ``` slot ``` prop change (bevause on resize end I incalidate slots query) but first 38 (during resizing) have no highlighted reason of rerender)",
    "author_id":5549,
    "publication_date":1754327502000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Korer",
    "author_reputation":327.0,
    "tags":"reactjs, react-context, memo",
    "text_length":5979,
    "title_length":62,
    "num_tags":3
  },
  {
    "id":6042,
    "title":"Questions about acquireTokenByClientCredential caching",
    "link":"https:\/\/stackoverflow.com\/questions\/79725101\/questions-about-acquiretokenbyclientcredential-caching",
    "text":"We are using acquireTokenByClientCredential to get an app-to-app token. We tried using the same user multiple times and it seemed to cache since I didn't see any extra calls going out to MS. But I did have a couple questions: What is the TTL of this cached token? Is there any documentation that explains how the cache for this works? From documentation found here I found this section: \"Since this is a “client credentials flow”, this uses the application token cache. This method takes care of verifying the application token cache before sending a request to the security token service (STS).\" But that's as much as I could find on it",
    "author_id":5548,
    "publication_date":1754327937000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user2271328",
    "author_reputation":3.0,
    "tags":"msal.js, msal-react, clientcredential",
    "text_length":637,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":6041,
    "title":"Why does cpquery() work manually but fail when used inside a function with get() in bnlearn?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725104\/why-does-cpquery-work-manually-but-fail-when-used-inside-a-function-with-get",
    "text":"I'm working with Bayesian networks using the bnlearn package in R. I use cpquery() to compute conditional probabilities. The code works when run manually, but fails or returns incorrect results when wrapped in a function that uses get() for variable names. Here’s a minimal reproducible example showing the issue. Manual version (works as expected): ``` library(bnlearn) # Create toy data set.seed(42) data <- data.frame( x = runif(20, 0, 2), y = rnorm(20), z = runif(20, 0, 2) ) # Create a simple DAG dag <- model2network(\"[x][y][z|x:y]\") # Fit the Bayesian network fitted_bn <- bn.fit(dag, data) # Set threshold and interval x_value <- 1 lower <- 0.5 upper <- 1.5 # cpquery call with hardcoded variable names – this works prob <- cpquery( fitted_bn, event = (z >= lower & z < upper), evidence = (x <= x_value) ) print(prob) ``` Function version (fails): ``` # Define function that fails inside cpquery() when using get() test_cpquery_function <- function(data, dag, var_x = \"x\", var_z = \"z\", var_y = \"y\") { # Fit the Bayesian network fitted_bn <- bn.fit(dag, data) # Define a test threshold x_value <- 1 lower <- 0.5 upper <- 1.5 # Try cpquery using get() - this will not work as expected prob <- cpquery( fitted_bn, event = (get(var_z) >= lower & get(var_z) < upper), # ERROR evidence = (get(var_x) <= x_value) # ERROR ) print(prob) } # Run it test_cpquery_function(data, dag) } ``` ``` Error in eval(evidence, generated.data, parent.frame()) : object 'var_x' not found ``` How can I pass column names dynamically in a function to cpquery()? I assume the issue is with get() not resolving in the correct environment inside cpquery(). Is there a proper way to construct expressions for event and evidence when the variable names are stored in strings?",
    "author_id":5547,
    "publication_date":1754328353000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Terka Dračkov&#225;",
    "author_reputation":21.0,
    "tags":"r, bnlearn",
    "text_length":1753,
    "title_length":92,
    "num_tags":2
  },
  {
    "id":6040,
    "title":"Spring Cloud Gateway not able to redirect requests",
    "link":"https:\/\/stackoverflow.com\/questions\/79725111\/spring-cloud-gateway-not-able-to-redirect-requests",
    "text":"I am trying to build a microservices project in Spring Boot. Dependencies: Spring Boot Cloud Gateway Eureka Discovery Server Spring Boot Load Balancer (in the API gateway) I have AUTH-SERVICE and CHAT-SERVICE registered using ``` @EnableDiscoveryClient ``` . They are visible on the web GUI. I added JWT authentication in AUTH-SERVICE but have not used a filter since it is used in the API gateway. ``` SecurityConfig ``` for AUTH-SERVICE: ``` @Configuration @EnableWebSecurity public class SecurityConfig { @Autowired private UserService userService; @Bean PasswordEncoder passwordEncoder() { return new BCryptPasswordEncoder(); } @Bean SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception { http .csrf(AbstractHttpConfigurer::disable) .authorizeHttpRequests( requests -> requests .requestMatchers(\"\/auth\/**\", \"\/login\", \"\/register\") .permitAll() .anyRequest() .authenticated()) .sessionManagement(manager -> manager.sessionCreationPolicy(SessionCreationPolicy.STATELESS)) .authenticationProvider(getAuthenticationProvider()); return http.build(); } @Bean WebMvcConfigurer corsConfigurer() { return new WebMvcConfigurer() { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(\"\/**\") .allowedOrigins(\"*\") .allowedMethods(\"*\") .allowedHeaders(\"*\"); } }; } @Bean AuthenticationProvider getAuthenticationProvider() { DaoAuthenticationProvider provider = new DaoAuthenticationProvider(userService); provider.setPasswordEncoder(passwordEncoder()); return provider; } @Bean AuthenticationManager authenticationManager(AuthenticationConfiguration config) throws Exception { return config.getAuthenticationManager(); } } ``` ``` JwtAuthFilter ``` in API-GATEWAY: ``` @Component public class JwtAuthFilter implements GatewayFilter { @Value(\"${jwt.secret.key}\") private String secretKey; private Claims extractClaims(String token) { return Jwts.parser() .verifyWith((SecretKey) getSignInKey()) .build() .parseSignedClaims(token) .getPayload(); } private boolean isTokenValid(String token) { try { Claims claims = extractClaims(token); return claims.getExpiration().after(new Date()); } catch (Exception e) { System.out.println(\"Invalid JWT token: \" + e.getMessage()); } return false; } private Key getSignInKey() { final byte[] secretKey = Decoders.BASE64.decode(this.secretKey); return Keys.hmacShaKeyFor(secretKey); } @Override public Mono<Void> filter(ServerWebExchange exchange, GatewayFilterChain chain) { System.out.println(\"JWT Auth Filter is called\"); ServerHttpRequest request = exchange.getRequest(); \/\/ Check if the incoming request has an auth header if (!request.getHeaders().containsKey(\"Authorization\")) { System.out.println(\"No Authorization header found in the request\"); exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED); return exchange.getResponse().setComplete(); } \/\/ Extract the JWT Token from the auth header System.out.println(\"Extracting JWT Token from Authorization header\"); String token = request.getHeaders().getFirst(\"Authorization\").substring(7); if (isTokenValid(token)) { System.out.println(\"JWT Token is valid\"); Claims claims = extractClaims(token); String id = claims.get(\"id\", String.class); request = exchange.getRequest().mutate() .header(\"X-User-Token\", token) .header(\"X-User-Id\", id) .build(); } else { System.out.println(\"JWT Token is invalid or expired\"); exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED); return exchange.getResponse().setComplete(); } System.out.println(\"JWT Token is valid, proceeding with the request\"); return chain.filter(exchange.mutate().request(request).build()); } } ``` When I directly access AUTH-SERVICE's login endpoint (which has ``` @RequestMapping(\"\/auth\") ``` and ``` @PostMapping(\"\/login\") ``` ) using Postman, I get the token and the user object. But hitting the endpoint via API-GATEWAY I get: 403 Forbidden - Access Denied Routing configurations for API-GATEWAY: ``` @Configuration public class ApiGatewayConfig { @Autowired private JwtAuthFilter jwtFilter; @Bean RouteLocator customRouteLocator(RouteLocatorBuilder builder) { return builder.routes() .route(\"auth-service\", r -> r.path(\"\/auth\/**\") .uri(\"lb:\/\/AUTH-SERVICE\")) .route(\"chat-service-api\", r -> r.path(\"\/api\/chats\/**\") .filters(f -> f.stripPrefix(1).filter(jwtFilter)) .uri(\"lb:\/\/CHAT-SERVICE\")) .route(\"chat-service-ws\", r -> r.path(\"\/chat\/ws\/**\") .filters(f -> f.stripPrefix(1).filter(jwtFilter)) .uri(\"lb:\/\/CHAT-SERVICE\")) .build(); } } ``` application.yaml file of API-GATEWAY: ``` spring: application: name : API-GATEWAY cloud: gateway: default-filters: - DedupeResponseHeader=Access-Control-Allow-Credentials Access-Control-Allow-Origin server: webmvc: routes: - id: auth-service uri: lb:\/\/AUTH-SERVICE predicates: - Path=\/auth\/** - id: chat-service uri: lb:\/\/CHAT-SERVICE predicates: - Path=\/api\/chats\/** - Path=\/chat\/ws\/** ```",
    "author_id":5546,
    "publication_date":1754328917000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"cruxi5",
    "author_reputation":1.0,
    "tags":"java, spring, spring-boot, spring-security, jwt",
    "text_length":4847,
    "title_length":50,
    "num_tags":5
  },
  {
    "id":6039,
    "title":"is there some opening tools can find non-traffic jar？",
    "link":"https:\/\/stackoverflow.com\/questions\/79725112\/is-there-some-opening-tools-can-find-non-traffic-jar",
    "text":"I would like to try to analyze and govern the non-traffic jars, that is, the jars that have no traffic passing through the production environment, to improve the compilation speed of the Java project. Is there any open source solution in this direction?",
    "author_id":5545,
    "publication_date":1754328986000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Prayer",
    "author_reputation":1.0,
    "tags":"java, jar",
    "text_length":253,
    "title_length":53,
    "num_tags":2
  },
  {
    "id":6038,
    "title":"scaling slick carousel within a container with centerMode not working",
    "link":"https:\/\/stackoverflow.com\/questions\/79725114\/scaling-slick-carousel-within-a-container-with-centermode-not-working",
    "text":"I want to have a slick carousel with three slides and centerMode, so that you see the middle slide and parts of the other two slides on the left and right. Everything works fine when I have the carousel go all the way across my screen. But when I put my carousel into a container that is 50vw, the images don't scale down -- they stay the same size, so you don't see anything but part of one slide. How do you make the carousel scale down to the containing div? Here is my codepen: https:\/\/codepen.io\/lschneiderman\/pen\/ByoRzqM html: ``` <div class=\"screen height-auto\"> <div style=\"width: 50vw;\"> <div class=\"ss-exposition\"> <div class=\"slide\"> <img class=\"slide-img\" src=\"\/\/newsinteractive.post-gazette.com\/.test2\/img\/recreation\/recreation-1890map2.jpg\" \/> <div class=\"caption text-center\">caption1<\/div> <\/div> <div class=\"slide\"> <img class=\"slide-img\" src=\"\/\/newsinteractive.post-gazette.com\/.test2\/img\/recreation\/recreation-gone-19031906-b.jpg\" \/> <div class=\"caption text-center\">caption2<\/div> <\/div> <div class=\"slide\"> <img class=\"slide-img\" src=\"\/\/newsinteractive.post-gazette.com\/.test2\/img\/recreation\/Grandstand_at_Recreation_Park_Expositionb.jpg\" \/> <div class=\"caption text-center\">caption3 <\/div> <\/div> <\/div> <\/div> <\/div> ``` css: ``` .screen { position: relative; width: 100%; height: 100vh; background-color: white ; z-index: 3; overflow: hidden; } .height-auto { height: auto; } .slide-img { padding-left: 10px; padding-right: 10px; } .fa-angle-left, .fa-angle-right { font-size: 7rem; color: #979a9c; position: absolute; top: 38%; cursor: pointer; z-index: 3; font-weight: 300; } .fa-angle-left { left: -80px; } .fa-angle-right { right: -80px; } .text-center { text-align: center; } ``` jquery: ``` $('.ss-exposition').slick({ dots: true, prevArrow: \"<i class='fa-solid fa-angle-left'><\/i>\", nextArrow: \"<i class='fa-solid fa-angle-right'><\/i>\", swipe: true, centerMode: true, slidesToShow: 1, centerPadding: '0px', variableWidth: true, variableHeight: true }); ```",
    "author_id":5544,
    "publication_date":1754329179000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"LauraNMS",
    "author_reputation":2924.0,
    "tags":"css, slick.js",
    "text_length":1987,
    "title_length":69,
    "num_tags":2
  },
  {
    "id":6037,
    "title":"I am trying to run invoke-sqlcmd in powershell in my laptop, getting error related to certificate",
    "link":"https:\/\/stackoverflow.com\/questions\/79725119\/i-am-trying-to-run-invoke-sqlcmd-in-powershell-in-my-laptop-getting-error-relat",
    "text":"i got a new laptop from my organization, whenever i try to run invoke-sqlcmd i used to get an error saying that the server certificate is not trusted, i had to keep \"-TrustServerCertificate\" flag in all of my invoke-sqlcmd cmd-lets, tried searching for permanent solution but found none, i am trying to document the solution.",
    "author_id":5543,
    "publication_date":1754329276000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"cross_handle",
    "author_reputation":505.0,
    "tags":"powershell, certificate, invoke-sqlcmd",
    "text_length":325,
    "title_length":97,
    "num_tags":3
  },
  {
    "id":6036,
    "title":"How to use InnoDependencyInstaller correct for both 32 \/ 64 bit installers",
    "link":"https:\/\/stackoverflow.com\/questions\/79725122\/how-to-use-innodependencyinstaller-correct-for-both-32-64-bit-installers",
    "text":"I thought that I had this issue catered for with my installer, but something is still awry. I use InnoDependencyInstaller . My application has a supporting console utility that is written with C# .NET8, and it is configured as AnyCPU : Now, my main application comes in 32 \/ 64 bit editions, each with their own installers. Here is a snippet from ``` [Files] ``` section: ``` ; MSA Tools Console Application Source: \"{#MSAToolsConsole}\\*.*\"; DestDir: \"{app}\\MSATools\"; Excludes: \"MSATools.dll,MSATools.exe\"; Flags: ignoreversion recursesubdirs Source: \"{#MSAToolsConsole}\\MSATools.dll\"; DestDir: \"{app}\\MSATools\"; Flags: ignoreversion signonce Source: \"{#MSAToolsConsole}\\MSATools.exe\"; DestDir: \"{app}\\MSATools\"; Flags: ignoreversion signonce ``` My ``` InitializeSetup ``` includes the following dependencies: ``` { .NET 8.0.3 } Dependency_AddDotNet80; { .NET Framework 4.8.0 } Dependency_AddDotNet481; { WebView2 } Dependency_AddWebView2; { Visual Studio Framework - version 2015 to 2022 } Dependency_AddVC2015To2022; ``` Now, I had a user that installed the 32 bit version of my application. And a certain feature that relied on my MSATools application wasn't doing anything. I connected to their computer and eventually brought up a console window to run the tool via the command line myself. Boom: ``` C:\\Program Files (x86)\\Meeting Schedule Assistant\\MSATools>msatools You must install .NET to run this application. App: C:\\Program Files (x86)\\Meeting Schedule Assistant\\MSATools\\MSATools.exe Architecture: x64 App host version: 8.0.18 .NET location: Not found Learn more: https:\/\/aka.ms\/dotnet\/app-launch-failed Download the .NET runtime: https:\/\/aka.ms\/dotnet-core-applaunch?missing_runtime=true&arch=x64&rid=win-x64&os=win10&apphost_version=8.0.18 ``` I followed the instructions and installed the requested .NET8 runtime and my console tool worked correctly. So I am confused because I thought it would all work. I appreciate there is not a full sample provided. I would have directed this to GitHub but there is no Issue tracker. What step have I missed? It has now happened on 2 user computers. When installing my 32 bit software on a 64 bit pc, the MSATools console utility will not run because it still needs .NEt8 64 bit installed. Even though I built the tool as AnyCPU. Since I don't force the installer (to my knowledge) to use 32 bit .net I thought it would be ok. In my script setup section I have: ``` #ifdef WIN64 ArchitecturesInstallIn64BitMode=x64compatible ArchitecturesAllowed=x64compatible #endif ``` I think it is relevant. In the docs it says: If you only deploy 32-bit binaries and dependencies you can also instead just not define ``` ArchitecturesInstallIn64BitMode ``` in ``` [Setup] ``` . So my 32 bit install is not defining that flag. Hence it is installing .net8 for 32 bit. But why can’t MSA Tools work with either .net as it is AnyCPU? Publish Settings",
    "author_id":5542,
    "publication_date":1754329319000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Andrew Truckle",
    "author_reputation":19339.0,
    "tags":".net-8.0, inno-setup, inno-dependency-installer",
    "text_length":2892,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":6035,
    "title":".NET Core is returning a build error related to the View() method",
    "link":"https:\/\/stackoverflow.com\/questions\/79725123\/net-core-is-returning-a-build-error-related-to-the-view-method",
    "text":"I'm trying to learn .NET Core \/ C# for the very first time and I'm doing it on a Debian machine with no IDE like ModoDevelop, VS Code or whatever. Here is my directory tree: ``` peter@laptop:\/data\/DotNet\/dotnettest$ tree -d . ├── bin │ └── Debug │ └── net9.0 ├── Controllers ├── Models ├── obj │ └── Debug │ └── net9.0 │ ├── ref │ ├── refint │ └── staticwebassets ├── Properties ├── Views │ └── Home └── wwwroot ``` My ``` program.cs ``` is simple: ``` var builder = WebApplication.CreateBuilder(args); builder.Services.AddControllersWithViews(); var app = builder.Build(); app.MapGet(\"\/\", () => \"Hello World!\"); app.MapGet(\"\/Home\", () => \"Hellow there\"); app.Run(); ``` I can make it print messages from ``` program.cs ``` file directly. The other code is here - my model class: ``` namespace Blog.Models { public class Post : Model { public string? Title { get; set; } public int Views { get; set; } = 0; public string? Content { get; set; } public string? Excerpt { get; set; } public string? CoverImagePath { get; set; } public bool Public { get; set; } } } ``` And my controller: ``` public class Post { public string ActionIndex() { \/\/ return \"This is my default action...\"; return View(\"\/Home\/Index\"); } } ``` And my view - ``` index.cshtml ``` : ``` <html> <head> <body> <strong>testy westy<\/strong> <\/body> <\/head> <\/html> ``` Here is the build error I see: peter@laptop:\/data\/DotNet\/dotnettest$ dotnet build Restore complete (0.3s) dotnettest failed with 1 error(s) (0.5s) \/data\/DotNet\/dotnettest\/Controllers\/PostController.cs(6,16): error CS0103: The name 'View' does not exist in the current context Build failed with 1 error(s) in 1.2s Does anyone know what I need to do to get the ``` View() ``` method to work, I'm hoping I just need an include or to use the ``` dotnet ``` command to install a package or assembly reference",
    "author_id":4342,
    "publication_date":1754329335000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user3183111",
    "author_reputation":331.0,
    "tags":"c#, .net-core",
    "text_length":1839,
    "title_length":65,
    "num_tags":2
  },
  {
    "id":6034,
    "title":"Why won&#39;t &amp;dyn Iterator be accepted as an Iterator?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725125\/why-wont-dyn-iterator-be-accepted-as-an-iterator",
    "text":"The other day I thought about having Iterators over the same type, but of different types be passed in a list, to be then flattened and finally consumed. So today I decided whether that was possible on a whim and stumbled across this: ``` fn main() { let a: [u8; 5] = [1, 2, 3, 4, 5]; let b: Vec<u8> = vec![6, 7, 8, 9]; let c: [&dyn Iterator<Item = u8>; 2] = [&a.into_iter(), &b.into_iter()]; for v in c.into_iter().flatten() { println!(\"{v}\") } } ``` ``` error[E0277]: `&dyn Iterator<Item = &u8>` is not an iterator --> src\/main.rs:5:28 | 5 | for v in c.into_iter().flatten() { | ^^^^^^^ `&dyn Iterator<Item = u8>` is not an iterator | = help: the trait `Iterator` is not implemented for `&dyn Iterator<Item = u8>` = note: `Iterator` is implemented for `&mut dyn Iterator<Item = u8>`, but not for `&dyn Iterator<Item = u8>` = note: required for `&dyn Iterator<Item = u8>` to implement `IntoIterator` ``` Playground So, why? Is it because the dyn is behind a shared reference?",
    "author_id":5541,
    "publication_date":1754329426000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"USR",
    "author_reputation":33.0,
    "tags":"rust",
    "text_length":976,
    "title_length":59,
    "num_tags":1
  },
  {
    "id":6033,
    "title":"Quarto render to docx, md_container not rendering",
    "link":"https:\/\/stackoverflow.com\/questions\/79725127\/quarto-render-to-docx-md-container-not-rendering",
    "text":"I'm trying to render a quarto document to docx. I have no issue rendering to html. But when I tried docx one of the tables looks like this: Here is what it looks like when rendered to html: I'm not sure what to troubleshoot and I'm having a hard time searching about this issue.",
    "author_id":5484,
    "publication_date":1754329689000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"bdeonovic",
    "author_reputation":4272.0,
    "tags":"docx, r, quarto, markdown",
    "text_length":278,
    "title_length":49,
    "num_tags":4
  },
  {
    "id":6032,
    "title":"Build the multipage taipy application with login page",
    "link":"https:\/\/stackoverflow.com\/questions\/79725131\/build-the-multipage-taipy-application-with-login-page",
    "text":"After clicking the login button, the application does not redirect to the intended page, and no action appears to occur. It seems that the button click event is either not being handled correctly or the logic to navigate to the next page is missing or not functioning as expected. Below is the code details app.py ``` # app.py from taipy.gui import Gui from pages.login_page import login_page, username, password, login_error, on_login_action from pages.overview_page import overview_page from pages.analysis_page import analysis_page # App state logged_in = False page = \"login\" # Pages dictionary with tgb pages pages = { \"login\": login_page, \"overview\": overview_page, \"analysis\": analysis_page } def before_page_display(state, page_name): if not state.logged_in and page_name != \"login\": state.page = \"login\" gui = Gui(pages=pages) gui.run(title=\"Secure Taipy App with Builder\", port=5000, use_reloader=True, before_page_display=before_page_display) ``` login_page.py ``` # pages\/login_page.py import taipy.gui.builder as tgb from taipy.gui import notify VALID_USERNAME = \"admin\" VALID_PASSWORD = \"1234\" username = \"\" password = \"\" login_error = \"\" def on_login_action(state): if state.username == VALID_USERNAME and state.password == VALID_PASSWORD: state.logged_in = True state.login_error = \"\" state.page = \"overview\" else: state.login_error = \"Invalid username or password.\" notify(state, \"error\", state.login_error) with tgb.Page() as login_page: tgb.text(\"🔒 Login\", mode=\"md\") with tgb.layout(columns=\"1 1 1\"): tgb.text(\"Username\") tgb.input(value=\"{username}\") with tgb.layout(columns=\"1 1 1\"): tgb.text(\"Password\") tgb.input(value=\"{password}\", password=True) tgb.button(\"Login\", on_action=on_login_action) tgb.text(\"{login_error}\", color=\"red\") ``` analysis_page.py ``` # pages\/analysis_page.py import taipy.gui.builder as tgb with tgb.Page() as analysis_page: tgb.text(\"📈 Analysis\", mode=\"md\") tgb.text(\"This is the analysis page content.\") ``` overview_page.py ``` # pages\/overview_page.py import taipy.gui.builder as tgb with tgb.Page() as overview_page: tgb.text(\"📊 Overview\", mode=\"md\") tgb.text(\"This is the overview page content.\") ``` Could you help identify the issue with this code and suggest the necessary changes to ensure that the login button functions as intended?",
    "author_id":5540,
    "publication_date":1754329757000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user3219244",
    "author_reputation":13.0,
    "tags":"python, flask, taipy",
    "text_length":2293,
    "title_length":53,
    "num_tags":3
  },
  {
    "id":6031,
    "title":"Airflow + Keycloak SSO: User with &quot;Admin&quot; role gets &quot;Viewer&quot; role after login",
    "link":"https:\/\/stackoverflow.com\/questions\/79725142\/airflow-keycloak-sso-user-with-admin-role-gets-viewer-role-after-login",
    "text":"We’re using Apache Airflow 3.0.2 deployed on Kubernetes with the official Airflow Helm chart (1.17.0) . We’ve integrated Keycloak for SSO using OAuth. Users are able to log in successfully via SSO, but even if they have the \"Admin\" role in Keycloak, they are logged in with only the Viewer role in Airflow. What we’ve tried: In ``` webserver_config.py ``` , we've enabled OAuth and set up role mapping as follows: ``` import os from flask_appbuilder.security.manager import AUTH_OAUTH AUTH_TYPE = AUTH_OAUTH OAUTH_PROVIDERS = [{ 'name': 'keycloak', 'token_key': 'access_token', 'icon': 'fa-circle-o', 'remote_app': { 'client_id': os.getenv(\"AIRFLOW_OAUTH_CLIENT_ID\"), 'client_secret': os.getenv(\"AIRFLOW_OAUTH_CLIENT_SECRET\"), 'api_base_url': 'https:\/\/<keycloak-domain>\/realms\/<realm-name>\/protocol\/openid-connect', 'client_kwargs': { 'scope': 'openid profile email' }, 'access_token_url': 'https:\/\/<keycloak-domain>\/realms\/<realm-name>\/protocol\/openid-connect\/token', 'authorize_url': 'https:\/\/<keycloak-domain>\/realms\/<realm-name>\/protocol\/openid-connect\/auth', 'request_token_url': None, 'server_metadata_url': os.getenv(\"AIRFLOW_METADATA_URL\"), } 'role_keys': 'realm_access.roles', }] AUTH_ROLES_MAPPING = { \"Viewer\": [\"Viewer\"], \"Admin\": [\"Admin\"], \"User\": [\"User\"], \"Public\": [\"Public\"], \"Op\": [\"Op\"], } # Uncomment to setup Full admin role name AUTH_ROLE_ADMIN = \"Admin\" # Uncomment to setup Public role name, no authentication needed AUTH_ROLE_PUBLIC = \"Public\" # Will allow user self registration AUTH_USER_REGISTRATION = True # The default user self registration role AUTH_USER_REGISTRATION_ROLE = \"Viewer\" AUTH_ROLES_SYNC_AT_LOGIN = True ``` We also tried customising the security manager to override the role mapping like this: ``` from airflow.providers.fab.auth_manager.security_manager.override import FabAirflowSecurityManagerOverride from base64 import b64decode import jwt import requests from cryptography.hazmat.primitives import serialization OIDC_ISSUER = os.getenv(\"OIDC_ISSUER\") # Fetch public key req = requests.get(OIDC_ISSUER) key_der_base64 = req.json()[\"public_key\"] key_der = b64decode(key_der_base64.encode()) public_key = serialization.load_der_public_key(key_der) class CustomSecurityManager(FabAirflowSecurityManagerOverride): def get_oauth_user_info(self, provider, response): if provider == \"keycloak\": token = response[\"access_token\"] me = jwt.decode(token, public_key, algorithms=[\"HS256\", \"RS256\"], audience=os.getenv(\"SSO_CLIENT_ID\")) # Extract roles from resource access realm_access = me.get(\"realm_access\", {}) groups = realm_access.get(\"roles\", []) log.info(\"groups: {0}\".format(groups)) if not groups: groups = [\"Viewer\"] userinfo = { \"username\": me.get(\"preferred_username\"), \"email\": me.get(\"email\"), \"first_name\": me.get(\"given_name\"), \"last_name\": me.get(\"family_name\"), \"role_keys\": groups, } log.info(\"user info: {0}\".format(userinfo)) return userinfo else: return {} SECURITY_MANAGER_CLASS = \"my_module.CustomSecurityManager\" ``` And added it in ``` webserver_config.py ``` : Each SSO user is assigned the ``` Admin ``` role, but there are two records for user role mapping in the PostgreSQL table ``` ab_user_role ``` : one for Admin and another for Viewer. What we want: If a user has the \"Admin\" role in Keycloak, they should get the Admin role in Airflow. Additional info: The Keycloak token contains roles under ``` realm_access.roles ``` Example decoded token snippet: ``` \"realm_access\": { \"roles\": [\"offline_access\", \"uma_authorization\", \"admin\"] } ``` Questions: How can we extract the Keycloak realm roles correctly from the token and map them to Airflow roles? Is there a better way to define role mapping with Keycloak and Airflow? Do we need to modify ``` FAB ``` (Flask AppBuilder) role sync or extend the OAuthUserInfo object? Any help or working example would be appreciated!",
    "author_id":5539,
    "publication_date":1754330135000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ravindra Gupta",
    "author_reputation":1552.0,
    "tags":"python, airflow, keycloak",
    "text_length":3843,
    "title_length":97,
    "num_tags":3
  },
  {
    "id":6030,
    "title":"How can I make my Window resizable when the .plain style is used in MacOS 15+?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725143\/how-can-i-make-my-window-resizable-when-the-plain-style-is-used-in-macos-15",
    "text":"``` @main struct ResizableWindowApp: App { var body: some Scene { WindowGroup(for: String.self) { id in ContentView() .ignoresSafeArea() .frame(minWidth: 320, maxWidth: 1024, minHeight: 568, maxHeight: 768) } .windowLevel(.floating) .windowStyle(.plain) .windowResizability(.contentSize) .defaultSize(width: 320, height: 568) } } ``` When windowStyle is set to ``` plain ``` I am unable to resize the window. Is there a way to get this behaviour back?",
    "author_id":4624,
    "publication_date":1754330155000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mark",
    "author_reputation":18334.0,
    "tags":"swiftui, macos, window",
    "text_length":451,
    "title_length":78,
    "num_tags":3
  },
  {
    "id":6029,
    "title":"The conditional edge in LangGraph causes the reduce function to be invoked twice",
    "link":"https:\/\/stackoverflow.com\/questions\/79725144\/the-conditional-edge-in-langgraph-causes-the-reduce-function-to-be-invoked-twice",
    "text":"I am a beginner with LangGraph and am learning to create a graph to compute the Fibonacci sequence. The program has two nodes: one responsible for updating the Fibonacci sequence, and the other for controlling the loop. The complete code is as follows: ``` from langchain_core.runnables import Runnable from collections.abc import Iterable from langgraph.graph import StateGraph from pydantic import BaseModel from typing import Any, Dict, Annotated from langgraph.constants import END def append_to_list(a : list, b) -> list: print(f\"append_to_list:{a}, {b}\") if isinstance(b, Iterable) and not isinstance(b, str): a.extend(b) else: a.append(b) return a class Node(Runnable): def __init__(self, name : str): self.name = name def invoke(self, input: Any, config: Dict | None = None) -> Any: c = input.a + input.b print(f\"{self.name}:{input}\") return {\"a\":input.b, \"b\":c, \"fibonacci_list\":c} class ConditionalNode(Runnable): def __init__(self, name : str, threshold: int): self.threshold = threshold self.name = name def invoke(self, input: Any, config) -> Any: print(f\"{self.name}:{input}\") return {\"path\" : input.b >= self.threshold} class State(BaseModel): a : int b : int fibonacci_list : Annotated[list[int], append_to_list] graph = StateGraph(State) graph.add_node(\"n1\", Node(name = \"n1\")) graph.add_conditional_edges(\"n1\", ConditionalNode(name = \"CN\", threshold = 10), {True : END, False: \"n1\"}) graph.set_entry_point(\"n1\") g = graph.compile() s = g.invoke({\"a\":1, \"b\":1, \"fibonacci_list\":[1, 1]}) print(s) ``` The n1 node is responsible for computing the latest value in the Fibonacci sequence and returning updates to the graph state. The append_to_list function is intended to add the newly generated fibonacci_list value to the fibonacci_list list in the graph state. I originally thought that append_to_list would be called only once after the n1 node finishes execution, and that the final result would be a complete Fibonacci sequence. However, in reality, the append_to_list function is also called every time the CN node finishes executing, and the arguments passed to it are the same as those passed after the n1 node finishes. This results in duplicate values being added to the Fibonacci sequence. For example, I expected to get [1, 1, 2, 3, 5, 8, 13], but instead I got [1, 1, 2, 2, 3, 3, 5, 5, 8, 8, 13, 13]. How can I prevent the conditional edge from triggering the reduce function?",
    "author_id":5538,
    "publication_date":1754330185000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"lei hu",
    "author_reputation":323.0,
    "tags":"python, langgraph",
    "text_length":2404,
    "title_length":80,
    "num_tags":2
  },
  {
    "id":6028,
    "title":"Can I have 2 `.navigationTransition`s on a single view?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725145\/can-i-have-2-navigationtransitions-on-a-single-view",
    "text":"I have a ``` List ``` inside a ``` NavigationStack ``` and there's also a ``` + ``` button in the toolbar. On tap on this button a new item is added and it's detail view is immediately pushed to navigation. If I tap the item, it's just pushed to navigation as usual. I would like to have 2 different transitions for these 2 cases. If I press the ``` + ``` button I'd like the detail view transition from it (the button) but when I tap on the existing item in the list, then I want it to transition from the row using ``` .matchedTransitionSource ``` and ``` .navigationTransition ``` . Can I achieve it? Here's what I do but the view doesn't get \"appeared\" from the ``` + ``` button. The row tap works fine: ``` NavigationStack(path: $path) { List { ForEach(items) { item in NavigationLink(value: item) { Text(item.content) .matchedTransitionSource(id: \"read-\\(item.id.uuidString)\", in: namespace) } } } .navigationDestination(for: Item.self) { item in ItemEditingView(item: item) .navigationTransition( .zoom(sourceID: \"add\", in: namespace) ) .navigationTransition( .zoom(sourceID: \"read-\\(item.id.uuidString)\", in: namespace) ) } .toolbar { ToolbarItem { Button(action: addItem) { Label(\"Add Item\", systemImage: \"plus\") .matchedTransitionSource(id: \"add\", in: namespace) } } } } ```",
    "author_id":5537,
    "publication_date":1754330231000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ramzesenok",
    "author_reputation":7137.0,
    "tags":"swiftui",
    "text_length":1284,
    "title_length":55,
    "num_tags":1
  },
  {
    "id":6027,
    "title":"CORS issue when syncing PouchDB with Capella",
    "link":"https:\/\/stackoverflow.com\/questions\/79725146\/cors-issue-when-syncing-pouchdb-with-capella",
    "text":"When trying to sync \/ replicate PouchDB from Capella (from frontend\/browswer), using App Services, we get a CORS Error: ``` Acces to fetch ____ has been blocked by CORS policy: Response to preflight request doesn't pass access control check: No 'Access-Control-Allow-Origin' header is present on the requested resource. ``` We have added the URL origin in the App Services \/ Advanced tab + the following custom headers: ``` Content-Type, Authorization, Access-Control-Allow-Origin ``` => now there is an ``` Acces-Control-Allow-Origin=\"\" ``` error. Has anyone faced this issue? The single solution we found so far, was to set up a proxy server which fixes the CORS issues. I do mention, we have also posted on the Capella Forum, but still waiting for response. Any help here would be much appreciated! :)",
    "author_id":5536,
    "publication_date":1754330378000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Alex Bran",
    "author_reputation":501.0,
    "tags":"couchbase, pouchdb, couchbase-sync-gateway",
    "text_length":804,
    "title_length":44,
    "num_tags":3
  },
  {
    "id":6026,
    "title":"Implement a custom runtime memory manager with Emscripten &amp; Wasmtime?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725153\/implement-a-custom-runtime-memory-manager-with-emscripten-wasmtime",
    "text":"I am using WebAssembly to implement a sandboxed plugin system in a C application. Users can write plugins in any language that will compile to WASM, and we just say \"you can import a, b, c functions, and you must export x, y, z, functions\". At the moment, WASM modules must export ``` malloc ``` and ``` free ``` so that the runtime environment can allocate space in WASM memory for passing arguments in, and free space used by arguments or return values passed out. That seems rather inefficient, as it involves every module having its own duplicated copy of the memory management functions, as well as wasting space in WASM memory with heap management overhead and running memory functions in the WASM VM rather than as native code. I would like to be able to be able to implement my own memory manager that runs in native code and keeps track of heap overhead externally, but that requires knowing something about the static memory layout of the module. I'm using Wasmtime for the embedded WASM VM, and I would like to be able to give users documentation on how to produce a compliant module with Emscripten. Emscripten has an option to omit a ``` malloc ``` implementation and provide your own, but I can't find any documentation on how to actually do the \"provide your own\" part. I cannot figure out how to get it to export a ``` __heap_base ``` symbol, and I've found conflicting claims about how the stack layout works, which complicates figuring out how to grow memory (do we just add more, or does the stack segment need to be copied to expand space between the stack and heap? And if the latter, how would the WASM program be informed of the address change?) I thought I might be able to compute a heap base on my own by inspecting the static data declarations in a WASM module to figure out where that ends, but Wasmtime doesn't seem to provide any means of doing so. So, how can I actually do this? Is there an example I can reference somewhere of replacing ``` malloc ``` and ``` free ``` with imported functions?",
    "author_id":5535,
    "publication_date":1754330773000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Logan R. Kearsley",
    "author_reputation":940.0,
    "tags":"c, memory-management, emscripten, wasmtime",
    "text_length":2026,
    "title_length":73,
    "num_tags":4
  },
  {
    "id":6025,
    "title":"Why can&#39;t I assign textures to Terrain3D object?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725157\/why-cant-i-assign-textures-to-terrain3d-object",
    "text":"I installed Terrain3D for Godot. I followed this YouTube video ; I processed my textures fine, and when I got to about 14:10, I realized that I cannot assign my textures in the texture array. I was able to clear the arrays by right clicking the assets and clearing, but I haven't managed to modify them more than that. I initially installed 4.4, then reset my project and tried with 4.3.stable, since Terrain3D seems to be made for 4.3. I have restarted my project a couple of times like the video says I should to no avail. Has anyone ever encountered this issue?",
    "author_id":5534,
    "publication_date":1754331233000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dandjix",
    "author_reputation":3.0,
    "tags":"godot, godot4",
    "text_length":564,
    "title_length":52,
    "num_tags":2
  },
  {
    "id":6024,
    "title":"How to connect docker app container to server through mariadb socket",
    "link":"https:\/\/stackoverflow.com\/questions\/79725158\/how-to-connect-docker-app-container-to-server-through-mariadb-socket",
    "text":"So I am trying to dockerize my Python program, which is a RESTful API simulating a Login page using Flask with MariaDB functionality. Dockerfile ``` FROM python:3.11-slim RUN mkdir \/login WORKDIR \/login RUN apt update && apt install python3-pip libmariadb3 libmariadb-dev -y COPY . . RUN pip install -r requirements.txt --break-system-packages CMD [ \"python3\", \"-m\" , \"main.py\"] ``` docker-compose.yaml ``` version: '3.8' services: app: build: . depends_on: db: condition: service_healthy environment: - DB_HOST=localhost - DB_USER=root - DB_PASSWORD=root - DB=user_db ports: - 5000:5000 db: image: mariadb:latest environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: user_db ports: - \"3308:3306\" volumes: - mariadb_data:\/var\/lib\/mysql healthcheck: test: \"mariadb $$MYSQL_DATABASE -uroot -p$$MYSQL_ROOT_PASSWORD -e 'SELECT 1;'\" interval: 2s timeout: 2s retries: 10 volumes: mariadb_data: driver: local ``` Relevant code ``` import mariadb import argon2 import re import os CONNECTION_SETTINGS = { \"host\": os.environ[\"DB_HOST\"], \"port\": 3308, \"user\": os.environ[\"DB_USER\"], \"password\": os.environ[\"DB_PASSWORD\"], \"database\": os.environ[\"DB\"], } try: self.conn = mariadb.connect(**CONNECTION_SETTINGS) except mariadb.Error as e: #print(\"An error has occurred while connecting to the database.\\nPlease check the inputted credentials and try again.\") print(e) exit() self.mycursor = self.conn.cursor() ``` With the above code in mind, I run ``` docker compose up --build --force-recreate -d ``` . which does build the 2 containers and the network neccesary, login_default, login-db-1 and login-app-1. However, while login_default and login-db-1 work and remain operational, login-app-1 always exits upon creation, and when I examine the container using do logs, I always get this error ``` Can't connect to local server through socket '\/run\/mysqld\/mysqld.sock' (2) ``` Any ideas on how to fix this? NOTE: For what it's worth, I am running all this on Windows using Windows PowerShell in VSCode",
    "author_id":5533,
    "publication_date":1754331267000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MZX",
    "author_reputation":1.0,
    "tags":"docker, mariadb",
    "text_length":1990,
    "title_length":68,
    "num_tags":2
  },
  {
    "id":6023,
    "title":"Merge 2 API JSON responses into one based on a unique Id present in both API responses using NiFi Jolt",
    "link":"https:\/\/stackoverflow.com\/questions\/79725160\/merge-2-api-json-responses-into-one-based-on-a-unique-id-present-in-both-api-res",
    "text":"Merge 2 API JSON responses into one based on a unique Id present in both API responses using NiFi Jolt. If the Aud_Id not found in both API response, then ignore them in the final output. The merge should happen if the endDate (from API response 1) is less than 30 days. API-1: ``` [ { \"Aud_id\": \"7f9a6541\", \"aud_name\": \"Holders_Current\", \"startDate\": \"2025-05-09\", \"endDate\": \"2026-01-31\" }, { \"Aud_id\": \"463e9d0c\", \"aud_name\": \"Expiring_Current\", \"startDate\": \"2025-05-15\", \"endDate\": \"2025-07-31\" }, { \"Aud_id\": \"4c973b25\", \"aud_name\": \"Reminder_Current\", \"startDate\": \"2025-05-15\", \"endDate\": \"2025-07-31\" }, { \"Aud_id\": \"6c915c60\", \"aud_name\": \"Inactive_14-60days\", \"startDate\": \"2025-05-09\", \"endDate\": \"2025-12-31\" }, { \"Aud_id\": \"87d32b36\", \"aud_name\": \"Inactive_14-60days\", \"startDate\": \"2024-12-02\", \"endDate\": \"2025-01-04\" } ``` ] API-2: [ { \"Aud_id\": \"7f9a6541\", \"Aud_count\": \"42526\" }, { \"Aud_id\": \"463e9d0c\", \"Aud_count\": \"63737\" }, { \"Aud_id\": \"4c973b25\", \"Aud_count\": \"728282\" }, { \"Aud_id\": \"6c915c60\", \"Aud_count\": \"282\" }, { \"Aud_id\": \"87d32b36\", \"Aud_count\": \"27278\" } ] Expected Output: ``` [ { \"Aud_id\": \"7f9a6541\", \"aud_name\": \"Holders_Current\", \"startDate\": \"2025-05-09\", \"endDate\": \"2026-01-31\", **\"Aud_count\": \"42526\"** }, { \"Aud_id\": \"463e9d0c\", \"aud_name\": \"Expiring_Current\", \"startDate\": \"2025-05-15\", \"endDate\": \"2025-07-31\", **\"Aud_count\": \"63737\"** }, { \"Aud_id\": \"4c973b25\", \"aud_name\": \"Reminder_Current\", \"startDate\": \"2025-05-15\", \"endDate\": \"2025-07-31\", **\"Aud_count\": \"728282\"** }, { \"Aud_id\": \"6c915c60\", \"aud_name\": \"Inactive_14-60days\", \"startDate\": \"2025-05-09\", \"endDate\": \"2025-12-31\", **\"Aud_count\": \"282\"** }, { \"Aud_id\": \"87d32b36\", \"aud_name\": \"Inactive_14-60days\", \"startDate\": \"2024-12-02\", \"endDate\": \"2025-01-04\", **\"Aud_count\": \"27278\"** } ``` ]",
    "author_id":5532,
    "publication_date":1754331319000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sam",
    "author_reputation":21.0,
    "tags":"apache-nifi, jolt",
    "text_length":1800,
    "title_length":102,
    "num_tags":2
  },
  {
    "id":6022,
    "title":"How to use GPUDirect RDMA from Bluefield 2 DPU?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725162\/how-to-use-gpudirect-rdma-from-bluefield-2-dpu",
    "text":"I’m working on a research experiment where I need to directly interact with an Nvidia GPU from a Bluefield DPU, bypassing the host CPU. Specifically, I want to: Write data directly into GPU memory from the Bluefield. Launch a CUDA kernel on the GPU from the Bluefield. Retrieve the kernel results back into the Bluefield. I know Nvidia Converged Accelerators DPUs can own the PCIe downstream so the GPU appears directly in the DPU, but my setup (Bluefield 2 + Nvidia T4 on the same node\/root-complex in a Dell R740xd) does not support that. I’ve read about using GPUDirect RDMA, and found references to ``` nvidia-peermem ``` and ``` DMA-buf ``` , but there’s no clear beginner-friendly documentation or step-by-step guide on how to configure and enable this for my use case. I’m running Nvidia DOCA development containers on both host and DPU. Can anyone provide instructions or best practices on how to: Enable GPUDirect RDMA for Bluefield to GPU communication? Perform direct GPU memory writes from the DPU? Launch CUDA kernels and fetch results on the DPU side? Thanks in advance!",
    "author_id":5531,
    "publication_date":1754331483000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mohammad Siavashi",
    "author_reputation":1272.0,
    "tags":"cuda, dma, rdma, gpudirect",
    "text_length":1084,
    "title_length":47,
    "num_tags":4
  },
  {
    "id":6021,
    "title":"Custom Border in CSS",
    "link":"https:\/\/stackoverflow.com\/questions\/79725167\/custom-border-in-css",
    "text":"I want to make the border-radius of a Button in CSS sharp like 45 deg instead of round. I tried it with clip-path, but it turned out only to be the background and not the border. Are there any solutions or some ways to make this possible? https:\/\/i.sstatic.net\/eArjNmwv.png ``` border: 1px solid var(--cyberRed); clip-path: polygon(0 0, 100% 0, 100% calc(100% - 6px), calc(100% - 6px) 100%, 0 100%, 0 0); ```",
    "author_id":5530,
    "publication_date":1754331679000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"burgerexe18",
    "author_reputation":11.0,
    "tags":"css, border-radius, button, border",
    "text_length":408,
    "title_length":20,
    "num_tags":4
  },
  {
    "id":6020,
    "title":"Vercel Deployment issue",
    "link":"https:\/\/stackoverflow.com\/questions\/79725169\/vercel-deployment-issue",
    "text":"I'm currently having deployment issues with Vercel throwing this error. The code works nicely locally as its login and register site backed by payload cms. ``` Type error: Type 'PageProps' does not satisfy the constraint 'import(\"C:\/Users\/User\/Documents\/GitHub\/new-payload-ecom\/.next\/types\/app\/(frontend)\/(account)\/login\/page\").PageProps'. Types of property 'searchParams' are incompatible. Type '{ [key: string]: string | undefined; } | undefined' is not assignable to type 'Promise<any> | undefined'. Type '{ [key: string]: string | undefined; }' is missing the following properties from type 'Promise<any>': then, catch, finally. ``` Heres my code, I have tried every solution but to no sucess ``` import React from 'react' import LoginForm from '.\/components\/loginForm' import { getUser } from '@\/app\/(frontend)\/(auth)\/actions\/getUser' import { redirect } from 'next\/navigation' interface SearchParams { [key: string]: string | undefined; } export default async function Page({searchParams}: {searchParams: SearchParams}): Promise<React.ReactElement> { const user = await getUser() if (user) { redirect('\/dashboard') } const {message} = await searchParams return <div > <div >{message && <p >{message}<\/p>}<\/div> <LoginForm \/> <\/div> } ```",
    "author_id":5529,
    "publication_date":1754331877000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MacDee501",
    "author_reputation":1.0,
    "tags":"next.js, typescript, vercel",
    "text_length":1243,
    "title_length":23,
    "num_tags":3
  },
  {
    "id":6019,
    "title":"Getting connection error when trying to trigger desktop flow using API",
    "link":"https:\/\/stackoverflow.com\/questions\/79725176\/getting-connection-error-when-trying-to-trigger-desktop-flow-using-api",
    "text":"I am trying to trigger desktop flow using API and getting error as below:- User with object identifier 'xxxx-yyyy' does not have access to the connection 'abcdefr' I have followed the steps mentioned in below link https:\/\/learn.microsoft.com\/en-us\/power-automate\/developer\/desktop-flow-public-apis . 1)Created the app in Azure entra id. 2)Able to get access token. 3)added the app in power automate in user and provided system admin. 4)when trying to trigger the desktop flow via post man getting the error saying the identifier(app created in azure) does not have access to connection",
    "author_id":5528,
    "publication_date":1754332350000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user3686276",
    "author_reputation":35.0,
    "tags":"power-automate, dataverse, power-automate-desktop",
    "text_length":585,
    "title_length":70,
    "num_tags":3
  },
  {
    "id":6018,
    "title":"Azure CosmosDB Bulk insert in Python",
    "link":"https:\/\/stackoverflow.com\/questions\/79725177\/azure-cosmosdb-bulk-insert-in-python",
    "text":"I have 50,000 entries that I want to store in my CosmosDB container. As this takes a while so far, I am looking for a fast way to achieve this. I am using Python and couldn't find a native bulk insert support. I currently do this, but it's taking a while and I was wondering if there is a faster approach: ``` class CosmosDB: def __init__(self) -> None: self.client = CosmosClient(url=\"...\", credential=config.azure_cosmosdb_access_key) database = self.client.get_database_client(\"...\") self.container = database.get_container_client(\"...\") ... async def bulk_insert(self, data: List[Dict[str, Any]], batch_size: int = 1000) -> None: semaphore = asyncio.Semaphore(100) # only 100 at once async def _limited_upsert(item: Dict[str, Any]): # to avoid cosmos db limits async with semaphore: return await self.container.upsert_item(item) successful_writes = 0 failed_writes = 0 for i in range(0, len(data), batch_size): # to limit system load batch = data[i:i + batch_size] operations = [_limited_upsert(item) for item in batch] results = await asyncio.gather(*operations, return_exceptions=True) for j, result in enumerate(results): item_id = data[j].get('id', 'N\/A') if isinstance(result, CosmosHttpResponseError): failed_writes += 1 logger.error( f\"Error writing item '{item_id}': Status {result.status_code}, Message: {result.message}\" ) if result.status_code == StatusCodes.CONFLICT: logger.warning(f\" Item '{item_id}' might already exist.\") elif isinstance(result, Exception): failed_writes += 1 logger.error(f\"Unexpected error for item '{item_id}': {result}\") else: successful_writes += 1 logger.info(f\"\\nBulk operation completed. Successful: {successful_writes}, Failed: {failed_writes}\") ```",
    "author_id":5527,
    "publication_date":1754332440000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jan",
    "author_reputation":31.0,
    "tags":"python, azure-cosmosdb, bulkinsert, azure-cosmosdb-sqlapi",
    "text_length":1695,
    "title_length":36,
    "num_tags":4
  },
  {
    "id":6017,
    "title":"How to delay initialization of SQL-based configuration in ASP.NET Core startup without using BuildServiceProvider?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725178\/how-to-delay-initialization-of-sql-based-configuration-in-asp-net-core-startup-w",
    "text":"I am using an ASP.NET Core 9.0 Web API with C#. My application setup uses L1 Cache using FusionAPI and L2 is Garnet Server for distributed layer of cache. The following code snippet is from my ``` Startup.cs ``` class. Some of my configurations are coming from SQL as this is a business requirement. I have simplified the code shown here: ``` public void ConfigureServices(IServiceCollection services) { services.AddSingleton<IDbConnectionFactory, DbConnectionFactory>(); services.AddSingleton<ISqlRepository, SqlRepository>(); ConfigParamStore paramStore = null; using (var tempProvider = services.BuildServiceProvider()) { var sqlRepo = tempProvider.GetRequiredService<ISqlRepository>(); paramStore = sqlRepo.GetConfigParamStoreAsync().GetAwaiter().GetResult(); } var garnetConnectionString = paramStore[ConfigParamNames.GarnetServerConnectionString].GetValueAsString(); var sslHostName = paramStore[ConfigParamNames.GarnetServerSslHostName].GetValueAsString(); ConfigurationOptions garnetConfigOptions = new ConfigurationOptions { EndPoints = { garnetConnectionString }, User = paramStore[ConfigParamNames.GarnetServerAclUserName].GetValueAsString(), Password = paramStore[ConfigParamNames.GarnetServerAclPassword].GetValueAsString(), Ssl = true, SslHost = sslHostName, ConnectTimeout = paramStore[ConfigParamNames.GarnetServerConnectionTimeoutInMilliseconds].GetValueAsInt(), SyncTimeout = paramStore[ConfigParamNames.GarnetServerSyncTimeoutInMilliseconds].GetValueAsInt(), ConnectRetry = paramStore[ConfigParamNames.GarnetServerConnectionRetries].GetValueAsInt(), AllowAdmin = false }; var connectionMultiplexer = ConnectionMultiplexer.Connect(garnetConfigOptions); services.AddControllers().AddOData(opt => { }); services.AddHttpContextAccessor(); \/\/ Removed other configurations to keep code small. services.AddSingleton<IFusionCacheSerializer, FusionCacheNewtonsoftJsonSerializer>(); services.AddFusionCache() .WithDistributedCache(new RedisCache(new RedisCacheOptions { ConfigurationOptions = garnetConfigOptions })) .WithDefaultEntryOptions(new FusionCacheEntryOptions { Duration = TimeSpan.FromMilliseconds(paramStore[ConfigParamNames.FusionCacheDurationInMilliseconds].GetValueAsInt()), DistributedCacheDuration = TimeSpan.FromMilliseconds(paramStore[ConfigParamNames.GarnetCacheDurationInMilliseconds].GetValueAsInt()), AllowTimedOutFactoryBackgroundCompletion = true, IsFailSafeEnabled = true, FailSafeMaxDuration = TimeSpan.FromMilliseconds(paramStore[ConfigParamNames.FusionFailSafeMaxDurationInMilliseconds].GetValueAsInt()), DistributedCacheFailSafeMaxDuration = TimeSpan.FromMilliseconds(paramStore[ConfigParamNames.GarnetFailSafeMaxDurationInMilliseconds].GetValueAsInt()), FailSafeThrottleDuration = TimeSpan.FromMilliseconds(paramStore[ConfigParamNames.GarnetFailSafeThrottleDurationInMilliseconds].GetValueAsInt()), ReThrowDistributedCacheExceptions = true }); services.AddSingleton<IConnectionMultiplexer>(connectionMultiplexer); } ``` My question is: how can I avoid writing this line of code: ``` var tempProvider = services.BuildServiceProvider() ``` I want to avoid this line, and I want to initialize ``` paramStore ``` bit late so that all DIs would be proper, but in this case how would I use the database values in method ``` services.AddFusionCache() ``` ? Variable ``` paramStore ``` is fetching and holding values from the SQL database, which is used in initializing the properties of method call ``` services.AddFusionCache() ``` . How to fetch the values from the database and still not doing early initialization like this: ``` using (var tempProvider = services.BuildServiceProvider()) ``` Please guide.",
    "author_id":5415,
    "publication_date":1754332487000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Creative Learner",
    "author_reputation":33.0,
    "tags":"asp.net-core, dependency-injection, distributed-caching, web-hosting, fusion-cache",
    "text_length":3642,
    "title_length":114,
    "num_tags":5
  },
  {
    "id":6016,
    "title":"Raycast not detecting the collision when hitting ground",
    "link":"https:\/\/stackoverflow.com\/questions\/79725184\/raycast-not-detecting-the-collision-when-hitting-ground",
    "text":"I have a ground object that has a Collider2D attached to the ground game object, this game object has a layer name of \"Ground\". I have a player object that walks on the ground but when the player moves in the air to fall on the ground the collision does not occur using the Raycast: Here is the code in the fixed update: ``` Debug.DrawRay(transform.position, transform.TransformDirection(-Vector2.up) * 0.7f, UnityEngine.Color.red); RaycastHit2D hit = Physics2D.Raycast(transform.position, transform.TransformDirection(-Vector2.up), 0.7f); \/\/ If it hits something... if (hit && hit.transform.gameObject != gameObject && hit.transform.gameObject.layer == LayerMask.NameToLayer(\"Ground\")) { Debug.Log(\"Hit \" + hit.transform.gameObject.tag); } ``` Here is the DrawRay inside the game when viewing the scene: You can see the raycast of the red colour which is intersecting the player and the ground. Where am I going wrong. When i remove \"&& hit.transform.gameObject.layer == LayerMask.NameToLayer(\"Ground\")\", I get a hit when the player is on top of the ladder and moves down from the ladder.",
    "author_id":5526,
    "publication_date":1754332965000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"redoc01",
    "author_reputation":2461.0,
    "tags":"unity-game-engine",
    "text_length":1089,
    "title_length":55,
    "num_tags":1
  },
  {
    "id":6015,
    "title":"How do I filter out audio spillage from audio extraction?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725186\/how-do-i-filter-out-audio-spillage-from-audio-extraction",
    "text":"I created a transcription and audio extraction Python script using Claude AI and Whisper. I get audio spillage that comes into the audio extracted (the wave form at the start and end of the screenshot). How do I create a script to eliminate this? A detector that detects very little sounds as unwanted hasn't worked. How do I create a script to remove audio spillage? ``` import os import numpy as np from pydub import AudioSegment from pydub.silence import detect_nonsilent def analyze_audio_levels(audio_file): \"\"\"Analyze audio to suggest optimal thresholds\"\"\" audio = AudioSegment.from_file(audio_file) # Convert to numpy array for analysis samples = np.array(audio.get_array_of_samples()) if audio.channels == 2: samples = samples.reshape((-1, 2)) samples = samples.mean(axis=1) # Convert to float and normalize samples = samples.astype(np.float32) samples = samples \/ (2**15) # Normalize 16-bit audio # Calculate RMS in sliding windows window_size = int(audio.frame_rate * 0.1) # 100ms windows rms_values = [] for i in range(0, len(samples) - window_size, window_size): window = samples[i:i + window_size] rms = np.sqrt(np.mean(window ** 2)) if rms > 0: rms_db = 20 * np.log10(rms) rms_values.append(rms_db) if rms_values: # Suggest thresholds based on analysis noise_floor = np.percentile(rms_values, 20) # 20th percentile as noise floor suggested_threshold = noise_floor + 10 # 10dB above noise floor print(f\"\\n📊 AUDIO ANALYSIS:\") print(f\" Noise floor: {noise_floor:.1f} dB\") print(f\" Suggested threshold: {suggested_threshold:.1f} dB\") print(f\" RMS range: {min(rms_values):.1f} to {max(rms_values):.1f} dB\") return suggested_threshold return -40 # Default fallback def remove_silence_with_padding(audio_file, silence_thresh=-40, min_silence_len=500, min_sound_len=1000, use_adaptive=True, start_padding=100, end_padding=200, fade_in_duration=50, fade_out_duration=100): \"\"\" Advanced silence removal with padding to preserve natural audio transitions Args: audio_file (str): Path to audio file silence_thresh (int): Threshold in dB min_silence_len (int): Minimum silence duration in ms min_sound_len (int): Minimum sound duration to keep in ms use_adaptive (bool): Use adaptive threshold based on audio analysis start_padding (int): Extra time to keep before audio starts (ms) end_padding (int): Extra time to keep after audio ends (ms) fade_in_duration (int): Fade in duration for smooth start (ms) fade_out_duration (int): Fade out duration for smooth end (ms) \"\"\" print(f\"🔍 Analyzing: {os.path.basename(audio_file)}\") # Load audio audio = AudioSegment.from_file(audio_file) original_duration = len(audio) \/ 1000 # Auto-detect better threshold if requested if use_adaptive: suggested_thresh = analyze_audio_levels(audio_file) if suggested_thresh > silence_thresh: print(f\" Using adaptive threshold: {suggested_thresh:.1f} dB (instead of {silence_thresh})\") silence_thresh = int(suggested_thresh) # Detect non-silent chunks nonsilent_chunks = detect_nonsilent( audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh ) if not nonsilent_chunks: print(f\" ❌ No audio content detected\") return None print(f\" Found {len(nonsilent_chunks)} audio segments\") # Filter chunks by duration and apply padding filtered_chunks = [] total_removed_time = 0 for i, (start_time, end_time) in enumerate(nonsilent_chunks): segment_duration = end_time - start_time # Extract segment for additional analysis segment = audio[start_time:end_time] # Calculate segment RMS samples = np.array(segment.get_array_of_samples(), dtype=np.float32) if segment.channels == 2: samples = samples.reshape((-1, 2)).mean(axis=1) rms = np.sqrt(np.mean((samples \/ 32768) ** 2)) segment_db = 20 * np.log10(rms) if rms > 0 else -float('inf') # Apply duration and level filtering if segment_duration >= min_sound_len and segment_db > silence_thresh: # Apply padding to preserve natural transitions padded_start = max(0, start_time - start_padding) padded_end = min(len(audio), end_time + end_padding) filtered_chunks.append((padded_start, padded_end)) print(f\" ✓ Kept segment {i+1}: {start_time\/1000:.1f}s-{end_time\/1000:.1f}s → {padded_start\/1000:.1f}s-{padded_end\/1000:.1f}s ({segment_db:.1f}dB)\") else: total_removed_time += segment_duration \/ 1000 reason = \"too short\" if segment_duration < min_sound_len else \"too quiet\" print(f\" ❌ Removed segment {i+1}: {start_time\/1000:.1f}s-{end_time\/1000:.1f}s ({reason}, {segment_db:.1f}dB)\") if not filtered_chunks: print(f\" ❌ No segments passed filtering\") return None # Merge overlapping chunks (after padding, some might overlap) merged_chunks = [] for start, end in sorted(filtered_chunks): if merged_chunks and start <= merged_chunks[-1][1]: # Overlapping or adjacent - merge merged_chunks[-1] = (merged_chunks[-1][0], max(merged_chunks[-1][1], end)) else: merged_chunks.append((start, end)) print(f\" 📎 Merged into {len(merged_chunks)} continuous segments\") # Combine all valid segments with smooth transitions cleaned_audio = AudioSegment.empty() for i, (start_time, end_time) in enumerate(merged_chunks): segment = audio[start_time:end_time] # Apply fade in\/out for smooth transitions if i == 0 and fade_in_duration > 0: # Fade in at the very beginning segment = segment.fade_in(min(fade_in_duration, len(segment))) if i == len(merged_chunks) - 1 and fade_out_duration > 0: # Fade out at the very end segment = segment.fade_out(min(fade_out_duration, len(segment))) cleaned_audio += segment final_duration = len(cleaned_audio) \/ 1000 time_saved = original_duration - final_duration print(f\" 📊 Results:\") print(f\" Original: {original_duration:.1f}s\") print(f\" Cleaned: {final_duration:.1f}s\") print(f\" Removed: {time_saved:.1f}s ({time_saved\/original_duration*100:.1f}%)\") print(f\" Applied padding: start +{start_padding}ms, end +{end_padding}ms\") return cleaned_audio def process_folder_with_padding(input_folder, output_folder=None, **kwargs): \"\"\"Process folder with padding settings\"\"\" if output_folder is None: output_folder = os.path.join(input_folder, \"cleaned_audio_padded\") os.makedirs(output_folder, exist_ok=True) audio_extensions = ['.mp3', '.wav', '.flac', '.m4a', '.ogg', '.wma'] processed_count = 0 skipped_count = 0 for filename in os.listdir(input_folder): if any(filename.lower().endswith(ext) for ext in audio_extensions): input_path = os.path.join(input_folder, filename) try: cleaned_audio = remove_silence_with_padding(input_path, **kwargs) if cleaned_audio is not None: name, ext = os.path.splitext(filename) output_filename = f\"{name}_cleaned{ext}\" output_path = os.path.join(output_folder, output_filename) # Export with same format if ext.lower() == '.mp3': cleaned_audio.export(output_path, format=\"mp3\", bitrate=\"192k\") elif ext.lower() == '.flac': cleaned_audio.export(output_path, format=\"flac\") else: cleaned_audio.export(output_path, format=\"wav\") print(f\" ✅ Saved: {output_filename}\") processed_count += 1 else: print(f\" ⚠️ Skipped: {filename} (no content)\") skipped_count += 1 except Exception as e: print(f\" ❌ Error processing {filename}: {str(e)}\") skipped_count += 1 else: print(f\" ⚠️ Skipped: {filename} (unsupported format)\") skipped_count += 1 print(f\"\\n📈 SUMMARY:\") print(f\" Processed: {processed_count} files\") print(f\" Skipped: {skipped_count} files\") print(f\" Output folder: {output_folder}\") def main(): print(\"=\" * 70) print(\" ENHANCED AUDIO SILENCE REMOVER WITH PADDING\") print(\"=\" * 70) print(\"This version preserves natural audio transitions and prevents abrupt cuts.\") print() # Get input folder input_folder = input(\"Enter input folder path: \").strip().strip('\"\\'') if not os.path.exists(input_folder): print(\"❌ Folder does not exist!\") return # Get output folder output_input = input(f\"Enter output folder (Enter for default): \").strip().strip('\"\\'') output_folder = output_input if output_input else None print(\"\\n🎛️ ADVANCED SETTINGS:\") print(\"For your waveform, try these settings:\") print() # Get threshold settings print(\"Silence threshold recommendations:\") print(\" -45 dB: Aggressive (recommended for your case)\") print(\" -35 dB: Very aggressive\") print(\" -30 dB: Extremely aggressive\") thresh_input = input(\"Silence threshold in dB (default: -35): \").strip() silence_thresh = -35 if not thresh_input else int(thresh_input) print(\"\\nMinimum silence length:\") print(\" 300ms: Aggressive (recommended)\") print(\" 500ms: Moderate\") silence_len_input = input(\"Min silence length in ms (default: 300): \").strip() min_silence_len = 300 if not silence_len_input else int(silence_len_input) print(\"\\nMinimum sound length:\") print(\" 800ms: Keep substantial audio\") print(\" 1200ms: Very selective\") sound_len_input = input(\"Min sound length in ms (default: 1000): \").strip() min_sound_len = 1000 if not sound_len_input else int(sound_len_input) # NEW: Padding settings print(\"\\n🛡️ PADDING SETTINGS (to prevent abrupt cuts):\") print(\"Start padding (extra time before audio starts):\") print(\" 100ms: Conservative\") print(\" 200ms: Safe (recommended)\") start_pad_input = input(\"Start padding in ms (default: 150): \").strip() start_padding = 150 if not start_pad_input else int(start_pad_input) print(\"\\nEnd padding (extra time after audio ends):\") print(\" 200ms: Conservative\") print(\" 300ms: Safe (recommended)\") print(\" 500ms: Very safe\") end_pad_input = input(\"End padding in ms (default: 300): \").strip() end_padding = 300 if not end_pad_input else int(end_pad_input) print(\"\\nFade durations for smooth transitions:\") fade_in_input = input(\"Fade in duration in ms (default: 50): \").strip() fade_in_duration = 50 if not fade_in_input else int(fade_in_input) fade_out_input = input(\"Fade out duration in ms (default: 100): \").strip() fade_out_duration = 100 if not fade_out_input else int(fade_out_input) adaptive_input = input(\"\\nUse adaptive threshold detection? (Y\/n): \").strip().lower() use_adaptive = adaptive_input != 'n' print(f\"\\n🚀 PROCESSING with settings:\") print(f\" Silence threshold: {silence_thresh} dB\") print(f\" Min silence length: {min_silence_len} ms\") print(f\" Min sound length: {min_sound_len} ms\") print(f\" Start padding: {start_padding} ms\") print(f\" End padding: {end_padding} ms\") print(f\" Fade in: {fade_in_duration} ms\") print(f\" Fade out: {fade_out_duration} ms\") print(f\" Adaptive detection: {use_adaptive}\") print() confirm = input(\"Start processing? (y\/N): \").strip().lower() if confirm == 'y': process_folder_with_padding( input_folder, output_folder, silence_thresh=silence_thresh, min_silence_len=min_silence_len, min_sound_len=min_sound_len, use_adaptive=use_adaptive, start_padding=start_padding, end_padding=end_padding, fade_in_duration=fade_in_duration, fade_out_duration=fade_out_duration ) else: print(\"❌ Processing cancelled.\") if __name__ == \"__main__\": main() ```",
    "author_id":5525,
    "publication_date":1754333053000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"stanonstack",
    "author_reputation":1.0,
    "tags":"python, audio, openai-whisper",
    "text_length":10678,
    "title_length":57,
    "num_tags":3
  },
  {
    "id":6014,
    "title":"How to monitor JS thread, UI thread, and frame drops in a React Native app with Hermes?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725193\/how-to-monitor-js-thread-ui-thread-and-frame-drops-in-a-react-native-app-with",
    "text":"I'm working on a production React Native app and want to monitor performance issues like: JS thread stalls UI thread responsiveness Memory usage and frame drops Detecting ANRs on Android Hermes is enabled in the app. What are the practical ways or APIs in React Native or Android\/iOS tooling to track these issues? For example, how to capture JS execution bottlenecks or measure long frames? I'm not asking for tool recommendations, but for platform-native or React Native APIs\/workflows that can help with diagnosing these performance concerns. Has anyone implemented frame drop detection or JS thread blocking analysis with Hermes?",
    "author_id":5524,
    "publication_date":1754333730000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Arijit das",
    "author_reputation":75.0,
    "tags":"react-native, performance",
    "text_length":633,
    "title_length":87,
    "num_tags":2
  },
  {
    "id":6013,
    "title":"Only match phrases when not part of other phrases with regex",
    "link":"https:\/\/stackoverflow.com\/questions\/79725194\/only-match-phrases-when-not-part-of-other-phrases-with-regex",
    "text":"I want to do the following with Javascript regex: Match words or phrases inside larger texts. However, if a word\/phrase is part of a set of excluded phrases, skip the match. For example: I want to match \"hell\" when part of \"The landscape of hell is quite dull\", but not when part of \"what the hell are you doing\". For this, each potential \"match\" has a list of the following properties: The phrase we want to match A list of excluded phrases Here's one of the regexes our system creates: ``` \/(?<=^|[\\s\"'‘’…_*<>~=\\p{P}])(hell(?:['‘’]?s)?)(?=[\\s\"'‘’…_*<>~=\\p{P}]|$)\/gimu ``` If we now want to add the exclusions, we found that there are a couple cases: An exclusion might only want to exclude a certain prefix (e.g. \"fricking hell\", \"bloody hell\"), or only a certain suffix (e.g. \"hell yeah\", \"hell no\") It doesn't matter if a prefix and suffix exist at the same time (e.g. \"bloody hell no\") - we can use regular negative lookbehinds and lookaheads for this and it works fine. The issue arises when there's a full phrase like \"the hell out of\". We can't merge this into the existing lookbehind and lookaheads, as this would also negate phrases like just \"the hell\" - and matching against \"the hell is a hot place\" will fail because of the matched prefix being just \"the\". I tried various approaches to see if I can \"negative match\" the full phrases first, and then go onto the lookbehind-lookahead block, but since the JS Regex engine backtracks when it fails, it will still match just fine for the original word if we use the following regex: ``` \/(?<=^|[\\s\"'‘’…_*<>~=\\p{P}])((?:(?!the\\s+hell(?:['‘’]?s)?\\s+out\\s+of)|(?<!(?:fricking|bloody)\\s+)hell(?:['‘’]?s)?(?!\\s+(?:yeah|no))))(?=[\\s\"'‘’…_*<>~=\\p{P}]|$)\/gimu ``` It also has the issue of matching results with zero length because of the negation. Here is a playground link to see what I mean: https:\/\/regex101.com\/r\/7zQeXs\/1 The only thing that works is when we DO match the excluded phrases, but then discard the match right after within our app logic itself (since there's no valid entry to lookup). How can I specify this matching condition with just regex?",
    "author_id":5523,
    "publication_date":1754333753000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"spaceemotion",
    "author_reputation":1548.0,
    "tags":"javascript, regex, regex-lookarounds, regex-negation",
    "text_length":2113,
    "title_length":60,
    "num_tags":4
  },
  {
    "id":6012,
    "title":"Specifying subscription from a variable",
    "link":"https:\/\/stackoverflow.com\/questions\/79725196\/specifying-subscription-from-a-variable",
    "text":"I'm using Terraform to create resources in Azure across multiple subscriptions. Ideally I'd like to be able to take the variables from a CSV, but I'm having trouble figuring out how to tell it which subscription to deploy into, without hardcoding it into the deployment block. I've added the subscriptions to my providers and used aliases. If I hard code an alias into the deployment, it does seem to work. If I pull the variable from my CSV or a map, it doesn't it keeps saying The provider argument requires a provider type name, optionally followed by a period and then a configuration alias. CoPilot + Gemini both tell me I can do somethig like this: ``` providers = { azurerm = local.provider_map[each.value.Subscription] } ``` But I get the same error each time. How would you achieve this?",
    "author_id":5522,
    "publication_date":1754333884000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"darknessshampoo",
    "author_reputation":21.0,
    "tags":"azure, terraform, subscription, azure-rm",
    "text_length":796,
    "title_length":39,
    "num_tags":4
  },
  {
    "id":6011,
    "title":"Unable to Authenticate to Docker image via Golang",
    "link":"https:\/\/stackoverflow.com\/questions\/79725198\/unable-to-authenticate-to-docker-image-via-golang",
    "text":"I am new to Docker and Golang. I am trying to run Golang server locally and connect it with my postgres docker image.Basically I want the DB to run as docker container with persitant volume Error ``` PS C:\\Users\\usero\\Desktop\\Operation_GatiShakti\\goBackEnd> go run . mongodb:\/\/mongoadmin:password@localhost:27017\/?tls=false 2025\/08\/04 22:08:16 POSTGRES_URI: postgres:\/\/myuser:mypassword@localhost:5432\/mydatabase?sslmode=disable 2025\/08\/04 22:08:16 Failed to connect to PostgreSQL: failed to connect to `user=myuser database=mydatabase`: [::1]:5432 (localhost): failed SASL auth: FATAL: password authentication failed for user \"myuser\" (SQLSTATE 28P01) exit status 1 ``` I have created my docker-compose.yml file as - ``` services: postgres: image: postgres:latest container_name: my-postgres environment: POSTGRES_DB: mydatabase POSTGRES_USER: myuser POSTGRES_PASSWORD: mypassword volumes: - postgres_data:\/var\/lib\/postgresql\/data ports: - \"5432:5432\" restart: unless-stopped volumes: postgres_data: ``` Inside my Golang code the .env file has ``` POSTGRES_URI=postgres:\/\/myuser:mypassword@localhost:5432\/mydatabase?sslmode=disable ``` The main.go file: ``` func main() { var pgPool *pgxpool.Pool \/\/====================================== \/\/ ENVIRONMENT VARIABLE \/\/====================================== err := godotenv.Load(\".env\") if err != nil { log.Fatal(\"Error loading .env file\") } \/\/====================================== \/\/ MongoDB connection \/\/====================================== client, err := mongoFunctions.ConnectMongoDB() if err != nil { log.Fatalf(\"Failed to connect to MongoDB: %v\", err) } defer func() { if err = client.Disconnect(context.Background()); err != nil { log.Fatalf(\"Failed to disconnect from MongoDB: %v\", err) } }() \/\/ Set global MongoDB collection handle MONGO_DB_UPSC := client.Database(\"UPSC\") \/\/====================================== \/\/ PostgreSQL connection \/\/====================================== pgPool, err = connectPostgres() if err != nil { log.Fatalf(\"Failed to connect to PostgreSQL: %v\", err) } defer pgPool.Close() smtpConfig := resetPassword.SMTPConfig{ Host: \"smtp.hostinger.com\", Port: \"587\", Username: \"office@somemail.com\", Password: os.Getenv(\"SMTP_PASSWORD\"), From: \"office@somemail.com\", } resetService := resetPassword.NewService(pgPool, smtpConfig) \/\/ Setup logger logger := slog.New(slog.NewJSONHandler(os.Stdout, nil)) slog.SetDefault(logger) \/\/ Setup Gin router router := gin.Default() router.Use(corsMiddleware) \/\/----------------------------------- \/\/ LOGIN \/\/----------------------------------- router.POST(\"\/login\", ratelimit.LoginMiddleware(), func(c *gin.Context) { authentication.LoginUser(c, pgPool) }) router.GET(\"\/refresh\", func(c *gin.Context) { authentication.RefreshHandler(c.Writer, c.Request) }) router.GET(\"\/me\", func(c *gin.Context) { userID, err := authentication.ValidateAccessToken(c.Request) if err != nil { c.JSON(http.StatusUnauthorized, gin.H{\"error\": \"Unauthorized\"}) return } c.JSON(http.StatusOK, gin.H{\"id\": userID}) }) \/\/----------------------------------- \/\/ PASSWORD RESET \/\/----------------------------------- router.POST(\"\/resetpassword\", resetService.ResetPasswordHandler) \/\/----------------------------------- \/\/ MONGO \/\/----------------------------------- router.GET(\"\/UPSC_History\", func(c *gin.Context) { mongoFunctions.GetAllDocument(c, MONGO_DB_UPSC, \"History\") }) \/\/--------------------------------------- \/\/ POSTGRES \/\/--------------------------------------- router.GET(\"\/pg-users\", func(c *gin.Context) { registration.GetPostgresUsersHandler(c, pgPool) }) router.POST(\"\/pg-users\", func(c *gin.Context) { registration.CreatePostgresUserHandler(c, pgPool) }) router.POST(\"\/pg-users\/check\", func(c *gin.Context) { registration.CheckUserExistsHandler(c, pgPool) }) router.Run(\":8080\") } \/\/ ✅ CORS middleware func corsMiddleware(c *gin.Context) { origin := c.Request.Header.Get(\"Origin\") if origin != \"\" { c.Writer.Header().Set(\"Access-Control-Allow-Origin\", origin) c.Writer.Header().Set(\"Access-Control-Allow-Credentials\", \"true\") c.Writer.Header().Set(\"Access-Control-Allow-Headers\", \"Content-Type\") c.Writer.Header().Set(\"Access-Control-Allow-Methods\", \"POST, GET, OPTIONS\") } if c.Request.Method == \"OPTIONS\" { c.AbortWithStatus(204) return } c.Next() } \/\/ ✅ PostgreSQL connection func connectPostgres() (*pgxpool.Pool, error) { uri := os.Getenv(\"POSTGRES_URI\") log.Println(\"POSTGRES_URI:\", os.Getenv(\"POSTGRES_URI\")) if uri == \"\" { log.Fatal(\"POSTGRES_URI environment variable is required\") } ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() pool, err := pgxpool.New(ctx, uri) if err != nil { return nil, err } if err := pool.Ping(ctx); err != nil { return nil, err } return pool, nil } ``` I have tried removing image and volumes and restarting container, creating a new user as well. Tried Localhost, 127.0.0.1 and 0.0.0.0 as Ip.Postgres Docker image logs show it runs on 0.0.0.0 This fails even on typing correct password ``` psql -h localhost -p 5432 -U myuser ``` . But commands like - ``` docker exec -it postgres_demo psql -U postgres ``` work",
    "author_id":5521,
    "publication_date":1754334046000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"harsh9607",
    "author_reputation":11.0,
    "tags":"docker-compose, postgresql, docker, go, pgx",
    "text_length":5095,
    "title_length":49,
    "num_tags":5
  },
  {
    "id":6010,
    "title":"Why does ffmpeg command work in terminal but not subprocess.call",
    "link":"https:\/\/stackoverflow.com\/questions\/79725199\/why-does-ffmpeg-command-work-in-terminal-but-not-subprocess-call",
    "text":"I reviewed the \"similar questions\", but those seem to refer to using the full path for ffmpeg to ensure using the same version, which I am already doing so posting my question. I am using a Mac runnning Sequoia 15.5, python 3, and ffmpeg 4.2.1. I'm trying to write a script that can convert flac to mp3 320k. If I use the following command in terminal everything works as intended: ``` \/usr\/local\/bin\/ffmpeg -i \"\/Volumes\/MainData\/Media\/Media Server\/_Stage\/03 - People Like Us - Aaron Tippin.flac\" -b:a 320k -map_metadata 0 -id3v2_version 3 \"\/Volumes\/MainData\/Media\/Media Server\/_Stage\/03 - People Like Us - Aaron Tippin.mp3\" ``` But when trying to do the same thing with subprocess.call with the following: ``` command = ['\/usr\/local\/bin\/ffmpeg', '-i', '\/Volumes\/MainData\/Media\/Media Server\/_Stage\/03 - People Like Us - Aaron Tippin.flac', '-b:a 320k', '-map_metadata 0', '-id3v2_version 3', '\/Volumes\/MainData\/Media\/Media Server\/_Stage\/03 - People Like Us - Aaron Tippin.mp3'] p = subprocess.call(command) ``` I get this as a result: ``` Unrecognized option 'id3v2_version 3'. Error splitting the argument list: Option not found ``` If I remove the '-id3v2_version 3', then the subprocess.call results in the following error: ``` [mp3 @ 0x7fae2c008200] Invalid stream specifier: a 320k. ``` Why does Terminal understand but subprocess.call doesn't when I've verified I'm using the same version of ffmpeg?",
    "author_id":5520,
    "publication_date":1754334092000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Brian",
    "author_reputation":117.0,
    "tags":"python, ffmpeg, subprocess",
    "text_length":1405,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":6009,
    "title":"Recursive function to create and populate dynamic-depth nested dictionary",
    "link":"https:\/\/stackoverflow.com\/questions\/79725201\/recursive-function-to-create-and-populate-dynamic-depth-nested-dictionary",
    "text":"I am attempting to build and populate a data structure consisting of multiple, but variable count, nested dictionaries in C#. I am currently using a recursive function to do so. The data stored in the structure is the contents of a PrintQueue's Location field parsed according to a naming convention. In this case, the convention is: Campus-Building-Floor-Room-Subroom The basic data structure would therefore be (Dict)[Campus, (Dict)[Building, (Dict)[Floor, (Dict)[Room, (Dict)[Subroom, (List)[PrintQueue]]]]]] Unfortunately I can't assume every campus will have the same naming convention, and getting each campus to switch is not feasible. Hence the need to make this dynamic. Building the data structure initially works fine but populating it with data is proving an issue for me. Everything works as intended for the initial drill down, but I'm struggling to wrap my head around how the function actually works so that I can add entries to the dictionaries when a key already exists. My code right now is: ``` private void popData() { var poppedData = new SortedDictionary<string, object>(); var keyListList = new List<List<string>>() { new () {\"UCSC\",\"123\",\"1\",\"102\",\"N\/A\"}, new () {\"UCSC\",\"456\",\"2\",\"210\",\"N\/A\"}, new () {\"UCSC\",\"789\",\"3\",\"302\",\"1\"} }; foreach (var list in keyListList) { var queue = \"test\"; var queueList = new List<string>(); poppedData = createDataDictionary(list, queue, queueList, poppedData); } } private dynamic createDataDictionary(List<string> keyList, string queue, List<string> queueList, SortedDictionary<string, object> tempData) { string temp = keyList.ElementAt(0).ToUpper(); if (tempData.ContainsKey(temp)) { keyList.RemoveAt(0); return createDataDictionary(keyList, queue, queueList, (SortedDictionary<string, object>)tempData[temp]); } else if (keyList.Count() > 1) { keyList.RemoveAt(0); tempData.Add(temp, createDataDictionary(keyList, queue, queueList, tempData)); return tempData; } else if (keyList.Count() == 1) { queueList.Add(queue); return new SortedDictionary<string, List<string>>() { { temp, queueList } }; } else { keyList.RemoveAt(0); return new SortedDictionary<string, object> { { temp, createDataDictionary(keyList, queue, queueList, tempData) } }; } } ``` As I said, it does not currently work, and this version is actually worse than the one that functioned more or less correctly, but I lost that version for reasons. I have tried many, many versions of this function but I am not good at visualizing recursion so most of it has just been banging my head against the wall with occasional incremental improvements. My question is this: How do I adapt this recursive function to allow it to add relevant dictionaries at levels in the structure that have existing keys?",
    "author_id":5519,
    "publication_date":1754334157000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"buca117",
    "author_reputation":21.0,
    "tags":"c#, dictionary, recursion",
    "text_length":2727,
    "title_length":73,
    "num_tags":3
  },
  {
    "id":6008,
    "title":"Can&#39;t write to user mode address space from kernel using MmCopyMemory",
    "link":"https:\/\/stackoverflow.com\/questions\/79725203\/cant-write-to-user-mode-address-space-from-kernel-using-mmcopymemory",
    "text":"I've got this software based kernel mode driver (KMDF) and I've called MmcopyMemory like the following (see below). Essentially I would like to patch a user mode process in memory. The two passed parameters ``` PVOID TargetMemory ``` and ``` HANDLE ProcessId ``` passed to this function ( ``` WriteToMemoryMappedSection ``` ) are gathered by way of callback routine (see below). So I'm getting a BSOD ``` IRQL_NOT_LESS_OR_EQUAL ``` in VMWare Workstation Guest OS Windows 11 24H2. I can post the crash dump if anyone makes the request. Hopefully this all makes sense. My apologies if I am not clear. I will edit my question if need be. ``` VOID WriteToMemoryMappedSection(HANDLE ProcessId, PVOID TargetMemoryAddress) { PEPROCESS process = NULL; KAPC_STATE apcState; MM_COPY_ADDRESS virtualSourceAddress = { 0 }; size_t NumberOfBytesTransferred = 0; NTSTATUS status = STATUS_SUCCESS; UNREFERENCED_PARAMETER(status); char Temp[2] = { 0x90, 0x90 }; PsLookupProcessByProcessId(ProcessId, &process); KeStackAttachProcess(process, &apcState); virtualSourceAddress.VirtualAddress = (PVOID)(Temp); __try { status = MmCopyMemory ( (PVOID)((ULONG64)TargetMemoryAddress + (ULONG64)0x1000), virtualSourceAddress, 2, MM_COPY_MEMORY_VIRTUAL, &NumberOfBytesTransferred ); } __except (EXCEPTION_EXECUTE_HANDLER) { DbgPrint(\"Exception 2: 0x%08X\\n\", GetExceptionCode()); } KeUnstackDetachProcess(&apcState); ObDereferenceObject(process); } \/\/ declaration of my callback routine VOID MyLoadImageNotifyRoutine ( _In_opt_ PUNICODE_STRING FullImageName, _In_ HANDLE ProcessId, _In_ PIMAGE_INFO ImageInfo ); ```",
    "author_id":5518,
    "publication_date":1754334198000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"blogger13",
    "author_reputation":97.0,
    "tags":"process, kernel, 64-bit, usermode",
    "text_length":1587,
    "title_length":73,
    "num_tags":4
  },
  {
    "id":6007,
    "title":"FlashList React Native rendering mismatch",
    "link":"https:\/\/stackoverflow.com\/questions\/79725205\/flashlist-react-native-rendering-mismatch",
    "text":"Im using flashlist from shopify to contain simple card components with a vertical scroll, the cards have an image and a title shown inmediately. I am having an issue where when I get to around 10 cards, while scrolling, the image for the new card entering the screen is swapped with the image of another card for less than a second until it changes to its original image. Note that only the image changes, the title and everything else stays the same the whole time. I was thinking this may be due to the way it handles its virtual list but I cannot seem to find any issue with this. Here is my implementation FlashList wrapper ``` export function HorizontalFlatList<T = any>({ containerStyle, contentContainerStyle, data, keyExtractor = (_, index) => index.toString(), renderItem, emptyState, style, footer, header, onScroll, removeClippedSubviews, estimatedItemSize, drawDistance, disableAutoLayout = false, initialScrollIndex, }: Props<T>) { return ( <View style={[tw(\"w-full\", \"flex-1\"), containerStyle]}> <FlashList showsVerticalScrollIndicator={false} data={data} keyExtractor={keyExtractor} \/\/ renderItem={(info) => <>{renderItem(info)}<\/>} renderItem={renderItem} ListEmptyComponent={emptyState} keyboardShouldPersistTaps=\"handled\" style={[tw(\"h-full\"), style]} contentContainerStyle={contentContainerStyle} ListFooterComponent={footer} ListHeaderComponent={header} onScroll={onScroll} scrollEventThrottle={1} removeClippedSubviews={removeClippedSubviews} estimatedItemSize={estimatedItemSize} drawDistance={drawDistance} \/\/ Distance to render items ahead\/behind disableAutoLayout={disableAutoLayout} \/\/ Let FlashList handle layout optimization initialScrollIndex={initialScrollIndex} \/> <\/View> ); } ``` Parent component with fetching logic ``` export const BrowseContent = () => { const activeServiceFilter = useAppSelector(selectActiveServiceFilter); const filters = useAppSelector(selectSitterFilters); const { data: sitters, isLoading } = useFetchSitters(filters); const { data: user } = useFetchUserUnsafeScreen(); const headerContent = useMemo( () => ( <View style={tw(\"mb-6\")}> ... <\/View> ), [activeServiceFilter, heightAnimation], ); const renderItem = useCallback( ({ item }: { item: User }) => <Card sitter={item} user={user} \/>, [user], ); return ( <SafeAreaViewStable style={tw(\"flex-1\", \"h-full\", \"bg-gray-050\")} edges={[\"top\", \"right\", \"left\"]} > <View style={tw(\"gap-6\", \"pt-0\", \"flex-1\")}> <HorizontalFlatList data={sitters ?? []} emptyState={isLoading ? <BrowseTabLoadingState \/> : <BrowseTabEmptyState \/>} keyExtractor={(sitter) => `recent-activity-${sitter.id}`} renderItem={renderItem} contentContainerStyle={tw(\"gap-6\", \"px-4\")} removeClippedSubviews estimatedItemSize={350} drawDistance={400} header={headerContent} \/> <\/View> <\/SafeAreaViewStable> ); }; ```",
    "author_id":5517,
    "publication_date":1754334286000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jose Izarra",
    "author_reputation":11.0,
    "tags":"react-native, ui-virtualization, mobile-development, react-virtualized, flash-list",
    "text_length":2790,
    "title_length":41,
    "num_tags":5
  },
  {
    "id":6006,
    "title":"Git branch -vv except showing committerdate::relative and sort by",
    "link":"https:\/\/stackoverflow.com\/questions\/79725208\/git-branch-vv-except-showing-committerdaterelative-and-sort-by",
    "text":"I want a pretty ``` git branch -vv ``` except with committerdate::relative in it. With all the same colors and pretty formatting This person had exactly the same desire I did. But they never got to a working example. My usual methods struck out…robots are bad at writing this one.",
    "author_id":5516,
    "publication_date":1754334421000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Eric Auld",
    "author_reputation":1318.0,
    "tags":"git",
    "text_length":280,
    "title_length":65,
    "num_tags":1
  },
  {
    "id":6005,
    "title":"Workmanager task running every 15 minutes even when I configure it to run less often",
    "link":"https:\/\/stackoverflow.com\/questions\/79725210\/workmanager-task-running-every-15-minutes-even-when-i-configure-it-to-run-less-o",
    "text":"I use workmanager package to schedule background tasks, I have this setup for setting background autobackup task to run eeither daily, weekly or monthly, but it runs every 15 minutes not matter what frequency I set. this is my code: ``` if(value) final frequency = state.autoBackupFrequency; Duration duration = frequency == 'daily' ? Duration(days: 1) : frequency == 'weekly' ? Duration(days: 7) : Duration(days: 30); \/\/ This log also prints correct duration MiniLogger.dp( 'Registering background task: frequency: $frequency, duration: $duration', ); await Workmanager().registerPeriodicTask( mAutoBackup, mAutoBackup, initialDelay: Duration(seconds: 5), \/\/ Here I am setting the duration frequency: duration, ); } else { MiniLogger.dp('Cancelling background task'); await Workmanager().cancelByUniqueName(mAutoBackup); } } on GoogleClientNotAuthenticatedError { if (context.mounted) showErrorToast(context, 'Sign in to continue!'); } } ``` How to solve this?",
    "author_id":4384,
    "publication_date":1754334517000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Husen Patel",
    "author_reputation":38.0,
    "tags":"flutter, flutter-workmanager",
    "text_length":961,
    "title_length":84,
    "num_tags":2
  },
  {
    "id":6004,
    "title":"The request is frozen on the, {status: “pending”, isLoading: true, data: undefined}",
    "link":"https:\/\/stackoverflow.com\/questions\/79725212\/the-request-is-frozen-on-the-status-pending-isloading-true-data-undefin",
    "text":"The request is frozen on the pending status, the loading status does not change in the component {status: “pending”, isLoading: true, data: undefined}, I see the response in devTools and in my API, but I don't see it in the component. I think there's a problem with the cache, but I can't find what the problem is. All code can be found on GitHub this my API ``` getNewEvents: builder.query<Event[], number>({ queryFn: async (size, { getState }, _extraOptions, baseQuery) => { const state = getState() as RootState; const city = state.filter.city; const url = `events\/new?size=${size ?? 20}${!city || city === 'All city' ? '' : '&cityName=' + city}`; const result = await baseQuery({ url, method: 'GET' }); if (result?.error) { return { error: result.error }; } let content = result.data as Event[]; if (content.length === 0) { const url = `events\/new?size=${size ?? 20}`; const result = await baseQuery({ url, method: 'GET' }); if (result.error) { return { error: result.error }; } content = result.data as Event[]; } return { data: content }; }, providesTags: ['NewEvent'], }), ``` hooks in my component ``` const { data: newEvent, isLoading: newEventIsLoading, isFetching: newEventIsFetching, } = useGetNewEventsQuery(10); const { isLoading, isFetching, data: events, } = useGetAllEventsFilteredQuery({ page: 0, size: 21, filter: {}, }); ```",
    "author_id":5515,
    "publication_date":1754334776000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mykola Kosobutskyi",
    "author_reputation":11.0,
    "tags":"redux, redux-toolkit, rtk-query",
    "text_length":1344,
    "title_length":83,
    "num_tags":3
  },
  {
    "id":6003,
    "title":"How to ensure rollback consistency and scalability when inserting into multiple SQL Server tables using JPA and JDBC in Spring Boot?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725217\/how-to-ensure-rollback-consistency-and-scalability-when-inserting-into-multiple",
    "text":"I'm working on a system where I need to store processed data across approximately 30 tables in a SQL Server database. For frequently used tables , I'm using JPA , while the majority of the insertions are handled using JDBC due to performance and control needs. Currently, I wrap the entire operation in a Spring ``` @Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRED) ``` annotation. This approach ensures that any failure during the operation rolls back all the changes — which is good. However, it locks all the involved tables until the full transaction completes , which negatively affects scalability and performance . What I need: A way to persist data across multiple tables efficiently. Maintain transactional integrity — if any part of the process fails, the entire operation should roll back and leave the database in the exact previous state . Minimize long-held locks or table blocking that affect scalability. Questions: Is there a better way to manage transactions when working with a mix of JPA and JDBC across many tables? Can ``` @Transactional ``` be used in a smarter way (e.g., splitting into smaller units with proper rollback)? Would programmatic transaction management , nested transactions, or custom rollback logic be more suitable here? Are there any best practices for this kind of multi-table write operation to balance consistency and performance? I’m looking for a scalable and clean approach to ensure rollback behaves exactly as expected, without locking the entire database section for the duration of the transaction.",
    "author_id":5514,
    "publication_date":1754335036000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tech Savy",
    "author_reputation":17.0,
    "tags":"sql-server, spring-boot, spring-data-jpa, jdbctemplate",
    "text_length":1581,
    "title_length":132,
    "num_tags":4
  },
  {
    "id":6002,
    "title":"How to prevent custom useUnmount cleanup hook from running prematurely in React 18 StrictMode?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725219\/how-to-prevent-custom-useunmount-cleanup-hook-from-running-prematurely-in-react",
    "text":"I'm building a custom React hook ``` useUnmount ``` to handle cleanup logic on component unmount. It works great in production, but in development (with React 18's ``` <StrictMode> ``` ), it runs the cleanup right after mount due to the intentional double-invocation behavior. In dev mode (with StrictMode), the cleanup function inside ``` useEffect ``` is invoked immediately after the first mount. This causes the cleanup logic (like resetting Redux state) to run even though the component hasn’t actually unmounted yet. How can I modify this hook to only run the cleanup logic on a real unmount, without breaking the dev-mode behavior or relying on discouraged workarounds like useRef flags? I’m looking for a clean and idiomatic solution that works in both dev and prod, and still allows multiple callbacks or a single one as supported in the current version. ``` import { useEffect, useRef } from 'react'; import { isArray, isFunction } from 'lodash-es'; import type { TCallback, TCallbackReturn } from '@\/types'; export function useUnmount(onUnmountCallback: () => TCallbackReturn): void { const callbackRef = useRef(onUnmountCallback); callbackRef.current = onUnmountCallback; useEffect(() => { return () => { const returnedCallbacks = callbackRef.current?.(); if (!returnedCallbacks) return; let callbacks: TCallback[] = []; switch (true) { case isFunction(returnedCallbacks): callbacks = [returnedCallbacks]; break; case isArray(returnedCallbacks): callbacks = returnedCallbacks.filter(isFunction); break; default: callbacks = []; } callbacks.forEach((callback) => callback()); }; }, []); } ```",
    "author_id":5513,
    "publication_date":1754335210000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"RAHUL KUNDU",
    "author_reputation":935.0,
    "tags":"reactjs, react-hooks",
    "text_length":1603,
    "title_length":94,
    "num_tags":2
  },
  {
    "id":6001,
    "title":"Unable to render image in GitHub Actions workflow summary",
    "link":"https:\/\/stackoverflow.com\/questions\/79725224\/unable-to-render-image-in-github-actions-workflow-summary",
    "text":"I am trying to render an image in my GitHub Actions workflow summary. Here are the things I tried: ``` - name: Run CLI command id: evals run: | ... results_html=\"\" for entry in \"${evals_results[@]}\"; do heading=\"${entry%%:*}\" img=\"${entry#*:}\" results_html+=\"<h3>$heading<\/h3><img src='data:image\/png;base64,$(base64 -w 0 -i \"$img\")' width='500'>\" done echo \"results_html<<EOF\" >> $GITHUB_OUTPUT echo \"$results_html\" >> $GITHUB_OUTPUT echo \"EOF\" >> $GITHUB_OUTPUT shell: bash - name: Generate Summary markdown run: | cat <<EOF > summary.md ## :bar_chart: Eval Results ${{ steps.evals.outputs.results_html }} EOF shell: bash - name: Display Workflow Summary run: | cat summary.md >> $GITHUB_STEP_SUMMARY shell: bash ``` I also tried to upload as an artefact and render the image using the markdown syntax - ``` [alt-text](<artefact-url) ``` . This doesn't work either. How do I render the image in the workflow summary? The reason behind doing this rather than displaying tables is because I have a lot of test cases and adding images makes it easier to read the results.",
    "author_id":5512,
    "publication_date":1754335696000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"tharika",
    "author_reputation":21.0,
    "tags":"github-actions",
    "text_length":1070,
    "title_length":57,
    "num_tags":1
  },
  {
    "id":6000,
    "title":"Using the MELSEC Communication Protocol, Correct way to Read Multiple Data Register for Mitsubishi FX5UC-32MT\/DSS-TS PLC",
    "link":"https:\/\/stackoverflow.com\/questions\/79725226\/using-the-melsec-communication-protocol-correct-way-to-read-multiple-data-regis",
    "text":"I'm working with a Mitsubishi FX5UC-32MT\/DSS-TS PLC and trying to read data from multiple D registers using the MC protocol, specifically the 0406 command (QnA-compatible 3E frame for multiple-block batch read). Currently, I can successfully read one register at a time using command 0401, but I want to optimize this by reading multiple blocks in a single request. I followed a YouTube tutorial ( https:\/\/www.youtube.com\/watch?v=El1h99SCKns ) and implemented the code in VB.NET. I also reviewed the MELSEC Communication Protocol manual, but I’m having trouble understanding how to correctly structure the binary frame for 0406. Here’s a simplified version of my code: ``` Private Sub MultiReadDataRegister(rdAddressTextBox As TextBox, disTextBox As TextBox, rdValue As TextBox) Dim DeviceNo_D As String Dim DeviceNo2_D As String Dim DeviceNo15_D As String Dim DeviceNo16_D As String Dim DeviceNo17_D As String DeviceNo_D = rdAddressTextBox.Text If String.IsNullOrWhiteSpace(DeviceNo_D) OrElse Val(DeviceNo_D) < 0 Then rdAddressTextBox.Text = \"0\" Else DeviceNo_D = rdAddressTextBox.Text End If DeviceNo_D = rdAddressTextBox.Text DeviceNo2_D = Hex(DeviceNo_D).PadLeft(6, \"0\").Substring(0, 6) DeviceNo15_D = DeviceNo2_D.Substring(4, 2) DeviceNo16_D = DeviceNo2_D.Substring(2, 2) DeviceNo17_D = DeviceNo2_D.Substring(0, 2) Dim sendmessage4(22) As String 'Binary Code sendmessage4(0) = \"50\" 'Subheader - Set the fixed value (request message: '5000', response message: 'D000') sendmessage4(1) = \"00\" 'Subheader - Set the fixed value (request message: '5000', response message: 'D000') sendmessage4(2) = \"00\" 'NetWork No. sendmessage4(3) = \"FF\" 'Pc No. sendmessage4(4) = \"FF\" 'Request destination module I\/O No. sendmessage4(5) = \"03\" 'Request destination module I\/O No. sendmessage4(6) = \"00\" 'Request destination Module station No sendmessage4(7) = \"0C\" 'Requested Data Length sendmessage4(8) = \"00\" 'Requested Data Length sendmessage4(9) = \"10\" 'Monitoring timer sendmessage4(10) = \"00\" 'Monitoring timer sendmessage4(11) = \"06\" 'Command sendmessage4(12) = \"04\" 'Command sendmessage4(13) = \"00\" 'SubCommand sendmessage4(14) = \"00\" 'SubCommand sendmessage4(15) = \"02\" 'Number of word device blocks sendmessage4(16) = \"00\" 'Number of bit device blocks sendmessage4(17) = DeviceNo15_D sendmessage4(18) = DeviceNo16_D sendmessage4(19) = DeviceNo17_D sendmessage4(20) = \"A8\" 'Device Code in Hexadecimal format. It indecate Data register D^* sendmessage4(21) = \"02\" 'Number of Device Points to Read - Low byte Example: user input is D200 so it implies 3 consecutive device points, possibly D200,D201,D202 'sendmessage4(22) = \"00\" 'Number of Device Points to Read - High byte 'Socket connected then it send And recive the message If _socket.Connected Then Dim bytereceiveMessage4 = SendAndReceive(sendmessage4) If bytereceiveMessage4 IsNot Nothing Then Dim arrays As Byte() = {bytereceiveMessage4(11), bytereceiveMessage4(12)} rdValue.Text = BitConverter.ToUInt16(arrays, 0) If rdValue.Text = 0 Then rdValue.BackColor = Color.Blue rdValue.ForeColor = Color.White Else rdValue.BackColor = Color.White rdValue.ForeColor = Color.Black End If disTextBox.Text = rdValue.Text \/ 10 End If End If End Sub ``` ``` Private Function SendAndReceive(sendMessage As String()) As Byte() Dim byteSendMessage As Byte() = StringsToBytes(sendMessage) _socket.Send(byteSendMessage, byteSendMessage.GetLength(0), SocketFlags.None) Dim byteReceiveMessage As Byte() Dim ReceiveSize As Integer = 0 Do byteReceiveMessage = New Byte(_socket.Available - 1) {} ReceiveSize = _socket.Receive(byteReceiveMessage, byteReceiveMessage.GetLength(0), SocketFlags.None) Loop While ReceiveSize = 0 Return byteReceiveMessage End Function Private Function StringsToBytes(src() As String) As Byte() Dim returnBytes(src.Length - 1) As Byte Dim i As Integer For i = 0 To src.Length - 1 returnBytes(i) = Convert.ToByte(src(i), 16) Next Return returnBytes End Function ``` What I tried: I implemented a VB.NET socket communication routine based on a YouTube tutorial to send a 3E frame using command 0406 (multiple-block batch read). I constructed the binary frame manually, including the subheader, network and station numbers, command code, and device address formatting. I verified that the PLC is in RUN mode, and that MC protocol communication is enabled. I tested the same setup using command 0401 for single-block batch read, and it worked correctly — I was able to read one D register at a time. What I expected: I expected the PLC to return data for multiple D registers (e.g., D200, D201, D202) in a single response frame when using command 0406. I anticipated that the response would contain the values of all requested registers, allowing me to parse and display them efficiently. What actually happened: In some cases, the socket received a frame, but its not a actual value. sometimes socket received this ``` 0 - 208 1 - 0 2 - 0 3 - 255 4 - 255 5 - 3 6 - 0 7 - 11 8 - 0 9 - 97 10 - 192 11 - 0 12 - 255 13 - 255 14 - 3 15 - 0 16 - 6 17 - 4 18 - 0 19 - 0 ``` I suspect the issue may be due to incorrect frame formatting.",
    "author_id":5511,
    "publication_date":1754335894000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user31209074",
    "author_reputation":11.0,
    "tags":"automation, vb.net, sockets, protocols, plc",
    "text_length":5079,
    "title_length":120,
    "num_tags":5
  },
  {
    "id":5999,
    "title":"Does Frama-c support GNU-C fixed point arithmetic extension?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725228\/does-frama-c-support-gnu-c-fixed-point-arithmetic-extension",
    "text":"I am currently trying to use Frama-c with MPLAB, to check a code embedded on a microcontroller. After struggling a bit to make Frama-c work with the compiler included inside MPLAB, I have a syntax error message obviously due to the unrecognized ``` _Frac ``` type (a GNU C fixed point type ): ``` syntax error: Location: line 28, between columns 15 and 16, before or at token: _Fract 26 27 typedef CHANNEL_TYPE Channel_t; 28 typedef XXXX_TYPE Xxxx_t; ^ ``` with: ``` #define XXXX_TYPE sat fract ``` (and ``` fract ``` is also a define for ``` _Fract ``` .) I still have the possibility to treat the fixed point types as simple integers, but if possible I want to avoid modifying my code for that. 31.0 (Gallium) gcc version 8.3.1 (Microchip XC32 Compiler v4.21)",
    "author_id":5510,
    "publication_date":1754336130000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"wilfrid",
    "author_reputation":21.0,
    "tags":"gcc, fixed-point, frama-c, mplab-x",
    "text_length":761,
    "title_length":60,
    "num_tags":4
  },
  {
    "id":5998,
    "title":"User Powershell to get Power BI Paginated Report Parameter Names",
    "link":"https:\/\/stackoverflow.com\/questions\/79725229\/user-powershell-to-get-power-bi-paginated-report-parameter-names",
    "text":"I have a need to know what parameter names are required for each paginated report -- the actual names, not what is displayed to the user -- but I don't want to open Power BI Report Builder for every report in the system. I thought I'd use the Power BI REST API, but that doesn't seem to do what I need. I have been searching the Power BI REST API documentation (for example, https:\/\/learn.microsoft.com\/en-us\/rest\/api\/power-bi\/reports\/get-report-in-group ) and I'm not seeing what I need. I found a \"solution\" on Fabric Community . But when I use the Get Reports in Group API call, I don't see parameter names. Is there a way a script can inspect a paginated report to determine what the parameter names are? Update I am a Power BI Admin. I am NOT a Fabric Admin, Entra Admin, or M365 Admin. Using the MS Government cloud. (So some features are not available and most examples posted online won't work for me because URLs are not the same and what they should be is unclear or inconsistent.)",
    "author_id":5509,
    "publication_date":1754336169000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dougp",
    "author_reputation":3163.0,
    "tags":"powershell, powerbi, powerbi-paginated-reports",
    "text_length":991,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":5997,
    "title":"Getting typescript error &quot;No overload matches this call&quot; on Postgres INSERT\/UPDATE statement into JSONB column",
    "link":"https:\/\/stackoverflow.com\/questions\/79725236\/getting-typescript-error-no-overload-matches-this-call-on-postgres-insert-upda",
    "text":"I'm an experienced programmer (mainly c# and mysql, then moved to javascript\/JQuery and now more recently nextjs), but new to nextjs and postgres. I'm trying to insert an object into a JSONB column, which works while running in dev, but typescript is complaining. I can insert other data and run SELECT queries even SELECT queries to JSONB columns, with no problem shown by typescript. I created a minimal example to demonstrate. Using this schema on a neon postgres database: ``` CREATE TABLE \"test\" (\"mycol\" jsonb); ``` At first, when creating my example, I inserted as follows: ``` import postgres from 'postgres'; const sql = postgres(process.env.POSTGRES_URL!, { ssl: 'require' }); const result = await sql`INSERT INTO test (mycol) VALUES (${ {a:1, b:\"fred\"} }) `; ``` With this VSCode shows the following problem when hovering over 'sql' (which is red underlined): ``` No overload matches this call. Argument of type '[{ a: number; b: string; }]' is not assignable to parameter of type 'never'. Overload 2 of 2, '(template: TemplateStringsArray, ...parameters: readonly ParameterOrFragment<never>[]): PendingQuery<Row[]>', gave the following error. Object literal may only specify known properties, and 'a' does not exist in type 'Date | Uint8Array<ArrayBufferLike> | Helper<any, any[]> | Parameter<any> | ArrayParameter<readonly any[]> | readonly SerializableParameter<...>[] | Fragment | Fragment[]'.ts(2769) ``` Then changed my example to be as follows: ``` import postgres from 'postgres'; const sql = postgres(process.env.POSTGRES_URL!, { ssl: 'require' }); const testdata = {a:1, b:\"fred\"} await sql`INSERT INTO test (mycol) VALUES (${ testdata }) ``` Here, VSCode showed a very similar error (below) but interestingly over the const, 'testdata' (inside the braces). ``` No overload matches this call. Argument of type '[{ a: number; b: string; }]' is not assignable to parameter of type 'never'. Overload 2 of 2, '(template: TemplateStringsArray, ...parameters: readonly ParameterOrFragment<never>[]): PendingQuery<Row[]>', gave the following error. Argument of type '{ a: number; b: string; }' is not assignable to parameter of type 'ParameterOrFragment<never>'.ts(2769) ``` I'm sure I must be missing something obvious, but after lots of searching to try to find other people with the same problem, I'm getting nothing. I thought perhaps I'm supposed to be JSON.stringify'ing the object in the INSERT statement, but when I do this postgres seems to store the value as a string and doesn't allow querying of the JSON in the usual way, ie: ``` SELECT mycol->>'a' FROM test ``` Just shows empty rows. Also, when I do: ``` SELECT * FROM test ``` if I had inserted the data using ``` JSON.stringify(testdata) }::jsonb ``` I get: ``` \"{\\\"a\\\":1,\\\"b\\\":\\\"fred\\\"}\" ``` When I insert without JSON.stringify, (just using ``` ... VALUES (${ testdata }) ``` ) I get: ``` {\"a\": 3, \"b\": \"bob\"} ``` Any thoughts\/suggestions?",
    "author_id":5508,
    "publication_date":1754336390000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"VMeldrew",
    "author_reputation":1.0,
    "tags":"next.js, jsonb, typescript, postgres.js",
    "text_length":2922,
    "title_length":120,
    "num_tags":4
  },
  {
    "id":5996,
    "title":"How to deploy an app to shinyapps.io and select files interactively?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725237\/how-to-deploy-an-app-to-shinyapps-io-and-select-files-interactively",
    "text":"``` library(shiny) library(readxl) library(ggplot2) library(dplyr) library(ggthemes) cars <- read_xlsx(file.choose()) carprice <- cars %>% select(Origin,Make,MSRP) data <- aggregate(carprice,MSRP~Origin+Make,FUN = mean) asiadata <- data %>% arrange(MSRP) %>% filter(Origin == 'Asia') %>% select(Origin,Make,MSRP) eurdata <- data %>% arrange(MSRP) %>% filter(Origin == 'Europe') %>% select(Origin,Make,MSRP) usadata <- data %>% filter(Origin == 'USA') %>% arrange(MSRP) %>% select(Origin,Make,MSRP) asia <- asiadata asia$Make <- factor(asia$Make, levels = asia$Make[order(asia$MSRP)]) europe <- eurdata europe$Make <- factor(europe$Make, levels = europe$Make[order(europe$MSRP)]) usa <- usadata usa$Make <- factor(usa$Make, levels = usa$Make[order(usa$MSRP)]) ui <- fluidPage( titlePanel('The Graph Generator'), sidebarLayout( sidebarPanel( selectInput('choose','Select the Origin',choices = unique(cars$Origin)) ), mainPanel( conditionalPanel( condition = \"input.choose == 'Asia'\",plotOutput('asia')), conditionalPanel( condition = \"input.choose == 'Europe'\",plotOutput('europe')), conditionalPanel( condition = \"input.choose == 'USA'\",plotOutput('usa') ) ) ) ) server <- function(input, output, session) { output$usa <- renderPlot({ggplot(usa,aes(Make,MSRP))+ geom_bar(stat = 'identity')+ geom_label(aes(Make,MSRP,label = round(MSRP)))+ coord_flip()+ theme_pander()+ theme(plot.subtitle = element_text(face = 'bold', color = '#822b2b'), plot.caption = element_text(face = 'italic'))+ labs(title = 'Mean MSRP of Manufacturers in USA', subtitle = 'Mean MSRP V\/S Make', caption = 'Source : dataset sourced from sashelp.cars dataset from SAS Studio', x = 'Manufacturers in USA', y = 'Mean MSRP')}) output$europe <- renderPlot({ggplot(europe,aes(Make,MSRP))+ geom_bar(stat = 'identity')+ geom_label(aes(Make,MSRP,label = round(MSRP)))+ coord_flip()+ theme_pander()+ theme(plot.subtitle = element_text(face = 'bold', color = '#822b2b'), plot.caption = element_text(face = 'italic'))+ labs(title = 'Mean MSRP of Manufacturers in Europe', subtitle = 'Mean MSRP V\/S Make', caption = 'Source : dataset sourced from sashelp.cars dataset from SAS Studio', x = 'Manufacturers in Europe', y = 'Mean MSRP') }) output$asia <- renderPlot({ggplot(asia,aes(Make,MSRP))+ geom_bar(stat = 'identity')+ geom_label(aes(Make,MSRP,label = round(MSRP)))+ coord_flip()+ theme_pander()+ theme(plot.subtitle = element_text(face = 'bold', color = '#822b2b'), plot.caption = element_text(face = 'italic'))+ labs(title = 'Mean MSRP of Manufacturers in Asia', subtitle = 'Mean MSRP V\/S Make', caption = 'Source : dataset sourced from sashelp.cars dataset from SAS Studio', x = 'Manufacturers in Asia', y = 'Mean MSRP')}) } shinyApp(ui, server) ``` enter image description here This the code and output after launching the app I'm getting an error: ``` AN ERROR HAS OCCURED. APPLICATION FAILED TO START. EXIT STATUS 1 ``` I really think this is due to the raw code I put outside the UI and Server. But even after putting all the raw code in the Server part of the ShinyApp the app was running but the ``` selectinput ``` was not showing any option.",
    "author_id":5507,
    "publication_date":1754336433000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Nipun Nagpal",
    "author_reputation":1.0,
    "tags":"r",
    "text_length":3114,
    "title_length":68,
    "num_tags":1
  },
  {
    "id":5995,
    "title":"Convert GTSUMMARY object to FLEXTABLE",
    "link":"https:\/\/stackoverflow.com\/questions\/79725240\/convert-gtsummary-object-to-flextable",
    "text":"I am trying to convert ``` gtsummary ``` to ``` flextable ``` using ``` as_flex_table ``` . It works by default passing on information from table body. Example code is provided below: ``` # Load packages for program execution pkg.load <- c(\"gtsummary\", \"cards\", \"labelled\", \"admiral\", \"tidyverse\", \"flextable\", \"officer\", \"clinify\") invisible(lapply(pkg.load, library, character.only = TRUE)) # Create dummy dataset adsl <- pharmaverseadam::adsl |> filter(!grepl('Screen', TRT01A)) %>% select(c(USUBJID, TRT01A, SAFFL, EOSSTT)) %>% mutate( RANDFL = sample(c(\"Y\", NA), size = n(), replace = TRUE, prob = c(0.95, 0.05)), ITTFL = sample(c(\"Y\", NA), size = n(), replace = TRUE, prob = c(0.95, 0.05)) %>% {ifelse(is.na(RANDFL), NA, .)}, TRT01AN = case_when( TRT01A == 'Xanomeline High Dose' ~ 11, TRT01A == 'Xanomeline Low Dose' ~ 21, TRT01A == 'Placebo' ~ 31), COMPLFL = EOSSTT %>% factor(labels = c(\"COMPLETED\", \"DISCONTINUED\")), DSREAS = ifelse( COMPLFL == \"DISCONTINUED\", sample(c(\"Death\", \"Adverse event\", \"Withdrawal by subject\", \"Lost to Follow-up\"), size = n(), replace = TRUE, prob = c(0.05, 0.3, 0.15, 0.5)), NA)) %>% set_variable_labels(DSREAS = \"PRIMARY REASON FOR DISCONTINUATION\") %>% mutate(DSREAS = if_else(TRT01A == 'Placebo' & DSREAS == 'Death', NA, DSREAS)) # Create one-row summary tbl.smry <- adsl %>% tbl_summary( by = TRT01A, include = c(RANDFL, COMPLFL, ITTFL, SAFFL), value = list(ITTFL = \"Y\", SAFFL = \"Y\", RANDFL = \"Y\"), label = list(RANDFL = \"RANDOMIZED\", ITTFL = \"INTENT-TO-TREAT\", SAFFL = \"SAFETY\"), missing = \"no\") %>% add_overall(last = TRUE) tbl.final <- tbl.smry |> modify_table_body( ~ .x %>% mutate( var_ord = match(variable, unique(variable))) |> group_by(var_ord) |> mutate( seq1 = cur_group_id(), seq2 = row_number()) |> ungroup()) ## Convert to flextable object tst1 <- as_flex_table( x = tbl.final) tst2 <- as_flex_table( x = tbl.final, include = c(\"seq1\", \"seq2\")) ``` When I try to pass on the additional 'user-generated' columns ``` seq1 ``` & ``` seq2 ``` to flextable using ``` include ``` it doesn't pass-through on tst2. Please tell me what I am missing here.",
    "author_id":5506,
    "publication_date":1754336652000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ranjan Karmakar",
    "author_reputation":135.0,
    "tags":"r, gtsummary, r-flextable",
    "text_length":2101,
    "title_length":37,
    "num_tags":3
  },
  {
    "id":5994,
    "title":"File read\/write with Avalonia WebAssembly",
    "link":"https:\/\/stackoverflow.com\/questions\/79725241\/file-read-write-with-avalonia-webassembly",
    "text":"I'm currently using Avalonia (with C# and .NET) successfully to produce both Windows and Linux versions of our source code converters and have just started to look at targeting WebAssembly. This would be greatly preferred since it would run anywhere a modern browser exists and would do away with the need for users to install our software. However, the biggest drawback I've seen so far is regarding file system access (as expected for anything running in the browser). I know that I can (and currently do) use the Avalonia StorageProvider methods, but suppose the user selects a .csproj file (C# project file) using StorageProvider.OpenFilePickerAsync - how do I access all the files explicit or implicit within that .csproj file? Currently, for the Windows or Linux target, I can just use StreamReader with a path to read the file and StreamWriter to write the converted file. How would this be done for the WebAssembly target?",
    "author_id":5505,
    "publication_date":1754336828000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dave Doknjas",
    "author_reputation":6564.0,
    "tags":"avalonia, webassembly",
    "text_length":930,
    "title_length":41,
    "num_tags":2
  },
  {
    "id":5993,
    "title":"Reading a wav file and then writing the wav file to another file",
    "link":"https:\/\/stackoverflow.com\/questions\/79725242\/reading-a-wav-file-and-then-writing-the-wav-file-to-another-file",
    "text":"I have legacy code where a ``` .wav ``` file is read as follows into a string: ``` string ReadAllBytes() { var s = File.OpenRead(\"Alarm01.wav\"); var reader = new StreamReader(s, Encoding.Default); return BitConverter.ToString(Encoding.UTF8.GetBytes(reader.ReadToEnd())); } ``` I need to now take this string and convert it back to a ``` .wav ``` file, however I'm not able to do it. Thus far I have come up with this, but unfortunately there is something wrong with this method: ``` void WriteAllBytes(string str) { \/\/ Step 1: Convert hex string back to UTF-8 byte array string[] hexValues = str.Split('-'); byte[] utf8Bytes = new byte[hexValues.Length]; for (int i = 0; i < hexValues.Length; i++) { utf8Bytes[i] = Convert.ToByte(hexValues[i], 16); } \/\/ Step 2: Decode UTF-8 bytes into string string decodedText = Encoding.UTF8.GetString(utf8Bytes); \/\/ Step 3: Re-encode string using Encoding.Default byte[] originalBytes = Encoding.Default.GetBytes(decodedText); \/\/ Step 4: Write to .wav file File.WriteAllBytes(\"output3.wav\", originalBytes); } ``` If someone could help me point out where I go wrong in the ``` WriteAllBytes(string str) ``` method, that would be greatly appreciated.",
    "author_id":5504,
    "publication_date":1754336874000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MMDR",
    "author_reputation":11.0,
    "tags":"c#, wav",
    "text_length":1185,
    "title_length":64,
    "num_tags":2
  },
  {
    "id":5992,
    "title":"Detecting theme change for XAML-based update",
    "link":"https:\/\/stackoverflow.com\/questions\/79725247\/detecting-theme-change-for-xaml-based-update",
    "text":"In my AvaloniaUI application I have a user control that's basically a ``` PathIcon ``` with a ``` TextBlock ``` caption, the contents of which are bound to dependency properties. One DP is ``` IsActive ``` , which when true should have the icon and text be the theme's accent color, and normal theme foreground when false. That all works, except when the theme switches to light or dark, the colors don't update until I change the state bound to ``` IsActive ``` . I need the icon's and text's color to automatically update to what they should be when the theme toggles. I would rather not use a converter that inputs the state and theme. I would also rather not have to track all bound state flags and double-toggle them, which seems inefficient and can create side effects if other operations are executed on the togglings. User control XAML: ``` <UserControl xmlns=\"https:\/\/github.com\/avaloniaui\" xmlns:x=\"http:\/\/schemas.microsoft.com\/winfx\/2006\/xaml\" xmlns:d=\"http:\/\/schemas.microsoft.com\/expression\/blend\/2008\" xmlns:mc=\"http:\/\/schemas.openxmlformats.org\/markup-compatibility\/2006\" mc:Ignorable=\"d\" d:DesignWidth=\"48\" d:DesignHeight=\"48\" xmlns:views=\"clr-namespace:MyApp.Views\" x:Class=\"MyApp.Views.CaptionedToggleIcon\"> <UserControl.Styles> <Style Selector=\"views|CaptionedToggleIcon\"> <Setter Property=\"Template\"> <Setter.Value> <ControlTemplate TargetType=\"views:CaptionedToggleIcon\"> <Panel VerticalAlignment=\"Stretch\"> <Rectangle Fill=\"Transparent\" \/> <StackPanel> <PathIcon Width=\"{Binding IconWidth, RelativeSource={RelativeSource Mode=FindAncestor, AncestorType={x:Type views:CaptionedToggleIcon}}}\" Height=\"{Binding IconHeight, RelativeSource={RelativeSource Mode=FindAncestor, AncestorType={x:Type views:CaptionedToggleIcon}}}\" Data=\"{Binding IconGeo, RelativeSource={RelativeSource Mode=FindAncestor, AncestorType={x:Type views:CaptionedToggleIcon}}}\"> <Interaction.Behaviors> <DataTriggerBehavior Binding=\"{TemplateBinding IsActive}\" ComparisonCondition=\"Equal\" Value=\"True\"> <!--AccentButtonBackground is darker than SystemAccentColor during light theme and lighter during dark theme--> <ChangePropertyAction PropertyName=\"Foreground\" Value=\"{DynamicResource AccentButtonBackground}\" \/> <\/DataTriggerBehavior> <DataTriggerBehavior Binding=\"{TemplateBinding IsActive}\" ComparisonCondition=\"Equal\" Value=\"False\"> <ChangePropertyAction PropertyName=\"Foreground\" Value=\"{DynamicResource TextFillColorPrimary}\" \/> <\/DataTriggerBehavior> <\/Interaction.Behaviors> <\/PathIcon> <TextBlock Text=\"{Binding Caption, RelativeSource={RelativeSource Mode=FindAncestor, AncestorType={x:Type views:CaptionedToggleIcon}}}\"> <Interaction.Behaviors> <DataTriggerBehavior Binding=\"{TemplateBinding IsActive}\" ComparisonCondition=\"Equal\" Value=\"True\"> <ChangePropertyAction PropertyName=\"Foreground\" Value=\"{DynamicResource AccentButtonBackground}\" \/> <\/DataTriggerBehavior> <DataTriggerBehavior Binding=\"{TemplateBinding IsActive}\" ComparisonCondition=\"Equal\" Value=\"False\"> <ChangePropertyAction PropertyName=\"Foreground\" Value=\"{DynamicResource TextFillColorPrimary}\" \/> <\/DataTriggerBehavior> <\/Interaction.Behaviors> <\/TextBlock> <\/StackPanel> <\/Panel> <\/ControlTemplate> <\/Setter.Value> <\/Setter> <\/Style> <\/UserControl.Styles> <\/UserControl> ``` Code-behind: ``` using Avalonia; using Avalonia.Controls; using Avalonia.Data; using Avalonia.Media; namespace MyApp.Views; public partial class CaptionedToggleIcon : UserControl { public static readonly StyledProperty<string> CaptionProperty = AvaloniaProperty.Register<CaptionedToggleIcon, string>(nameof(IsActive), defaultValue: string.Empty); public static readonly StyledProperty<Geometry> IconProperty = AvaloniaProperty.Register<CaptionedToggleIcon, Geometry>(nameof(IconGeo)); public static readonly StyledProperty<bool> IsActiveProperty = AvaloniaProperty.Register<CaptionedToggleIcon, bool>(nameof(IsActive), defaultValue: false); public static readonly StyledProperty<double> IconWidthProperty = AvaloniaProperty.Register<CaptionedToggleIcon, double>(nameof(IconWidth), defaultValue: 24d); public static readonly StyledProperty<double> IconHeightProperty = AvaloniaProperty.Register<CaptionedToggleIcon, double>(nameof(IconHeight), defaultValue: 24d); public string Caption { get => GetValue(CaptionProperty); set => SetValue(CaptionProperty, value); } public Geometry IconGeo { get => GetValue(IconProperty); set => SetValue(IconProperty, value); } public bool IsActive { get => GetValue(IsActiveProperty); set => SetValue(IsActiveProperty, value); } public double IconWidth { get => GetValue(IconWidthProperty); set => SetValue(IconWidthProperty, value); } public double IconHeight { get => GetValue(IconHeightProperty); set => SetValue(IconHeightProperty, value); } public CaptionedToggleIcon() { InitializeComponent(); } } ``` Example usage in XAML, where a button toggles a viewmodel flag's state and lights up in the accent color while the state is ``` true ``` : ``` <Button Command=\"{Binding ToggleFlag}\"> <Button.Content> <views:CaptionedToggleIcon IconWidth=\"20\" IconHeight=\"20\" Caption=\"Text below icon\" IsActive=\"{Binding IsFlagSet}\" IconGeo=\"{StaticResource IconStreamGeometry}\" \/> <\/Button.Content> <\/Button> ``` Relevant code of viewmodel that inherits from ``` ObservableRecipient ``` : ``` [RelayCommand] public void ToggleFlag() { IsFlagSet = !IsFlagSet; } ```",
    "author_id":5503,
    "publication_date":1754337487000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Eric Eggers",
    "author_reputation":92.0,
    "tags":"xaml, avaloniaui, avalonia, themes",
    "text_length":5338,
    "title_length":44,
    "num_tags":4
  },
  {
    "id":5991,
    "title":"Create unique row subtotals in Powerquery or Excel Data Model",
    "link":"https:\/\/stackoverflow.com\/questions\/79725252\/create-unique-row-subtotals-in-powerquery-or-excel-data-model",
    "text":"I've set up a data model in excel with a query joining multiple sources and lookup tables The data is a P&L (or G\/L) with each expense opened by FA, account, cost center, etc My dataset's rows have no totalizing\/controlling accounts for the FAs, meaning that i don't have lines for \"Net Trade Sales\" or \"Material Variance\", only its components I want to, when listing FAs as rows in a Pivot table, to have subtotals for things like Net Sales and Gross Profit I know i can map that in my FA lookup table, which looks like this: FA FA_DESC FA+DESC PL GROUP FA_Grouping_1 F100100 Trade Sales F100100 Trade Sales 1. Sales F100115 Net Trade Sales F100102 Licensing and Royalty Revenue F100102 Licensing and Royalty Revenue 1. Sales F100115 Net Trade Sales F100105 Less: Sales returns and allowances F100105 Less: Sales returns and allowances 1. Sales F100115 Net Trade Sales F100110 Sales Discounts F100110 Sales Discounts 1. Sales F100115 Net Trade Sales F100115 Net Trade Sales F100115 Net Trade Sales 1. Sales F100120 Intercompany Group Sales F100120 Intercompany Group Sales 1. Sales F100131 Net Intercompany Sales F100125 Intracompany Group Sales Europe F100125 Intracompany Group Sales Europe 1. Sales F100131 Net Intercompany Sales F100126 Intracompany Group Sales Asia F100126 Intracompany Group Sales Asia 1. Sales F100131 Net Intercompany Sales FA lookup table FA by row in pivot But there's 2 problems with that: to fully map it out it would take several degrees of hierarchy, which would all be needed in the pivot fields, and the fact that i'd get a lot of \"blanks\" subtotals for things that don't fit in each degree of the hierarchy I'm also aware that i can use this mapping - or something similar - to create custom measures that add up the correspondent rows (something like ``` CALCULATE(amount, dataset[fa_grouping] = \"Net Sales\") ``` ), but i'm not sure about how to add that to my pivot Any ideas? I have full access to the tables, queries and the data model",
    "author_id":5502,
    "publication_date":1754337649000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Rodrigo",
    "author_reputation":21.0,
    "tags":"dax, powerquery, m, excel",
    "text_length":1974,
    "title_length":61,
    "num_tags":4
  },
  {
    "id":5990,
    "title":"Incorrect received bytes order from peripheral device with protocol osdp v1.5.0",
    "link":"https:\/\/stackoverflow.com\/questions\/79725255\/incorrect-received-bytes-order-from-peripheral-device-with-protocol-osdp-v1-5-0",
    "text":"I am writing a program for a control panel. I wrote functions send\/recv for CP: ``` static int cp_send(void *data, uint8_t *buf, int len) { (void)(data); int ret = 0; \/\/ write 1 to RW_SYS_CONTROL write(rw_control_fd, \"1\", sizeof(\"1\")); \/\/ len = shift_bytes_left(buf, len, 1); \/\/ Write to rs-485 ret = write(dev_file_fd, (unsigned char *)buf, len); #ifdef DEBUG printf(\"RS-485: func: %s; Sended buf:\\n\", __func__); printf_hex_dump(buf, len); #endif return ret; } static int cp_recv(void *data, uint8_t *buf, int maxLen) { (void)(data); int ret = 0; \/\/ Write 0 to RW_SYS_CONTROL write(rw_control_fd, \"0\", sizeof(\"0\")); \/\/ Read from rs-485 to buf ret = read(dev_file_fd, (unsigned char *)buf, maxLen); #ifdef DEBUG printf(\"RS-485: func: %s; Received buf:\\n\", __func__); printf_hex_dump(buf, maxLen); #endif return ret; } ``` After initialization I can't receive incorrect bytes order. Initialization all devices in libosdp by itself: ``` RS-485: func: cp_send; Sended buf: [ FF 53 01 09 00 04 61 00 60 23 ] RS-485: func: cp_send; Sended buf: [ FF 53 02 09 00 04 61 00 80 ED ] RS-485: func: cp_send; Sended buf: [ FF 53 03 09 00 04 61 00 20 A8 ] ... RS-485: func: cp_send; Sended buf: [ FF 53 7C 09 00 04 61 00 BF 28 ] RS-485: func: cp_send; Sended buf: [ FF 53 7D 09 00 04 61 00 1F 6D ] RS-485: func: cp_send; Sended buf: [ FF 53 7E 09 00 04 61 00 FF A3 ] ``` I connected only 1 device to CP, so only 1 device reply Reply by device: ``` RS-485: func: cp_recv; Received buf: [ FF 53 81 14 00 04 61 00 60 23 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00000000 00 00 00 00 00 00 00 00 ] ``` Test card reader time: ``` RS-485: func: cp_recv; Received buf: [ 45 42 53 52 38 01 00 00 00 00 00 01 08 8A F9 A6 00 0C 01 40 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00000000 00 00 00 00 00 00 00 00 ] OSDP: ERROR: PD[1]: PHY: Invalid SOM 0x45 ``` I tried to shift bytes left when I receive messages, but libosdp writes error \"Incorrect CRC\". Also, I thought that I need to change bits order from little-endian to big-endian, but it's not helpful. If error in bits order I could see byte 0xCA (reverse SOM-byte 0x53) in byte order. Also, I saw that libosdp uses SKIP_MARK_BYTE (0xFF) at the start of all messages. What should I do to receive the correct byte order?",
    "author_id":5501,
    "publication_date":1754337999000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ebash",
    "author_reputation":29.0,
    "tags":"c, rs485",
    "text_length":2268,
    "title_length":79,
    "num_tags":2
  },
  {
    "id":5989,
    "title":"Bash logging out on login in tty",
    "link":"https:\/\/stackoverflow.com\/questions\/79725257\/bash-logging-out-on-login-in-tty",
    "text":"I wrote simple pam module to ran my own program, when session was created. When I login in GUI (by SDDM), everything works. Problem occurs, when login on tty (CTRL+ALT+Fn). In this case, bash wrote text, my program wrote text and I was moved to login prompt again (both programs end). I try to figure out, what happens. My program receive SIGCONT and SIGHUP next. When I wrote my own empty handler of SIGHUP, then my program do not exits, but still login prompt appears and bash end working. I think problem is bash checks if my tty is controlling terminal for some process and then exits, because my program opens it. I am not sure. How to solve this case and what really happens? Thanks! ``` \/* * SPDX-FileCopyrightText: 2023-2025 Sławomir Lach <nintyfan19@gmail.com> * SPDX-FileContributor: Sławomir Lach <slawek@lach.art.pl> * * SPDX-License-Identifier: GPLv3 *\/ \/* * SecureHome - Asks, what to do on file access * Copyright (C) 2023 Sławomir Lach <slawek@lach.art.pl> * * This program is free software: you can redistribute it and\/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 3 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program. If not, see <https:\/\/www.gnu.org\/licenses\/>. *\/ #include <string.h> #include <stdlib.h> #include <stdio.h> #include <errno.h> #include <pwd.h> #include <unistd.h> #include <security\/pam_appl.h> #include <security\/pam_modules.h> #include <security\/pam_ext.h> static const char *password; static const char *username; static bool debug = false; PAM_EXTERN int pam_sm_open_session(pam_handle_t *handle, int flags, int argc, const char *argv[argc]) { pam_get_data(handle, \"SECUREHOME_PASSWORD_OBTAINED\", (const void*)&password); if (NULL == password && NULL == username) { return PAM_IGNORE; } if (NULL == password) { return PAM_IGNORE; } if (NULL == username) { return PAM_IGNORE; } if (0 == fork()) { puts(\"WILL SLEEP\"); system(\"sleep 1000\"); exit(1); } return PAM_IGNORE; } PAM_EXTERN int pam_sm_close_session(pam_handle_t *pamh, int flags, int argc, const char *argv[argc]) { return PAM_IGNORE; } PAM_EXTERN int pam_sm_authenticate(pam_handle_t *handle, int flags, int argc, const char **argv){ int result; const void *data; if (PAM_SUCCESS == pam_get_data(handle, \"SECUREHOME_PASSWORD_OBTAINED\",&data)) { return PAM_IGNORE; } result = pam_get_user(handle, &username, NULL); if (PAM_SUCCESS != result) { \/\/ fprintf(log_file, \"Unable to get user name: %s\\n\", \/\/ pam_strerror(handle, result)); \/\/fflush(log_file); return PAM_IGNORE; } result = pam_get_authtok(handle, PAM_AUTHTOK, &password, NULL); \/\/result = pam_get_item(handle, PAM_AUTHTOK, (const void**)&password); if (PAM_SUCCESS != result) { \/\/fprintf(log_file, \"Unable to get user password: %s\\n\", \/\/ pam_strerror(handle, result)); \/\/fflush(log_file); return PAM_IGNORE; } result = pam_set_data(handle, \"SECUREHOME_PASSWORD_OBTAINED\", strdup(password), NULL); if (result != PAM_SUCCESS) { \/\/fprintf(log_file, \"Imposible to store session data: %s\\n\", \/\/ pam_strerror(handle, result)); \/\/fflush(log_file); } return PAM_IGNORE; } ``` Edit1: Put Pam configuration ``` auth optional pam_kwallet5.so auth optional my_module.so auth required pam_unix.so try_first_pass session required pam_limits.so session optional pam_systemd.so session required pam_unix.so try_first_pass session optional pam_umask.so session optional pam_kwallet5.so session optional pam_env.so session optional my_module.so ``` Logs: ``` Started Session 5 for User slawomir. LOGIN ON tty1 BY slawomir pam_unix(login:session): session closed for user slawomir pam_kwallet5(login:session): pam_kwallet5 pam_sm_close_session session-5.scope: Deactivated successfully. ``` Last working version: ``` \/* * SPDX-FileCopyrightText: 2023-2025 Sławomir Lach <nintyfan19@gmail.com> * SPDX-FileContributor: Sławomir Lach <slawek@lach.art.pl> * * SPDX-License-Identifier: GPLv3 *\/ \/* * SecureHome - Asks, what to do on file access * Copyright (C) 2023 Sławomir Lach <slawek@lach.art.pl> * * This program is free software: you can redistribute it and\/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation, either version 3 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program. If not, see <https:\/\/www.gnu.org\/licenses\/>. *\/ #include <string.h> #include <stdlib.h> #include <stdio.h> #include <errno.h> #include <pwd.h> #include <sys\/types.h> #include <sys\/wait.h> #include <unistd.h> #include <security\/pam_appl.h> #include <security\/pam_modules.h> #include <security\/pam_ext.h> static const char *password; static const char *username; static bool debug = false; PAM_EXTERN int pam_sm_open_session(pam_handle_t *handle, int flags, int argc, const char *argv[argc]) { pid_t process_id; pam_get_data(handle, \"SECUREHOME_PASSWORD_OBTAINED\", (const void*)&password); if (NULL == password && NULL == username) { return PAM_IGNORE; } if (NULL == password) { return PAM_IGNORE; } if (NULL == username) { return PAM_IGNORE; } if (0 == (process_id = fork())) { if (0 == fork()) { setsid(); close(STDIN_FILENO); close(STDOUT_FILENO); close(STDERR_FILENO); \/\/ Do exec here with your program path system(\"sleep 1000\"); exit(1); } exit(0); } waitpid(process_id, NULL, 0); return PAM_IGNORE; } PAM_EXTERN int pam_sm_close_session(pam_handle_t *pamh, int flags, int argc, const char *argv[argc]) { return PAM_IGNORE; } PAM_EXTERN int pam_sm_authenticate(pam_handle_t *handle, int flags, int argc, const char **argv){ int result; const void *data; if (PAM_SUCCESS == pam_get_data(handle, \"SECUREHOME_PASSWORD_OBTAINED\",&data)) { return PAM_IGNORE; } result = pam_get_user(handle, &username, NULL); if (PAM_SUCCESS != result) { \/\/ fprintf(log_file, \"Unable to get user name: %s\\n\", \/\/ pam_strerror(handle, result)); \/\/fflush(log_file); return PAM_IGNORE; } result = pam_get_authtok(handle, PAM_AUTHTOK, &password, NULL); \/\/result = pam_get_item(handle, PAM_AUTHTOK, (const void**)&password); if (PAM_SUCCESS != result) { \/\/fprintf(log_file, \"Unable to get user password: %s\\n\", \/\/ pam_strerror(handle, result)); \/\/fflush(log_file); return PAM_IGNORE; } result = pam_set_data(handle, \"SECUREHOME_PASSWORD_OBTAINED\", strdup(password), NULL); if (result != PAM_SUCCESS) { \/\/fprintf(log_file, \"Imposible to store session data: %s\\n\", \/\/ pam_strerror(handle, result)); \/\/fflush(log_file); } return PAM_IGNORE; } ```",
    "author_id":5500,
    "publication_date":1754338043000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"nintyfan",
    "author_reputation":462.0,
    "tags":"bash, tty, pam",
    "text_length":7067,
    "title_length":32,
    "num_tags":3
  },
  {
    "id":5988,
    "title":"How do I access a PDS on zOS using PL\/I?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725258\/how-do-i-access-a-pds-on-zos-using-pl-i",
    "text":"I am trying to compose a simple PL\/I procedure that calculates the average of numbers that are stored in a member of a PDS. This seems trivially simple but I cannot get it to work. Here is the PL\/I: ``` AVERAGE: PROCEDURE OPTIONS(MAIN); DECLARE SYSIN2 FILE INPUT EXTERNAL ENVIRONMENT(SYSIN2); DECLARE A FLOAT; DECLARE B FLOAT; DECLARE C FLOAT; DECLARE D FLOAT; DECLARE E FLOAT; DECLARE SUM FLOAT; DECLARE AVERAGE_GRADE FLOAT; GET FILE(SYSIN2) LIST(A, B, C, D, E); SUM = A + B + C + D + E; AVERAGE_GRADE = SUM \/ 5; PUT LIST ('The average is ', AVERAGE_GRADE); END AVERAGE; ``` Here is the JCL: ``` \/\/PLITEST JOB (ACCT),'PL\/I TEST',CLASS=A,MSGCLASS=A,NOTIFY=&SYSUID \/\/* COMPILE STEP \/\/COMPILE EXEC PROC=IBMZCPLG,REGION=64M \/\/SYSIN DD DSN=Z74114.PLI(AVERAGE),DISP=SHR \/\/SYSPRINT DD SYSOUT=* \/\/SYSLMOD DD DSN=Z74114.LOAD(AVERAGE),DISP=SHR \/\/* RUN STEP \/\/RUN EXEC PGM=AVERAGE \/\/STEPLIB DD DSN=Z74114.LOAD,DISP=SHR \/\/SYSIN2 DD DSN=Z74114.SOURCE(AVERAGE),DISP=SHR \/\/SYSPRINT DD SYSOUT=* ``` The SYSIN2 DD is not being picked up in the PL\/I procedure. The program otherwise compiles and executes properly. The error I get is this: IBM0204S ONCODE=84 The UNDEFINEDFILE condition was raised because a DD statement or CMS FILEDEF was not used in (FILE = SYSIN2). Seems like this ought to work. Any ideas?",
    "author_id":5499,
    "publication_date":1754338084000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Thomas Delaney",
    "author_reputation":1.0,
    "tags":"zos, pl-i",
    "text_length":1293,
    "title_length":40,
    "num_tags":2
  },
  {
    "id":5987,
    "title":"GUI HTTP Clients not sending requests to server in WSL2",
    "link":"https:\/\/stackoverflow.com\/questions\/79725261\/gui-http-clients-not-sending-requests-to-server-in-wsl2",
    "text":"I've been using HTTPie like normal ever since I started building APIs and I do all my development in WSL2. For some unknown reason, today, it just stopped working. I send a simple request, any request and it just hangs. The request never reaches the server, it just loads and loads. Both Postman and HTTPie are having this issue for me but not in the terminal. HTTPie's CLI works fine and sends requests and wget is also sending requests just fine. It's the GUI http clients for some reason, just incase it's important info, I use Ubuntu 24.04.1 LTS on WSL2 and all my servers are done on localhost ports. Interestingly, I changed the request URL from localhost to my WSL hostname IP which I got from running 'wsl hostname -I' in powershell and it works. So instead of doing ``` localhost:3000\/ ``` , I do ``` 172.00.00.000:3000\/ ``` and it works, but why?? I was using ``` localhost:3000\/ ``` just fine 30 minutes ago and now it stopped working out of nowhere? Any tips on how to fix this? I could probably do some stuff to get it to work again but I'm mainly just trying to go back and get it to work how it was.",
    "author_id":5498,
    "publication_date":1754338345000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"codi-ng",
    "author_reputation":11.0,
    "tags":"postman, windows-subsystem-for-linux, httpie",
    "text_length":1114,
    "title_length":55,
    "num_tags":3
  },
  {
    "id":5986,
    "title":"How to fsync a directory?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725268\/how-to-fsync-a-directory",
    "text":"I have a write-ahead log that stores some metadata using atomic renames. Currently I am only fsyncing the file before rename, but it is my understanding that I should also be fsyncing the directory after rename to ensure this atomic operation is flushed to disk. I have the following: ``` import fs from \"fs\/promises\"; ``` I tried two ways to ``` fsync ``` a directory. Calling ``` sync ``` on the ``` fs.Dir ``` object: ``` const dir = await fs.opendir(...); await dir.sync(); \/\/ error: no sync method exists ``` Calling ``` fsync ``` and passing in file descriptor: ``` const dir = await fs.opendir(...); await fs.fsync(dir.fd); \/\/ error: no fsync method exists ``` How do I fsync my directory using Node.js ``` fs\/promises ``` or is there any other viable solution? This Reddit post is related but isn't referring to the promises API which is what I am working with. You might ask, why do I believe I need to ``` fsync ``` a directory? According to the linux man pages : As well as flushing the file data, ``` fsync() ``` also flushes the metadata information associated with the file (see inode(7)). Calling ``` fsync() ``` does not necessarily ensure that the entry in the directory containing the file has also reached disk. For that an explicit ``` fsync() ``` on a file descriptor for the directory is needed.",
    "author_id":5497,
    "publication_date":1754339070000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"David Callanan",
    "author_reputation":5858.0,
    "tags":"javascript, node.js, node.js-fs",
    "text_length":1317,
    "title_length":25,
    "num_tags":3
  },
  {
    "id":5985,
    "title":"Removing the ... from RadzenDataGrid header in case the cell is to small to show the whole text",
    "link":"https:\/\/stackoverflow.com\/questions\/79725271\/removing-the-from-radzendatagrid-header-in-case-the-cell-is-to-small-to-show",
    "text":"I'm displaying a table with RadzenDataGrid and I don't want to show the ``` ... ``` of the column name. E.g. Name becomes \"N...\". I tried adding this to the Blazor ``` app.css ``` : ``` .rz-datatable td, .rz-datatable th, .rz-cell-data { text-overflow: clip !important; white-space: nowrap !important; overflow: hidden !important; } ``` figured it out, the correct selector is .rz-column-title-content",
    "author_id":5496,
    "publication_date":1754339202000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"berti",
    "author_reputation":1.0,
    "tags":"blazor, radzen",
    "text_length":401,
    "title_length":95,
    "num_tags":2
  },
  {
    "id":5984,
    "title":"import().then not working after upgrade to Angular 19",
    "link":"https:\/\/stackoverflow.com\/questions\/79725274\/import-then-not-working-after-upgrade-to-angular-19",
    "text":"I need to import libraries that are not ESM compliant. One of them is called xlsx (SheetJS). When working with Angular 17 and earlier versions, I just did: ``` import('xlsx').then(xlsx => { const ws: any = xlsx.utils.json_to_sheet(json); const wb: any = xlsx.utils.book_new(); xlsx.utils.book_append_sheet(wb, ws, 'teste9'); xlsx.writeFile(wb, 'test.xlsx'); }); ``` Now that I upgraded the project to version 19, the same code results in error: ``` ERROR TypeError: Cannot read properties of undefined (reading 'json_to_sheet') ``` During ng serve, the code works fine. Only after ng build this starts hapenning. I've tried: Utilizing relative path: import('..\/..\/..\/..\/node_modules\/xlsx').then(xlsx => { Changing configurations from the builder Creating a new project with standard configurations then importing xlsx in the same way Nothing worked.",
    "author_id":5495,
    "publication_date":1754339561000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Gustavo Branco",
    "author_reputation":61.0,
    "tags":"angular19, angular-builder",
    "text_length":849,
    "title_length":53,
    "num_tags":2
  },
  {
    "id":5983,
    "title":"How can I download a Leaflet map as PNG from a published Shiny app?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725277\/how-can-i-download-a-leaflet-map-as-png-from-a-published-shiny-app",
    "text":"I published a Shiny app in shinyapps.io. I am able to download the map as HTML, however when I try to download as PNG the server gets disconnected. Is it possible to download a map on a format other than HTML? Locally both download buttons work. The code: ``` library(shiny) library(mapview) library(leaflet) library(htmlwidgets) #Ensure webshot is installed and PhantomJS is set up if (!webshot::is_phantomjs_installed()) { webshot::install_phantomjs() } ui <- fluidPage( sidebarLayout( sidebarPanel( actionButton(\"print_map\", \"Download png\"), actionButton(\"print_map2\", \"Download html\")), mainPanel( leafletOutput(\"mymap\")) ) ) server <- function(input, output, session) { output$mymap <- renderLeaflet({ mapview(breweries)@map }) # Create a new map based off of the user's zoom level user.created.map <- reactive({ mapview(breweries)@map %>% setView( lng = input$mymap_center$lng , lat = input$mymap_center$lat , zoom = input$mymap_zoom ) }) observeEvent(input$print_map2, { tryCatch( mapshot2(user.created.map(), file = \"map_zoom2.webp\", remove_controls = c(\"zoomControl\",\"layersControl\"),vwidth = input$dimension[1], vheight = input$dimension[2]), error = function(e) conditionMessage(e)) }) observeEvent(input$print_map, { tryCatch( mapshot2(user.created.map(), file = \"map_zoom.png\", remove_controls = c(\"zoomControl\", \"layersControl\"), vwidth = input$dimension[1], vheight = input$dimension[2]), error = function(e) conditionMessage(e)) }) } shinyApp(ui, server) ```",
    "author_id":5441,
    "publication_date":1754339746000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Salvador",
    "author_reputation":1923.0,
    "tags":"r, shiny, r-leaflet, r-mapview",
    "text_length":1474,
    "title_length":67,
    "num_tags":4
  },
  {
    "id":5982,
    "title":"Horizontal cum sum + unnest bug in polars",
    "link":"https:\/\/stackoverflow.com\/questions\/79725287\/horizontal-cum-sum-unnest-bug-in-polars",
    "text":"When I use horizontal cum sum followed by unnest, a \"literal\" column is formed that stays in the schema even when dropped. Here is a mwe: ``` import polars as pl def test_literal_bug(): print(\"Polars version:\", pl.__version__) # Create simple test data df = pl.DataFrame({ \"A\": [1, 2, 3], \"T0\": [0.1, 0.2, 0.3], \"T1\": [0.4, 0.5, 0.6], \"T2\": [0.7, 0.8, 0.9], }) time_cols = [\"T0\", \"T1\", \"T2\"] print(\"Original columns:\", df.columns) print(\"Time columns:\", time_cols) lazy_df = df.lazy() print(\"Schema before cumsum:\", lazy_df.collect_schema().names()) result = ( lazy_df.select(pl.cum_sum_horizontal(time_cols)) .unnest(\"cum_sum\") .rename({col: f\"C{col}\" for col in time_cols}) ) print(\"Schema after cumsum:\", result.collect_schema().names()) # This will fail with: ColumnNotFoundError: \"literal\" not found try: collected = result.collect() print(\"v1: No bug reproduced\") except pl.exceptions.ColumnNotFoundError as e: print(f\"v1: BUG REPRODUCED: {e}\") result_2 = result.drop(\"literal\") result_2 = pl.concat([pl.LazyFrame({\"B\": [1, 2, 3]}), result_2], how=\"horizontal\") print(\"Schema after drop and concat:\", result_2.collect_schema().names()) try: collected_2 = result_2.collect() print(\"v2: No bug reproduced\") except pl.exceptions.ColumnNotFoundError as e: print(f\"v2: BUG REPRODUCED: {e}\") if __name__ == \"__main__\": test_literal_bug() ``` which has output ``` Polars version: 1.31.0 Original columns: ['A', 'T0', 'T1', 'T2'] Time columns: ['T0', 'T1', 'T2'] Schema before cumsum: ['A', 'T0', 'T1', 'T2'] Schema after cumsum: ['CT0', 'CT1', 'CT2', 'literal'] v1: BUG REPRODUCED: \"literal\" not found Schema after drop and concat: ['B', 'CT0', 'CT1', 'CT2'] v2: BUG REPRODUCED: \"literal\" not found ``` What is going on? Am I doing something wrong or is it a bug? What follows is just to avoid the stackoverflow error \"Too much code in your post\": aaa",
    "author_id":5494,
    "publication_date":1754340588000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Nicol&#242; Cavalleri",
    "author_reputation":203.0,
    "tags":"python, python-polars, polars",
    "text_length":1850,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":5981,
    "title":"Instancing in Godot",
    "link":"https:\/\/stackoverflow.com\/questions\/79725294\/instancing-in-godot",
    "text":"How big is the difference between instancing a scene with c# compared to GDScript? I noticed on the docs that it does say that when instancing a scene \"Preloading the scene can improve the user's experience as the load operation happens when the compiler reads the script and not at runtime. This feature is only available with GDScript.\" But how big is this difference of using the preloader? I am planning on instancing multiple scenes with tile map layers where at once there can be 10-1000 tile map layers all adjacent to each other at once depending on the zoom set to the camera, so I was wondering how big a difference in performance there would be in just doing c#.",
    "author_id":5493,
    "publication_date":1754341275000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Alvin Lee",
    "author_reputation":23.0,
    "tags":"c#, godot, gdscript, godot4",
    "text_length":673,
    "title_length":19,
    "num_tags":4
  },
  {
    "id":5980,
    "title":"dosresmeta package errors with &#39;positive length&#39; and missing arguments",
    "link":"https:\/\/stackoverflow.com\/questions\/79725295\/dosresmeta-package-errors-with-positive-length-and-missing-arguments",
    "text":"I am trying to run a dose-response meta-analysis using the dosresmeta package in R, but I'm encountering a frustrating two-part error. My goal is to model the relationship between basilar artery (BA) diameter and the risk of cardiovascular events from a meta-analysis of several studies. First, when I try to run the function with just the log-Hazard Ratios (logHR) and dose, I get the following error: ``` Error in dosresmeta(formula = log_Effect_Size \\~ Mean_BA_Diameter_mm, id = Study, : Arguments cases, n, and type are required when covariance equal to 'gl' or 'h' ``` This makes sense, as the dosresmeta function needs the raw data (number of events and total subjects) to perform its calculations when using a generalized least squares model. To fix the first error, I updated my code to include ``` cases ``` , ``` n ``` , and ``` type ``` arguments, which are available in my dataset. However, after making this change, I encountered a new error that prevented the function from running at all: ``` Error in diag(cx[v != 0] + cx[v == 0], nrow = sum(v != 0)) : 'x' must have positive length ``` This second error suggests that my data frame is becoming empty after the data cleaning and filtering steps, which means the function has no data to operate on. I have tried to make my code more robust by carefully selecting columns and using ``` na.omit() ``` , but I am still running into this issue. I expected the corrected code to successfully run the ``` dosresmeta ``` function and produce a dose-response model. Here is a reproducible example with sample data: ``` # Load necessary package library(dosresmeta) # Sample data drma_input <- data.frame( Study = c(\"StudyA\", \"StudyB\", \"StudyC\", \"StudyD\", \"StudyE\"), Mean_BA_Diameter_mm = c(\"2.73\", \"2.85 (imputed)\", \"2.7\", NA, \"4.3\"), Effect_Size_HR = c(1.55, 1.23, 1.02, 1.756, 3.69), CI_Lower = c(1.12, 1.07, 0.31, 1.244, 1.63), CI_Upper = c(2.14, 1.41, 3.32, 2.478, 8.38), Events = c(91, 54, 30, 22, 54), Total_N = c(493, 466, 164, 115, 466) ) # Data preparation drma_data_subset <- drma_input[, c(\"Study\", \"Mean_BA_Diameter_mm\", \"Effect_Size_HR\", \"CI_Lower\", \"CI_Upper\", \"Events\", \"Total_N\")] drma_data_subset$dose <- as.numeric(gsub(\" \\\\(imputed\\\\)| \\\\(threshold\\\\)| \\\\(average\\\\)|\", \"\", drma_data_subset$Mean_BA_Diameter_mm)) drma_data_subset$logHR <- log(drma_data_subset$Effect_Size_HR) valid_ci_rows <- !is.na(drma_data_subset$CI_Upper) & !is.na(drma_data_subset$CI_Lower) & !is.infinite(drma_data_subset$CI_Upper) & !is.infinite(drma_data_subset$CI_Lower) & drma_data_subset$CI_Upper > 0 & drma_data_subset$CI_Lower > 0 drma_data_subset$se_logHR <- NA_real_ drma_data_subset$se_logHR[valid_ci_rows] <- (log(drma_data_subset$CI_Upper[valid_ci_rows]) - log(drma_data_subset$CI_Lower[valid_ci_rows])) \/ (2 * 1.96) # Final filtering step final_drma_data <- na.omit(drma_data_subset[, c(\"Study\", \"dose\", \"logHR\", \"se_logHR\", \"Events\", \"Total_N\")]) # Check the final data frame print(final_drma_data) # Call dosresmeta model <- dosresmeta( formula = logHR ~ dose, se = se_logHR, id = Study, data = final_drma_data, cases = Events, n = Total_N, type = 'loghr', covariance = 'gl' ) ``` What is the desired output? I want the ``` dosresmeta ``` model to run successfully and produce a summary of the dose-response relationship, similar to the output of ``` summary(model) ``` .",
    "author_id":5492,
    "publication_date":1754341364000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Anonymous Human",
    "author_reputation":1.0,
    "tags":"r",
    "text_length":3334,
    "title_length":78,
    "num_tags":1
  },
  {
    "id":5979,
    "title":"Vaadin 24 compile issue with CustomField java generics",
    "link":"https:\/\/stackoverflow.com\/questions\/79725296\/vaadin-24-compile-issue-with-customfield-java-generics",
    "text":"I'm upgrading a Vaadin 23.2 app to Vaadin 24.8. I have a compilation error on one of my components. It's a simple custom field with this signature: ``` public class MyField extends CustomField<String> { ``` IntelliJ IDEA doesn't show any error, but Gradle compilation (JDK17) throws: ``` error: HasValue cannot be inherited with different arguments: <?,java.lang.String> and <com.vaadin.flow.component.AbstractField.ComponentValueChangeEvent<?,java.lang.String>,java.lang.String> ``` without any indication of class name or line number, but when I remove the class the issue is gone. I see that ``` CustomField ``` now effectively inherits from ``` HasValue ``` twice: by extending ``` AbstractField ``` and by implementing ``` InputField ``` , but I fail to see why these clash, and what I can do about that. I'm on JDK17 (Eclipse Termurin), I've tried the latest Corretto 17 release but that doesn't help. Update In my custom field, I have three text fields: ``` private final TextField part1 = new TextField(); private final TextField part2 = new TextField(); private final TextField part3 = new TextField(); ``` I'm adding a focus listener to ``` this ``` and each part using this line: ``` Stream.of(this, part1, part2, part3).forEach(c -> c.addFocusListener(event -> this.focused = true)); ``` Commenting out that line makes the error go away. When I hover over the ``` c ``` lambda parameter, IntelliJ IDEA shows its type: ``` com.vaadin.flow.component.AbstractField<? extends com.vaadin.flow.component.AbstractField<?, ?> & com.vaadin.flow.component.Focusable<? extends com.vaadin.flow.component.AbstractField<?, ?> & com.vaadin.flow.component.Focusable<?> & com.vaadin.flow.component.shared.InputField<? extends com.vaadin.flow.component.AbstractField.ComponentValueChangeEvent<? extends com.vaadin.flow.component.AbstractField<?, ?> & com.vaadin.flow.component.Focusable<?> & com.vaadin.flow.component.shared.InputField<?, ?> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties, String>, String> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties> & com.vaadin.flow.component.shared.InputField<? extends com.vaadin.flow.component.AbstractField.ComponentValueChangeEvent<? extends com.vaadin.flow.component.AbstractField<?, ?> & com.vaadin.flow.component.Focusable<?> & com.vaadin.flow.component.shared.InputField<?, ?> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties, String>, String> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties, String> & com.vaadin.flow.component.Focusable<? extends com.vaadin.flow.component.AbstractField<?, ?> & com.vaadin.flow.component.Focusable<?> & com.vaadin.flow.component.shared.InputField<? extends com.vaadin.flow.component.AbstractField.ComponentValueChangeEvent<? extends com.vaadin.flow.component.AbstractField<?, ?> & com.vaadin.flow.component.Focusable<?> & com.vaadin.flow.component.shared.InputField<?, ?> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties, String>, String> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties> & com.vaadin.flow.component.shared.InputField<? extends com.vaadin.flow.component.AbstractField.ComponentValueChangeEvent<? extends com.vaadin.flow.component.AbstractField<?, ?> & com.vaadin.flow.component.Focusable<?> & com.vaadin.flow.component.shared.InputField<?, ?> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties, String>, String> & com.vaadin.flow.component.HasTheme & com.vaadin.flow.component.shared.HasValidationProperties c ```",
    "author_id":5491,
    "publication_date":1754341424000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Herman Bovens",
    "author_reputation":12465.0,
    "tags":"java, generics, vaadin, vaadin-flow",
    "text_length":3740,
    "title_length":54,
    "num_tags":4
  },
  {
    "id":5978,
    "title":"SAM CLI Lambda container cannot connect to LocalStack Secrets Manager on macOS:",
    "link":"https:\/\/stackoverflow.com\/questions\/79725298\/sam-cli-lambda-container-cannot-connect-to-localstack-secrets-manager-on-macos",
    "text":"\"Could not connect to the endpoint URL: http:\/\/localhost:4566\/ I'm developing a FastAPI-based Lambda function using AWS SAM CLI for local testing on macOS, with LocalStack emulating Secrets Manager. The deployment script starts LocalStack in Docker mode, creates a secret, and launches the API with ``` sam local start-api ``` . The health endpoint works, but the \/login endpoint (which fetches a secret from LocalStack) fails with a 500 error and timeout. Lambda logs show: ``` [ERROR] 2025-08-03T17:56:02.055Z bfbe9fac-b862-4586-8b22-7f959a7cce4b 15. Error fetching user secret: Could not connect to the endpoint URL: \"http:\/\/localhost:4566\/\" [ERROR] 2025-08-03T17:56:09.151Z bfbe9fac-b862-4586-8b22-7f959a7cce4b 33. Unexpected error in login: Could not connect to the endpoint URL: \"http:\/\/localhost:4566\/\" Function 'AuthFunction' timed out after 20 seconds ``` The SAM CLI Lambda container seems unable to connect to LocalStack at localhost:4566, while ``` awslocal ``` commands from the host work fine. Environment: macOS Sonoma 14.5 (M3 chip) Docker Desktop 28.3.2 SAM CLI 1.124.0 LocalStack CLI 4.7.0 Python 3.12 Boto3 1.40.1 Relevant code in ``` main.py ``` : ``` secrets_endpoint = \"http:\/\/localhost:4566\" if is_local else None secrets_client = boto3.client(\"secretsmanager\", endpoint_url=secrets_endpoint, region_name=\"us-east-1\") ``` ``` def get_user(username: str) -> Optional[dict]: logger.info(\"11. Fetching user secret for: %s\", username) try: response = secrets_client.get_secret_value(SecretId=\"UserCredentials\") users = json.loads(response[\"SecretString\"]) logger.info(\"12. Secret retrieved: %s\", {k: v for k, v in users.items() if k == username}) return users.get(username) except Exception as e: logger.error(\"13. Error fetching user secret: %s\", str(e)) return None ``` ``` 05_manage_api.sh ``` : ``` if [ \"$LOCAL_ONLY\" == \"true\" ]; then echo \"🚀 Starting local API...\" TEMPLATE_FILE=\"${PWD}\/.aws-sam\/build\/template.yaml\" ENV_PATH=\"${PWD}\/env.json\" PORT=3000 while lsof -i :$PORT > \/dev\/null 2>&1; do echo \"⚠️ Port $PORT is in use. Trying port $((PORT + 1))...\" PORT=$((PORT + 1)) done echo \"✅ Using port $PORT for API.\" sam local start-api \\ --template-file \"${TEMPLATE_FILE}\" \\ --docker-network host \\ --env-vars \"${ENV_PATH}\" \\ --port \"${PORT}\" \\ --host 127.0.0.1 \\ --static-dir public \\ --layer-cache-basedir \"${HOME}\/.aws-sam\/layers-pkg\" \\ --container-host host.docker.internal else echo \"🚀 Remote API setup will be handled by CloudFormation stack (no action taken here).\" fi ``` LocalStack is running (status: ``` ✔ running, name: \"localstack-main\", IP: 172.17.0.2 ``` ), and ``` awslocal secretsmanager get-secret-value --secret-id UserCredentials ``` works from the host. What I've tried: --docker-network host in sam local start-api --container-host host.docker.internal Increased sleep times (15 seconds) and readiness checks for LocalStack Verified LocalStack logs show no errors and \"Ready.\" Tested with dummy credentials in ``` ~\/.aws\/credentials ``` How can I make the SAM CLI Lambda container connect to LocalStack on macOS? Is there a better way to configure the Docker network or endpoint? Any insights would be appreciated!",
    "author_id":5490,
    "publication_date":1754341480000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Daniel Dow",
    "author_reputation":11.0,
    "tags":"python, docker, aws-lambda, aws-secrets-manager, sam",
    "text_length":3161,
    "title_length":79,
    "num_tags":5
  },
  {
    "id":5977,
    "title":"How to receive raw input from terminal? It still echos after crossterm::terminal::enable_raw_mode()",
    "link":"https:\/\/stackoverflow.com\/questions\/79725303\/how-to-receive-raw-input-from-terminal-it-still-echos-after-crosstermterminal",
    "text":"I was making a text editor in Rust. I am taking input in raw mode. But whenever I try to enter a word or a sentence, it just prints each letter as soon as I type it, not even letting me finish the word. I want to print the value of the thing I type as binary and char as of now then think of future functionality. This is my current code: ``` use crossterm::terminal::{disable_raw_mode, enable_raw_mode}; use std::io::{self, Bytes, Read, stdin}; fn main() { enable_raw_mode().unwrap(); for x in io::stdin().bytes() { match x { Ok(x) => { let c = x as char; println!(\"Binary: {0:08b} ASCII: {0:#03} Character: {1:#?}\\r\", x, c); } Err(error) => println!(\"ERROR PROCESSING YOUR REQUEST\"), } } disable_raw_mode().unwrap(); } ``` I have tried various methods, and looked at Documentation, tried testing directly from the terminal, tried it from VS CODE. I am using windows 11.",
    "author_id":5489,
    "publication_date":1754342180000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ARJAN CHAKRABORTY",
    "author_reputation":9.0,
    "tags":"rust, terminal, crossterm",
    "text_length":871,
    "title_length":99,
    "num_tags":3
  },
  {
    "id":5976,
    "title":"pg8000 conn.run argument error\/ SQL syntax issues inserting data with AWS Lambda",
    "link":"https:\/\/stackoverflow.com\/questions\/79725304\/pg8000-conn-run-argument-error-sql-syntax-issues-inserting-data-with-aws-lambda",
    "text":"I am using AWS Lambda with Python and pg8000 to insert records into a PostgreSQL RDS database. When ``` conn.run() ``` is called, I get argument errors or syntax errors. INSERT_SQL: ``` INSERT INTO crypto_prices (coin, price_usd, timestamp, day_of_week) VALUES ($1, $2, $3, $4) ON CONFLICT (coin, timestamp) DO NOTHING; ``` 'params' looks like: ``` ('bitcoin', 115251, '2025-08-04T18:30:40.926246+00:00', 'Monday') ``` Error in AWS logs (if I use ``` conn.run(INSERT_SQL, params) ``` ): Failed to insert bitcoin: list index out of range Error in AWS logs (if I use ``` conn.run(INSERT_SQL, *params) ``` ): Failed to insert bitcoin: Connection.run() takes from 2 to 4 positional arguments but 6 were given Any help is super appreciated, thank you.",
    "author_id":5488,
    "publication_date":1754342278000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Lil&#39; Big Boy",
    "author_reputation":19.0,
    "tags":"sql, aws-lambda, amazon-web-services, amazon-rds, pg8000",
    "text_length":746,
    "title_length":80,
    "num_tags":5
  },
  {
    "id":5975,
    "title":"How to pass vector from Python to Duckdb for vector similarity search",
    "link":"https:\/\/stackoverflow.com\/questions\/79725307\/how-to-pass-vector-from-python-to-duckdb-for-vector-similarity-search",
    "text":"I am using Duckdb with the VSS extension to store document embeddings, it works normally however when I try to do a similarity search to a vector passed from Python, I am getting a TypeError. The query looks like this: ``` query = text('select document from embedding_duckdb where collection_name = :collection_name order by array_cosine_similarity(embedding, :embedding::FLOAT[1024])) limit :num_docs') result = self.session.execute(query, {'collection_name': collection, 'embedding': array_type(response[\"embedding\"]), 'num_docs': num_docs}) pages = [row[0] for row in result.fetchall()] ``` here is TypeError I get: ``` E TypeError: array_type(): incompatible function arguments. The following argument types are supported: E 1. (type: duckdb.duckdb.typing.DuckDBPyType, size: int, *, connection: duckdb.DuckDBPyConnection = None) -> duckdb.duckdb.typing.DuckDBPyType E E Invoked with: [0.8049294352531433, -0.5165860652923584, -0.25891393423080444, -0.8668404221534729, 0.050446540117263794, -0.060985639691352844, -0.2768419682979584, -0.3037632703781128, 0.7962498664855957, -0.30102455615997314, 0.13201330602169037, -0.35436326265335083, -0.2618299722671509, -0.030353419482707977, -0.009162794798612595, -0.37244415283203125, -0.505980372428894, -0.2313883751630783, -0.4354000389575958, 0.7349468469619751, 0.18996360898017883, 0.47891461849212646, -0.009091421961784363, ... ``` I tried other ways of casting the vector, for example using the ``` cast ``` function. AFAICT I am doing exactly as the docs instruct. Don't know what else to try.",
    "author_id":5487,
    "publication_date":1754342521000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"fccoelho",
    "author_reputation":6262.0,
    "tags":"python, sql, duckdb",
    "text_length":1553,
    "title_length":69,
    "num_tags":3
  },
  {
    "id":5974,
    "title":"illegal condition for field filter error in pineconeStore similaritySearch",
    "link":"https:\/\/stackoverflow.com\/questions\/79725309\/illegal-condition-for-field-filter-error-in-pineconestore-similaritysearch",
    "text":"I am building an AI agent and the retrieval node causing issues when I am adding filter object. Here is the core code or retrieval node : ``` const filter = { info: state.info , }; const embeddings = new GoogleGenerativeAIEmbeddings({ apiKey: state.apiKey, model: \"models\/embedding-001\", }); console.log(\"Created the embedding model\"); const vectorStore = await PineconeStore.fromExistingIndex(embeddings, { pineconeIndex: index, maxConcurrency: 5, }); console.log(\"Created the vectors store\"); const retriever = { withConfig: (_cfg: RunnableConfig) => ({ invoke: (query: string, _config?: RunnableConfig) => vectorStore.similaritySearch(query, 3, { filter}), }), }; console.log(\"Created retriever\"); relatedDocuments = await retriever .withConfig({ runName: \"FetchRelevantDocuments\" }) .invoke(state.question, config); ``` Here is what I am upserting in pinecone : ``` const now = new Date().toISOString(); const upsertData: VectorData[] = vectors.map((values, i) => ({ id: `doc-${i}-${Date.now()}`, values, metadata: { info: info, date: now, text: docs[i], }, })); ``` When I run postman with this query : ``` { \"info\":\"profile\/Tamalckb531\", \"question\":\"What is the most popular repo here ?\"} ``` It shows this error : ``` illegal condition for field filter, got {\"info\":\"profile\/Tamalckb531\"} ``` Running fine without the filter.",
    "author_id":5486,
    "publication_date":1754342874000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tamal Chakraborty",
    "author_reputation":21.0,
    "tags":"typescript, langchain, pinecone, honojs",
    "text_length":1332,
    "title_length":74,
    "num_tags":4
  },
  {
    "id":5973,
    "title":"Vscode\/Cursor keeps reformatting the python file when I press enter inside a list",
    "link":"https:\/\/stackoverflow.com\/questions\/79725310\/vscode-cursor-keeps-reformatting-the-python-file-when-i-press-enter-inside-a-lis",
    "text":"I am seeing this weird behaviour by vscode (Cursor actually) where it reformats the file as soon as I press enter. Showcased it in this video: https:\/\/youtu.be\/kACXeZoYZoQ I have not been able to figure out what setting is causing this. I have disabled format-on-save just to be sure. Can someone point me towards right setting or atleast some way to monitor vscode event logs so I know what is causing this behaviour.",
    "author_id":5485,
    "publication_date":1754343037000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Yash Aggarwal",
    "author_reputation":88.0,
    "tags":"visual-studio-code, cursor-ide",
    "text_length":418,
    "title_length":81,
    "num_tags":2
  },
  {
    "id":5972,
    "title":"Newline in GT table, rendered in word",
    "link":"https:\/\/stackoverflow.com\/questions\/79725319\/newline-in-gt-table-rendered-in-word",
    "text":"I have some table cells in a GT table I would like to have a new line in. Works great when output to html but when I tried docx the newlines are not showing up. MWE: ``` --- title: \"Testing docx render\" date: today format: docx: default --- ```{r} library(tidyverse) library(gtsummary) library(gt) set.seed(123) d <- tibble( x = rep(c(\"A\",\"B\"), each=10), y = rnorm(20), z = sample(c(\"C\",\"D\"), 20, replace=TRUE) ) t1 <- d |> tbl_summary(by=x, statistic = list(y ~ \"{median} ({min}-{max})<br>n = {N_nonmiss}\")) |> as_gt() t1 t2 <- d |> tbl_summary(by=x, statistic = list(y ~ \"{median} ({min}-{max})<br>n = {N_nonmiss}\")) |> as_gt() |> fmt_markdown(columns = c(contains(\"stat_\"))) t2 t3 <- d |> tbl_summary(by=x, statistic = list(y ~ md(\"{median} ({min}-{max})<br>n = {N_nonmiss}\"))) |> as_gt() t3 ``` Output as docx I get: Compared to HTML:",
    "author_id":5484,
    "publication_date":1754343463000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"bdeonovic",
    "author_reputation":4272.0,
    "tags":"docx, r, quarto, gtsummary, gt",
    "text_length":838,
    "title_length":37,
    "num_tags":5
  },
  {
    "id":5971,
    "title":"Laravel 7 pusher Error 401 and Internal Server Error [500]",
    "link":"https:\/\/stackoverflow.com\/questions\/79725325\/laravel-7-pusher-error-401-and-internal-server-error-500",
    "text":"I have a project in Laravel 7, I want to count when user chat and appears sender name on section my blade.php looks like ``` <div class=\"card\"> <div class=\"card-header head Nunito-family\" id=\"ChatSectionAccordion\"> <i class=\"fa fa-comments fa-lg pl-1\"><\/i> <a class=\"pl-1 text-light\" data-toggle=\"collapse\" href=\"#collapseChat\" aria-expanded=\"true\">{{ __('messages.Chat Section') }} <span class=\"data-count='0'\">0<\/span> <i class=\"fa fa-chevron-down pull-right float-right\"><\/i><\/a> <\/div> <\/div> ``` in app.blade looks like ``` <script src=\"{{ asset('\/js\/jquery.min.js') }}\"><\/script> <script src=\"{{ asset('\/js\/app.js') }}\"><\/script> <script src=\"{{ asset('\/js\/myjs.js') }}\"><\/script> <script src=\"{{ asset('\/js\/all.js') }}\"><\/script> <script src=\"https:\/\/js.pusher.com\/8.4.0\/pusher.min.js\"><\/script> <script> Pusher.logToConsole = true; var pusher = new Pusher('59ef8c5b08ddb96760db', { cluster: 'mt1' encrypted: false }); <\/script> <script src=\"{{ asset('\/js\/pusherNotifications.js') }}\"><\/script> ``` my event chats looks like ``` <?php namespace App\\Events; use Illuminate\\Broadcasting\\Channel; use Illuminate\\Broadcasting\\InteractsWithSockets; use Illuminate\\Broadcasting\\PresenceChannel; use Illuminate\\Broadcasting\\PrivateChannel; use Illuminate\\Contracts\\Broadcasting\\ShouldBroadcast; use Illuminate\\Foundation\\Events\\Dispatchable; use Illuminate\\Queue\\SerializesModels; class ChatNotify implements ShouldBroadcast { use Dispatchable, InteractsWithSockets, SerializesModels; public $senderMSG; public function __construct(array $data) { $this->senderMSG = $data['senderMSG']; } \/** * Get the channels the event should broadcast on. * * @return \\Illuminate\\Broadcasting\\Channel|array *\/ public function broadcastOn() { return ['realtime']; } public function broadcastAs() { return 'chat-notify'; } } ``` in controller file looks like ``` public function userChat(Request $request) { if (Auth::guard('admin')->check()) { $user = Auth::guard('admin')->user()->name; } if (Auth::guard('web')->check()) { $user = Auth::user()->name; } $user_id = $request->user_id; $purpose = $request->purpose; if ($purpose == 'find-user') { $msg = Chat::where('from', $user)->where('to', $user_id) ->orWhere('from', $user_id)->Where('to', $user) ->get(); \/\/ dd($msg); return response()->json($msg); } elseif ($purpose == 'admin-user' && $user_id != 'allUsers') { $val = $request->val; \/\/ $val = $request->input('val'); or $val = $request->val; $chat = Chat::create([ 'message' => $val, 'from' => $user, 'to' => $user_id ]); $data= [ 'senderMSG' => $user, ]; event(new ChatNotify($data)); return response()->json(['user' => $user, 'msg' => $val]); } elseif ($purpose == 'admin-user' && $user_id == 'allUsers') { $val = $request->val; if (Auth::guard('admin')->check()) { $admin = Auth::guard('admin')->user()->name; } $users = User::all(); if($users->count() > 0) { foreach($users as $user) { $chat = Chat::create([ 'message' => $val, 'from' => $admin, 'to' => $user->name ]); } return response()->json(['user' => $admin, 'msg' => $val]); } else return false; } } ``` in my pusherNotifications.js lokks like ``` var notificationsWrapper = $('div#ChatSectionAccordion'); var notificationsCountElem = notificationsWrapper.find('span.data-count'); var notificationsCount = parseInt(notificationsCountElem.data('count')); \/\/ Subscribe to the channel we specified in our Laravel Event var channel = pusher.subscribe('realtime'); \/\/ Bind a function to a Event (the full Laravel class) channel.bind('App\\\\Events\\\\ChatNotify', function (data) { var sender = data.senderMSG; notificationsCount +=1; notificationsCountElem.attr('data-count', notificationsCount + ' ' + sender); }); ``` in my broadcasting.php looks like ``` 'pusher' => [ 'driver' => 'pusher', 'key' => env('PUSHER_APP_KEY'), 'secret' => env('PUSHER_APP_SECRET'), 'app_id' => env('PUSHER_APP_ID'), 'options' => [ 'cluster' => env('PUSHER_APP_CLUSTER'), \/\/ 'useTLS' => true, 'encrypted' => true, ], ], ``` But I get a message error 401 when I chat",
    "author_id":5483,
    "publication_date":1754343923000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ahmad almasri",
    "author_reputation":51.0,
    "tags":"pusher, laravel-7",
    "text_length":3991,
    "title_length":58,
    "num_tags":2
  },
  {
    "id":5970,
    "title":"Rename branch that uses # at the beginning of the name",
    "link":"https:\/\/stackoverflow.com\/questions\/79725329\/rename-branch-that-uses-at-the-beginning-of-the-name",
    "text":"I have a branch using ``` # ``` at the beginning of the branch name. I try to rename using: ``` git branch -m #test-branch2 ``` The message I receive is: ``` fatal: branch name required ``` How can I get rid of this message and rename the branch?",
    "author_id":5482,
    "publication_date":1754344193000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"&#194;ngelo Rigo",
    "author_reputation":2191.0,
    "tags":"git",
    "text_length":246,
    "title_length":54,
    "num_tags":1
  },
  {
    "id":5969,
    "title":"NSEvent.mouseEvent coordinates affected by display scaling - how to click consistently?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725332\/nsevent-mouseevent-coordinates-affected-by-display-scaling-how-to-click-consis",
    "text":"I'm using ``` NSEvent.mouseEvent() ``` to click specific coordinates in another app's window, but when I change display resolution\/scaling, the same coordinates (500, 300) click different spots in the window. ``` func sendNSEventClick(at point: NSPoint, pid: pid_t, windowNumber: Int) { guard let mouseDown = NSEvent.mouseEvent( with: .leftMouseDown, location: point, modifierFlags: [], timestamp: ProcessInfo.processInfo.systemUptime, windowNumber: windowNumber, context: nil, eventNumber: 0, clickCount: 1, pressure: 1.0 ) else { return } mouseDown.cgEvent?.postToPid(pid) \/\/ ... mouseUp similar } ``` Simple test that fails: ``` \/\/ This should click window center, but clicks different spots at different resolutions let centerPoint = NSPoint(x: windowFrame.width\/2, y: windowFrame.height\/2) sendNSEventClick(at: centerPoint, pid: pid, windowNumber: windowNumber) ``` How do I make coordinates resolution\/scaling independent? Do I need to apply some scaling factor to the coordinates before passing to ``` NSEvent.mouseEvent() ``` ?",
    "author_id":5481,
    "publication_date":1754344286000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"SamHoque",
    "author_reputation":3164.0,
    "tags":"swift, macos, nsevent",
    "text_length":1035,
    "title_length":87,
    "num_tags":3
  },
  {
    "id":5968,
    "title":"Update JSON Schema in Power Automate Flow",
    "link":"https:\/\/stackoverflow.com\/questions\/79725335\/update-json-schema-in-power-automate-flow",
    "text":"I am using Power Automate to run checks on different servers. The flow receives an HTTP request from a PowerShell script and parses through the JSON payload. Some servers, for example, have several Network Interface Cards, while others only have one. The issue I run into is that, for the JSON schema I use to parse the input, I have to keep changing the \"type\" value from \"object\" to \"array\" for the Network Interface Cards or vice versa depending on what server I'm running the script on. Is there a way to update the schema as it receives the input to remove the need of having to edit the flow each time?",
    "author_id":5480,
    "publication_date":1754344345000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"mimi m",
    "author_reputation":71.0,
    "tags":"powershell, json, power-automate",
    "text_length":608,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":5967,
    "title":"How to load firebase service worker with different configurations (test\/prod)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725337\/how-to-load-firebase-service-worker-with-different-configurations-test-prod",
    "text":"Depending on the environment, I have two different configurations I would like to use (test and prod). Unfortunately, firebase only works with the one service worker in the domain root folder ( ``` \/firebase-messaging-sw.js ``` ). The dev environment is in a subfolder of the domain. What I tried is sending a message to the sw as soon as it is registered and ready with the correct config - that works. However, initializing the firebase app throws warnings if it is not done immediately (don't know why that is) and the listener ``` messaging.onBackgroundMessage ``` stops working. Warnings in Console from Firebase ( ``` ...\/firebasejs\/messaging\/src\/helpers\/register.ts ``` ) register.ts:80 Event handler of 'push' event must be added on the initial evaluation of worker script. register.ts:83 Event handler of 'pushsubscriptionchange' event must be added on the initial evaluation of worker script. register.ts:86 Event handler of 'notificationclick' event must be added on the initial evaluation of worker script. app.js Sending the config ``` navigator.serviceWorker.register('\/firebase-messaging-sw.js') .then(() => navigator.serviceWorker.ready) .then(reg => reg.active.postMessage({ firebaseConfig })) ``` firebase-messaging-sw.js This gets me the right config, but initializing firebase fails ``` self.addEventListener('message', ({ data }) => { if(data?.firebaseConfig) initFirebaseMessaging(data.firebaseConfig); }); ``` And this is the main part of initializing firebase ``` importScripts('https:\/\/www.gstatic.com\/firebasejs\/9.21.0\/firebase-app-compat.js'); importScripts('https:\/\/www.gstatic.com\/firebasejs\/9.21.0\/firebase-messaging-compat.js'); const initFirebaseMessaging = function(config){ firebase.initializeApp(config); const messaging = firebase.messaging(); messaging.onBackgroundMessage(message => { ... }); }; ``` When I hard code the config for one environment in the sw file and call ``` initFirebaseMessaging(...) ``` right away after importing the scripts, everything works fine.",
    "author_id":4722,
    "publication_date":1754344530000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Simon Ferndriger",
    "author_reputation":5098.0,
    "tags":"javascript, firebase, service-worker, firebase-cloud-messaging",
    "text_length":2007,
    "title_length":77,
    "num_tags":4
  },
  {
    "id":5966,
    "title":"What am I missing to create a loop that works?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725338\/what-am-i-missing-to-create-a-loop-that-works",
    "text":"I have multiple simple DFs with survey results (I've made up test data below that yields the same result). I'm trying to write a loop to select two of the seven columns so I don't have to write the same thing out each time. ``` test1 = { \"Strongly Agree\": [4], \"Agree\": [2], \"Neither Agree nor Disagree\": [1], \"Disagree\": [2], \"Strongly Disagree\": [1], \"Top 2 Box\": [6], \"Label\": [\"One\"] } test2 = { \"Strongly Agree\": [6], \"Agree\": [2], \"Neither Agree nor Disagree\": [1], \"Disagree\": [0], \"Strongly Disagree\": [1], \"Top 2 Box\": [8], \"Label\": [\"Two\"] } df1 = pd.DataFrame(test1) df2 = pd.DataFrame(test2) df_list = [df1, df2] for df in df_list: df = df[[\"Top 2 Box\", \"Label\"]] for df in df_list: print(df.head()) ``` (Or just df1.head() and df2.head()... they yield the same results as the attempted loop above.) I'm trying to update the DFs so they just have the two columns in the order dictated. Clearly I'm missing something. When I run the above, I just get the same DFs I started with.",
    "author_id":5479,
    "publication_date":1754344532000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"regulationboomerang",
    "author_reputation":11.0,
    "tags":"python, dataframe, loops, for-loop",
    "text_length":990,
    "title_length":46,
    "num_tags":4
  },
  {
    "id":5965,
    "title":"Order of columns in a plotnine bar plot using a polars dataframe",
    "link":"https:\/\/stackoverflow.com\/questions\/79725340\/order-of-columns-in-a-plotnine-bar-plot-using-a-polars-dataframe",
    "text":"I'm quite new to the packages polars and plotnine and have the following code: ``` import polars as pl import polars.selectors as cs from plotnine import * df = pl.read_csv('https:\/\/raw.githubusercontent.com\/Brinkhuis\/Zorguitgaven\/refs\/heads\/master\/zorguitgaven.csv') df = df.unpivot(cs.numeric(), index='Category') ( ggplot() + geom_bar( data=df.filter(pl.col('variable').is_in(['Mannen 2040', 'Vrouwen 2040'])), mapping=aes(x='Category', y='value', fill='variable'), stat='identity' ) + scale_fill_manual(values=['#007BC7', '#CA005D']) + coord_flip() ) ``` It runs without errors. However, the Category order is not right. The \"5-10\" value show be on \"row 3\". My questions: Is there a way to set the categorical order in a polars dataframe to fix this? Is there a way to set the categorical order in plotnine?",
    "author_id":4984,
    "publication_date":1754344790000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ren&#233;",
    "author_reputation":4919.0,
    "tags":"python, dataframe, python-polars, plotnine, polars",
    "text_length":811,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":5964,
    "title":"How to correctly integrate java source into a leiningen project?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725342\/how-to-correctly-integrate-java-source-into-a-leiningen-project",
    "text":"This is my java source file: ``` package main; class ModelLoader { public static void foo() { System.out.println(\"foo\"); } } ``` which is located at ``` src\/main\/java\/ModelLoader.java ``` . In ``` project.clj ``` I added this line: ``` :java-source-paths [\"src\/main\"] ``` Compilation seems to be working: ``` Compiling 1 source files to \/home\/me\/projects\/learnopengl\/target\/classes ``` which produces ``` target\/classes\/main\/ModelLoader.class ``` . In ``` core.clj ``` I've imported the class and added this line: ``` (ns learnopengl.core (:import [main ModelLoader])) ... (ModelLoader\/foo) ``` And this doesn't work: ``` Execution error (IllegalAccessError) at learnopengl.core\/eval490 (core.clj:20). failed to access class ModelLoader from class learnopengl.core$eval490 (ModelLoader is in unnamed module of loader 'app'; learnopengl.core$eval490 is in unnamed module of loader clojure.lang.DynamicClassLoader @4228bf58) ``` What does this error mean? Specifically, what is this \"in unnamed module of loader\" business? How to fix it?",
    "author_id":5163,
    "publication_date":1754344876000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user1785730",
    "author_reputation":3801.0,
    "tags":"clojure, leiningen",
    "text_length":1035,
    "title_length":64,
    "num_tags":2
  },
  {
    "id":5963,
    "title":"scanimage leads to sane_start: Invalid argument with Device passed in Arguments",
    "link":"https:\/\/stackoverflow.com\/questions\/79725348\/scanimage-leads-to-sane-start-invalid-argument-with-device-passed-in-arguments",
    "text":"I am able to scan using the scaneimage command only when i do not define a device otherwise it throughs an sane_start: Invalid argument. Any Ideas what could be the cause of that issue. The following works fine ``` scanimage -o 'test.tif' ``` How ever the following is not working ``` scanimage -d 'canon_dr:libusb:005:012' -o 'test.tif' scanimage: sane_start: Invalid argument ``` This is the Device list just for completness ``` scanimage -L device `canon_dr:libusb:005:012' is a CANON P-208II scanner ``` Running Debian 12, Kernel 6.1.0-37-amd64 and sane\/stable 1.0.14-17 amd64",
    "author_id":5478,
    "publication_date":1754345341000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Kai D&#246;lger",
    "author_reputation":9.0,
    "tags":"debian, invalid-argument, sane",
    "text_length":580,
    "title_length":79,
    "num_tags":3
  },
  {
    "id":5962,
    "title":"Not using side effects, immediately emit State based on condition, then debounce the user input and make a server request for new State",
    "link":"https:\/\/stackoverflow.com\/questions\/79725360\/not-using-side-effects-immediately-emit-state-based-on-condition-then-debounce",
    "text":"Given: an ``` Observable ``` that emits the user input; 3 states: ``` Success ``` , ``` Error ``` , ``` Idle ``` (shows nothing, may be treated as some progress). I want the following behavior: as soon as the user types, check if the input is too short or the last state was ``` Error ``` , then immediately emit ``` Idle ``` , otherwise debounce the input and make a server request. The main requirement is to AVOID side effects. Also, if it's possible, do not keep the state outside the chain. If you try to bypass ``` debounce ``` with ``` timer ``` , please consider the following case: if the user types \"abc\", receives the ``` Success ``` result, and then they type \"abcd\", then rapidly (within the debounce) remove the last character, a server request should not be sent. Also, make sure there are no races. I have the following implementation, but the ``` Observable.merge ``` may cause races, I guess: ``` inputAction .map { it.input } .publish { shared -> Observable.merge( \/\/ (!) no order guaranteed clearState(shared), processInput(shared) ) } .scan(State.idle()) { prev, reducer -> reducer(prev) } .distinctUntilChanged() fun processInput(source: Observable<String>): Observable<Reducer> { return source .debounce(1, TimeUnit.SECONDS) .distinctUntilChanged() .switchMapSingle { input -> serverRequest(input) .map { State.success(it) } .onErrorReturnItem(State.error()) } .map { newState -> { _ -> newState } } private fun clearState(source: Observable<String>): Observable<Reducer> { return source .map { if (input.isShouldBeIgnored()) { { State.idle() } } else { { prev: State -> when (prev) { is Error -> State.idle() else -> prev } } } } } ```",
    "author_id":5477,
    "publication_date":1754346367000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ivan8m8",
    "author_reputation":457.0,
    "tags":"kotlin, rx-java",
    "text_length":1659,
    "title_length":135,
    "num_tags":2
  },
  {
    "id":5961,
    "title":"Check referential integrity between two GridDB containers",
    "link":"https:\/\/stackoverflow.com\/questions\/79725366\/check-referential-integrity-between-two-griddb-containers",
    "text":"I have two GridDB Cloud containers participating in a one-to-many relationship. The tables layout is shown below: Table ``` Customer ``` (collection container) Name Type Nullity Key Customer_ID INTEGER NOT NULL TRUE Customer_Name STRING NOT NULL FALSE Customer_Email STRING NULL FALSE Table ``` CustomerData ``` (timeseries container) Name Type Nullity Key CustomerData_TS TIMESTAMP NOT NULL TRUE Customer_ID INTEGER NOT NULL FALSE CustomerData_Val1 DOUBLE NOT NULL FALSE CustomerData_Val2 DOUBLE NOT NULL FALSE ``` CustomerData.Customer_ID ``` is a foreign key to the primary key of table ``` Customer ``` . I´d like use a single SQL statement that inserts a line in the ``` CustomerData ``` table only if the value for ``` Customer_ID ``` exists in table ``` Customer ``` . Additionally, I’d like to automatically provide the equivalent of ANSI's ``` CURRENT_TIMESTAMP ``` value, in order to populate the ``` CustomerData_TS ``` column. I tried to create the ``` CustomerData ``` table specifying declarative defaults and referential integrity (below), but GridDB throws an error: The invalid query was specified This is my table structure: ``` Create table CustomerData ( CustomerData_TS TIMESTAMP NOT NULL PRIMARY KEY DEFAULT CURRENT_TIMESTAMP, Customer_ID INTEGER NOT NULL FOREIGN KEY REFERENCES Customer (Customer_ID), CustomerData_Val1 DOUBLE NOT NULL, CustomerData_Val2 DOUBLE NOT NULL ) ``` Any ideas on how to accomplish that?",
    "author_id":5476,
    "publication_date":1754346844000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Danilo Silva",
    "author_reputation":91.0,
    "tags":"sql, cloud, griddb, referential-integrity",
    "text_length":1436,
    "title_length":57,
    "num_tags":4
  },
  {
    "id":5960,
    "title":"Not able to click the deny_cookies_button",
    "link":"https:\/\/stackoverflow.com\/questions\/79725371\/not-able-to-click-the-deny-cookies-button",
    "text":"I’m scraping easyjet.com but clicking the “Deny Cookies” button on the second GDPR\/cookie consent popup isn't working: ``` BASE_URL = 'easyjet.com' ``` ``` EASYJET | |-run.py |-EasyJet (folder) |-easyjet.py ``` run.py: ``` from Easyjet.easyjet import EasyJet with EasyJet(teardown=False) as bot_easyjet: try: bot_easyjet.land_home_page() bot_easyjet.reject_cookies() bot_easyjet.origin_and_destination(origin='Zurich', destination='Madrid') bot_easyjet.deaparture_dates(date='5-9-2025') bot_easyjet.return_date(date='10-9-2025') bot_easyjet.passengers(2,0,0) bot_easyjet.submit_info() bot_easyjet.reject_second_cookies() except Exception as e: raise ``` easyjet.py: ``` from selenium import webdriver from selenium.webdriver.chrome.service import Service from webdriver_manager.chrome import ChromeDriverManager from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys import EASYJET.constants as const import time from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC class EasyJet(webdriver.Chrome): def __init__(self, teardown=False): self.teardown = teardown #prevent google log-in messages options = webdriver.ChromeOptions() options.add_experimental_option('excludeSwitches', ['enable-logging']) super(EasyJet,self).__init__(service=Service(ChromeDriverManager().install()), options=options) self.implicitly_wait(15) self.maximize_window() def __exit__(self, exc_type, exc, traceback): if self.teardown: self.quit() def land_home_page(self): self.get(const.BASE_URL) def reject_cookies(self): decline_element = self.find_element( By.ID, 'ensRejectAll' ) decline_element.click() def origin_and_destination(self, origin:str, destination:str): origin_element_box = self.find_element( By.ID, 'from' ) origin_element_box.clear() origin_element_box.click() origin_element_box.send_keys(f'{origin}') select_first_element = self.find_element( By.CSS_SELECTOR, 'label[data-testid=\"airport-label\"]' ) select_first_element.click() destination_element_box = self.find_element( By.ID, 'to' ) destination_element_box.click() destination_element_box.send_keys(f'{destination}') select_first_element = self.find_element( By.CSS_SELECTOR, 'label[data-testid=\"airport-label\"]' ).click() def deaparture_dates(self, date:str): select_departure_element_box = self.find_element( By.ID, 'when' ) select_departure_element_box.click() select_departure_day = self.find_element( By.CSS_SELECTOR, f'button[data-testid=\"{date}\"]' #day-month-year; ej: 2-9-2025 ) select_departure_day.click() def return_date(self, date:str): select_departure_day = self.find_element( By.CSS_SELECTOR, f'button[data-testid=\"{date}\"]' ) select_departure_day.click() def passengers(self, adults:int, children:int, infants:int): select_passengers_element_box = self.find_element( By.ID, 'who' ) select_passengers_element_box.click() adults_on_trip = self.find_element( By.CSS_SELECTOR, 'button[aria-label=\"Add one adult\"]' ) for adult in range(adults - 1): adults_on_trip.click() apply_changes_button = self.find_element( By.CSS_SELECTOR, 'button[data-testid=\"close-button\"]' ) apply_changes_button.click() def submit_info(self): submit_button = self.find_element( By.CSS_SELECTOR, 'button[data-testid=\"submit\"]' ) submit_button.click() def reject_second_cookies(self): deny_button = self.find_element( By.CSS_SELECTOR, 'button[data-testid=\"uc-deny-all-button\"]' ) deny_button.click() ```",
    "author_id":5475,
    "publication_date":1754347307000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user_none",
    "author_reputation":1.0,
    "tags":"selenium-webdriver, python, web-scraping",
    "text_length":3445,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":5959,
    "title":"BLIP Fine-Tuning: Special Token Always Biased to One Class in Generated Caption",
    "link":"https:\/\/stackoverflow.com\/questions\/79725374\/blip-fine-tuning-special-token-always-biased-to-one-class-in-generated-caption",
    "text":"I'm trying to fine-tune Hugging Face BLIP (Bootstrapped Language-Image Pretraining) to classify pizza boxes as either recyclable (clean) or non-recyclable (contaminated) by generating captions that explicitly include a custom special token. Objective Have BLIP generate captions including special tokens such as \"A greasy pizza box with leftover cheese. [XXY_CONTAM]\" \"A clean, dry pizza box. [CTX_CLEAN]\" Training Code (train.py) ``` from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments from datasets import Dataset from PIL import Image import torch, os, json # === Paths === model_path = \"D:\/models\/blip-caption\" data_path = \"D:\/BLIP\/captions.json\" image_root = \"D:\/BLIP\" output_dir = \"D:\/BLIP\/blip-finetuned-pizza\" log_dir = \"D:\/BLIP\/logs\" # === Load dataset JSON === with open(data_path, \"r\") as f: data = json.load(f) dataset = Dataset.from_list(data) # === Load processor and model === processor = BlipProcessor.from_pretrained(model_path) model = BlipForConditionalGeneration.from_pretrained(model_path) # === Add special tokens === special_tokens_dict = {\"additional_special_tokens\": [\"[CTX_CLEAN]\", \"[XXY_CONTAM]\"]} processor.tokenizer.add_special_tokens(special_tokens_dict) model.resize_token_embeddings(len(processor.tokenizer)) # === Preprocess function === def preprocess(example): image_path = os.path.join(image_root, example[\"image\"]) image = Image.open(image_path).convert(\"RGB\") inputs = processor( images=image, text=example[\"caption\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=32 ) inputs = {k: v.squeeze(0) for k, v in inputs.items()} inputs[\"labels\"] = inputs[\"input_ids\"] return inputs processed_dataset = dataset.map(preprocess) # === Training arguments === training_args = TrainingArguments( output_dir=output_dir, per_device_train_batch_size=2, num_train_epochs=10, logging_dir=log_dir, logging_steps=5, save_steps=20, save_total_limit=1, remove_unused_columns=False, fp16=torch.cuda.is_available() ) # === Trainer === trainer = Trainer( model=model, args=training_args, train_dataset=processed_dataset, ) # === Train and Save === trainer.train() model.save_pretrained(output_dir) processor.tokenizer.save_pretrained(output_dir) processor.save_pretrained(output_dir) print(\"Fine-tuned model saved to\", output_dir) print(\"Special token IDs:\", processor.tokenizer.convert_tokens_to_ids([\"[CTX_CLEAN]\", \"[XXY_CONTAM]\"])) ``` Inference Code () ``` from transformers import BlipProcessor, BlipForConditionalGeneration from PIL import Image import torch # === Load model and processor === model_path = \"D:\/BLIP\/blip-finetuned-pizza\" image_path = \"D:\/Models\/pizzabox.jpg\" processor = BlipProcessor.from_pretrained(model_path) model = BlipForConditionalGeneration.from_pretrained(model_path) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) image = Image.open(image_path).convert(\"RGB\") inputs = processor(images=image, return_tensors=\"pt\").to(device) with torch.no_grad(): output = model.generate(**inputs, max_length=32) caption = processor.decode(output[0], skip_special_tokens=False) print(\"Generated Caption:\", caption) ``` Problem Despite having: Balanced data Clear special tokens ([CTX_CLEAN], [XXY_CONTAM]) Proper token registration with tokenizer\/model The model always outputs only one of the tokens (whichever comes first in additional_special_tokens). For example: ``` special_tokens_dict = {\"additional_special_tokens\": [\"[CTX_CLEAN]\", \"[XXY_CONTAM]\"]} ``` Always outputs: [CTX_CLEAN] If reversed: ``` special_tokens_dict = {\"additional_special_tokens\": [\"[XXY_CONTAM]\", \"[CTX_CLEAN]\"]} ``` Always outputs: [XXY_CONTAM] So the model is not learning to generate the correct label token based on the image content, but just consistently prefers the first token in the list. Question How can I make BLIP learn to output the correct special token based on the image? Is there something wrong with how I’m handling special tokens, captions, or preprocessing? Do I need to set token embeddings manually, or enforce token use during generation? Any insights or suggestions would be greatly appreciated! Thanks in advance.",
    "author_id":5474,
    "publication_date":1754347620000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Wow Wow",
    "author_reputation":11.0,
    "tags":"huggingface, huggingface-transformers, huggingface-trainer",
    "text_length":4160,
    "title_length":79,
    "num_tags":3
  },
  {
    "id":5958,
    "title":"Unable to get show and hide columns button to work",
    "link":"https:\/\/stackoverflow.com\/questions\/79725375\/unable-to-get-show-and-hide-columns-button-to-work",
    "text":"I am trying to add a show\/hide button to show\/hidden columns in an angular material table. I can't get the button to function properly. When clicking on the button it does show the columns in the table I want to show\/hide. I'm also trying to get the button to be placed as the last column header of the table after the 'position' column header on the right or whatever column is the last column. Also want the column names for the button to show vertically instead of horizontally bunched together like they are now. html ``` <button mat-button [matMenuTriggerFor]=\"menu\" class=\"table-config-menu\"> <mat-icon aria-hidden=\"false\" aria-label=\"Example home icon\">settings<\/mat-icon> <mat-menu #menu=\"matMenu\"> <span class=\"table-config-menu-label\">Edit Columns<\/span> <div class=\"table-config-menu-options\"> <mat-checkbox *ngFor=\"let cd of columnDefinitions; let i = index\" (click)=\"$event.stopPropagation()\" [(ngModel)]=\"cd.visible\"> {{cd.label}} <\/mat-checkbox> <\/div> <\/mat-menu> <\/button> <table mat-table [dataSource]=\"dataSource\" multiTemplateDataRows class=\"mat-elevation-z8\" > <ng-container matColumnDef=\"{{column}}\" *ngFor=\"let column of columnsToDisplay\" > <ng-container *ngIf=\"column !== 'action'; else action\"> <th mat-header-cell *matHeaderCellDef>{{column}}<\/th> <td mat-cell *matCellDef=\"let element\">{{element[column]}}<\/td> <\/ng-container> <ng-template #action> <th mat-header-cell *matHeaderCellDef>Actions<\/th> <td mat-cell *matCellDef=\"let element\" class=\"checkbox-spacing\"> <mat-icon (click)=\"expandedElement = expandedElement === element ? null : element\" >{{expandedElement === element ? 'expand_less' : 'expand_more'}}<\/mat-icon > <\/td> <\/ng-template> <\/ng-container> <ng-container matColumnDef=\"select\"> <th mat-header-cell *matHeaderCellDef> <mat-checkbox (change)=\"$event ? toggleAllRows() : null\" [checked]=\"selection.hasValue() && isAllSelected()\" [indeterminate]=\"selection.hasValue() && !isAllSelected()\" [aria-label]=\"checkboxLabel()\" > <\/mat-checkbox> <\/th> <td mat-cell *matCellDef=\"let row\"> <mat-checkbox (click)=\"$event.stopPropagation()\" (change)=\"$event ? selection.toggle(row) : null\" [checked]=\"selection.isSelected(row)\" [aria-label]=\"checkboxLabel(row)\" > <\/mat-checkbox> <\/td> <\/ng-container> <!-- Expanded Content Column - The detail row is made up of this one column that spans across all columns --> <ng-container matColumnDef=\"expandedDetail\"> <td mat-cell *matCellDef=\"let element\" [attr.colspan]=\"columnsToDisplay.length\" > <div class=\"example-element-detail\" [@detailExpand]=\"element == expandedElement ? 'expanded' : 'collapsed'\" > <div class=\"example-element-diagram\"> <div class=\"example-element-position\">{{element.position}}<\/div> <div class=\"example-element-symbol\">{{element.symbol}}<\/div> <div class=\"example-element-name\">{{element.name}}<\/div> <div class=\"example-element-weight\">{{element.weight}}<\/div> <\/div> <div class=\"example-element-description\"> {{element.description}} <span class=\"example-element-description-attribution\"> -- Wikipedia <\/span> <\/div> <\/div> <\/td> <\/ng-container> <tr mat-header-row *matHeaderRowDef=\"columnsToDisplayWithExpand\"><\/tr> <tr mat-row *matRowDef=\"let element; columns: columnsToDisplayWithExpand;\" class=\"example-element-row\" [class.example-expanded-row]=\"expandedElement === element\" ><\/tr> <tr mat-row *matRowDef=\"let row; columns: ['expandedDetail']\" class=\"example-detail-row\" ><\/tr> <!-- <tr mat-header-row *matHeaderRowDef=\"getDisplayedColumns()\"> <\/tr> <tr mat-row *matRowDef=\"let row; columns: getDisplayedColumns()\"><\/tr> --> <\/table> ``` ts ``` import { SelectionModel } from '@angular\/cdk\/collections'; import { MatTableDataSource } from '@angular\/material\/table'; import { Component } from '@angular\/core'; import { MatCheckboxModule } from '@angular\/material\/checkbox'; import { animate, state, style, transition, trigger, } from '@angular\/animations'; \/** * @title Table with expandable rows *\/ @Component({ selector: 'table-expandable-rows-example', styleUrls: ['table-expandable-rows-example.css'], templateUrl: 'table-expandable-rows-example.html', imports: [MatCheckboxModule], animations: [ trigger('detailExpand', [ state('collapsed', style({ height: '0px', minHeight: '0' })), state('expanded', style({ height: '*' })), transition( 'expanded <=> collapsed', animate('225ms cubic-bezier(0.4, 0.0, 0.2, 1)') ), ]), ], }) export class TableExpandableRowsExample { columnsToDisplay: string[] = [ 'action', 'Name', 'weight', 'symbol', 'position', ]; columnsToDisplayWithExpand = ['select', ...this.columnsToDisplay]; dataSource = new MatTableDataSource<PeriodicElement>(ELEMENT_DATA); expandedElement: PeriodicElement | null; selection = new SelectionModel<PeriodicElement>(true, []); \/** Whether the number of selected elements matches the total number of rows. *\/ isAllSelected() { const numSelected = this.selection.selected.length; const numRows = this.dataSource.data.length; return numSelected === numRows; } \/** Selects all rows if they are not all selected; otherwise clear selection. *\/ toggleAllRows() { if (this.isAllSelected()) { this.selection.clear(); return; } this.selection.select(...this.dataSource.data); } \/** The label for the checkbox on the passed row *\/ checkboxLabel(row?: PeriodicElement): string { if (!row) { return `${this.isAllSelected() ? 'deselect' : 'select'} all`; } return `${this.selection.isSelected(row) ? 'deselect' : 'select'} row ${ row.position + 1 }`; } columnDefinitions = [ { def: 'name', label: 'Name', visible: true }, { def: 'weight', label: 'Weight', visible: true, }, { def: 'symbol', label: 'Symbol', visible: true, }, { def: 'position', label: 'Position', visible: true, }, ]; getDisplayedColumns(): string[] { return this.columnDefinitions .filter((cd) => cd.visible) .map((cd) => cd.def); } } export interface PeriodicElement { Name: string; position: number; weight: number; symbol: string; description: string; } const ELEMENT_DATA: PeriodicElement[] = [ { position: 1, Name: 'Hydrogen', weight: 1.0079, symbol: 'H', description: `Hydrogen is a chemical element with symbol H and atomic number 1. With a standard atomic weight of 1.008, hydrogen is the lightest element on the periodic table.`, }, { position: 2, Name: 'Helium', weight: 4.0026, symbol: 'He', description: `Helium is a chemical element with symbol He and atomic number 2. It is a colorless, odorless, tasteless, non-toxic, inert, monatomic gas, the first in the noble gas group in the periodic table. Its boiling point is the lowest among all the elements.`, }, { position: 3, Name: 'Lithium', weight: 6.941, symbol: 'Li', description: `Lithium is a chemical element with symbol Li and atomic number 3. It is a soft, silvery-white alkali metal. Under standard conditions, it is the lightest metal and the lightest solid element.`, }, ]; \/** * Control column ordering and which columns are displayed. *\/ ``` Demo code is on StackBlitz . I have commented out the getDisplayedColumns in the html so the rest of the table is still functioning properly. When I uncomment them the columns on the table disappear",
    "author_id":4450,
    "publication_date":1754347639000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"developer8492",
    "author_reputation":97.0,
    "tags":"typescript, angular, angular-material",
    "text_length":7039,
    "title_length":50,
    "num_tags":3
  },
  {
    "id":5957,
    "title":"Why do I get timeout error while fetching HTTP request inside Angular App?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725376\/why-do-i-get-timeout-error-while-fetching-http-request-inside-angular-app",
    "text":"I am using trying to create a small project based on Angular 20, where I provide front-end for data and charts from Fintacharts API , using SSR (Server-side Rendering), because CORS policies didn't allow me to fetch data using a normal built. I have encountered a problem of fetching data for the templates using HttpClient . I have implemented a Fintech service , code below provided: ``` import { isPlatformServer } from '@angular\/common'; import { Inject, Injectable, makeStateKey, OnInit, PLATFORM_ID, TransferState } from '@angular\/core'; import { environment } from '..\/..\/environments\/environment.development'; import { HttpClient, HttpHeaders } from '@angular\/common\/http'; import { Observable, ReplaySubject } from 'rxjs'; import { get } from 'http'; @Injectable({ providedIn: 'root' }) export class Fintech {\/\/implements OnInit { private Token: string=\"\"; private TokenType: string=\"\"; private Providers: Array<string> = []; constructor(private http: HttpClient) { this.getToken(); } private async getToken(){ let headers: HttpHeaders; headers = new HttpHeaders(); headers = headers.set('Content-Type', 'application\/x-www-form-urlencoded'); let body = new URLSearchParams({ \"grant_type\": \"password\", 'client_id': 'app-cli', 'username': environment.USERNAME, 'password': environment.PASSWORD }).toString(); let ExpireTime: number = 0; this.http.post(`${environment.URI}\/identity\/realms\/fintatech\/protocol\/openid-connect\/token`, body, {headers: headers}).subscribe({ next: (data) => { console.log(data); let Object_data = Object(data); this.Token = Object_data[\"access_token\"]; this.TokenType = Object_data[\"token_type\"]; ExpireTime = Object_data[\"expires_in\"]; }, error: (error) => { console.log(error); } }); if (ExpireTime != 0) { setTimeout(() => this.getToken(), ExpireTime * 1000); } } public getProviders() { let headers: HttpHeaders; headers = new HttpHeaders(); headers = headers.set('Authorization', `${this.TokenType} ${this.Token}`); return this.http.get(`${environment.URI}\/api\/instruments\/v1\/providers`, { headers: headers }) } } ``` I tried to fetch data from components that utilize this Service, but it only enters endless loop of the same error upon fetch (sometimes it actually fetches data, but only at the beginning). I have figured out that component call the ``` GetProviders() ``` methods 6 times before , and 6 times after the Token is finally fetched, but I have no idea why it keeps going in case it encounters an error. Especially when I tried to place ``` return; ``` to exit the method. Aside from the timeout problem, I have no idea what causes this error, and how to handle\/fix it: ``` HttpErrorResponse { headers: _HttpHeaders { headers: Map(0) {}, normalizedNames: Map(0) {}, lazyInit: undefined, lazyUpdate: null }, status: 0, statusText: 'Unknown Error', url: 'https:\/\/platform.fintacharts.com\/api\/instruments\/v1\/providers', ok: false, type: undefined, name: 'HttpErrorResponse', message: 'Http failure response for https:\/\/platform.fintacharts.com\/api\/instruments\/v1\/providers: 0 undefined', error: TypeError: fetch failed at node:internal\/deps\/undici\/undici:15422:13 at _ZoneDelegate.invoke (eval at runInlinedModule (file:\/\/\/mnt\/d\/Programs\/VSC%20Projects\/StockServer\/node_modules\/vite\/dist\/node\/module-runner.js:988:20), :336:158) at ZoneImpl.run (eval at runInlinedModule (file:\/\/\/mnt\/d\/Programs\/VSC%20Projects\/StockServer\/node_modules\/vite\/dist\/node\/module-runner.js:988:20), :105:35) at eval (eval at runInlinedModule (file:\/\/\/mnt\/d\/Programs\/VSC%20Projects\/StockServer\/node_modules\/vite\/dist\/node\/module-runner.js:988:20), :1040:30) at _ZoneDelegate.invokeTask (eval at runInlinedModule (file:\/\/\/mnt\/d\/Programs\/VSC%20Projects\/StockServer\/node_modules\/vite\/dist\/node\/module-runner.js:988:20), :362:171) at ZoneImpl.runTask (eval at runInlinedModule (file:\/\/\/mnt\/d\/Programs\/VSC%20Projects\/StockServer\/node_modules\/vite\/dist\/node\/module-runner.js:988:20), :143:37) at drainMicroTaskQueue (eval at runInlinedModule (file:\/\/\/mnt\/d\/Programs\/VSC%20Projects\/StockServer\/node_modules\/vite\/dist\/node\/module-runner.js:988:20), :522:23) at processTicksAndRejections (node:internal\/process\/task_queues:105:5) at runNextTicks (node:internal\/process\/task_queues:69:3) at listOnTimeout (node:internal\/timers:569:9) { [cause]: AggregateError [ETIMEDOUT]: at internalConnectMultiple (node:net:1134:18) at internalConnectMultiple (node:net:1210:5) at Timeout.internalConnectMultipleTimeout (node:net:1742:5) at listOnTimeout (node:internal\/timers:610:11) at process.processTimers (node:internal\/timers:543:7) { code: 'ETIMEDOUT', [errors]: [Array] } } } ```",
    "author_id":5473,
    "publication_date":1754347722000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Denis Berezniuk",
    "author_reputation":19.0,
    "tags":"typescript, angular, angular-httpclient, rxjs",
    "text_length":4588,
    "title_length":74,
    "num_tags":4
  },
  {
    "id":5956,
    "title":"Enabling autocomplete in PyCharm Terminal",
    "link":"https:\/\/stackoverflow.com\/questions\/79725383\/enabling-autocomplete-in-pycharm-terminal",
    "text":"I am running PyCharm 2025.1.3.1, opening up a remotely-hosted project via JetBrains Gateway 2025.1.2, under macOS 15.5. When I open the Terminal within PyCharm, the Tab key does not provide autocompletion of paths or commands. Autocompletion works with a terminal session opened within Apple Terminal (2.14). I am using ``` \/bin\/zsh ``` as the shell on the remote host. What settings do I enable (or commands do I run) to get autocompletion working in PyCharm Terminal?",
    "author_id":5472,
    "publication_date":1754348043000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Alex Reynolds",
    "author_reputation":97143.0,
    "tags":"autocomplete, pycharm, terminal",
    "text_length":469,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":5955,
    "title":"Identify which cells were based on accepted AI suggestions",
    "link":"https:\/\/stackoverflow.com\/questions\/79725390\/identify-which-cells-were-based-on-accepted-ai-suggestions",
    "text":"Is it possible to identify which cells in a Google Colab were based on accepted AI suggestions? I am considering how I might inspect the JSON returned by students to determine if cell(s) were the result of AI-generated code.",
    "author_id":5471,
    "publication_date":1754348831000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Btibert3",
    "author_reputation":40386.0,
    "tags":"google-colaboratory",
    "text_length":224,
    "title_length":58,
    "num_tags":1
  },
  {
    "id":5954,
    "title":"Format-Table that ignores terminal width?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725392\/format-table-that-ignores-terminal-width",
    "text":"I just want to print out a table of values. I don't want \"smart\" truncation, or automatic choice of the \"right\" properties. I don't care if it all fits on one line. I just want to print a table to the terminal. Is there some option I can provide to Format-Table or an alternative that someone has coded that will just print the data that I asked it to? Things that don't work include: Piping to ``` Out-String -Width 6000 ``` . This seems to work sometimes, but not others, and no, it's not because my data is more than 6000 characters wide Specifying which properties to print with an argument to ``` -Property ``` . I'm not sure this makes any difference at all. Using ``` -AutoSize ``` Using ``` -Wrap ```",
    "author_id":5470,
    "publication_date":1754348872000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"wfaulk",
    "author_reputation":1784.0,
    "tags":"powershell",
    "text_length":708,
    "title_length":41,
    "num_tags":1
  },
  {
    "id":5953,
    "title":"How to count time on active tab in Safari web extension?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725397\/how-to-count-time-on-active-tab-in-safari-web-extension",
    "text":"I'm creating simple Safari web extension that tracks time on any site and calculate time per day. Unfortunately it doesn't work right, as it doesn't count time when I'm on active tab and do nothing. As I understand, it is a ``` manifest.json ``` configuration: ``` { \"permissions\": [ \"tabs\", \"activeTab\", \"storage\" ], \"background\": { \"scripts\": [\"background.js\"], \"persistent\": false } } ``` ``` \"persistent\": false ``` , and I can't set it true in Safari due to security reasons. What is the proper way to let extension work when you do nothing on web page?",
    "author_id":5469,
    "publication_date":1754349137000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sergey Bakotin",
    "author_reputation":489.0,
    "tags":"safari-extension",
    "text_length":558,
    "title_length":56,
    "num_tags":1
  },
  {
    "id":5952,
    "title":"std::format in a mex file",
    "link":"https:\/\/stackoverflow.com\/questions\/79725398\/stdformat-in-a-mex-file",
    "text":"I'm trying to use ``` std::format ``` in a mex file but get an ``` unresolved external symbol ``` error when I use ``` std ``` as a module. ``` \/\/ test_mex.cpp #include \"mex.h\" import std; void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]) { std::string temp{std::format(\"{:.2f}\", 21.234)}; mexPrintf(\"%s\\n\", temp.c_str()); plhs[0] = mxCreateDoubleMatrix(10, 10, mxREAL); } ``` compiled with ``` !cl \/std:c++latest \/O2 \/Oi \/EHsc \/MD \/nologo \/c -I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.26100.0\\ucrt\" -I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.44.35207\\include\" \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.44.35207\\modules\\std.ixx\" mex -O COMPFLAGS=\"$COMPFLAGS -std:c++latest \/O2 \/Oi \/EHsc \/Wall \/MD\" test_mex.cpp; ``` gives this error ``` Error using mex Creating library test_mex.lib and object test_mex.exp test_mex.obj : error LNK2019: unresolved external symbol \"public: static int const std::_General_precision_tables_2<double>::_Max_P\" (?_Max_P@?$_General_precision_tables_2@N@std@@2HB) referenced in function \"struct std::to_chars_result __cdecl std::_Floating_to_chars_general_precision<double>(char *,char * const,double,int)\" (??$_Floating_to_chars_general_precision@N@std@@YA?AUto_chars_result@0@PEADQEADNH@Z) test_mex.obj : error LNK2019: unresolved external symbol \"public: static int const std::_General_precision_tables_2<float>::_Max_P\" (?_Max_P@?$_General_precision_tables_2@M@std@@2HB) referenced in function \"struct std::to_chars_result __cdecl std::_Floating_to_chars_general_precision<float>(char *,char * const,float,int)\" (??$_Floating_to_chars_general_precision@M@std@@YA?AUto_chars_result@0@PEADQEADMH@Z) test_mex.mexw64 : fatal error LNK1120: 2 unresolved externals ``` If you don't use ``` std::format ``` and instead just specify the string, for example ``` std::string temp{\"test\"} ``` it compiles and runs. Furthermore, instead of using a module if you replace ``` import std; ``` with ``` #include <format> ``` again it compiles and runs. I'm using the Microsoft Visual C++ 2022 compiler.",
    "author_id":5468,
    "publication_date":1754349163000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dogAwakeCat",
    "author_reputation":384.0,
    "tags":"c++, matlab, mex",
    "text_length":2121,
    "title_length":25,
    "num_tags":3
  },
  {
    "id":5951,
    "title":"Is it possible to override APISIX default response using a custom plugin?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725401\/is-it-possible-to-override-apisix-default-response-using-a-custom-plugin",
    "text":"Apache APISIX by default gives inconsistent responses when returning errors. Here are some examples: ``` 404 Not Found ``` : ``` {\"error_msg\":\"404 Route Not Found\"} ``` ``` 502 Bad Gateway ``` : ``` <html> <head><title>502 Bad Gateway<\/title><\/head> <body> <center><h1>502 Bad Gateway<\/h1><\/center> <hr><center>openresty<\/center> <p><em>Powered by <a href=\"https:\/\/apisix.apache.org\/\">APISIX<\/a>.<\/em><\/p><\/body> <\/html> ``` As widely discussed in GitHub issues, this can be configured via a custom nginx configuration. Below are some references: help request: customize the return description and error page help request: How to have custom 404 page in apisix? help request: Some questions about error_page Is it possible to override this default behaviour using a custom plugin? How? As an example, here's APISIX in Standalone Mode, running on Docker: File structure: ``` . ├── conf\/ │ ├── apisix.yaml │ └── config.yaml └── compose.yaml ``` ``` compose.yaml ``` : ``` name: apisix-standalone services: apisix: image: apache\/apisix:3.13.0-ubuntu stdin_open: true tty: true volumes: - .\/conf\/config.yaml:\/usr\/local\/apisix\/conf\/config.yaml - .\/conf\/apisix.yaml:\/usr\/local\/apisix\/conf\/apisix.yaml ports: - \"9080:9080\/tcp\" - \"9443:9443\/tcp\" networks: apisix: httpbin: image: kennethreitz\/httpbin:latest ports: - \"3000:80\/tcp\" networks: apisix: networks: apisix: driver: bridge ``` ``` config.yaml ``` : ``` deployment: role: data_plane role_data_plane: config_provider: yaml ``` ``` apisix.yaml ``` : ``` upstreams: - id: httpbin_internal nodes: \"httpbin:80\": 1 type: roundrobin routes: - id: base_internal uri: \/anything upstream_id: httpbin_internal - id: apisix_status uri: \/apisix_status\/* upstream_id: httpbin_internal plugins: serverless-post-function: phase: access functions: - | return function(conf, ctx) local core = require(\"apisix.core\") local status_code = 200 local matched = ngx.re.match(ngx.var.uri, \"^\/apisix_status\/([2-5][0-9]{2})$\") if matched then status_code = tonumber(matched[1]) end core.response.exit(status_code) end ``` Run services: ``` docker compose up ``` Test APISIX default responses for some status codes: Status code 404: ``` $ curl localhost:9080 -i HTTP\/1.1 404 Not Found Content-Type: text\/plain; charset=utf-8 ... Server: APISIX\/3.13.0 {\"error_msg\":\"404 Route Not Found\"} ``` Status code 500: ``` $ curl localhost:9080\/apisix_status\/500 -i HTTP\/1.1 500 Internal Server Error Content-Type: text\/html; charset=utf-8 ... Server: APISIX\/3.13.0 <html> <head><title>500 Internal Server Error<\/title><\/head> <body> <center><h1>500 Internal Server Error<\/h1><\/center> <hr><center>openresty<\/center> <p><em>Powered by <a href=\"https:\/\/apisix.apache.org\/\">APISIX<\/a>.<\/em><\/p><\/body> <\/html> ``` Status code 502: ``` $ curl localhost:9080\/apisix_status\/502 -i HTTP\/1.1 502 Bad Gateway Content-Type: text\/html; charset=utf-8 ... Server: APISIX\/3.13.0 <html> <head><title>502 Bad Gateway<\/title><\/head> <body> <center><h1>502 Bad Gateway<\/h1><\/center> <hr><center>openresty<\/center> <p><em>Powered by <a href=\"https:\/\/apisix.apache.org\/\">APISIX<\/a>.<\/em><\/p><\/body> <\/html> ``` As we can notice, the responses are not normalized: we get a JSON for ``` 404 ``` , with ``` Content-Type: text\/plain ``` we get a HTML for ``` 500 ``` , ``` 502 ``` (and many other status codes) with ``` Content-Type: text\/html ```",
    "author_id":5467,
    "publication_date":1754349336000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"mikyll98",
    "author_reputation":2479.0,
    "tags":"nginx, lua, openresty, apache-apisix",
    "text_length":3335,
    "title_length":73,
    "num_tags":4
  },
  {
    "id":5950,
    "title":"VSCode WSL Laravel: configure Ignition or Ray to open files directly in VSC debugger",
    "link":"https:\/\/stackoverflow.com\/questions\/79725403\/vscode-wsl-laravel-configure-ignition-or-ray-to-open-files-directly-in-vsc-debu",
    "text":"I am using VSCode on windows with WSL Linux where the project files are and PHP and the XDebug debugger are running. When Ray or Ignition display debug information with a filename, I would like the links of Ray and Ignition to open the file in VSCode with the file opened in WSL ``` \/home\/... ``` (not in the windows filesystem ``` \\\\wsl$\\Ubuntu\\home ``` ). This is needed to be able to set XDebug breakpoints. Searching internet and asking AIs do not help me. I tried a lot of ``` .env ``` sttings. ``` IGNITION_EDITOR=vscode IGNITION_REMOTE_SITES_PATH=\/home\/paul\/www\/ecoob_laravel_worktree IGNITION_LOCAL_SITES_PATH=\"\\\\\\\\\\\\\\\\wsl$\\\\Ubuntu\\\\home\\\\paul\\\\www\\\\ecoob_laravel_worktree\" ``` or ``` IGNITION_EDITOR=vscode IGNITION_REMOTE_SITES_PATH=\/home\/paul\/www\/ecoob_laravel_worktree IGNITION_LOCAL_SITES_PATH=\\\\\\\\wsl$\\\\Ubuntu\/home\/paul\/www\/ecoob_laravel_worktree ``` or ``` IGNITION_EDITOR=vscode IGNITION_REMOTE_SITES_PATH=\/home\/paul\/www\/ecoob_laravel_worktree IGNITION_LOCAL_SITES_PATH='\\\\\\\\wsl$\\\\Ubuntu\\\\home\\\\paul\\\\www\\\\ecoob_laravel_worktree' ``` or ``` IGNITION_EDITOR=vscode IGNITION_REMOTE_SITES_PATH=\/home\/paul\/www\/ecoob_laravel_worktree IGNITION_LOCAL_SITES_PATH=vscode:\/\/vscode-remote\/wsl+Ubuntu\/home\/paul\/www\/ecoob_laravel_worktree ``` In the best case, a new instance of VSCode is openen with a windows file. But I want my current VSCode instance to show the file in WSL. Same with Spatie Ray: ``` RAY_REMOTE_PATH=\/home\/paul\/www\/ecoob_laravel_worktree RAY_LOCAL_PATH=\"\\\\\\\\wsl$\\\\Ubuntu\\\\home\\\\paul\\\\www\\\\ecoob_laravel_worktree\" ``` opens a new VSCode instance, with a windows path, not WSL path I cannot run the PHP debugger in this case, because php is executed in the WSL environment.",
    "author_id":5466,
    "publication_date":1754349486000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"PaulH",
    "author_reputation":3099.0,
    "tags":"php, visual-studio-code, laravel, windows-subsystem-for-linux",
    "text_length":1696,
    "title_length":84,
    "num_tags":4
  },
  {
    "id":5949,
    "title":"How to install PostgreSQL 16 client on RedHat UBI 9 (arm64\/amd64)?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725414\/how-to-install-postgresql-16-client-on-redhat-ubi-9-arm64-amd64",
    "text":"I'm trying to install the PostgreSQL 16 client on RedHat UBI 9 with Python 3.9. I've followed several tutorials and blog posts, but I’m still facing issues. I'm currently testing on a Mac M1 (arm64), but the code will be deployed in production on an amd64 architecture. Here are the steps I'm using: ``` dnf install -y https:\/\/download.postgresql.org\/pub\/repos\/yum\/reporpms\/EL-9-x86_64\/pgdg-redhat-repo-latest.noarch.rpm dnf -qy module disable postgresql dnf install -y postgresql16 ``` However, I'm not sure if these are the correct instructions, especially considering the architecture differences. During execution, I encounter this error when disabling the default PostgreSQL module: ``` (app-root) dnf -qy module disable postgresql Importing GPG key 0x08B40D20: Userid : \"PostgreSQL RPM Repository <pgsql-pkg-yum@lists.postgresql.org>\" Fingerprint: D4BF 08AE 67A0 B4C7 A1DB CCD2 40BC A2B4 08B4 0D20 From : \/etc\/pki\/rpm-gpg\/PGDG-RPM-GPG-KEY-RHEL Importing GPG key 0x08B40D20: Userid : \"PostgreSQL RPM Repository <pgsql-pkg-yum@lists.postgresql.org>\" Fingerprint: D4BF 08AE 67A0 B4C7 A1DB CCD2 40BC A2B4 08B4 0D20 From : \/etc\/pki\/rpm-gpg\/PGDG-RPM-GPG-KEY-RHEL Error: Failed to download metadata for repo 'pgdg16': repomd.xml GPG signature verification error: gpgme_op_verify() error: No data ``` It seems to be a GPG verification issue related to the PostgreSQL repository metadata. Has anyone successfully installed the PostgreSQL 16 client on UBI 9 (either on arm64 or amd64)? Any idea how to fix or work around this GPG verification error without disabling GPG checks entirely? PS I tried also the instruction: ``` dnf install -y --nogpgcheck postgresql16 ``` and it works, however, my UBI image should go in a production environment and I want to avoid to disable the check.",
    "author_id":5465,
    "publication_date":1754350659000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Salvatore D&#39;angelo",
    "author_reputation":1167.0,
    "tags":"postgresql, redhat, ubi",
    "text_length":1781,
    "title_length":66,
    "num_tags":3
  },
  {
    "id":5948,
    "title":"Trouble with NeoVim theme looking weird",
    "link":"https:\/\/stackoverflow.com\/questions\/79725415\/trouble-with-neovim-theme-looking-weird",
    "text":"I am a longtime vim user trying to learn how to use neovim. I have been trying to do so with Neovim Chad. Right now, the color scheme looks super wacky, and I can't figure out how to fix it. I've tried 10 different schemes so far and all of them look similarly weird and unpleasant. A photo is attached. If anyone recognizes this, please let me know how I can get the themes to show up correctly. Terminal with broken theme",
    "author_id":5464,
    "publication_date":1754350798000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"saltine",
    "author_reputation":1.0,
    "tags":"neovim, nvchad",
    "text_length":423,
    "title_length":39,
    "num_tags":2
  },
  {
    "id":5947,
    "title":"Get a YouTube artist image using Angular YouTube package",
    "link":"https:\/\/stackoverflow.com\/questions\/79725416\/get-a-youtube-artist-image-using-angular-youtube-package",
    "text":"Google Provides an Angular Material based component for Video Handling See documentation here from which a user can extract a video thumbnail image given a video URL However, I failed to find a YouTube Channel handling component from which I could get the thumbnail image of the artist. Is there a different YouTube package providing this service ? Other TypeScript packages doing anything similar ?",
    "author_id":5463,
    "publication_date":1754350898000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"shlomoa",
    "author_reputation":520.0,
    "tags":"angular-material, youtube-api",
    "text_length":399,
    "title_length":56,
    "num_tags":2
  },
  {
    "id":5946,
    "title":"Generic Function Returning Null",
    "link":"https:\/\/stackoverflow.com\/questions\/79725422\/generic-function-returning-null",
    "text":"I have a generic function that makes an ajax call. When I do a console.log in the success. The object prints out correctly. But when another function calls that function and tries to assign it to a variable and I do a console.log of the variable. It keeps coming up null. Can't seem to figure out why this is. But I'm new to TypeScript, so I might be missing something. Any help would be appreciated. ``` private ajax<T>(url: string, verb: string, data: string | undefined): T | null { $.ajax({ headers: { 'Accept': 'application\/json', 'Content-Type': 'application\/json', }, url: url, type: verb, dataType: 'json', data: data, success: function (response: T) { console.log(response); return response || null; }, error: function (error) { console.log(error); } }); return null; } ajaxGet<T>(url: string, data?: any | undefined): T | null { let content: string = (data !== undefined) ? JSON.stringify(data) : JSON.stringify(''); let value: T | null = this.ajax<T>(url, 'GET', content); console.log(value); return value; } ``` The console.log(response); line prints out the object in the Chrome debugger, but the console.log(value) line comes up null.",
    "author_id":5462,
    "publication_date":1754351636000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user2780362",
    "author_reputation":77.0,
    "tags":"typescript, angular, typescript-generics",
    "text_length":1148,
    "title_length":31,
    "num_tags":3
  },
  {
    "id":5945,
    "title":"Shell script to execute gradle build and move to .m2",
    "link":"https:\/\/stackoverflow.com\/questions\/79725427\/shell-script-to-execute-gradle-build-and-move-to-m2",
    "text":"I want to create a shell script to do a gradle build and then deploy to my local .m2. However, my attempt fails. My directory structure is as follows: ``` my-app # root of project + build +libs - my-jar.1.0.0 ``` I am attempting to write a shell script to build the gradle jar and second to deploy it to my local .m2 so I can pull it into other projects locally Here is my attempt which fails: ``` #!\/bin\/bash .\/gradlew jar cd .\/ GROUP_ID=\"com.tech\" ARTIFACT_ID=\"my-app\" VERSION=\"1.0.0-SNAPSHOT\" PACKAGING=\"jar\" JAR_FILE=\"lib\/libs\/my-app.1.0.0-SNAPSHOT.jar\" mvn install:install-file \\ -Dfile=\"$JAR_FILE\" \\ -DgroupId=\"$GROUP_ID\" \\ -DartifactId=\"$ARTIFACT_ID\" \\ -Dversion=\"$VERSION\" \\ -Dpackaging=\"$PACKAGING\" if [ $? -eq 0 ]; then echo \"Successfully installed $ARTIFACT_ID-$VERSION.jar to local repo\" else echo \"Failed to install $ARTIFACT_ID-$VERSION.jar.\" fi ``` I get an error that the file cannot be found because the script is searching from my home directory using the absolute path ``` [ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.4:install-file (default-cli) on project standalone-pom: The specified file '\/users\/d-dane\/projects\/my-app\/lib\/libs\/my-app-1.0.0-SNAPSHOT.jar' not exists -> [Help 1] ```",
    "author_id":5461,
    "publication_date":1754352164000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"BreenDeen",
    "author_reputation":728.0,
    "tags":"linux, shell, sh",
    "text_length":1239,
    "title_length":52,
    "num_tags":3
  },
  {
    "id":5944,
    "title":"Jenkins and JSR-352 (Java batch) combined?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725436\/jenkins-and-jsr-352-java-batch-combined",
    "text":"I have a JSR-352 application. It performs Batch Processing and thus is a command line application writing logs to stdout. This goes nicely with Jenkins that provides a perfect UI to configure\/run the jobs, analyze the log output etc. But I'd like to display the Batch Job's steps similar to Jenkins pipeline phases. Is there some way to accomplish this?",
    "author_id":5460,
    "publication_date":1754353181000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"queeg",
    "author_reputation":9712.0,
    "tags":"jenkins, jsr352",
    "text_length":353,
    "title_length":42,
    "num_tags":2
  },
  {
    "id":5943,
    "title":"Files unable to be opened from the taskbar search box in Windows 11",
    "link":"https:\/\/stackoverflow.com\/questions\/79725437\/files-unable-to-be-opened-from-the-taskbar-search-box-in-windows-11",
    "text":"On a laptop that runs on Windows 11 Pro (version 10.0.26100 Build 26100), I have lately been unable to open files from the taskbar search box. When I type in a file name, that file is found. When the name is hovered over, the cursor becomes a hand, and the file path is shown. However, when I press Enter or double-click the file to open it, nothing happens. The box to the right of the menu also stays blank. Nothing of the sort is happening with apps or web search results--these can be launched from the search box as always. I've tried rebuilding the search index, running the system file checker and the DISM tool, and resetting Windows search, as well as restoring the system to an earlier point. One of those steps--unfortunately I don't know which one--apparently fixed this, but not for long. The next day, the problem reappeared, and none of the above solutions worked. What could be going wrong here? Thanks everyone for any advice on this.",
    "author_id":5459,
    "publication_date":1754353315000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mhoram",
    "author_reputation":431.0,
    "tags":"windows-11, taskbar, windows-search",
    "text_length":951,
    "title_length":67,
    "num_tags":3
  },
  {
    "id":5942,
    "title":"Python packages not autocompleting in Pycharm for one project, but works fine for another",
    "link":"https:\/\/stackoverflow.com\/questions\/79725439\/python-packages-not-autocompleting-in-pycharm-for-one-project-but-works-fine-fo",
    "text":"I'm using Pycharm 2025.1.3.1. I created a new project that uses the random package and the turtle package. In one project, turtle classes and methods autocomplete, and in the other, they don't. I've checked the interpreter settings for both and neither has a turtle interpreter. From what I've read, since turtle is included in python (3.13), it should autocomplete without having an interpreter. The same is true for the random package. However basics like ``` print ``` do autocomplete. Project A: Project B: I am new to python and learning through Udemy.",
    "author_id":5458,
    "publication_date":1754353566000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"jzadra",
    "author_reputation":4354.0,
    "tags":"python, pycharm",
    "text_length":557,
    "title_length":89,
    "num_tags":2
  },
  {
    "id":5941,
    "title":"Convert (many) integer-valued rows into binary indicator columns using Pandas",
    "link":"https:\/\/stackoverflow.com\/questions\/79725440\/convert-many-integer-valued-rows-into-binary-indicator-columns-using-pandas",
    "text":"This seems a little to me like one-hot encoding, but notably different. What I want to do is take a row of integers from a Pandas DataFrame and produce a binary column with 1's at the index locations specified by the integers and 0's everywhere else. If possible, I would like to do this for many rows at the same time. So a trivial example would be given as taking index A B C 0 1 4 7 1 2 5 8 2 3 6 9 and producing index .0 .1 .2 0 0 0 0 1 1 0 0 2 0 1 0 3 0 0 1 4 1 0 0 5 0 1 0 6 0 0 1 7 1 0 0 8 0 1 0 9 0 0 1 I have tried ``` new_df = pandas.DataFrame(range(old_df.max(axis=None)+1)).isin(list(old_df.iloc[0])) ``` which works for a single row (the first row of the old_df in this case), but doesn't seem easily scalable to an arbitrary number of rows. Is there a built in function that does something similar to this?",
    "author_id":5457,
    "publication_date":1754353917000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"lanerogers",
    "author_reputation":165.0,
    "tags":"python, dataframe, pandas",
    "text_length":820,
    "title_length":77,
    "num_tags":3
  },
  {
    "id":5940,
    "title":"Trying to keep only the duplicate rows of data in Excel based on a single columns values",
    "link":"https:\/\/stackoverflow.com\/questions\/79725441\/trying-to-keep-only-the-duplicate-rows-of-data-in-excel-based-on-a-single-column",
    "text":"I've got 4 columns of data where some of the rows were accidentally duplicated, but the only duplicated value was column A. I need to basically find an identify all the duplicates, which Excel can do by default, but instead of removing the duplicates I want to remove everything but the duplicates. I'll then take that list and remove them from my SQL database. Here is a small example of what the data looks like: This is what I would expect to see after doing whatever is needed to get just a duplicate row showing.",
    "author_id":5456,
    "publication_date":1754354039000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tyler Cohen",
    "author_reputation":87.0,
    "tags":"excel, excel-formula, duplicates",
    "text_length":517,
    "title_length":88,
    "num_tags":3
  },
  {
    "id":5939,
    "title":"Why am I getting this RecursionError?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725442\/why-am-i-getting-this-recursionerror",
    "text":"I'm working on making a program to read a chart of accounts and turn it into a tree of ``` Account ``` objects, which I will be doing stuff with later. The chart of accounts in question has several levels of sub-accounts, with the name of each account being indented three spaces more than its parent account. For example: ``` Current Assets Cash 1000000.00 - Cash 1000010.00 - Bank Account #1 ... Other Current Assets Receivables 100800.00 - Accounts Receivable ... ``` I've created a class to represent an account, as follows: ``` class Account: parent = None num: int | None name: str # Name after stripping whitespace and splitting off account number raw: str # Name prior to ^ ... # Assorted irrelevant class variables acct_type: str | None detail_type: str | None _header: bool = False # Flags whether an account's a header account descendants: list = list() # Holds account's children. Should be empty if _header == False. def __init__(self, instr: str, parent=None, acct_type: str = None, detail_type: str = None): ''' Initialize an Account. - parent: Account. `None` if top-level. - instr: raw string from ACS CoA. - acct_type: QBO Account Type - detail_type: QBO Detail Type ''' self.raw = instr self.parent = parent self.acct_type = acct_type self.detail_type = detail_type stripped = instr.strip() ... # Assorted initialization code, incl. manipulating stripped # to get values for self.num and self.name ... # Assorted irrelevant methods def is_header(self) -> bool: return self._header def depth(self) -> int: return (len(self.raw) - len(self.raw.strip())) \/ 3 # self.raw def add_descendant(self, instr, acct_t: str = None, detail: str = None): '''Add descendant to account. If `acct_type` and `detail_type` are supplied, will override parent account type. ''' if len(self.descendants) > 0: if calc_depth(instr) > self.descendants[-1].depth(): self.descendants[-1].add_descendant(instr, acct_t = acct_t, detail = detail) else: self.descendants.append(Account( instr, parent=self, acct_type = self.acct_type if acct_t is None else acct_t, detail_type = self.detail_type if detail is None else detail )) else: self.descendants.append(Account( instr, parent=self, acct_type = self.acct_type if acct_t is None else acct_t, detail_type = self.detail_type if detail is None else detail )) return if __name__ == \"__main__\": accounts: list[Account] = list() for (acs_name, account_type, detail_type) in get_data(): # Gets data as list[tuple[str, str, str]] d = len(acs_name) - len(acs_name.lstrip())) \/ 3 # Depth of current row's account if d == 0: # Overall header accounts.append(Account(acs_name, parent=None, acct_type=account_type, detail_type=detail_type)) else: if account_type is not None: # Account metadata supplied in CoA spreadsheet accounts[-1].add_descendant( acs_name, acct_t = account_type, detail = detail_type) else: # Impute account metadata from parent accounts[-1].add_descendant(acs_name) ``` However, when I run this, I get a rather nasty RecursionError: ``` Exception has occurred: RecursionError maximum recursion depth exceeded KeyError: 'C:\\\\Users\\\\...\\\\Account.py' During handling of the above exception, another exception occurred: KeyError: 'C:\\\\Users\\\\...\\\\Account.py' During handling of the above exception, another exception occurred: KeyError: 'C:\\\\Users\\\\...\\\\Account.py' During handling of the above exception, another exception occurred: KeyError: 'C:\\\\Users\\\\...\\\\Account.py' During handling of the above exception, another exception occurred: File \"C:\\Users\\...\\Account.py\", line 104, in add_descendant self.descendants[-1].add_descendant(instr, acct_t = acct_t, detail = detail) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\...\\Account.py\", line 104, in add_descendant self.descendants[-1].add_descendant(instr, acct_t = acct_t, detail = detail) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\...\\Account.py\", line 104, in add_descendant self.descendants[-1].add_descendant(instr, acct_t = acct_t, detail = detail) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [Previous line repeated 980 more times] File \"C:\\Users\\...\\Account.py\", line 155, in <module> acs_name, ^^^^^^^^^ ...<3 lines>... detail=detail_type RecursionError: maximum recursion depth exceeded ``` I've spent the last 1.5 hours trying to fix this, but to no avail. Looking in my debugger, it appears that while the first two levels get added properly, that second level makes itself its own descendant ad infinitum . For example, in my most recent debugging run the tree looks like this: ``` <__main__.Account object at 0x0000015631511BE0> # Current Assets <__main__.Account object at 0x0000015631796710> # Cash <__main__.Account object at 0x0000015631796710> # Cash <__main__.Account object at 0x0000015631796710> # Yet more Cash ... # 980-ish more Cash ``` For reference, it should actually look like this: ``` <__main__.Account object at 0x0000015631511BE0> # Current Assets <__main__.Account object at 0x0000015631796710> # Cash <__main__.Account object at 0x0000015631XXXXXX> # 1000000.00 - Cash ``` What am I doing wrong? As far as I can tell, there isn't anywhere where I am adding ``` self ``` to ``` self.descendants ``` . (As a final note, this isn't my full code; irrelevant bits have been omitted or simplified.)",
    "author_id":5455,
    "publication_date":1754354041000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"In Hoc Signo",
    "author_reputation":529.0,
    "tags":"python, recursion, infinite-recursion",
    "text_length":5358,
    "title_length":37,
    "num_tags":3
  },
  {
    "id":5938,
    "title":"How to create a numpy dtype object array from a python list without copying data?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725444\/how-to-create-a-numpy-dtype-object-array-from-a-python-list-without-copying-data",
    "text":"As the numpy docs describe for the object dtype , arrays created with the object dtype are simply references to an underlying data store like a python list. The ``` tobytes() ``` method on such an object returns pointers to this data store. I was wondering if it's possible to create an ndarray object from a python list without creating a copy on creation. For example, trying to create an ndarray from a list then assigning ``` copy=False ``` to np.asarray raises an exception: ``` import numpy as np l = ['spam', 'eggs'] arr = np.asarray(l, dtype='object', copy=False) # raises ValueError ``` I don't know how numpy is storing the underlying data, but it seems like it should be very similar (if not identical) to a python list.",
    "author_id":5454,
    "publication_date":1754354187000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dryden",
    "author_reputation":118.0,
    "tags":"python, arrays, numpy",
    "text_length":731,
    "title_length":81,
    "num_tags":3
  },
  {
    "id":5937,
    "title":"View modifier that displays content as a popover",
    "link":"https:\/\/stackoverflow.com\/questions\/79725445\/view-modifier-that-displays-content-as-a-popover",
    "text":"I know this is somewhat of a strange thing to do, however because of our apps mix of SwiftUI and UIKit I have found myself writing views that on iPad are technically greedy but center a white backgrounded view and on phone should be displayed as a popover. An additional thing I tend to do is measure the size of the view to define how large the popover should be. However, even before working on the view height measuring the view modifier just doesnt work. I thought this would be easy however bizzarly if I put something like ``` Text(\"HI\") ``` in place of content I do get a popover that has the text \"HI\". If I put ``` content.popover { content } ``` I can see the content but the popover is blank. Because of this ability to display \"hi\" I at least know that you can put a popover inside of a view modifier. For whatever reason SwiftUI seems to prefer to not show the content in the popover leaving it blank... Why is this happening and how might I get around it? ``` public struct DisplayAsPopoverModifier: ViewModifier { @Binding var isPresented: Bool var onDismiss: (() -> Void)? @ViewBuilder public func body(content: Content) -> some View { Color.clear .popover(isPresented: $isPresented) { content } .onChange(of: isPresented) { _, newValue in if !newValue { onDismiss?() } } } } public extension View { func displayAsPopover( isPresented: Binding<Bool>, onDismiss: (() -> Void)? = nil ) -> some View { modifier(DisplayAsPopoverModifier( isPresented: isPresented, onDismiss: onDismiss )) } } #if DEBUG struct DisplayAsPopoverModifier_Previews: PreviewProvider { static var previews: some View { VStack(spacing: 16) { Text(\"Title\") .font(.largeTitle) Image(systemName: \"star\") .font(.largeTitle) } .displayAsPopover(isPresented: .constant(true)) } } #endif ```",
    "author_id":4396,
    "publication_date":1754354296000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"CalebK",
    "author_reputation":663.0,
    "tags":"swiftui, viewmodifier",
    "text_length":1771,
    "title_length":48,
    "num_tags":2
  },
  {
    "id":5936,
    "title":"Azure Pipelines insufficient memory in Docker after August 1 update",
    "link":"https:\/\/stackoverflow.com\/questions\/79725448\/azure-pipelines-insufficient-memory-in-docker-after-august-1-update",
    "text":"Since the August 1 2025 update in Azure DevOps pipelines, I am getting a memory exception in a quarter of my project's process in the Docker build section. I use Ubuntu 22.04, but I have tried others. After displaying these messages, the process stops: ##[warning]Free memory is lower than 5%; Currently used: 95.00% ##[warning]Free memory is lower than 5%; Currently used: 95.00% ##[warning]Free memory is lower than 5%; Currently used: 95.00% ##[warning]Free memory is lower than 5%; Currently used: 95.00% ##[warning]Free memory is lower than 5%; Currently used: 95.00% Has there been a drastic change in the characteristics of the machines? Is there anything additional that needs to be configured to make it work properly again? I have spent a long time reducing the load on the solution, and I have managed to extend the process, but I cannot get it to complete.",
    "author_id":5453,
    "publication_date":1754354567000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MANUEL SOLER PUERTO",
    "author_reputation":1.0,
    "tags":"azure, docker, memory, devops, pipeline",
    "text_length":868,
    "title_length":67,
    "num_tags":5
  },
  {
    "id":5935,
    "title":"Adding pre-made logarithmic equation to a plot",
    "link":"https:\/\/stackoverflow.com\/questions\/79725451\/adding-pre-made-logarithmic-equation-to-a-plot",
    "text":"I have a dataset in R that I've already plotted and fitted a logarithmic model to. I'd like to add an additional line on the graph comparing the logarithmic model to a different log equation that I have from another study, using my data. How can I do this? I am using base R for plotting but am open to using other options. My code for plot and adding the log model is: ``` x <- mydata$length y <- mydata$EstEggs log_model <- lm(y ~ log(x)) summary(log_model) coefficients <- coef(log_model) cat(\"Equation: y =\", coefficients[1], \"+\", coefficients[2], \"*log(x)\\n\") plot(x, y, main = \"Fecundity\", xlab = \"Length (cm)\", ylab = \"Number of Eggs\", pch = 16) curve(coefficients[1] + coefficients[2] * log(x), add = TRUE, col = \"black\", lwd = 2) ``` I would like it to add this equation to the plot as well: log(X) = -0.4254 + 3.2857log(Y) Using my data set, where X and Y are two columns of data (length and eggs, as defined above. (both logs in the equation I'm adding are base 10, I was not sure how to type that in the code section)",
    "author_id":5452,
    "publication_date":1754354923000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"fishing4wall3y3",
    "author_reputation":53.0,
    "tags":"r, plot, logarithm",
    "text_length":1029,
    "title_length":46,
    "num_tags":3
  },
  {
    "id":5934,
    "title":"How to connect an external PostgreSQL database into my NextJs app to query data",
    "link":"https:\/\/stackoverflow.com\/questions\/79725454\/how-to-connect-an-external-postgresql-database-into-my-nextjs-app-to-query-data",
    "text":"I am building a software with NextJS Supabase, Prisma --- using typescript. My goal is simple, connect one or multiple EXTERNAL PostgreSQL databases to my software's UI. Once connected, I can write queries from the database as if I am querying the data in the database, but I am doing it in my own software. For example, in VSCode, the PostgreSQL extension already does this. I enter my own Username, Password, Host, Port into the extension and it connects and I have access to an external PostgreSQL Database within the extension. I want the same functionality for my software, but I am unsure how to make it work --- since it just brings lots of error when I attempt to connect. What is easiest\/fastest way to connect and write the code to properly connect external PostgreSQL databases?",
    "author_id":5133,
    "publication_date":1754355186000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"The Bar",
    "author_reputation":15.0,
    "tags":"next.js, postgresql, typescript, supabase, software-design",
    "text_length":789,
    "title_length":79,
    "num_tags":5
  },
  {
    "id":5933,
    "title":"How to email an opened Excel file as attachment using VBA?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725456\/how-to-email-an-opened-excel-file-as-attachment-using-vba",
    "text":"Is there any way to e-mail an opened Excel file as attachment using VBA? I use the following macro, but a debugger shows up when I run it, because obviously the file is open - so it won't allow it to be sent as attachment. ``` Sub simpleEmail() Dim OutApp As Object, MailItem As Object Set OutApp = CreateObject(\"Outlook.Application\") Set MailItem = OutApp.CreateItem(0) 'Create Email MailItem.To = \"email.com\" MailItem.Subject = \"subject\" MailItem.Body = \"body\" MailItem.attachments.Add \"file path\" MailItem.send End Sub ``` Is there any way around this?",
    "author_id":5451,
    "publication_date":1754355387000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"kxmoosekx",
    "author_reputation":21.0,
    "tags":"automation, vba, excel, outlook",
    "text_length":555,
    "title_length":58,
    "num_tags":4
  },
  {
    "id":5932,
    "title":"Vibrancy effect flicking during withAnimation text update",
    "link":"https:\/\/stackoverflow.com\/questions\/79725459\/vibrancy-effect-flicking-during-withanimation-text-update",
    "text":"I'm working on a macOS app using SwiftUI. I have a Text view with a vibrancy effect to blend nicely with the system background When the text changes, I animate the update using withAnimation, like this: ``` var body: some View { ZStack { \/\/ Always-present vibrancy background RoundedRectangle(cornerRadius: 12) .fill(.ultraThinMaterial) .blur(radius: 0) \/\/ Keep this fixed to avoid re-rendering VStack(alignment: .leading, spacing: 1) { MarqueeText(text: songName, font: .headline, speed: 30, width: PlayerSizes.shared.originalSize.width * 0.7, height: 15, delay: 3) .lineLimit(1) Text(artistName) .foregroundStyle(.tertiary) .font(.subheadline) .lineLimit(1) } } .blur(radius: blurRadius) .scaleEffect(scaleRadius) .onChange(of: playerViewModel.currentModel.pendingSongName) { oldValue, newValue in print(\"The new value \\(newValue)\") withAnimation(.easeOut(duration: 0.3)) { blurRadius = 5.0 scaleRadius = 0.92 } DispatchQueue.main.asyncAfter(deadline: .now() + 0.15) { songName = pendingName withAnimation(.easeOut(duration: 0.25)) { blurRadius = 0.0 scaleRadius = 1.0 } resetSongChanged() } } } ``` However, during the animation, the vibrancy effect disappears from the Text view and only returns after the animation completes. I tried using .background(.clear) instead, but that removes the vibrancy altogether. Is there a way to keep the vibrancy effect visible while animating the text update?",
    "author_id":5450,
    "publication_date":1754355908000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Avi Rok",
    "author_reputation":568.0,
    "tags":"swift, swiftui, macos",
    "text_length":1399,
    "title_length":57,
    "num_tags":3
  },
  {
    "id":5931,
    "title":"ListView in selected cell background color",
    "link":"https:\/\/stackoverflow.com\/questions\/79725463\/listview-in-selected-cell-background-color",
    "text":"In my .NET 9 MAUI application, I have a few ``` ListView ``` and when I select a cell in Android, the background of this cell is ``` White ``` . I defined the ``` ListView ``` like ``` <ListView x:Name=\"listDictionaries\" CachingStrategy=\"RecycleElement\" Grid.Row=\"1\" HasUnevenRows=\"True\" HorizontalOptions=\"FillAndExpand\" IsGroupingEnabled=\"True\" ItemSelected=\"listDictionaries_ItemSelected\" ItemsSource=\"{Binding DictionaryGroup}\" RefreshCommand=\"{Binding RefreshCommand}\" SelectionMode=\"Single\" VerticalOptions=\"FillAndExpand\"> <ListView.ItemTemplate> <DataTemplate x:DataType=\"dt:Dictionary\"> <ViewCell> <ListView.ItemTemplate> <DataTemplate x:DataType=\"dt:Dictionary\"> <ViewCell> <\/ViewCell> <\/DataTemplate> <\/ListView.ItemTemplate> <\/ViewCell> <\/DataTemplate> <\/ListView.ItemTemplate> <\/ListView> ``` In the ``` Colors.xml ``` I have those colors ``` <Color x:Key=\"Primary\">#173880<\/Color> <Color x:Key=\"Secondary\">#DFD8F7<\/Color> <Color x:Key=\"PrimaryBackground\">#E5E8F9<\/Color> <Color x:Key=\"SecondaryBackground\">#E5E8F9<\/Color> <Color x:Key=\"Tertiary\">#0f2555<\/Color> <Color x:Key=\"White\">White<\/Color> ``` For ``` Android ``` , the ``` colors.xml ``` is like this one ``` <?xml version=\"1.0\" encoding=\"utf-8\"?> <resources> <color name=\"colorPrimary\">#173880<\/color> <color name=\"colorPrimaryDark\">#0f2555<\/color> <color name=\"colorAccent\">#4273dd<\/color> <\/resources> ``` How can I define the color of the background using ``` AppThemeBinding ``` for each platform?",
    "author_id":4871,
    "publication_date":1754356297000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Enrico",
    "author_reputation":6592.0,
    "tags":"c#, android, maui",
    "text_length":1474,
    "title_length":42,
    "num_tags":3
  },
  {
    "id":5930,
    "title":"Weasyprint Turn Off all the Warnings",
    "link":"https:\/\/stackoverflow.com\/questions\/79725464\/weasyprint-turn-off-all-the-warnings",
    "text":"Using WeasyPrint but it is always overwhelming me with INFO, DEBUB messages. How do I turn them all off? Nothing i have tried works. (Using Django) Thanks",
    "author_id":5449,
    "publication_date":1754356346000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"diogenes",
    "author_reputation":2181.0,
    "tags":"django, weasyprint",
    "text_length":154,
    "title_length":36,
    "num_tags":2
  },
  {
    "id":5929,
    "title":"IconImageSource and ToolbarItem not working as expected",
    "link":"https:\/\/stackoverflow.com\/questions\/79725472\/iconimagesource-and-toolbaritem-not-working-as-expected",
    "text":"In my .NET 9 MAUI application, I added some ``` Toolbar ``` in a few pages. There is a strange behaviour. If I add the ``` IconImageSource ``` using ``` FontAwesome ``` for the icons, the result is what I expect for each platform. ``` <ContentPage.ToolbarItems> <ToolbarItem x:Name=\"AddNewDictionary\" Clicked=\"AddNewDictionary_Clicked\" Order=\"Primary\" Priority=\"0\"> <ToolbarItem.IconImageSource> <FontImageSource FontFamily=\"FAS\" Glyph=\"{x:Static hlp:FA7Solid.Plus}\" Size=\"35\" Color=\"{AppThemeBinding Light={StaticResource Black}, Dark={StaticResource White}}\" \/> <\/ToolbarItem.IconImageSource> <\/ToolbarItem> <\/ContentPage.ToolbarItems> ``` The only issue is with ``` Windows ``` because the icon is not very on focus. So, I tried to replace the ``` FontImageSource ``` with a ``` IconImageSource=\"add_book.png\" ``` , in iOS the image is place in the middle. If I use something like that ``` IconImageSource=\"{AppThemeBinding Light='add_book.png', Dark='add_white_book.png'} ``` it is not working at all.vHow can I fix it?",
    "author_id":4871,
    "publication_date":1754357572000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Enrico",
    "author_reputation":6592.0,
    "tags":"c#, ios, maui",
    "text_length":1023,
    "title_length":55,
    "num_tags":3
  },
  {
    "id":5928,
    "title":"Android Version Catalog and Gradle File Syntax",
    "link":"https:\/\/stackoverflow.com\/questions\/79725473\/android-version-catalog-and-gradle-file-syntax",
    "text":"I am in need of guidance on implementing a version catalog in an Android app. In this documentation there is an example of a version catalog implementation: ``` [versions] ktx = \"1.9.0\" [libraries] androidx-ktx = { group = \"androidx.core\", name = \"core-ktx\", version.ref = \"ktx\" } ``` It states that the dependency is defined as this in the gradle file: ``` implementation(libs.androidx.ktx) ``` I don't understand why that is because I don't see the exact match \"libs.androidx.ktx\" in the version catalog ( ``` libs.versions.toml ``` file) - the only thing close to that is \"androidx-ktx\". Would someone be able to explain the syntax of the version catalog and gradle build file as well as provide a version catalog example for this dependency as well? ``` implementation(libs.androidx.lifecycle.runtime.ktx) ```",
    "author_id":5448,
    "publication_date":1754357660000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"mfusco",
    "author_reputation":109.0,
    "tags":"android, kotlin",
    "text_length":813,
    "title_length":46,
    "num_tags":2
  },
  {
    "id":5927,
    "title":"MAUI ToolbarItem settings text color",
    "link":"https:\/\/stackoverflow.com\/questions\/79725474\/maui-toolbaritem-settings-text-color",
    "text":"In my .NET 9 MAUI application, I have in a few pages a ``` ToolbarItems ``` defined as ``` <ContentPage.ToolbarItems> <ToolbarItem x:Name=\"AddNewDictionary\" Clicked=\"AddNewDictionary_Clicked\" Order=\"Primary\" Priority=\"0\"> <ToolbarItem.Text> <OnIdiom x:TypeArguments=\"x:String\" Desktop=\"{x:Static lang:AppResources.Add}\" Phone=\"\" Tablet=\"{x:Static lang:AppResources.Add}\" \/> <\/ToolbarItem.Text> <ToolbarItem.IconImageSource> <FontImageSource FontFamily=\"FontAwesomeIcons\" Glyph=\"{x:Static hlp:FA7Solid.Plus}\" Size=\"30\" Color=\"{AppThemeBinding Light={StaticResource Black}, Dark={StaticResource White}}\" \/> <\/ToolbarItem.IconImageSource> <\/ToolbarItem> <\/ContentPage.ToolbarItems> ``` The result is like in the following screenshot How can I change the color of the ``` ToolbarItem ``` and the navigation?",
    "author_id":4871,
    "publication_date":1754358171000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Enrico",
    "author_reputation":6592.0,
    "tags":"c#, windows, .net-9.0, maui",
    "text_length":803,
    "title_length":36,
    "num_tags":4
  },
  {
    "id":5926,
    "title":"How to use page width in a template?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725479\/how-to-use-page-width-in-a-template",
    "text":"I am using Hugo Universal theme, but saw that code in many other themes, too. To lay out a \"table\" of features, they use ``` [params.features] cols = 2 # Default: 3, Available values 2,3,4,6 ``` That kinda works when resizing the page, but I would prefer to change number of columns based on the page width. I was thinking about creating a CSS variable and set it from multiple ``` @media (max-width: 991px) { --col: 3 } @media (max-width: 600px) { --col: 2 } ``` But I can't find a way to use that var in a template. Is it possible? Maybe with some JS code? Or another way to use 2 columns on small, 3 on medium and 4 on large?",
    "author_id":5447,
    "publication_date":1754358701000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vlad Feinstein",
    "author_reputation":11303.0,
    "tags":"hugo",
    "text_length":628,
    "title_length":36,
    "num_tags":1
  },
  {
    "id":5925,
    "title":"Redis Cloud and AWS EC2 in the same region — will I face latency or AWS data transfer fees?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725482\/redis-cloud-and-aws-ec2-in-the-same-region-will-i-face-latency-or-aws-data-tra",
    "text":"I'm using Redis Cloud (by Redis Inc.) as my managed Redis instance, and my server is hosted on AWS EC2. Both the Redis Cloud instance and the EC2 server are in the same AWS region (e.g., us-east-1). I'm accessing Redis over rediss:\/\/ using TLS. ❓Questions: Since both Redis and my server are in the same AWS region, will there still be noticeable latency during Redis GET and SET operations? Will I incur AWS data transfer (\"data out\") charges when sending or receiving data from Redis Cloud, even though both are in the same region?",
    "author_id":4854,
    "publication_date":1754358953000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Pawandeep Singh",
    "author_reputation":427.0,
    "tags":"redis, amazon-web-services, amazon-ec2",
    "text_length":533,
    "title_length":91,
    "num_tags":3
  },
  {
    "id":5924,
    "title":"Learning ASP.NET MVC without a way to compile code",
    "link":"https:\/\/stackoverflow.com\/questions\/79725483\/learning-asp-net-mvc-without-a-way-to-compile-code",
    "text":"So as the title says, I'm looking for ways to learn .NET without actually coding. This might be more of a Reddit question but since reddit is blocked on the network I'm currently on I will post it here. About 8 months ago I started learning .NET from a free website that teaches .NET by doing some actual projects instead of just reading or doing purpose-less projects. I kept going forward while looking for an internship at the same time, unfortunately I never found an internship at where I'm from so I decided to just keep growing up as a dev and keep applying for Jobs\/Internships. 2 months ago I found a job as an IT Service Desk, which is unrelated to programming but I need a bit of cash to keep running around, this job nature requires me to work in ABC shifts, and most of the C shifts I found out I have plenty of time on my 9Hrs shift soo I figure I can learn throughout the shift and invest in my time. Here's the problem: all coding tools (IDEs, SDKs, compilers) are blocked on the company network, and bringing my personal laptop is not allowed. So now I’m stuck in a loop where I have time but no coding environment.",
    "author_id":5446,
    "publication_date":1754359079000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"AhmadJer99",
    "author_reputation":1.0,
    "tags":"c#, ide, visual-studio, asp.net-mvc",
    "text_length":1132,
    "title_length":50,
    "num_tags":4
  },
  {
    "id":5923,
    "title":"How does Oracle convert decimal values to float?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725486\/how-does-oracle-convert-decimal-values-to-float",
    "text":"If I have a ``` float(5) ``` column, why does 7.89 get rounded to 7.9 but 12.79 gets rounded to 13, not 12.8? Binary forms are as follows for 3 examples: ``` 7.89 0111.01011001 ------ round to------\\> 7.9 0111.01001 12.79 01100.01001111 ------ round to------\\> 12.8 01100.1 1.23 1.010111 ------ round to------\\> 1.2 1.1 ``` ``` CREATE TABLE t1 ( item FLOAT(5) ); INSERT INTO t1 VALUES ( 7.89 ); INSERT INTO t1 VALUES ( 12.79 ); INSERT INTO t1 VALUES ( 1.23 ); SELECT ITEM FROM T1; ITEM ---- 7.9 13 1.2 ```",
    "author_id":5445,
    "publication_date":1754360093000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tayebeh Zarif",
    "author_reputation":31.0,
    "tags":"oracle-database, precision, floating-point",
    "text_length":505,
    "title_length":48,
    "num_tags":3
  },
  {
    "id":5922,
    "title":"Image Scaling and Pan\/Zoom Issues in Dynamically Resizable Panes",
    "link":"https:\/\/stackoverflow.com\/questions\/79725495\/image-scaling-and-pan-zoom-issues-in-dynamically-resizable-panes",
    "text":"I am developing a \"Comic Book Editor\" application where users can add photos to vertically split panes and dynamically resize these panes using a draggable divider. I am facing significant issues managing the images within these panes. When the size of a pane changes, the image inside does not maintain the ``` scaledToFill() ``` behavior as expected, leading to black spaces appearing around the image. Furthermore, after a user has panned and zoomed an image, changing the pane's size causes the image's offset and scale to become incorrect. My goal is for the image to always fill the pane dynamically, and for its position and scale to be proportionally updated whenever the pane's dimensions change. The core problems are: Dynamic Sizing: When the pane's size changes, the Image inside fails to correctly re-adjust its position and scale to maintain the ``` scaledToFill() ``` aspect, resulting in unwanted black areas. Maintaining Position and Scale: After a user manually adjusts an image's pan and zoom using ``` DragGesture ``` and ``` MagnificationGesture ``` , these values (offset and scale) become incorrect when the pane's size is altered by dragging the divider. I need these values to be proportionally updated based on the new pane dimensions. Below are the relevant code snippets for the views that manage these panes, the divider, and the images. I am struggling to find the right approach to combine ``` GeometryReader ``` , ``` scaledToFill() ``` , and gesture-based modifications within ``` PaneView ``` . Relevant Code Snippets: (The code below is a minimal representation of the issue, focused on the key parts from your project.) ``` struct PaneModel: Identifiable { let id = UUID() var background: PaneBackground = .color(Color(white: 0.2)) var imageOffset: CGSize = .zero var imageScale: CGFloat = 1.0 \/\/ ... } ``` ``` struct PaneView: View { @Binding var pane: PaneModel let isActive: Bool let onTap: () -> Void @State private var previousSize: CGSize = .zero @GestureState private var gestureOffset: CGSize = .zero @GestureState private var gestureScale: CGFloat = 1.0 var body: some View { GeometryReader { geometry in ZStack { backgroundView(geometry: geometry) \/\/ ... } .clipped() .contentShape(Rectangle()) .onTapGesture(perform: isExporting ? {} : onTap) .onChange(of: geometry.size) { newSize in \/\/ This logic attempts to adjust the offset, but it's not working correctly. if previousSize == .zero { previousSize = newSize return } let oldSize = previousSize \/\/ ... calculations to re-adjust offset ... pane.imageOffset = newOffset previousSize = newSize } } } @ViewBuilder private func backgroundView(geometry: GeometryProxy) -> some View { if case .image(let uiImage) = pane.background { Image(uiImage: uiImage) .resizable() .scaledToFill() \/\/ <-- This is where the image should fill the pane .scaleEffect(pane.imageScale * gestureScale) .offset(x: pane.imageOffset.width + gestureOffset.width, y: pane.imageOffset.height + gestureOffset.height) .gesture( \/\/ Drag and Magnification Gestures are combined here ) } else if case .color(let color) = pane.background { color } else { Color(white: 0.2) } } } ``` ``` private func canvasBackgroundLayout(geometry: GeometryProxy, isForExport: Bool) -> some View { switch viewModel.layout { case .twoVertical: if viewModel.panes.count >= 2 && !viewModel.dividerPositions.isEmpty { ZStack(alignment: .top) { VStack(spacing: 0) { PaneView(pane: $viewModel.panes[0], isActive: viewModel.panes[0].id == viewModel.activePaneID) { handleTap(on: viewModel.panes[0]) } .frame(height: geometry.size.height * viewModel.dividerPositions[0]) PaneView(pane: $viewModel.panes[1], isActive: viewModel.panes[1].id == viewModel.activePaneID) { handleTap(on: viewModel.panes[1]) } .frame(maxHeight: .infinity) } \/\/ ... The DividerView that modifies pane sizes. } } else { EmptyView() } \/\/ ... Other layouts ... } } ``` I would appreciate any guidance on how to correctly manage the image's offset and scale in PaneView so that it remains consistent and the ``` scaledToFill() ``` behavior is preserved, even when the parent pane's size changes. What is the right approach to dynamically update the image properties in response to changes in ``` GeometryReader ``` ? Thank you in advance for your help! The image and cleanliness I want are shown below. The Mematic app has done this very well, but I couldn't achieve this result.",
    "author_id":5444,
    "publication_date":1754360974000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Murat &#199;i&#231;ek",
    "author_reputation":21.0,
    "tags":"swiftui, drag-and-drop, gesture, geometryreader",
    "text_length":4389,
    "title_length":64,
    "num_tags":4
  },
  {
    "id":5921,
    "title":"error NU1102: Unable to find package Microsoft.NETCore.App.Runtime.win-x64 with version (= 8.0.19)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725502\/error-nu1102-unable-to-find-package-microsoft-netcore-app-runtime-win-x64-with",
    "text":"Using GitHub actions I'm building for Windows on an Ubuntu agent, this is a snippet from the build.yml: ``` jobs: build: runs-on: ubuntu-latest - name: \"⚙ Setup .NET\" uses: actions\/setup-dotnet@v4 with: dotnet-version: '8.0.x' ``` Today I started getting this error: error NU1102: Unable to find package Microsoft.NETCore.App.Runtime.win-x64 with version (= 8.0.19) How to fix it?",
    "author_id":5443,
    "publication_date":1754361789000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jeremy Thompson",
    "author_reputation":66140.0,
    "tags":".net, build, github-actions, nuget, continuous-integration",
    "text_length":380,
    "title_length":98,
    "num_tags":5
  },
  {
    "id":5920,
    "title":"How do I import a library to my react native project?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725505\/how-do-i-import-a-library-to-my-react-native-project",
    "text":"I have this library downloaded here . Which is just this library . I want to use it in my react-native project, however I am having a terrible time trying to get it to work. I have hosted an HTML file, that is referenced in my project, and that can be called, and displayed fine in the web. However, I think on android I get errors with security. I am trying to host the library right in my project, but just can't seem to get anything to register correctly or actually import correctly. I have tried require(), using tags and directly referencing it, using import 'types' from 'a package i tried to publish and install locally'. And nothing works. I am trying to have something akin to this, but for android ``` export default function WebWorks() { const containerRef = useRef<HTMLDivElement>(null); useEffect(() => { const container = containerRef.current; if (container && window.FamilyTree) { const family = new window.FamilyTree(container, { mouseScrool: window.FamilyTree.action.none, scaleInitial: window.FamilyTree.match.boundary, enableSearch: false, nodeBinding: { field_0: 'name' }, }); family.load([ { id: 2, pids: [3], gender: 'female', name: 'Dorothy' }, { id: 3, pids: [2], gender: 'male', name: 'George' }, ]); } }, []); return ( <View style={styles.container}> {\/* Use native HTML div directly here *\/} <div ref={containerRef} style={{ width: '100%', height: 600 }} \/> <\/View> ); } ' } ``` I understand this library isn't really meant for use in an android\/ios environment, but i'd be happy to just kind of \"force it\" with a webview, and then be able to interact sending messaging back and forth in order to change the html. Any suggestions would be greatly appreciated. Also, here is how my project is setup, ignore all the fluff, there is a ton of just testing i have all over the place, of all different attempts to get this to work. ProjectLayout",
    "author_id":5442,
    "publication_date":1754362216000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Joshua Holden",
    "author_reputation":1.0,
    "tags":"react-native, reactjs",
    "text_length":1867,
    "title_length":53,
    "num_tags":2
  },
  {
    "id":5919,
    "title":"How to save leaflet to html with same dimensions as mainPanel",
    "link":"https:\/\/stackoverflow.com\/questions\/79725507\/how-to-save-leaflet-to-html-with-same-dimensions-as-mainpanel",
    "text":"I am trying to save a map in html format from a shiny app, however, the map is saved alright but the dimensions from the map displayed on my mainPanel are different than the saved map. I would like to use ``` saveWidget() ``` instead of ``` mapshot2 ``` because for some unknown reason, the maps created with mapshot2 gives me errors once the shiny app is published. The problem that I am having is that I don't know how to pass the dimensions in ``` tags$head(tags$script ``` from the mainPanel to ``` saveWidget() ``` . See example below: ``` library(shiny) library(mapview) library(leaflet) library(htmlwidgets) #Ensure webshot is installed and PhantomJS is set up if (!webshot::is_phantomjs_installed()) { webshot::install_phantomjs() } ui <- fluidPage( tags$head(tags$script( 'var dimension = [0, 0]; $(document).on(\"shiny:connected\", function(e) { dimension[0] = document.getElementById(\"mymap\").clientWidth; dimension[1] = document.getElementById(\"mymap\").clientHeight; Shiny.onInputChange(\"dimension\", dimension); }); $(window).resize(function(e) { dimension[0] = document.getElementById(\"mymap\").clientWidth; dimension[1] = document.getElementById(\"mymap\").clientHeight; Shiny.onInputChange(\"dimension\", dimension); }); ')), sidebarLayout( sidebarPanel( downloadButton(\"print_map\", \"Download html\")), mainPanel( leafletOutput(\"mymap\")) ) ) server <- function(input, output, session) { output$mymap <- renderLeaflet({ mapview(breweries)@map }) # Create a new map based off of the user's zoom level user.created.map <- reactive({ mapview(breweries)@map %>% setView( lng = input$mymap_center$lng , lat = input$mymap_center$lat , zoom = input$mymap_zoom) }) #THE CODE BELOW WORKS DOWNLOADING THE MAP BUT THE MAP DOESN'T MATCH THE DIMENSIONS OF THE mainPanel output$print_map <- downloadHandler( filename = \"map_zoom.html\", content = function(file){ saveWidget(widget = user.created.map(), file = file) #remove_controls = c(\"zoomControl\", \"layersControl\"),vwidth #input$dimension[1], vheight = input$dimension[2]) } ) } shinyApp(ui, server) ``` mapshot2 takes the following arguments: ``` mapshot2(user.created.map(),file = file, remove_controls = c(\"zoomControl\", \"layersControl\"),vwidth = input$dimension[1], vheight = input$dimension[2]) ``` Is there a way to pass the ``` remove_control ``` along with ``` vwidth ``` and ``` vheight ``` to ``` saveWidget() ``` so the map get the same dimensions as the mainPanel?",
    "author_id":5441,
    "publication_date":1754362489000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Salvador",
    "author_reputation":1923.0,
    "tags":"r, shiny, r-leaflet, htmlwidgets, webshot",
    "text_length":2421,
    "title_length":61,
    "num_tags":5
  },
  {
    "id":5918,
    "title":"&#39;Error Code-11&#39; in Faro SDK,When load a last fls file",
    "link":"https:\/\/stackoverflow.com\/questions\/79725514\/error-code-11-in-faro-sdk-when-load-a-last-fls-file",
    "text":"``` #import \"C:\\...\\WinSxS\\...\\iQOpen.dll\" no_namespace ... CoInitialize(NULL); \/\/ FARO LS Licensing BSTR licenseCode = ... \/* FARO LS license code *\/; IiQLicensedInterfaceIfPtr liPtr(__uuidof(iQLibIf)); liPtr->License = licenseCode; IiQLibIfPtr libRef = static_cast<IiQLibIfPtr>(liPtr); libRef->load(\"c:\\\\temp\\\\demo.fls\"); double x, y, z, angle; libRef->getScanPosition(0, &x, &y, &z); libRef->getScanOrientation(0, &x, &y, &z, &angle); int numRows = libRef->getScanNumRows(0); int numCOls = libRef->getScanNumCols(0); \/\/ Access all points points by point for (int col=0; col<numCols; col++) for (int row=0; row<numRows; row++) { double x, y, z; int refl; \/\/ Access all points column per column in polar coordinates double* positions = new double[numRows*3]; int* reflections = new int[numRows]; for (int col=0; col<numCols; col++) { result = libRef->getPolarScanPoints(0, 0, col, numRows, positions, reflections); for (int row=0 ; row<numRows ; row++) { double r, phi, theta; int refl; r = positions[3*row+0]; phi = position[3*row+1]; theta = positions[3*row+2]; refl = reflections[row]; \/\/ ... } } delete[] positions; delete[] reflections; libRef = NULL; ``` libRef->load(\"c:\\temp\\demo.fls\");Return Error Code-11,No workspace ,It is easy to appear in a project that stops immediately and then in the last fls file without full chunking. How to solve it?",
    "author_id":5440,
    "publication_date":1754363725000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tang",
    "author_reputation":41.0,
    "tags":"c++",
    "text_length":1356,
    "title_length":61,
    "num_tags":1
  },
  {
    "id":5917,
    "title":"ASP.NET upgrade from .NET 4.0 to .NET 4.8",
    "link":"https:\/\/stackoverflow.com\/questions\/79725518\/asp-net-upgrade-from-net-4-0-to-net-4-8",
    "text":"I'm upgrading a legacy ASP.NET application written in VB.NET and running on .NET 4.0 to .NET 4.8. I'm not familiar with the structure. After I retarget to the project to .NET 4.8, how do I go about upgrading the dependencies? Frow what I can tell, it looks like I'll need to update the assembly versions in each ``` .aspx ``` file: ``` <%@ Register Assembly=\"System.Web.Extensions, Version=1.0.61025.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35\" Namespace=\"System.Web.UI\" TagPrefix=\"asp\" %> ``` There is also a ``` web.config ``` file, but the assembly references seem redundant: ``` <compilation debug=\"true\" strict=\"false\" explicit=\"true\" targetFramework=\"4.7.2\"> <buildProviders> <add extension=\".rdlc\" type=\"Microsoft.Reporting.RdlBuildProvider, Microsoft.ReportViewer.Common, Version=8.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a\"\/> <\/buildProviders> <assemblies> <add assembly=\"Microsoft.ReportViewer.ProcessingObjectModel, Version=8.0.0.0, Culture=neutral, PublicKeyToken=B03F5F7F11D50A3A\" \/> <add assembly=\"Microsoft.ReportViewer.Common, Version=8.0.0.0, Culture=neutral, PublicKeyToken=B03F5F7F11D50A3A\" \/> <add assembly=\"Microsoft.ReportViewer.WebForms, Version=8.0.0.0, Culture=neutral, PublicKeyToken=B03F5F7F11D50A3A\" \/> ... <\/assemblies> <\/compilation> ``` I deleted these assembly references in the web.config and it made no difference, at least to the build and start-up. Touching the assembly reference in an ``` .aspx ``` file (e.g. setting a bogus assembly version) did make a difference, and caused the build to fail. Am I missing something? Are those assembly references in ``` web.config ``` required? Do I really need to update every assembly reference in every ``` .aspx ``` file manually? Aside, if I try to re-add an assembly in the 'References' tab of the 'Property Pages' I get an error: The website is already referencing the assembly System.Web.Extensions I don't see the assembly references in ``` web.config ``` updated. It does work for an assembly that isn't imported in the ``` .aspx ``` pages (like ``` System.Web.Extensions.Design ``` ). Here's a glimpse of the solution folder.",
    "author_id":5439,
    "publication_date":1754364282000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jevon Kendon",
    "author_reputation":658.0,
    "tags":"asp.net, .net-4.8",
    "text_length":2134,
    "title_length":41,
    "num_tags":2
  },
  {
    "id":5916,
    "title":"Unable to zoom or hover map using nested subplot()",
    "link":"https:\/\/stackoverflow.com\/questions\/79725525\/unable-to-zoom-or-hover-map-using-nested-subplot",
    "text":"My interactive dashboard where multiple Plotly objects including maps are displayed using nested ``` subplot() ``` in R: ``` library(digest) library(sf) library(jsonlite) library(plotly) library(ggplot2) library(tidyr) library(dplyr) library(tibble) sa_final_dataset <- read.csv(\"state_data.csv\") final_dataset <- read.csv(\"nation_data.csv\") australia_data <- read_sf(\"australia_map.shp\") pivot_data <- read.csv(\"pivot_data.csv\") sa_map_data <- subset(australia_data, STE_NAME21 == \"South Australia\") # Interpolate and split into line segments interpolate_segments_as_lines <- function(df, steps = 50) { df %>% rowwise() %>% do({ x_vals <- seq(.$fromX, .$X, length.out = steps) y_vals <- seq(.$fromY, .$Y, length.out = steps) position <- seq(0, 1, length.out = steps) # Construct segments data.frame( x = head(x_vals, -1), y = head(y_vals, -1), xend = tail(x_vals, -1), yend = tail(y_vals, -1), position = head(position, -1), FROM_NAME = .$FROM_NAME, TO_NAME = .$TO_NAME, TOTAL = .$TOTAL, AGE_15_34 = .$AGE_15_34, AGE_35_49 = .$AGE_35_49, AGE_50_65 = .$AGE_50_65, AGE_65_PLUS = .$AGE_65_PLUS ) }) %>% ungroup() } # Apply interpolation sa_lines_segments <- interpolate_segments_as_lines(sa_final_dataset) sa_lines_segments <- sa_lines_segments %>% mutate( thickness = round(rescale(abs(TOTAL), to = c(3, 10))), color = rgb( colorRamp(c(\"#cc0b15ff\", \"#1cc00dff\"))(position), maxColorValue = 255 ) ) # Use add_segments with thickness mapped to line width sa_plotly <- plot_ly(height = 900, source = \"South_Australia_Map\") %>% add_sf( data = sa_map_data, fill = \"#007499\", line = list(color = \"black\"), showlegend = FALSE, hoverinfo = \"skip\" ) # Optionally, you can color by direction or other variable if needed for (t in sort(unique(sa_lines_segments$thickness))) { seg_data <- sa_lines_segments %>% filter(thickness == t ) for (g in unique(seg_data$color)) { seg_data_color <- seg_data %>% filter(color == g) if (nrow(seg_data_color) > 0) { sa_plotly <- sa_plotly %>% add_segments( data = seg_data_color, x = ~x, y = ~y, xend = ~xend, yend = ~yend, line = list( color = ~color, width = t ), opacity = 0.8, hovertext = paste0( \"From: \", seg_data_color$FROM_NAME, \"<br>\", \"To: \", seg_data_color$TO_NAME, \"<br>\", \"Age 15 - 34 Migrations: <b>\", seg_data_color$AGE_15_34, \"<\/b><br>\", \"Age 35 - 49 Migrations: <b>\", seg_data_color$AGE_35_49, \"<\/b><br>\", \"Age 50 - 65 Migrations: <b>\", seg_data_color$AGE_50_65, \"<\/b><br>\", \"Age 66+ Migrations: <b>\", seg_data_color$AGE_65_PLUS, \"<\/b><br>\", \"Net Migrations: <b>\", seg_data_color$TOTAL, \"<\/b>\" ), hoverinfo = \"text\", showlegend = FALSE, inherit = FALSE, yaxis=\"y\" ) } } } sa_plotly <- sa_plotly %>% layout( xaxis = list(title = \"\"), yaxis = list(title = \"\") ) interpolate_segments_as_lines_inter <- function(df, steps = 50) { df %>% rowwise() %>% do({ x_vals <- seq(.$fromX, .$X, length.out = steps) y_vals <- seq(.$fromY, .$Y, length.out = steps) position <- seq(0, 1, length.out = steps) data.frame( x = head(x_vals, -1), y = head(y_vals, -1), xend = tail(x_vals, -1), yend = tail(y_vals, -1), position = head(position, -1), group = .$group, thickness = .$thickness, SA3_NAME21 = .$SA3_NAME21, State = .$State, FinalValue = .$FinalValue, AGE_15_34 = .$AGE_15_34, AGE_35_49 = .$AGE_35_49, AGE_50_65 = .$AGE_50_65, AGE_65_PLUS = .$AGE_65_PLUS ) }) %>% ungroup() } inter_lines_segments <- interpolate_segments_as_lines_inter(final_dataset) inter_hovertexts <- paste0( \"SA3 Name: \", inter_lines_segments$SA3_NAME21, \"<br>\", \"Age 15 - 34 Migrations: <b>\", inter_lines_segments$AGE_15_34, \"<\/b><br>\", \"Age 35 - 49 Migrations: <b>\", inter_lines_segments$AGE_35_49, \"<\/b><br>\", \"Age 50 - 65 Migrations: <b>\", inter_lines_segments$AGE_50_65, \"<\/b><br>\", \"Age 66+ Migrations: <b>\", inter_lines_segments$AGE_65_PLUS, \"<\/b><br>\", \"State: <b>\", inter_lines_segments$State, \"<\/b><br>\", \"Net Value: <b>\", inter_lines_segments$FinalValue, \"<\/b>\" ) # Build plotly map for inter-state migration plotly_gg_map <- plot_ly(height = 900, source = \"Australia_Map\") %>% add_sf( data = subset(australia_data, STE_NAME21 != \"South Australia\"), fill = \"#007499\", line = list(color = \"black\"), showlegend = FALSE, hoverinfo = \"skip\" ) %>% add_sf( data = subset(australia_data, STE_NAME21 == \"South Australia\"), fill = \"#007499\", line = list(color = \"black\"), showlegend = FALSE, hoverinfo = \"skip\" ) groups <- c(\"Outgoing Migration\", \"Incoming Migration\") colors <- c(\"Outgoing Migration\" = \"#cc0b15ff\", \"Incoming Migration\" = \"#1cc00dff\") # Track if legend has been added for each group legend_added <- setNames(rep(FALSE, length(groups)), groups) for (g in groups) { for (t in sort(unique(inter_lines_segments$thickness))) { seg_data <- inter_lines_segments %>% filter(group == g, thickness == t) if (nrow(seg_data) > 0) { seg_hovertexts <- inter_hovertexts[which(inter_lines_segments$group == g & inter_lines_segments$thickness == t)] plotly_gg_map <- plotly_gg_map %>% add_segments( data = seg_data, x = ~x, y = ~y, xend = ~xend, yend = ~yend, line = list(color = colors[[g]], width = t), opacity = 0.7, hovertext = seg_hovertexts, hoverinfo = \"text\", name = g, legendgroup = g, showlegend = !legend_added[[g]] ) legend_added[[g]] <- TRUE } } } plotly_gg_map <- plotly_gg_map %>% layout( showlegend = TRUE, xaxis = list(title = \"\"), yaxis = list(overlaying=\"y2\"), legend = list(title = list(text = \"\")) ) plotly_combined <- subplot( plotly_gg_map, sa_plotly, nrows = 1 ) %>% layout( showlegend = TRUE, legend = list( orientation = \"h\", x = 0.5, y = 0.1, xanchor = \"center\", yanchor = \"top\", font = list(size = 12) ), annotations = list( list( text = \"<b>Inter State Migration Map<\/b>\", x = 0.185, y = 1.035, xref = \"paper\", yref = \"paper\", showarrow = FALSE ), list( text = \"<b>Intra State Migration Map<\/b>\", x = 0.825, y = 1.035, xref = \"paper\", yref = \"paper\", showarrow = FALSE ) ) ) # Plotly table table_plot <- plot_ly( source = \"Table 1\", type = \"table\", columnwidth = c(15, 10, 10, 10, 10, 10, 10, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20), header = list( values = colnames(pivot_data), align = \"center\", line = list(color = \"#000000ff\"), font = list(color = list( \"black\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\" ), size = 12), fill = list( color = list( c(\"#ffffffff\"), c(\"#6A625E\"), c(\"#6A625E\"), c(\"#6A625E\"), c(\"#6A625E\"), c(\"#6A625E\"), c(\"#6A625E\"), c(\"#6A625E\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\"), c(\"#333333\") ) ), height = 40 ), cells = list( values = rbind(t(as.matrix(unname(pivot_data))) ), align = \"center\", line = list(color = \"#000000ff\"), fill = list(color = \"#ffffffff\"), font = list(color = \"#000000ff\", size = 12) ) ) table_plot2 <- plot_ly( type = \"table\", source = \"Table 2\", header = list( values = list( c(\"<b>Sources:<\/b> Demo\") ), align = \"left\", font = list(family = \"Arial\", size = 12), height = 40, line = list(color = \"rgba(0,0,0,0)\") # Remove borders ), cells = list( line = list(color = \"rgba(0,0,0,0)\") # Remove borders ), domain = list( x = c(0, 1), y = c(0, 0.03) ) ) # Ensure hoverinfo is retained for all subplots by explicitly setting hoverinfo for each axis final_combined <- subplot( table_plot, plotly_combined, table_plot2, nrows = 3, heights = c(0.25, 0.7, 0.05), shareX = FALSE, shareY = FALSE, titleX = FALSE, titleY = FALSE ) %>% layout( annotations = list( list( text = \"<b>Average Monthly Net Migration<\/b>\", x = 0.5, y = 1.035, xref = \"paper\", yref = \"paper\", showarrow = FALSE, font = list(size = 16, color = \"#000000\") ) ) ) print(final_combined) ``` When using ``` subplot() ``` with multiple maps from sf objects I cannot zoom or pan into the maps and hover text does not appear. It works when the map is plotted individually. Files to reproduce the issue . The data has been modified to remove confidential information so please ignore inconsistency or logical mismatches. It's part of a larger architecture where R acts only as the backend, hence I am limited to Plotly, Leaflet and MapView. Is this a bug in nested ``` subplot() ``` when used with sf objects or map traces? How can I retain map interactivity (zoom, pan, hover) when embedded as part of a nested ``` subplot() ``` layout? I've tried using ``` subplot() ``` with ``` add_sf() ``` and ``` add_trace() ``` and modifying layout options (dragmode, uirevision, and geo anchoring). ``` final_combined ``` should be able to convert it into Plotly JSON using: ``` plotly_json <- plotly_json(final_combined, jsonedit = FALSE) ```",
    "author_id":5438,
    "publication_date":1754365154000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"User_NAHRUB",
    "author_reputation":21.0,
    "tags":"r, plotly, plot, shapefile, ggplotly",
    "text_length":8572,
    "title_length":50,
    "num_tags":5
  },
  {
    "id":5915,
    "title":"How to create integration tests with Keycloak and xUnit?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725534\/how-to-create-integration-tests-with-keycloak-and-xunit",
    "text":"I'm creating a new project with Keycloak and .NET Core 9. I've managed to get it working with my application, but I need to make it work with my integration tests. To integrate authorization and authentication, I used the following libraries: XML ``` <PackageVersion Include=\"Keycloak.AuthServices.Authentication\" Version=\"2.6.1\" \/> <PackageVersion Include=\"Keycloak.AuthServices.Authorization\" Version=\"2.6.1\" \/> <PackageVersion Include=\"Microsoft.AspNetCore.Authentication.JwtBearer\" Version=\"9.0.7\" \/> ``` To start the WebApi with authorization and authentication, I'm doing the following: C# ``` private static IServiceCollection AddAuthorizationInternal(this IServiceCollection services, IConfiguration configuration) { \/\/ RequireResourceRolesForClient is used for client related roles \/\/ eg.: .AddPolicy(\"policyName\", policy => policy.RequireResourceRolesForClient(\"cidade-inteligente-client\", [\"test-client-permission\"])) \/\/ RequireRealmRoles is used for realm relatede roles (above client related) \/\/ eg.: .AddPolicy(\"policyName\", policy => policy.RequireRealmRoles(\"test-realm-permission\")) \/\/ on the endpoint use the .RequireAuthorization(\"policyName\") services .AddAuthorization() .AddAuthorizationBuilder() .AddPolicy(\"teste\", policy => policy.RequireRealmRoles(\"test-permission\")); services .AddKeycloakAuthorization() .AddAuthorizationServer(configuration); return services; } private static IServiceCollection AddAuthenticationInternal( this IServiceCollection services, IConfiguration configuration) { services .AddAuthentication(JwtBearerDefaults.AuthenticationScheme) .AddKeycloakWebApi(configuration); return services; } ``` My configuration looks like this: JSON ``` \"Keycloak\": { \"realm\": \"my-realm\", \"auth-server-url\": \"http:\/\/localhost:8080\/\", \"ssl-required\": \"none\", \"resource\": \"my-client\", \"verify-token-audience\": true, \"Audience\": \"account\", \"credentials\": { \"secret\": \"test-secret\" }, \"confidential-port\": 0 } ``` And for tests, I'm using these packages: XML ``` <PackageVersion Include=\"Keycloak.AuthServices.Authentication\" Version=\"2.6.1\" \/> <PackageVersion Include=\"Keycloak.AuthServices.Authorization\" Version=\"2.6.1\" \/> <PackageVersion Include=\"Microsoft.NET.Test.Sdk\" Version=\"17.14.1\" \/> <PackageVersion Include=\"Respawn\" Version=\"6.2.1\" \/> <PackageVersion Include=\"Testcontainers.Keycloak\" Version=\"4.6.0\" \/> <PackageVersion Include=\"xunit.v3\" Version=\"3.0.0\" \/> <PackageVersion Include=\"xunit.runner.visualstudio\" Version=\"3.1.3\" \/> <PackageVersion Include=\"Shouldly\" Version=\"4.3.0\" \/> ``` I'm using ``` [assembly: AssemblyFixture(typeof(ApiTestFixture))] ``` in ``` Tests.Integration.Fixtures; ``` with the following configuration: C# ``` protected override void ConfigureWebHost(IWebHostBuilder builder) { builder.ConfigureServices(services => { \/\/ Remove the existing DbContext configuration var descriptor = services.SingleOrDefault( d => d.ServiceType == typeof(DbContextOptions<AppDbContext>)); var authDescriptor = services.SingleOrDefault(d => d.ServiceType == typeof(IAuthenticationSchemeProvider)); if (descriptor != null) { services.Remove(descriptor); } if (authDescriptor != null) { services.Remove(authDescriptor); } \/\/ Add DbContext using the existing, open DbConnection object services.AddDbContext<AppDbContext>(options => options.UseSqlServer(_sqlServerContainer.GetConnectionString())); services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme) .AddJwtBearer(options => { options.Audience = _keycloakContainer.GetBaseAddress(); options.Authority = _keycloakContainer.GetBaseAddress() + \"\/realms\/cidade-inteligente\"; }); \/\/ set keycloak settings services .AddAuthentication(\"BearerTest\") .AddKeycloakWebApi(options => { options.Realm = \"my-realm\"; options.AuthServerUrl = _keycloakContainer.GetBaseAddress(); options.SslRequired = \"none\"; options.Resource = \"my-client\"; options.VerifyTokenAudience = true; options.Audience = \"account\"; options.Credentials = new KeycloakClientInstallationCredentials { Secret = \"my-secret\" }; }); }); } ``` The ``` InitializeAsync ``` method: C# ``` public async ValueTask InitializeAsync() { \/\/ 1. Start the Testcontainer _sqlServerContainer = new MsSqlBuilder() .WithImage(\"mcr.microsoft.com\/mssql\/server:2022-latest\") .Build(); await _sqlServerContainer.StartAsync(); _keycloakContainer = new KeycloakBuilder() .WithImage(\"quay.io\/keycloak\/keycloak:latest\") .WithName(\"keycloak_test\") \/\/map the realm configuration file import.json. .WithResourceMapping(\"..\/..\/..\/..\/..\/keycloak-config\/my-realm.json\", \"\/opt\/keycloak\/data\/import\") .WithCommand(\"--import-realm\") .WithPortBinding(8443, 8443) .WithEnvironment(\"KEYCLOAK_ADMIN\", \"admin\") .WithEnvironment(\"KEYCLOAK_ADMIN_PASSWORD\", \"admin\") .WithUsername(\"admin\") .WithPassword(\"admin\") .WithCleanUp(true) .Build(); await _keycloakContainer.StartAsync(); } ``` The ``` testContainer ``` starts up, but when I try to run the tests, my ``` Program.cs ``` file throws the following error: ``` System.InvalidOperationException : Scheme already exists: Bearer at Microsoft.AspNetCore.Authentication.AuthenticationOptions.AddScheme(String name, Action`1 configureBuilder) at Microsoft.Extensions.Options.OptionsFactory`1.Create(String name) at Microsoft.Extensions.Options.UnnamedOptionsManager`1.get_Value() at Microsoft.AspNetCore.Authentication.AuthenticationSchemeProvider..ctor(IOptions`1 options, IDictionary`2 schemes) at InvokeStub_AuthenticationSchemeProvider..ctor(Object, Span`1) at System.Reflection.MethodBaseInvoker.InvokeWithOneArg(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture) at System.Reflection.RuntimeConstructorInfo.Invoke(BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope) at Microsoft.Extensions.DependencyInjection.ServiceProvider.CreateServiceAccessor(ServiceIdentifier serviceIdentifier) at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory) at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(ServiceIdentifier serviceIdentifier, ServiceProviderEngineScope serviceProviderEngineScope) at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(Type serviceType) at Microsoft.Extensions.DependencyInjection.ActivatorUtilities.ConstructorMatcher.CreateInstance(IServiceProvider provider) at Microsoft.Extensions.DependencyInjection.ActivatorUtilities.CreateInstance(IServiceProvider provider, Type instanceType, Object[] parameters) at Microsoft.AspNetCore.Builder.UseMiddlewareExtensions.ReflectionMiddlewareBinder.CreateMiddleware(RequestDelegate next) at Microsoft.AspNetCore.Builder.ApplicationBuilder.Build() at Microsoft.AspNetCore.Builder.ApplicationBuilder.Build() at Microsoft.AspNetCore.Hosting.GenericWebHostService.StartAsync(CancellationToken cancellationToken) at Microsoft.Extensions.Hosting.Internal.Host.<StartAsync>b__14_1(IHostedService service, CancellationToken token) at Microsoft.Extensions.Hosting.Internal.Host.ForeachService[T](IEnumerable`1 services, CancellationToken token, Boolean concurrent, Boolean abortOnFirstException, List`1 exceptions, Func`3 operation) at Microsoft.Extensions.Hosting.Internal.Host.StartAsync(CancellationToken cancellationToken) at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.RunAsync(IHost host, CancellationToken token) at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.RunAsync(IHost host, CancellationToken token) at Program.<Main>$(String[] args) in \/home\/breno-pimentel\/Área de trabalho\/Projetos\/Darojo\/CidadeInteligente\/src\/WebApi\/Program.cs:line 64 at Program.<Main>(String[] args) at InvokeStub_Program.<Main>(Object, Span`1) at System.Reflection.MethodBaseInvoker.InvokeWithOneArg(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture) --- End of stack trace from previous location --- at Microsoft.AspNetCore.Mvc.Testing.DeferredHostBuilder.DeferredHost.StartAsync(CancellationToken cancellationToken) at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.Start(IHost host) at Microsoft.AspNetCore.Mvc.Testing.WebApplicationFactory`1.CreateHost(IHostBuilder builder) at Microsoft.AspNetCore.Mvc.Testing.WebApplicationFactory`1.ConfigureHostBuilder(IHostBuilder hostBuilder) at Microsoft.AspNetCore.Mvc.Testing.WebApplicationFactory`1.EnsureServer() at Microsoft.AspNetCore.Mvc.Testing.WebApplicationFactory`1.CreateDefaultClient(DelegatingHandler[] handlers) at Microsoft.AspNetCore.Mvc.Testing.WebApplicationFactory`1.CreateDefaultClient(Uri baseAddress, DelegatingHandler[] handlers) at Microsoft.AspNetCore.Mvc.Testing.WebApplicationFactory`1.CreateClient(WebApplicationFactoryClientOptions options) at Microsoft.AspNetCore.Mvc.Testing.WebApplicationFactory`1.CreateClient() at Tests.Integration.BaseIntegrationTest..ctor(ApiTestFixture fixture) in \/home\/breno-pimentel\/Área de trabalho\/Projetos\/Darojo\/CidadeInteligente\/test\/Tests.Integration\/BaseIntegrationTest.cs:line 22 at Tests.Integration.Tests.PersonControllerTests..ctor(ApiTestFixture fixture) in \/home\/breno-pimentel\/Área de trabalho\/Projetos\/Darojo\/CidadeInteligente\/test\/Tests.Integration\/Tests\/PersonControllerTests.cs:line 16 at InvokeStub_PersonControllerTests..ctor(Object, Span`1) at System.Reflection.MethodBaseInvoker.InvokeWithOneArg(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture) ``` I've tried clearing the authentication services, but it still didn't work. How do I get Keycloak to work with my integration tests?",
    "author_id":5437,
    "publication_date":1754366130000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"BrePi",
    "author_reputation":49.0,
    "tags":"asp.net-core, .net-9.0, keycloak, xunit, testcontainers",
    "text_length":9953,
    "title_length":56,
    "num_tags":5
  },
  {
    "id":5914,
    "title":"how to calculate the gpu usage of kv-cache for one request in vllm-serving",
    "link":"https:\/\/stackoverflow.com\/questions\/79725536\/how-to-calculate-the-gpu-usage-of-kv-cache-for-one-request-in-vllm-serving",
    "text":"In the pagedAttension article, section-3.1 mentions a calculation which is ``` one-token-kvcache = 2 (key and value vector) * hidden-state-size * number-of-layers * data-type(int=8,float=32,bfloat=16) ``` . If multiplying the above result with ``` model-context-length(prompt and output) ``` , the final result becomes the gpu usage of kv-cache for one request. By the config.json of Qwen\/Qwen2.5-7B-Instruct-GPTQ-Int8, the kv-cache for one token requires ``` 2 * 3584 * 28 * 8 = 1605632 B = 1.53 MB ``` . If the above statement is wrong, please correct me. Currently, I am running Qwen\/Qwen2.5-7B-Instruct-GPTQ-Int8 by vllm-v0.9.2. By adjusting the ``` --gpu-memory-utilization ``` option, there is the measurement result: 10.6 GB supports 5 req\/s, 13.0 GB supports 20 req\/s and 18.1 GB supports 50 req\/s. Roughly, one request requires 0.16 GB for kv-cache, which is calculated as ``` (13 -10.6)\/(20-5) ``` and verified by ``` (13 -10.6)\/(20-5) = (18 -13)\/(50-20) ``` . Is this calculation correct?",
    "author_id":5436,
    "publication_date":1754366405000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Hobin C.",
    "author_reputation":783.0,
    "tags":"vllm",
    "text_length":999,
    "title_length":74,
    "num_tags":1
  },
  {
    "id":5913,
    "title":"How to cache tokio_postgres::Client without closing the connection?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725539\/how-to-cache-tokio-postgresclient-without-closing-the-connection",
    "text":"I'm writing a library for creating and dropping postgres databases. I use it in tests to spin up and tear down test databases. Here is a snippet that works (excuse the lack of sql sanitizing. it's only used by myself in tests): ``` use tokio_postgres::{Client, NoTls}; pub struct DbContainer { container_name: String, args: Vec<String>, db_config: DbConfig, } #[derive(Debug)] pub struct DbConfig { pub port: u16, pub name: String, pub user: String, pub postgres_password: String, pub db_password: String, } impl DbContainer { pub fn new<N>(container_name: N, db_config: DbConfig) -> Self where N: Into<String>, { Self { container_name: container_name.into(), args: vec![ \"--env\".to_string(), format!(\"POSTGRES_PASSWORD={}\", &db_config.postgres_password), \"--publish\".to_string(), format!(\"{}:5432\", db_config.port), ], db_config, } } pub async fn create_database(&self, name: &str) -> Result<(), tokio_postgres::Error> { let client = self.connect(None).await?; client .execute(&format!(\"CREATE DATABASE {name}\"), &[]) .await?; let client = self.connect(Some(name)).await?; client.batch_execute(load_schema()).await } pub async fn drop_database(&self, name: &str) -> Result<u64, tokio_postgres::Error> { let client = self.connect(None).await?; let terminate_sql = format!( \" SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '{name}' AND pid <> pg_backend_pid(); \", ); client.execute(&terminate_sql, &[]).await?; client.execute(&format!(\"DROP DATABASE {name}\"), &[]).await } async fn connect(&self, db_name: Option<&str>) -> Result<Client, tokio_postgres::Error> { let mut conn_str = format!( \"host=localhost port={} user=postgres password={}\", self.db_config.port, self.db_config.postgres_password ); if let Some(dbname) = db_name { conn_str.push_str(&format!(\" dbname={dbname}\")); } let (client, conn) = tokio_postgres::connect(&conn_str, NoTls).await?; tokio::spawn(async move { if let Err(e) = conn.await { eprintln!(\"Connection error: {e}\"); } }); Ok(client) } } ``` Calling ``` self.connect(None) ``` makes a new connection to the default postgres database every time. I would like to cache this client. My first approach was using ``` tokio::sync::OnceCell ``` : ``` pub struct DbContainer { \/\/ ... client: tokio::sync::OnceCell<Client>, } enum ClientWrapper<'a> { Owned(Client), Borrowed(&'a Client), } impl Deref for ClientWrapper<'_> { type Target = Client; fn deref(&self) -> &Self::Target { match self { Self::Owned(c) => c, Self::Borrowed(c) => c, } } } impl DbContainer { pub fn new<N>(container_name: N, db_config: DbConfig) -> Self where N: Into<String>, { Self { \/\/ ... client: tokio::sync::OnceCell::const_new(), } } async fn connect(&self, db_name: Option<&str>) -> Result<ClientWrapper, tokio_postgres::Error> { let mut conn_str = format!( \"host=localhost port={} user=postgres password={}\", self.db_config.port, self.db_config.postgres_password ); if let Some(dbname) = db_name { conn_str.push_str(&format!(\" dbname={dbname}\")); let (client, conn) = tokio_postgres::connect(&conn_str, NoTls).await?; tokio::spawn(async move { if let Err(e) = conn.await { eprintln!(\"Connection error: {e}\"); } }); Ok(ClientWrapper::Owned(client)) } else { let client = self .client .get_or_init(|| async { let (client, conn) = tokio_postgres::connect(&conn_str, NoTls) .await .expect(\"Failed to connect to database\"); tokio::spawn(async move { if let Err(e) = conn.await { eprintln!(\"Connection error: {e}\"); } }); client }) .await; Ok(ClientWrapper::Borrowed(client)) } } } ``` but then I would get the following error in ``` drop_database ``` about 40% of the time: ``` Error { kind: Closed, cause: None } ``` I also tried using ``` client: Arc<Mutex<Option<Client>>> ``` with the following code but the results were the same: ``` let mut locked_client = self.client.lock().await; if locked_client.is_none() { let (client, conn) = tokio_postgres::connect(&conn_str, NoTls).await?; tokio::spawn(async move { if let Err(e) = conn.await { eprintln!(\"Connection error: {e}\"); } }); *locked_client = Some(client); } Ok(Arc::clone(&self.client)) ``` I'm confused why the original code didn't have this issue. I checked the doc for ``` tokio_postgres::Connection ``` and it says (emphasis mine) ``` Connection ``` implements ``` Future ``` , and only resolves when the connection is closed, either because a fatal error has occurred, or because its associated ``` Client ``` has dropped and all outstanding work has completed. In both the ``` OnceCell ``` and ``` Arc ``` approach, the client should never be dropped until ``` DbContainer ``` goes out of scope, so shouldn't connection stay alive? How do I cache the Client properly without the connection closing?",
    "author_id":5435,
    "publication_date":1754367122000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Coco Liliace",
    "author_reputation":374.0,
    "tags":"rust, rust-tokio, tokio-postgres",
    "text_length":4684,
    "title_length":67,
    "num_tags":3
  },
  {
    "id":5912,
    "title":"ttsClient.SynthesizeLongAudio undefined (type *&quot;cloud.google.com\/go\/texttospeech\/apiv1&quot;.Client has no field or method SynthesizeLongAudio)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725543\/ttsclient-synthesizelongaudio-undefined-type-cloud-google-com-go-texttospeech",
    "text":"Triggering a Cloud Run Function 2nd Generation based on changes to files in a Google Cloud Storage bucket. go.mod ``` go 1.24 require ( cloud.google.com\/go\/storage v1.51.0 cloud.google.com\/go\/texttospeech v1.13.0 github.com\/GoogleCloudPlatform\/functions-framework-go v1.9.2 github.com\/cloudevents\/sdk-go\/v2 v2.16.1 github.com\/googleapis\/google-cloudevents-go v0.10.0 ) ``` function.go ``` \/\/ Package helloworld provides a set of Cloud Functions samples. package helloworld import ( \"context\" \"fmt\" \"io\" \"log\" \"os\" \"path\/filepath\" \"reflect\" \"strconv\" \"strings\" \"time\" \"cloud.google.com\/go\/storage\" texttospeech \"cloud.google.com\/go\/texttospeech\/apiv1\" texttospeechpb \"google.golang.org\/genproto\/googleapis\/cloud\/texttospeech\/v1beta1\" \"github.com\/GoogleCloudPlatform\/functions-framework-go\/functions\" \"github.com\/cloudevents\/sdk-go\/v2\/event\" ) var ( storageClient *storage.Client ttsClient *texttospeech.Client inputFolderPrefix = \"text-to-speech\/input\/\" outputFolderPrefix = \"text-to-speech\/output\/\" ) func init() { var err error \/\/ Initialize clients eagerly to optimize for performance. \/\/ The function framework will keep these clients alive for subsequent invocations. storageClient, err = storage.NewClient(context.Background()) if err != nil { log.Fatalf(\"Failed to create storage client: %v\", err) } ttsClient, err = texttospeech.NewClient(context.Background()) if err != nil { log.Fatalf(\"Failed to create text-to-speech client: %v\", err) } functions.CloudEvent(\"ProcessGCSEvent\", helloStorage) } \/\/ StorageObjectData contains metadata of the Cloud Storage object. type StorageObjectData struct { Bucket string `json:\"bucket,omitempty\"` Name string `json:\"name,omitempty\"` Metageneration int64 `json:\"metageneration,string,omitempty\"` TimeCreated time.Time `json:\"timeCreated,omitempty\"` Updated time.Time `json:\"updated,omitempty\"` } func helloStorage(ctx context.Context, e event.Event) error { log.Println(\"begin helloStorage\") defer log.Println(\"end helloStorage\") reflectValue := reflect.ValueOf(ttsClient) numOfMethods := reflectValue.NumMethod() fmt.Println(\"Methods for ttsClient:\") for i := 0; i < numOfMethods; i++ { method := reflectValue.Type().Method(i) fmt.Printf(\"- %s\\n\", method.Name) } log.Printf(\"Event ID: %s\", e.ID()) log.Printf(\"Event Type: %s\", e.Type()) var data StorageObjectData if err := e.DataAs(&data); err != nil { return fmt.Errorf(\"event.DataAs: %v\", err) } log.Printf(\"Bucket: %s\", data.Bucket) log.Printf(\"File: %s\", data.Name) log.Printf(\"Metageneration: %d\", data.Metageneration) log.Printf(\"Created: %s\", data.TimeCreated) log.Printf(\"Updated: %s\", data.Updated) \/\/ Filter to avoid infinite loops and process only relevant files. if !strings.HasPrefix(data.Name, inputFolderPrefix) { log.Printf(\"Ignoring file '%s' as it is not in the input folder '%s'.\", data.Name, inputFolderPrefix) return nil \/\/ Acknowledge event successfully, but do nothing. } ext := strings.ToLower(filepath.Ext(data.Name)) if ext != \".ssml\" && ext != \".xml\" { log.Printf(\"Ignoring file with unsupported extension: %s\", data.Name) return nil } \/\/ Create a context with a timeout. Long audio synthesis can take a while. ctx, cancel := context.WithTimeout(context.Background(), 15*time.Minute) defer cancel() ssmlContent, errRead := readFileFromGCS(ctx, data.Bucket, data.Name) if errRead != nil { return fmt.Errorf(\"readFileFromGCS: %w\", errRead) } outputObjectName := getOutputObjectName(data.Name) outputURI := fmt.Sprintf(\"gs:\/\/%s\/%s\", data.Bucket, outputObjectName) err2 := synthesizeText(ctx, string(ssmlContent), outputURI) if err2 != nil { \/\/ Returning an error will cause Eventarc to attempt a retry. return fmt.Errorf(\"synthesizeText failed for '%s': %w\", data.Name, err2) } log.Printf(\"Successfully processed and stored audio for: %s\", data.Name) return nil return nil } func readFileFromGCS(ctx context.Context, bucket, objectName string) ([]byte, error) { log.Println(\"begin readFileFromGCS\") defer log.Println(\"end readFileFromGCS\") ctx, cancel := context.WithTimeout(ctx, time.Second*30) defer cancel() rc, err := storageClient.Bucket(bucket).Object(objectName).NewReader(ctx) if err != nil {return nil, err}; defer rc.Close() return io.ReadAll(rc) } func getOutputObjectName(inputName string) string { log.Println(\"begin getOutputObjectName\") defer log.Println(\"end getOutputObjectName\") baseName := filepath.Base(inputName) nameWithoutExt := strings.TrimSuffix(baseName, filepath.Ext(baseName)) audioExt := \".mp3\" \/\/ Default based on MP3 env var if os.Getenv(\"TTS_AUDIO_ENCODING\") == \"OGG_OPUS\" { audioExt = \".ogg\" } else if os.Getenv(\"TTS_AUDIO_ENCODING\") == \"LINEAR16\" { audioExt = \".wav\" } return filepath.Join(outputFolderPrefix, nameWithoutExt+audioExt) } func synthesizeText(ctx context.Context, strSsml string, outputURI string) (error) { log.Println(\"begin synthesizeText\") defer log.Println(\"end synthesizeText\") rate, _ := strconv.ParseFloat(os.Getenv(\"TTS_AUDIO_RATE\"), 64) pitch, _ := strconv.ParseFloat(os.Getenv(\"TTS_AUDIO_PITCH\"), 64) req := &texttospeechpb.SynthesizeLongAudioRequest{ Parent: \"projects\/xxxx\/locations\/global\", Input: &texttospeechpb.SynthesisInput{InputSource: &texttospeechpb.SynthesisInput_Ssml{Ssml: strSsml}}, Voice: &texttospeechpb.VoiceSelectionParams{ LanguageCode: os.Getenv(\"TTS_LANGUAGE_CODE\"), Name: os.Getenv(\"TTS_VOICE_NAME\"), }, AudioConfig: &texttospeechpb.AudioConfig{ AudioEncoding: texttospeechpb.AudioEncoding(texttospeechpb.AudioEncoding_value[os.Getenv(\"TTS_AUDIO_ENCODING\")]), SpeakingRate: rate, Pitch: pitch, }, OutputGcsUri: outputURI, } op, errSynth := ttsClient.SynthesizeLongAudio(ctx, req) if errSynth != nil { return errSynth } log.Printf(\"Successfully submitted job: %s\", op.Name()) return nil } ``` The build log shows: ``` failed to build: (error ID: 6fba28bd): # example.com\/gcf .\/function.go:170:28: ttsClient.SynthesizeLongAudio undefined (type *\"cloud.google.com\/go\/texttospeech\/apiv1\".Client has no field or method SynthesizeLongAudio) ``` Documentation I have reviewed includes: https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/texttospeech\/latest\/apiv1 https:\/\/cloud.google.com\/text-to-speech\/docs\/create-audio-text-long-audio-synthesis#:~:text=gcloud%20init-,Synthesize%20long%20audio%20from%20text%20using%20the%20command%20line,authorization%20token%20for%20the%20request . https:\/\/cloud.google.com\/text-to-speech\/docs\/samples\/tts-synthesize-text?hl=en What am I doing incorrectly? What is the fix?",
    "author_id":5434,
    "publication_date":1754367568000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tom Scheifler",
    "author_reputation":89.0,
    "tags":"go, google-text-to-speech, google-cloud-speech, google-api-go-client",
    "text_length":6420,
    "title_length":148,
    "num_tags":4
  },
  {
    "id":5911,
    "title":"Certain div not being shown on deployed site in Firefox?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725552\/certain-div-not-being-shown-on-deployed-site-in-firefox",
    "text":"So I'm building a site and I deployed it, but for some reason Firefox doesn't want to show one of the divs (Follow Us) for the footer. This only happens on the deployed version, localhost shows \"follow us\" just fine and every other browser works just fine. This is in react JS in case anyone was curious. This first screenshot is what firefox shows for the deployed site: This second screenshot is what firefox shows for localhost: I have never encountered this bug before on any site I've made. There are no console errors. But I did notice in the dev tools that firefox has grayed out the div Here is the entire Footer styling code in SCSS: ``` $small: 640px; $medium: 768px; $large: 1040px; .footer-container { background-image: linear-gradient(rgba(0, 0, 0, 0.4)), url(\"..\/..\/assets\/Homepage-Images\/Section5-Background.png\"); background-position: center; background-size: cover; background-repeat: no-repeat; padding: 1em; color: white; min-height: 88dvh; display: grid; grid-template-columns: 1fr; :where(h4) { margin-block: 0.67em; font-size: 2em; } @media screen and (min-width: $medium) { grid-template-columns: repeat(3, 1fr); grid-template-rows: auto 1fr auto; } .social-link { display: flex; align-items: center; justify-content: center; gap: 0.5em; color: white; &:hover { color: orange; } } .clickable-icon { font-size: 25px; &:hover { cursor: pointer; color: orange; } } .footer-header { grid-column: 1; grid-row: 1; margin-bottom: 1em; .footer-logo { width: 25em; } } .footer-content-container { grid-column: 1 \/ -1; grid-row: 2; display: grid; grid-template-columns: 1fr; grid-auto-rows: auto; @media screen and (min-width: $medium) { grid-template-columns: repeat(3, 1fr); } .footer-column { display: flex; flex-direction: column; justify-content: space-between; height: fit-content; &.footer-follow-us { display: flex; align-items: flex-start; justify-content: center; :nth-child(2) { margin-left: 1em; } @media screen and (min-width: $medium) { align-items: center; text-align: center; } } &.footer-legal { grid-column: 1 \/ -1; grid-row: auto; } @media screen and (min-width: $medium) { margin-top: 0; } ul { list-style: none; padding: 0; width: 10em; margin-left: 1.5em; li { &:hover { cursor: pointer; color: #ff8c00; } } } .contact-email-copy { display: flex; justify-content: center; align-items: center; gap: 0.5rem; } .copy-button { margin-bottom: 0.5em; padding: 0.25rem 0.5rem; font-size: 1rem; } .contact-button { background-color: var(--page-bg); color: var(--text-color); border: none; padding: 0.5rem 1rem; border-radius: 5px; cursor: pointer; font-weight: bold; text-decoration: none; display: inline-block; margin-top: 1rem; &:hover { background-color: var(--page-bg-reverse); color: var(--text-color-reverse); } } } } } ``` And here is the jsx: ``` import \".\/Footer.scss\"; import { useNavigate } from \"react-router-dom\"; import { Image, Button } from \"react-bootstrap\"; import { FaLinkedin, FaGithub } from \"react-icons\/fa\"; import LogoDark from \"..\/..\/assets\/Navbar-Images\/Logo-Dark.png\"; export const Footer = () => { const emailAddress = \"testingTest@test.com\"; const navigate = useNavigate(); const handleCopyEmail = () => { navigator.clipboard.writeText(emailAddress); alert(\"Email address copied to clipboard!\"); }; return ( <div className=\"footer-container\"> <div className=\"footer-header\"> <Image src={LogoDark} alt=\"Logo\" className=\"footer-logo\" \/> <\/div> <div className=\"footer-content-container\"> <div className=\"footer-column\"> <h4>Quick Links<\/h4> <ul> <li> <p onClick={() => { window.location.reload(); }} > - Home <\/p> <\/li> <li> <p onClick={() => { navigate(\"\/services\"); }} > - Services <\/p> <\/li> <li> <p onClick={() => { navigate(\"\/products\"); }} > - Products <\/p> <\/li> <li> <p onClick={() => { navigate(\"\/our-story\"); }} > - About Us <\/p> <\/li> <\/ul> <\/div> <div className=\"footer-column\"> <h4>Contact Us<\/h4> <div className=\"contact-email-copy\"> <p>{emailAddress}<\/p> <Button onClick={handleCopyEmail} className=\"copy-button\"> Copy <\/Button> <\/div> <Button className=\"contact-button\" onClick={() => { window.location.href = `mailto:${emailAddress}?subject=Inquiry&body=Hi, I’d like to learn more about your services.`; }} > Contact Us <\/Button> <\/div> <div className=\"footer-column footer-follow-us\"> <h4>Follow Us<\/h4> <a href=\"https:\/\/www.linkedin.com\/\" target=\"_blank\" rel=\"noopener noreferrer\" className=\"social-link\" > LinkedIn <FaLinkedin className=\"clickable-icon\" \/> <\/a> <\/div> <div className=\"footer-column footer-legal\"> <h4>Legal<\/h4> <ul> <li> <p onClick={() => navigate(\"\/privacy-policy\")}> - Privacy Policy <\/p> <\/li> <li> <p onClick={() => navigate(\"\/terms-of-service\")}> - Terms of Service <\/p> <\/li> <\/ul> <\/div> <\/div> <\/div> ); }; ```",
    "author_id":5433,
    "publication_date":1754368377000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Stacklep1",
    "author_reputation":415.0,
    "tags":"reactjs, javascript, firefox, sass",
    "text_length":4710,
    "title_length":56,
    "num_tags":4
  },
  {
    "id":5910,
    "title":"What is wrong with this Python variable NameError: name &#39;thetas&#39; is not defined",
    "link":"https:\/\/stackoverflow.com\/questions\/79725556\/what-is-wrong-with-this-python-variable-nameerror-name-thetas-is-not-defined",
    "text":"I am trying to plot the result of project 5 in http:\/\/www.simpetus.com\/projects.html#meep_thermal_radiation but get en error message ``` NameError: name 'thetas' is not defined ``` on line 12 of the following program ``` import matplotlib.pyplot as plt import numpy as np import numpy.matlib import math lmin = 2.0 # source min wavelength lmax = 5.0 # source max wavelength fmin = 1\/lmax # source min frequency fmax = 1\/lmin # source max frequency fcen = 0.5*(fmin+fmax) thetas = np.append(thetas, range(0,35,5)) kx = [fcen*math.sin(t) for t in [math.radians(float(t)) for t in thetas]] Refl = np.empty((50,thetas.size)) Abs = np.empty((50,thetas.size)) theta_out = np.empty((50,thetas.size)) for k in range(thetas.size): f0 = np.genfromtxt(\"flux0_a4.3_theta{}.dat\".format(thetas[k]), delimiter=\",\") f = np.genfromtxt(\"flux_a4.3_r1.72_theta{}.dat\".format(thetas[k]), delimiter=\",\") Refl[:,k] = -f[:,1]\/f0[:,1] theta_out[:,k] = np.asarray([math.degrees(math.asin(kx[k]\/f0[j,0])) for j in range(50)]) Abs[:,k] = np.asarray([(1-Refl[j,k])*math.cos(math.radians(theta_out[j,k])) for j in range(50)]) Abs[Abs<0] = 0 wvl = 1\/f0[:,0] wvls = np.matlib.repmat(np.reshape(wvl,(wvl.size,1)),1,thetas.size) plt.figure(dpi=100) plt.pcolormesh(theta_out, wvls, Abs, cmap='hot_r', shading='gouraud', vmin=0, vmax=Abs.max()) plt.axis([theta_out.min(), theta_out.max(), wvl[-1], wvl[0]]) plt.xlabel(\"emission angle (degrees)\") plt.xticks([t for t in range(0,60,10)]) plt.ylabel(\"wavelength (um)\") plt.yticks([t for t in np.arange(2.0,5.5,0.5)]) plt.title(r\"emissivity: a=4.3 um, r=1.72 um\") cbar = plt.colorbar() cbar.set_ticks([t for t in np.arange(0,1.0,0.2)]) cbar.set_ticklabels([\"{:.1f}\".format(t) for t in np.arange(0,1.0,0.2)]) plt.show() ``` searching the reason of this error message...`",
    "author_id":5432,
    "publication_date":1754369294000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Quant",
    "author_reputation":7.0,
    "tags":"python",
    "text_length":1779,
    "title_length":87,
    "num_tags":1
  },
  {
    "id":5909,
    "title":"How to know a security provider supports a specific keysize without trying to init?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725565\/how-to-know-a-security-provider-supports-a-specific-keysize-without-trying-to-in",
    "text":"The Cipher states that Java platforms require to support following transformations and keysizes. ``` AES\/CBC\/NoPadding (128) AES\/CBC\/PKCS5Padding (128) AES\/ECB\/NoPadding (128) AES\/ECB\/PKCS5Padding (128) DES\/CBC\/NoPadding (56) DES\/CBC\/PKCS5Padding (56) DES\/ECB\/NoPadding (56) DES\/ECB\/PKCS5Padding (56) DESede\/CBC\/NoPadding (168) DESede\/CBC\/PKCS5Padding (168) DESede\/ECB\/NoPadding (168) DESede\/ECB\/PKCS5Padding (168) RSA\/ECB\/PKCS1Padding (1024, 2048) RSA\/ECB\/OAEPWithSHA-1AndMGF1Padding (1024, 2048) RSA\/ECB\/OAEPWithSHA-256AndMGF1Padding (1024, 2048) ``` I know that a JRE may support more than that. And let's say we need to do with ``` AES\/ECB\/PKCS5Padding ``` with a keysize of ``` 256 ``` bits. (Please do not say anything about the ``` ECB ``` mode. That's not my decision.) It'll be easy to specify a provider supports the keysize, say, ``` BC ``` , at the first place. How can I (know|pickup) a security provider that supports a keysize without trying to ``` init ``` the cipher? Do I have a pre-step with ``` Security#getProviders ``` ?",
    "author_id":5431,
    "publication_date":1754370730000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jin Kwon",
    "author_reputation":22242.0,
    "tags":"java, cryptography, jce",
    "text_length":1042,
    "title_length":83,
    "num_tags":3
  },
  {
    "id":5908,
    "title":"How to find path by given a filename input in CLI",
    "link":"https:\/\/stackoverflow.com\/questions\/79725568\/how-to-find-path-by-given-a-filename-input-in-cli",
    "text":"I want to create a CLI tool with cobra cli framework where i have subcommand that take argument filename. then i can write a content to the file argument is given. but how i suppose to know where the path filename is if the CLI tool already build and let's say the bin is in go directory. For example : i'm in home directory (but it could be anywhere) then in $home directory i run the cli tool : ``` CLI write \"foo.txt\" ``` (then the cli write a content to \"foo.txt\" file)",
    "author_id":5430,
    "publication_date":1754370922000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Faizisyellow",
    "author_reputation":3.0,
    "tags":"go, path, command-line-interface",
    "text_length":473,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":5907,
    "title":"How to I add an information button to a carousel feature so it expands properly?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725573\/how-to-i-add-an-information-button-to-a-carousel-feature-so-it-expands-properly",
    "text":"Here is what I’m trying to do. I have a carousel with seven images inside of it. For each image I want to add an information button. When the user views one of the images and then clicks the information button located in the body area of the viewport, I want an expandable section to open up and push or flex-shrink the image and navigational buttons over to the side so that the viewer can see both the image and the information in the expandable section at the same time. The problem I am having is combining the carousel feature with the information button feature. I coded the carousel feature first and now I want to add the information button\/ expandable feature to it. Right now when I run the code (see below) and click on the information button, the expandable section opens up and covers the navigational buttons and the image on the carousel doesn’t slide over to accommodate the width of the expandable section and text. Please help if you have time. Thank you! ``` \/*Carousel*\/ const buttons = document.querySelectorAll(\"[data-carousel-button]\") buttons.forEach(button => { button.addEventListener(\"click\", () => { const offset = button.dataset.carouselButton === \"next\" ? 1 : -1 const slides = button .closest(\"[data-carousel]\") .querySelector(\"[data-slides]\") const activeSlide = slides.querySelector(\"[data-active]\") let newIndex = [...slides.children].indexOf(activeSlide) + offset if (newIndex < 0) newIndex = slides.children.length - 1 if (newIndex >= slides.children.length) newIndex = 0 slides.children[newIndex].dataset.active = true delete activeSlide.dataset.active }) }) ``` ``` body { margin: 0; font-family: sans-serif; line-height: 1.5; } \/*Carousel*\/ .carousel > ul { margin: 0; padding: 0; list-style: none; } .slide { position: absolute; inset: 0; opacity: 0; transition: 200ms opacity ease-in-out; transition-delay: 200ms; } .slide > img { display: block; width: 100%; height: 100%; object-fit: cover; object-position: center; } .slide[data-active] { opacity: 1; z-index: 1; transition-delay: 0ms; } .carousel-button { position: absolute; z-index: 2; background: none; border: none; font-size: 4rem; top: 50%; transform: translateY(-50%); color: rgba(255, 255, 255, .5); cursor: pointer; border-radius: .25rem; padding: 0 .5rem; background-color: rgba(0, 0, 0, .1); } .carousel-button:hover, .carousel-button:focus { color: white; background-color: rgba(0, 0, 0, .2); } .carousel-button:focus { outline: 1px solid black; } .carousel-button.prev { left: 1rem; } .carousel-button.next { right: 1rem; } \/*Info Button*\/ .wrapper { display: flex; height: 100vh; width: 100vw; position: relative; } .main { flex-grow: 1; padding: 20px; overflow-y: auto; z-index: 888; } .side-panel-toggle { width: 40px; height: 40px; flex-shrink: 0; display: grid; place-items: center; z-index: 999; color: black; border: none; background-color: transparent; outline: none; cursor: pointer; transition: transform 0.75s ease-in-in; } .side-panel-toggle:hover { color: #007960; } .sp-icon-close { display: none !important; } .side-panel { z-index: 999; display: none; width: 400px; flex-shrink: 0; padding: 20px; color: #ffffff; background: tan; box-shadow: 0 0 10px rgba(0, 0, 0, 0.35); } .side-panel-open .side-panel { display: initial; } .side-panel-open .sp-icon-open { display: none !important; } .side-panel-open .sp-icon-close { display: initial !important; } .content{ width: 100%; height: 100%; display: flex; justify-content: center; align-items: center; } ``` ``` <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title> Carousel and Info Button <output><\/output><\/title> <link rel=\"stylesheet\" href=\"style.css\"> <link rel=\"stylesheet\" href=\"https:\/\/fonts.googleapis.com\/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200&icon_names=info\" \/> <\/head> <body> <!--Info Button--> <div class=\"wrapper side-panel-close\"> <div class=\"main\"> <div class=\"content\"> <!--Carousel--> <section aria-label=\"Newest Photos \"> <div class=\"carousel\" data-carousel> <button class=\"carousel-button prev\" data-carousel-button=\"prev\">&#8656;<\/button> <button class=\"carousel-button next\" data-carousel-button=\"next\">&#8658;<\/button> <ul data-slides> <li class=\"slide\" data-active> <img src=\"https:\/\/placehold.co\/600x400?text=image_1.jpg\" alt=\"#1\"> <\/li> <li class=\"slide\"> <img src=\"https:\/\/placehold.co\/600x400?text=image_2.jpg\" alt=\"#2\"> <\/li> <li class=\"slide\"> <img src=\"https:\/\/placehold.co\/600x400?text=image_3.jpg\" alt=\" #3\"> <\/li> <\/ul> <\/div> <!--Info Button--> <\/div> <\/div> <button class=\"side-panel-toggle\" type=\"button\"> <span class=\"material-symbols-outlined sp-icon-open\"> info <\/span> <span class=\"material-symbols-outlined sp-icon-close\"> info <\/span> <\/button> <div class=\"side-panel\"> <h1>Image One<\/h1> <p>This is my image<\/p> <\/div> <\/div> <!--Info Button--> <script> document.querySelector(\".side-panel-toggle\").addEventListener(\"click\", () => { document.querySelector(\".wrapper\").classList.toggle(\"side-panel-open\"); }); <\/script> <script src=\"index.js\"><\/script> <\/body> <\/html> ```",
    "author_id":5429,
    "publication_date":1754371779000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jen",
    "author_reputation":73.0,
    "tags":"javascript, html, css",
    "text_length":5125,
    "title_length":80,
    "num_tags":3
  },
  {
    "id":5906,
    "title":"Advice on how to handle KeyError occurring when using map Lambda and slicing for dictionary lookup",
    "link":"https:\/\/stackoverflow.com\/questions\/79725576\/advice-on-how-to-handle-keyerror-occurring-when-using-map-lambda-and-slicing-for",
    "text":"I'm creating a lookup table \/ dictionary from a static spreadsheet for upcoming labelling on input data, adding a new field for flavor: ``` lookup_table_data = pd.read_csv(r'C:\\Location\\format1.csv', sep=',') lookup_table_data['label'] = 'apache' ``` I create the dictionary ``` my_format = lookup_table_data.set_index('server').T.to_dict('list') print(my_format) {'ABC123': ['IBM', 1000, 'East Coast', 'apache'], 'ABC456': ['Dell', 800, 'West Coast', 'apache'], 'XYZ123': ['HP', 900, 'West Coast', 'apache']} ``` I read in the input data ``` my_data = pd.read_csv(r'C:\\Location\\my_data.csv') print(my_data) server busy datetime 0 ABC123 24% 6\/1\/2024 0:02 1 ABC456 45% 6\/1\/2024 4:01 2 GHI100 95% 6\/1\/2024 9:10 ``` When I then try to assign some labels via slicing method I end up with 'KeyError' as a server from the input data is not present in the dictionary ``` my_data['type'] = my_data['server'].map(lambda x: my_format[x][0]) my_data['cost'] = my_data['server'].map(lambda x: my_format[x][1]) my_data['location'] = my_data['server'].map(lambda x: my_format[x][2]) KeyError: 'GHI100' ``` If I remove GHI100 data the code works fine as intended. ``` server %busy datetime type cost location 0 ABC123 24% 6\/1\/2024 0:02 IBM 1000 East Coast 1 ABC456 45% 6\/1\/2024 4:01 Dell 800 West Coast ``` Any advice on how I can handle missing keys? Several articles mention using .get with a default value. But doing that I get \"Index out of list range\" for the slices above 0 (i.e. map(lambda x: my_format.get([x][1], None)). Plugging in the [x][0] as a line by itself ``` my_data['type'] = my_data['server'].map(lambda x: my_format.get([x][0], None)) ``` Gets the whole dictionary put into the type column though it does prevent a keyerror. ``` server %busy datetime type 0 ABC123 24% 6\/1\/2024 0:02 [IBM, 1000, East Coast] 1 ABC456 45% 6\/1\/2024 4:01 [Dell, 800, West Coast] 2 GHI100 95% 6\/1\/2024 9:10 None ``` My only other solution has been to do an inner merge on the format file and the data file before the lookup as that removes data with servers not in the dictionary.",
    "author_id":5428,
    "publication_date":1754372581000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Chester",
    "author_reputation":23.0,
    "tags":"python, dictionary, keyerror",
    "text_length":2065,
    "title_length":98,
    "num_tags":3
  },
  {
    "id":5905,
    "title":"Spring AI MCP Client not interacting with MCP Server",
    "link":"https:\/\/stackoverflow.com\/questions\/79725577\/spring-ai-mcp-client-not-interacting-with-mcp-server",
    "text":"I'm trying to implement a simple MCP Server and Client. MCP Server just has a single function to create a file Server Implementation ``` @Slf4j @Service public class FileSystemService { @Tool(name = \"Create File\", description = \"Create a file with the provided fileName on the file system\") public String createFile(String fileName) { log.info(\"Request to create a file: {}\", fileName); Path path = Paths.get(fileName); try { Files.createFile(path); log.info(\"File created: {}\", path.toAbsolutePath()); } catch (IOException e) { log.error(\"Error creating file: {}\", e.getMessage()); } return \"File [\" + fileName + \"] created successfully\"; } } ``` When I test it with Postman using MCP request, it works as expected. I have exposed it like: ``` @Configuration public class ToolConfiguration { @Bean public ToolCallbackProvider tools(FileSystemService fileSystemService) { return MethodToolCallbackProvider.builder() .toolObjects(fileSystemService) .build(); } } ``` My configurations include: ``` # Spring configurations spring: application: name: mcp-filesystem-server # Spring AI configurations ai: mcp: server: enabled: true stdio: false name: mcp-filesystem-server version: 0.0.1 resource-change-notification: true tool-change-notification: true prompt-change-notification: true sse-endpoint: \/api\/v1\/sse sse-message-endpoint: \/api\/v1\/mcp type: async ``` Client Implementation ``` spring: application: name: mcp-client-x # Spring AI configurations ai: # Azure OpenAI configurations azure: openai: chat: options: deployment-name: gpt-4o-mini endpoint: https:\/\/ai-foundry-deployment.openai.azure.com\/ api-key: bsbfldsablf embeddings: options: deployment-name: text-embedding-ada-002 # MCP Client to Server Connection Configuration mcp: client: enabled: true name: mcp-client-x version: 0.0.1 initialized: true request-timeout: 20s root-change-notification: true type: async toolcallback.enabled: true sse: connections: server1: url: http:\/\/localhost:8018 sse-endpoint: \/api\/v1\/sse ``` From the logs I can confirm that Client connects with the Server. Server received the request and connects it. My ChatClient configurations for ASYNC workflow ``` @Configuration public class ModelConfiguration { @Bean public ChatClient chatClient(AzureOpenAiChatModel azureOpenAiChatModel, List<ToolCallback> mcpAsyncToolCallbacks) { return ChatClient.builder(azureOpenAiChatModel) .defaultToolCallbacks(mcpAsyncToolCallbacks) .build(); } } ``` Calling LLM ``` @Slf4j @Service @RequiredArgsConstructor public class ChatServiceAzureOpenAi implements ChatService { private final ChatClient chatClient; @Override public Flux<String> process(UserPrompt userPrompt) { log.info(\"User prompt: {}\", userPrompt); var SYSTEM_PROMPT = \"\"\" You are a helpful assistant. Your task is to perform the task based on the user's prompt while making use of the available tools. Message: {message} \"\"\"; PromptTemplate promptTemplate = PromptTemplate.builder() .template(SYSTEM_PROMPT) .build(); Prompt prompt = promptTemplate.create(Map.of(\"message\", userPrompt.getMessage())); return chatClient .prompt(prompt) .stream() .content(); } } ``` I'm unable to figure out why there is no communication between the Server from the client. Is there something I have missed? Can someone help me figure out what could be the cause of this. EDIT 1: In the logs I can see ``` 2025-08-05T09:54:26.443+05:30 DEBUG 92639 --- [mcp-client-x] [ctor-http-nio-2] io.modelcontextprotocol.spec.McpSchema : Received JSON message: {\"jsonrpc\":\"2.0\",\"id\":\"8e7ffaaf-1\", \"result\":{\"tools\": [{***\"name\":\"Create File\", \"description\":\"Create a file with the provided fileName on the file system\",*** \"inputSchema\": {\"type\":\"object\",\"properties\":{\"arg0\":{\"type\":\"string\"}},\"required\":[\"arg0\"],\"additionalProperties\":false}}]}} ```",
    "author_id":5427,
    "publication_date":1754372595000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Gagan",
    "author_reputation":141.0,
    "tags":"model-context-protocol, spring-webflux, asynchronous, spring-ai, azure-ai-foundry",
    "text_length":3765,
    "title_length":52,
    "num_tags":5
  },
  {
    "id":5904,
    "title":"Why does require become require2 at runtime inside Vite&#39;s internal worker function?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725581\/why-does-require-become-require2-at-runtime-inside-vites-internal-worker-functi",
    "text":"Why does ``` require ``` become ``` require2 ``` at runtime inside Vite's internal worker function? I'm using Vite (v7.0.6) to build a JavaScript library via Gulp . While debugging an issue during the build process, I discovered that Vite’s internal worker function ends up referencing ``` require2 ``` at runtime , even though the original source clearly uses ``` require ``` . 🔍 Problem Inside Vite’s code (at build time), the worker function is written like this: ``` const makeWorker = () => new WorkerWithFallback(() => async (terserPath, code, options) => { const terser = require(terserPath); \/\/ <-- original source return terser.minify(code, options); }, { shouldUseFake() { ... }, max: maxWorkers }); ``` However, during execution, I log the ``` fn.toString() ``` passed to ``` eval() ``` in ``` WorkerWithFallback ``` , and I see: ``` const terser = require2(terserPath); \/\/ <-- ❗ becomes require2 ``` Then, at runtime, this throws: ``` ReferenceError: require2 is not defined ``` This means that somewhere between bundling and evaluation , the ``` require(...) ``` call was transformed into ``` require2(...) ``` . 🧠 My Analysis The original Vite source uses ``` require(...) ``` But at runtime , ``` eval(fn.toString()) ``` shows ``` require2(...) ``` This happens inside Vite's compiled chunk file: ``` node_modules\/.pnpm\/vite@7.0.6_...\/vite\/dist\/node\/chunks\/dep-BHkUv4Z8.js ``` I'm not modifying the code myself So it appears Vite (or its bundler, like Rollup or esbuild) is transforming ``` require ``` to ``` require2 ``` during its own build process However, ``` require2 ``` is not defined in the runtime context , so this results in a fatal error ❓ What I Want to Understand Why is ``` require ``` transformed to ``` require2 ``` during Vite’s own bundling? Where is ``` require2 ``` supposed to be defined? Is this chunk not meant to be directly executed in Node.js via eval? How can I safely invoke ``` vite.build() ``` programmatically without hitting this? Is this a bug, or am I misusing Vite internals in an unsupported way? Logging ``` fn.toString() ``` before it's passed to ``` eval() ``` — confirms that ``` require ``` becomes ``` require2 ``` Searching Vite and plugin code for ``` require2 ``` — no direct match found Trying to manually patch it with ``` global.require2 = require ``` — works, but a hack Reviewing Rollup\/esbuild output options — didn’t find config that explains this rewrite",
    "author_id":5426,
    "publication_date":1754372958000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MyPledge",
    "author_reputation":1.0,
    "tags":"typescript, javascript, node.js, vite, gulp",
    "text_length":2424,
    "title_length":87,
    "num_tags":5
  },
  {
    "id":5903,
    "title":"GPS satellite position calculation from RINEX navigation file produces unexpected ECI\/ECEF plots",
    "link":"https:\/\/stackoverflow.com\/questions\/79725584\/gps-satellite-position-calculation-from-rinex-navigation-file-produces-unexpecte",
    "text":"For a college project I have to calculate the position of GPS satellites for a period of time with navigation data. There may be or 4 or 6 hours in a given navigation file - it depends on number of epochs for a specific GPS. The more epoch the more time. I must calculate the GPS position over 6 hours with a navigation file. I have to compute GPS satellite positions for a period of 6 hours using a RINEX 3.02 navigation file. I wrote code to parse the navigation data and compute satellite positions in both ECI and ECEF frames. After days of working to read and save ``` rinex ``` navigation files I finally managed to calculate satellite position, and plotted positions (x, y, z in both eci and ecef). However, when I plot the positions over time, the satellite trajectory looks incorrect or chaotic, like this: What I expect is a smooth orbital path like this: Some key details: I compute ``` tk = t - toe ``` , where ``` toe ``` is the time of ephemeris and ``` t ``` is seconds of week for the epoch of interest. I attempted using the nearest ephemeris for each time epoch. When I fix the navigation data to only one epoch and calculate for all times using that, the shape looks better (still not perfect). My main issue is that if I compute the satellite positions using only one ephemeris epoch (within about ±2 hours of its reference time), the results are correct. This makes sense because the navigation parameters are valid for roughly that time span. However, for a 6‑hour time span or more, I need to use multiple ephemeris epochs . My approach has been: for each computation time (in my code I store this as ``` te ``` — time of emission), I first find the nearest ephemeris epoch to ``` te ``` and then use that epoch’s navigation parameters. But when I do this, the trajectory becomes distorted (like the first plot I uploaded). Is there a correct way to handle switching between multiple ephemeris epochs, or a standard method to avoid these distortions? What could be causing such distortion? Is it a mistake in handling ``` tk ``` , or do I need to interpolate parameters more carefully? Here's the core part of my code: ``` def calc_tk(t, toe): tk = t - toe if tk > 302400: tk -= 604800 elif tk < -302400: tk += 604800 return tk # satellite position calculation here toe_list = np.array(list(nav_data[prn].keys())) toe = toe_list[np.argmin(np.abs(te - toe_list))] #using this calculate nearest epoch ``` First two ephemeris records from my RINEX file: Direct link to full RINEX file: https:\/\/github.com\/shahriar-nzz\/shahriar-repository\/blob\/main\/GODS00USA_R_20240010000_01D_GN.rnx ``` 3.04 N: GNSS NAV DATA G: GPS RINEX VERSION \/ TYPE JPS2RIN v.2.1.223 JAVAD GNSS 20240102 000924 UTC PGM \/ RUN BY \/ DATE 18 LEAP SECONDS GODS MARKER NAME COMMENT 40451M128 MARKER NUMBER COMMENT 1130752.1541 -4831349.1034 3994098.9626 COMMENT This data is provided as a public service by NASA\/JPL. COMMENT No warranty is expressed or implied regarding suitability COMMENT for use. For further information, contact: COMMENT ggnops at jpl dot nasa dot gov COMMENT END OF HEADER G07 2024 01 01 01 59 44-2.613384276628D-05-9.436007530894D-12 0.000000000000D+00 4.400000000000D+01 1.200000000000D+01 4.854845080927D-09 6.239840220677D-01 8.177012205124D-07 1.773746171966D-02 8.910894393921D-06 5.153683597565D+03 9.358400000000D+04-2.365559339523D-07 1.512483407131D+00 1.322478055954D-07 9.498578426946D-01 2.058125000000D+02-2.159927596900D+00-8.354633718315D-09 2.221521106700D-10 1.000000000000D+00 2.295000000000D+03 0.000000000000D+00 2.000000000000D+00 0.000000000000D+00-1.117587000000D-08 4.400000000000D+01 8.649000000000D+04 4.000000000000D+00 G01 2024 01 01 02 00 00 1.650028862059D-04 7.958078640513D-13 0.000000000000D+00 7.700000000000D+01 7.565625000000D+01 3.961593587703D-09 1.254732735329D+00 3.958120942116D-06 1.305459404830D-02 6.889924407005D-06 5.154026269913D+03 9.360000000000D+04-2.197921276093D-07-1.609739761974D+00 1.154839992523D-07 9.903301560032D-01 2.633125000000D+02 9.986915388941D-01-8.050692486514D-09 1.357199389945D-10 1.000000000000D+00 2.295000000000D+03 0.000000000000D+00 2.000000000000D+00 6.300000000000D+01 5.122274000000D-09 7.700000000000D+01 8.649000000000D+04 4.000000000000D+00 ``` My code (full repo): https:\/\/github.com\/shahriar-nzz\/shahriar-repository\/tree\/main",
    "author_id":5425,
    "publication_date":1754373054000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"shahriar.n",
    "author_reputation":1.0,
    "tags":"python, gps, positioning, geodesic-sphere, satellite-navigation",
    "text_length":4316,
    "title_length":96,
    "num_tags":5
  },
  {
    "id":5902,
    "title":"How to get pasted content in the Monaco Editor?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725585\/how-to-get-pasted-content-in-the-monaco-editor",
    "text":"I'm trying to retrieve the content when people paste in the Monaco editor. How can I do this? When I console.log() and paste something like \"aaaa\" in the text editor, I get the image below the code and nothing comes out I tried doing this, but it doesn't work every time I paste something; it just shows empty, or it shows the clipboardData as being empty... I know the Monaco Editor captures the paste events internally, so I was wondering how I can see what is being pasted into a Monaco Editor. ``` editor.onDidPaste((e) => { const pastedText = e.clipboardEvent?.clipboardData?.getData(\"text\/plain\") ?? \"\"; console.log(pastedText) }); ```",
    "author_id":5424,
    "publication_date":1754373210000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"SirBillyBobJoe",
    "author_reputation":1.0,
    "tags":"reactjs, javascript, monaco-editor, react-monaco-editor",
    "text_length":641,
    "title_length":47,
    "num_tags":4
  },
  {
    "id":5901,
    "title":"TS7009: &#39;new&#39; expression, whose target lacks a construct signature when using keycloak-js in React + TypeScript (Keycloak JS)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725588\/ts7009-new-expression-whose-target-lacks-a-construct-signature-when-using-ke",
    "text":"I'm integrating Keycloak into my React TypeScript app using the official keycloak-js package. However, I'm getting the following TypeScript error when I try to initialize the Keycloak instance: ``` TS7009: 'new' expression, whose target lacks a construct signature, implicitly has an 'any' type. ``` Here's the relevant part of my code: ``` \/\/ keycloak.ts import Keycloak, { KeycloakInstance } from 'keycloak-js'; const keycloak: KeycloakInstance = new Keycloak({ url: 'http:\/\/localhost:8080\/auth', realm: 'my-realm', clientId: 'my-client-id', }); export default keycloak; ``` Even I tried below code with type: ``` import Keycloak, { KeycloakInstance } from 'keycloak-js'; const keycloak: KeycloakInstance = new Keycloak({ url: 'http:\/\/localhost:8080\/auth', realm: 'your-realm', clientId: 'your-client-id', }); ``` What I've tried: Installed the package: ``` npm install keycloak-js ``` Tried adding types: ``` npm install --save-dev @types\/keycloak-js ``` -> But it says no type definitions available Used KeycloakInstance explicitly Checked Keycloak version (I'm using keycloak-js@26.0.0) What I want to know: How do I properly initialize Keycloak in a React TypeScript project without hitting this TS7009 error? Is there an official type-safe way to do this? Is there any wrapper or workaround recommended? Environment: React 19 TypeScript keycloak-js 26.2.0 Any help would be greatly appreciated. Thanks!",
    "author_id":5423,
    "publication_date":1754373459000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Akshay Jain",
    "author_reputation":788.0,
    "tags":"reactjs, typescript, authentication, keycloak, keycloak-js",
    "text_length":1409,
    "title_length":133,
    "num_tags":5
  },
  {
    "id":5900,
    "title":"How to dynamically import a date-fns language module in an Angular Ionic application",
    "link":"https:\/\/stackoverflow.com\/questions\/79725596\/how-to-dynamically-import-a-date-fns-language-module-in-an-angular-ionic-applica",
    "text":"I am replacing ``` moment ``` with ``` date-fns ``` in my Ionic Angular project. I run with many possible languages, so did not want to statically import them all, and make the bundle bigger. I am trying the following... ``` private async setLocale(lang: string): Promise<void> { try { if (!lang) { return; } const localeMap: Record<string, string> = { en: 'enAU', \/\/ eslint-disable-next-line @typescript-eslint\/naming-convention 'en-US': 'enUS', \/\/ eslint-disable-next-line @typescript-eslint\/naming-convention 'en-AU': 'enAU', fr: 'fr', de: 'de', }; const localeKey = localeMap[lang] || lang; const localeModule = await import( \/* webpackChunkName: \"date-fns-locale-[request]\" *\/ `date-fns\/locale\/${localeKey}` ); const locale = localeModule.default; setDefaultOptions({ locale }); } catch (error) { this.logger.error( `${MODULE_NAME}.setLocale: Error setting locale to ${lang}: ${Utils.unknownTypeToString( error )}` ); } } ``` However I get the exception.. I am not 100% where the ``` await import ``` is trying to load from; surely not node_modules, which will not be present in the deployed bundles. The string I am passing in us just ``` en-US ``` . Anyone know what could be wrong here, or how I can diagnose? Thanks in advance",
    "author_id":4729,
    "publication_date":1754375005000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"peterc",
    "author_reputation":7973.0,
    "tags":"angular, date-fns",
    "text_length":1235,
    "title_length":84,
    "num_tags":2
  },
  {
    "id":5899,
    "title":"DB2 setup TSAMP cant find resource",
    "link":"https:\/\/stackoverflow.com\/questions\/79725598\/db2-setup-tsamp-cant-find-resource",
    "text":"I have two DB2 version 10.5 machines, one physical and one virtual. I’ve set up HADR on them, and they are in sync. When trying to set up TSAMP on the standby machine using the db2haicu command, after creating the domain and specifying the gateway as the quorum, I get a segmentation fault error. In the diagnostic log, I see the error 2610-407 No resources found. The database is at Fix Pack 10, TSAMP version is 4.1, Operating system is CentOS 7. The same machine was part of a two-node physical HA setup with TSAMP. We virtualized one of the machines (fresh install at the OS and database layer + backup restore + HA setup). Finally, during the TSAMP setup phase, I’m getting this error. Can anyone provide guidance?",
    "author_id":5422,
    "publication_date":1754375213000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Evan",
    "author_reputation":1.0,
    "tags":"db2",
    "text_length":719,
    "title_length":34,
    "num_tags":1
  },
  {
    "id":5898,
    "title":"how to finish Playwright Run tests as it continuously loading in case of failed test case in pipeline &#39;http:\/\/localhost:9323. Press Ctrl+C to quit",
    "link":"https:\/\/stackoverflow.com\/questions\/79725600\/how-to-finish-playwright-run-tests-as-it-continuously-loading-in-case-of-failed",
    "text":"When I execute test cases which will result pass test cases azure pipeline successfully run and show results under a min. when I enabled failed test cases azure pipeline keeps on loading- until I turned off. Please see the below screenshots Screen 1 -Related to successful execution only with pass test cases and 2 skip test cases which are failed. screen 2 - Related to failed test case - one is passed and one is fail. after showing result localhost link showing and than 'Run Playwright tests with report' keeps running. 'Serving HTML report at http:\/\/localhost:9323. Press Ctrl+C to quit' above statement is showing and than job is keeping running in pipeline. Here is my yml file ``` ``` ``` trigger: - main pool: name: MyPool steps: # Step 1: Install Node.js - task: NodeTool@0 inputs: versionSpec: '16.x' displayName: 'Install Node.js' # Step 2: Install npm dependencies - script: npm install displayName: 'Install npm packages' # Step 3: Install Playwright browsers - script: npx playwright install displayName: 'Install Playwright browsers' - script: npx playwright test --reporter=list,html displayName: 'Run Playwright tests with report' # Publish the Playwright report as artifact - task: PublishPipelineArtifact@1 inputs: targetPath: 'playwright-report' artifact: 'playwright-report' publishLocation: 'pipeline' displayName: 'Publish Playwright report' ``` Please any one suggest how to execute failed test cases locally- or test run should close like for passed test cases.",
    "author_id":5421,
    "publication_date":1754375272000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Zeeshan ilyas",
    "author_reputation":13.0,
    "tags":"playwright, azure-devops, azure-pipelines, automated-tests, yaml",
    "text_length":1487,
    "title_length":150,
    "num_tags":5
  },
  {
    "id":5897,
    "title":"On Windows Terminal, in Visual C++, how can I overwrite previous lines?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725603\/on-windows-terminal-in-visual-c-how-can-i-overwrite-previous-lines",
    "text":"I written a C++ program that search \"text\" in all files of a folder and his subfolder. When search is running, program displays current directories and name of file currently scanned. When file is matching \"text\" to search, program displays lines found in following rows of console. When not, filename is \"deleted\" and cursor is positionned at place of deleted filename. Example of running display: This can be easily done using following C++ code ``` HANDLE hStdout = GetStdHandle(STD_OUTPUT_HANDLE); COORD oCurrentCursorCoord; \/\/ GetConsoleCursorPosition(); CONSOLE_SCREEN_BUFFER_INFO infoConsole; GetConsoleScreenBufferInfo(hStdout,&infoConsole); oCurrentCursorCoord = infoConsole.dwCursorPosition; cout << sScannedFileName << endl; \/\/ ... search text in current file \/\/ if file is matching some text if (bTextInFile) { SetConsoleCursorPosition(hStdout,oCurrentCursorCoord); \/\/delete previous line or, in reality, previous row ! wprintf(L\"\\x1b[1M\"); } ``` This work \"perfectly\" until cursor is on last line of console ! When this happens, cursor position saved just before scanned filename is displayed is not more correct ! Saved ``` Y ``` position must be decremented by 1 to reflect new reality. I can calculate correct previous cursor position using following code ``` oCurrentCoord.Y -= 1; SetConsoleCursorPosition(oCurrentCoord); ``` But this stop to work for lines greater than console width (> 100 in my situation) ! In this case, the decrementation value can be greater than 1. The previous code can also works if I can get coordinate of first console's row in console data buffer ! The code would be ``` HANDLE hStdout = GetStdHandle(STD_OUTPUT_HANDLE); COORD oCurrentCursorCoord; CONSOLE_SCREEN_BUFFER_INFO infoConsole; -> CONSOLE_BUFFER_INFO infoBuffer; \/\/new \/\/ GetConsoleCursorPosition(); GetConsoleScreenBufferInfo(hStdout,&infoConsole); oCurrentCursorCoord = infoConsole.dwCursorPosition; -> GetConsoleBufferInfo(hStdout,&infoBuffer); \/\/new (not existe in Console Terminal API) -> oCurrentCursorCoord.Y += infoBuffer.dwFirstRowPosition.Y; \/\/new cout << sScannedFileName << endl; \/\/ ... search text in current file \/\/ if file is matching some text if (bTextInFile) { -> GetConsoleBufferInfo(hStdout,&infoBuffer); \/\/new -> oCurrentCursorCoord.Y -= infoBuffer.dwFirstRowPosition.Y; \/\/new SetConsoleCursorPosition(hStdout,oCurrentCursorCoord); \/\/delete previous line or, in reality, previous row ! wprintf(L\"\\x1b[1M\"); } ``` where added lines have been prefixed by ``` -> ``` string. In console Windows API, how can I get position of first displayed row in console relatively to output data buffer ? Definitions for this question OUTPUT-DATA-BUFFER contains all rows that are and that were displayed on Console. OUTPUT-SCREEN-BUFFER contains only all rows currently displayed on Console. I have already a solution for my program but it is tricky and limited to what I display. I post this question to found a proper solution. PS: this program works only if following code is added at begin of program. ``` DWORD dwMode = 0; GetConsoleMode(hStdout, &dwMode); dwMode |= ENABLE_VIRTUAL_TERMINAL_PROCESSING; SetConsoleMode(hStdout, dwMode); ``` For those that are interesting, the tricky code is following ``` void DeletePreviousPrintedLine() { HANDLE hStdout = GetStdHandle(STD_OUTPUT_HANDLE); XCharPtr szBuf(\" \"); char buf[6]; COORD coord = { 0,0 }; unsigned long iNrCharRead; CONSOLE_SCREEN_BUFFER_INFO infoConsole; while (true) { \/\/goto start of previous line(in reality it is previous row) wprintf(L\"\\x1b[1F\"); \/\/get cursor coordinate of new position GetConsoleScreenBufferInfo( hStdout, &infoConsole); \/\/get first 6 characters at begin of line ReadConsoleOutputCharacter(hStdout, buf, 6, infoConsole.dwCursorPosition, &iNrCharRead); \/\/delete all character of current row wprintf(L\"\\x1b[1M\"); szBuf.Assign(buf,6); if (szBuf.StartsWith(\">>>> +\")) { break; } } } ``` where ``` XCharPtr ``` is a String helper for ``` char* ``` that I use to avoid to use complicated string manipulation defined in C langage.",
    "author_id":5420,
    "publication_date":1754375708000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"schlebe",
    "author_reputation":3823.0,
    "tags":"visual-c++, console, display, overwrite",
    "text_length":4021,
    "title_length":71,
    "num_tags":4
  },
  {
    "id":5896,
    "title":"Update to: Generate a file path based on dynamic cell data",
    "link":"https:\/\/stackoverflow.com\/questions\/79725606\/update-to-generate-a-file-path-based-on-dynamic-cell-data",
    "text":"I have an invoicing file for a small company with about 20 customers. Each customer has their own ID# and invoicing is once a month. The Excel file has three worksheets: one that generates an invoice based on ID# and date by filling in the appropriate information from the other two worksheets containing customer info (name, address, email) and monthly activity (fees & payments) for each customer by using a combination of VLOOKUP and INDEX MATCH. The invoice date self generates once a month on a specific date. Creating the invoices works great--just enter the ID# and an entire invoice is generated for that month's date for that customer. Creating a macro to name the invoice as .pdf based on the invoice date and ID# is not a problem, the problem is each invoice will have its own path because each customer has their record of invoices stored in its own folder. If all invoices were stored in one folder (one path) then after a few years the one folder will have 20 x 12 x # of years; a nightmare to retrieve when needed. Instead, invoices have: ``` --path C:\\Invoices\\Customer 01\\2025\\Invoice Date--Customer ID#.pdf --path C:\\Invoices\\Customer 02\\2025\\Invoice Date--Customer ID#.pdf ``` etc. Then when the year changes the path overall changes: ``` --path C:\\Invoices\\Customer 01\\2026\\Invoice Date--Customer ID#.pdf --path C:\\Invoices\\Customer 02\\2026\\Invoice Date--Customer ID#.pdf ``` So I'm wondering if VBA can, in addition to generating a file name, generate a path based on Invoice Date and ID# Is it as simple as something like declaring that ``` path = C:\\Invoices\\Customer ID\\Date(Year)\\Invoice Date--customer ID#.pdf ``` Thank you Tim for your feedback, as per your suggestion, here is what I'm trying. I know there needs to be in the macro ``` =format date(yyyy) for the path ``` and ``` =format date(mm-dd-yy) for the file name ``` Though I'm not certain on just how and where to apply thes in the macro for proper results. Here's my code so far using the date 7\/12\/25 and member number 345B just for example on intent. ``` Sub SaveInvoicesAs_pdf --Dim InvoiceDate As String --Dim FileName As String --Dim Member As String --Dim Path As String --InvoiceDate = Range(\"H13\") ‘ The date format of the invoice worksheet displays as Jul 12, 2025 but to best save and sequence in the records folder the macro needs to take the date from cell H13 and format it as 07-12-25 for the file name --Member = Range(\"C13\") --Path = \"C:\\Invoice Records\\Member[C13]\\InvoiceDate[H13]\\\" ‘Here the path, after the Invoice Records folder (which is a static branch in the path), looks to cell C13 to follow that branch, then looks to cell H13 for the final branch but retrieves only the year to be formatted as yyyy. So the path generated would be C:\\Invoice Records\\345A\\2025\\ --FileName = Invoice Date[H13] & “---” & Member[C13} ‘Both for the path and the file name I’m uncertain how to take the date in H13 which is displayed as Jul 12, 2025 and have the macro set it as 2025 at the end of the path and 07-12-25 for the start of the file name and it is generated as 07-12-25---345B --activesheet.exportasfixedformat type:= xlTypePDF, ignoreprintareas:= false, File:= Path & Filename End Sub ``` I hope this better explains what I'm trying to do. Thanks",
    "author_id":5419,
    "publication_date":1754375966000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Winston Smith",
    "author_reputation":11.0,
    "tags":"vba, excel",
    "text_length":3254,
    "title_length":58,
    "num_tags":2
  },
  {
    "id":5895,
    "title":"Custom web server &lt;video&gt; tag plays audio but not video",
    "link":"https:\/\/stackoverflow.com\/questions\/79725614\/custom-web-server-video-tag-plays-audio-but-not-video",
    "text":"I am making a small custom web server that serves videos. I have an HTML ``` <video> ``` tag that displays the correct video length and plays the audio, but the video itself does not play. HTML snippet: ``` <video width=\"320\" height=\"240\" controls> <source src=\"video.mp4\" type=\"video\/mp4\"> <\/video> ``` Actual result on the webpage: Request: ``` GET \/video.mp4 HTTP\/1.1 Host: localhost User-Agent: Mozilla\/5.0 (X11; Linux x86_64; rv:133.0) Gecko\/20100101 Firefox\/133.0 Accept: video\/webm,video\/ogg,video\/*;q=0.9,application\/ogg;q=0.7,audio\/*;q=0.6,*\/*;q=0.5 Accept-Language: en-US,en;q=0.5 Range: bytes=0- Connection: keep-alive Referer: https:\/\/localhost\/cinema Sec-Fetch-Dest: video Sec-Fetch-Mode: no-cors Sec-Fetch-Site: same-origin Accept-Encoding: identity Priority: u=4 ``` Response: ``` HTTP\/1.1 206 Partial Content x-powered-by: Dart with package:shelf date: Tue, 05 Aug 2025 04:42:55 GMT accept-ranges: bytes content-length: 25111615 content-type: video\/mp4 x-frame-options: SAMEORIGIN x-xss-protection: 1; mode=block x-content-type-options: nosniff content-range: 0-25111614\/25111615 ``` Video and audio codecs: Is there anything wrong with these headers? Maybe the Content-Range and\/or Content-Length properties? I have verified that the video plays locally, so I haven't investigated whether this specific video is corrupt or has an incompatible codec. The audio plays and the video length displays correctly, so I believe the actual data transmission is in tact and well formed. The video file is under 30MB, the file is served from localhost, and I waited a sufficient amount of time, so I would think the entire video could be \"downloaded\" even if the headers are malformed in a way that would try to send the entire video file at once.",
    "author_id":5418,
    "publication_date":1754376954000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"donotread123",
    "author_reputation":133.0,
    "tags":"http, html, video, webserver",
    "text_length":1753,
    "title_length":61,
    "num_tags":4
  },
  {
    "id":5894,
    "title":"Firebase Realtime Database onValue listener triggers repeatedly on iOS without data changes in Flutter",
    "link":"https:\/\/stackoverflow.com\/questions\/79725615\/firebase-realtime-database-onvalue-listener-triggers-repeatedly-on-ios-without-d",
    "text":"I'm using Firebase Realtime Database in a Flutter app and listening for updates using the ``` onValue ``` stream. Here's the code I’m using: ``` _plansSubscription = _plansRef .orderByChild('userId') .equalTo(userId) .onValue .listen((DatabaseEvent event) { \/\/ Handle data }); ``` This works correctly on Android , where the listener is only triggered when actual changes happen in the data. However, on iOS , the same listener is being called multiple times , even when no changes are made to the data. This is causing unnecessary processing and inconsistent app behavior. I’m using this logic inside a Provider , not directly in a widget, and the listener is only set up once during provider initialization. What I’ve verified: Firebase is initialized properly in ``` main() ``` using: await Firebase.initializeApp(); The ``` GoogleService-Info.plist ``` file is correctly added to the Xcode project. iOS ``` Info.plist ``` has the required network permissions: ``` <key>NSAppTransportSecurity<\/key> <dict> <key>NSAllowsArbitraryLoads<\/key> <true\/> <\/dict> ``` Firebase rules in the console allow public access for testing: ``` { \"rules\": { \".read\": \"true\", \".write\": \"true\" } } ``` The listener is not duplicated. It's disposed of properly in the provider: Why does the ``` onValue ``` listener in Firebase Realtime Database get triggered repeatedly on iOS , even when there are no actual data changes ? How can I prevent this or handle it more efficiently?",
    "author_id":5417,
    "publication_date":1754377074000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user31206466",
    "author_reputation":11.0,
    "tags":"ios, flutter, firebase, firebase-realtime-database",
    "text_length":1460,
    "title_length":102,
    "num_tags":4
  },
  {
    "id":5893,
    "title":"JAXB (glassfish): Cannot invoke &quot;org.glassfish.jaxb.runtime.v2.runtime.Transducer.useNamespace()&quot; because &quot;xducer&quot; is null",
    "link":"https:\/\/stackoverflow.com\/questions\/79725616\/jaxb-glassfish-cannot-invoke-org-glassfish-jaxb-runtime-v2-runtime-transduce",
    "text":"I would like to request your help on a NPE problem with a rest service on glassfish 7.0.25 utilizing JAXB to generate a xml response when I include a List into a parent object. My entity class that I want to output an xml representation for: ``` package de.gfrev.conmanager.entities; import jakarta.persistence.Basic; import jakarta.persistence.CascadeType; import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.NamedQueries; import jakarta.persistence.NamedQuery; import jakarta.persistence.OneToMany; import jakarta.persistence.Table; import jakarta.xml.bind.annotation.XmlAccessType; import jakarta.xml.bind.annotation.XmlAccessorType; import jakarta.xml.bind.annotation.XmlAttribute; import jakarta.xml.bind.annotation.XmlRootElement; import jakarta.xml.bind.annotation.XmlTransient; import jakarta.xml.bind.annotation.XmlType; import java.io.Serial; import java.util.ArrayList; import java.util.List; @Entity @Table(name = \"ESERIES\") @NamedQueries({ @NamedQuery(name = \"getEventSeriesById\", query = \"SELECT o FROM EventSeries o WHERE o.id = :id\"), @NamedQuery(name = \"getEventSeriesByName\", query = \"SELECT o FROM EventSeries o WHERE o.name LIKE :name\"), @NamedQuery(name = \"getAllEventSerieses\", query = \"SELECT o FROM EventSeries o\") }) @XmlRootElement(name = \"eventseries\", namespace = \"http:\/\/gfrev.de\/conmanager\/eventseries\") @XmlType(propOrder = {\"id\", \"name\", \"version\", \"comments\"}, namespace = \"http:\/\/gfrev.de\/conmanager\/eventseries\") @XmlAccessorType(XmlAccessType.FIELD) public class EventSeries extends AbstractCommentable { @Serial private static final long serialVersionUID = 1L; @Basic(optional = false) @Column(name = \"NAME\", table = \"ESERIES\", unique = true, insertable = true, nullable = false, updatable = true, length = 100) @XmlAttribute(name = \"name\", required = true) private String name; @OneToMany(mappedBy = \"series\", orphanRemoval = false, targetEntity = Event.class, cascade = CascadeType.ALL) @XmlTransient \/\/ @XmlElementWrapper(name = \"events\", nillable = true, required = false) \/\/ @XmlElement(name = \"event\") private List<Event> events; public String getName() { return name; } public void setName(String name) { this.name = name; } public List<Event> getEvents() { if (events == null) { events = new ArrayList<>(); } return events; } public void setEvents(List<Event> events) { this.events = events; } public void addEvent(Event event) { getEvents().add(event); event.setSeries(this); } public void removeEvent(Event event) { getEvents().remove(event); event.setSeries(null); } @Override public String toString() { return \"EventSeries{\" + \" name=\" + name + '}'; } } ``` which extends an abstract class ``` package de.gfrev.conmanager.entities; import jakarta.persistence.CascadeType; import jakarta.persistence.Entity; import jakarta.persistence.Inheritance; import jakarta.persistence.InheritanceType; import jakarta.persistence.OneToMany; import jakarta.persistence.Table; import jakarta.xml.bind.annotation.XmlAccessType; import jakarta.xml.bind.annotation.XmlAccessorType; import jakarta.xml.bind.annotation.XmlElement; import jakarta.xml.bind.annotation.XmlElementWrapper; import jakarta.xml.bind.annotation.XmlTransient; import java.io.Serial; import java.util.ArrayList; import java.util.List; @Entity @Table(name = \"ABSTRCOMMENTABLE\") @Inheritance(strategy = InheritanceType.JOINED) @XmlTransient @XmlAccessorType(XmlAccessType.FIELD) public abstract class AbstractCommentable extends AbstractPersistable implements Commentable { @Serial private static final long serialVersionUID = 1L; @OneToMany(mappedBy = \"commentable\", orphanRemoval = false, targetEntity = Comment.class, cascade = CascadeType.ALL) @XmlElementWrapper(name = \"comments\", nillable = true, required = false) @XmlElement(name = \"comment\") private List<Comment> comments = new ArrayList<>(); @Override public List<Comment> getComments() { if (comments == null) { comments = new ArrayList<>(); } return comments; } @Override public void setComments(List<Comment> comments) { this.comments = comments; } public void addComment(Comment comment) { getComments().add(comment); comment.setCommentable(this); } public void removeComment(Comment comment) { getComments().remove(comment); comment.setCommentable(null); } } ``` The Comment Entity class: ``` package de.gfrev.conmanager.entities; import jakarta.persistence.Basic; import jakarta.persistence.CascadeType; import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.FetchType; import jakarta.persistence.JoinColumn; import jakarta.persistence.ManyToOne; import jakarta.persistence.Table; import jakarta.persistence.Temporal; import jakarta.persistence.TemporalType; import jakarta.xml.bind.annotation.XmlAttribute; import jakarta.xml.bind.annotation.XmlElement; import jakarta.xml.bind.annotation.XmlRootElement; import jakarta.xml.bind.annotation.XmlTransient; import jakarta.xml.bind.annotation.XmlType; import java.io.Serial; import java.util.Date; @Entity @Table(name = \"COMMENT\") @XmlRootElement(name = \"comment\", namespace = \"http:\/\/gfrev.de\/conmanager\/comment\") @XmlType(propOrder = {\"id\", \"version\", \"date\", \"text\", \"commentable\"}, namespace = \"http:\/\/gfrev.de\/conmanager\/comment\") public class Comment extends AbstractPersistable { @Serial private static final long serialVersionUID = 1L; @Basic(optional = false) @Column(name = \"DATE\", table = \"COMMENT\", unique = false, insertable = true, nullable = false, updatable = true) @Temporal(TemporalType.TIMESTAMP) @XmlAttribute(name = \"date\", required = true) private Date date; @Basic(optional = false) @Column(name = \"TEXT\", table = \"COMMENT\", unique = false, insertable = true, nullable = false, updatable = true, length = 2048) @XmlElement(name = \"text\", required = true) private String text; @ManyToOne(fetch = FetchType.LAZY, optional = false, targetEntity = User.class, cascade = CascadeType.ALL) \/\/ @JoinColumn(name=\"USER_ID\", nullable=false) \/\/ @XmlElement(name = \"user\", required = true) @XmlTransient private User user; @ManyToOne(fetch = FetchType.LAZY, optional = false, targetEntity = AbstractCommentable.class, cascade = CascadeType.ALL) @JoinColumn(name=\"COMMENTABLE_ID\", nullable=false) @XmlAttribute(name = \"commentable\", required = true) private AbstractCommentable commentable; public Date getDate() { return date; } public void setDate(Date date) { this.date = date; } public String getText() { return text; } public void setText(String text) { this.text = text; } public User getUser() { return user; } public void setUser(User user) { this.user = user; } public AbstractCommentable getCommentable() { return commentable; } public void setCommentable(AbstractCommentable commentable) { this.commentable = commentable; } } ``` and the Servlet: ``` package de.gfrev.conmanager.application.webapi; import java.math.BigInteger; import java.util.List; import de.gfrev.conmanager.entities.Comment; import de.gfrev.conmanager.entities.EventSeries; import jakarta.ejb.Stateless; import jakarta.persistence.EntityManager; import jakarta.persistence.PersistenceContext; import jakarta.ws.rs.GET; import jakarta.ws.rs.Path; import jakarta.ws.rs.PathParam; import jakarta.ws.rs.Produces; import jakarta.ws.rs.core.Context; import jakarta.ws.rs.core.MediaType; import jakarta.ws.rs.core.UriInfo; @Stateless @Path(\"\/eventserieses\") public class EventSeriesServlet { @Context private UriInfo context; @PersistenceContext(unitName = \"ConManagerPU\") private EntityManager entityManager; public EventSeriesServlet() { \/\/ Auto-generated constructor stub } @GET @Produces({MediaType.TEXT_PLAIN}) @Path(\"\/all\") public String getAllPlain() { StringBuilder output = new StringBuilder(); List<EventSeries> serieses = entityManager.createNamedQuery(\"getAllEventSerieses\", EventSeries.class).getResultList(); output.append(\"# EventSerieses: \").append(serieses.size()).append(\"\\n\"); for (EventSeries series : serieses) { output.append(series.getName()).append(\"\\n # Comments: \").append(series.getComments().size()).append(\"\\n\"); for (Comment comment : series.getComments()) { output.append(\" - \").append(comment.getDate()).append(\" by \").append(comment.getUser().getSignature()).append(\"\\n\"); } } return output.toString(); } @GET @Produces({MediaType.APPLICATION_XML, MediaType.APPLICATION_JSON}) @Path(\"\/all\") public List<EventSeries> getAll() { return entityManager.createNamedQuery(\"getAllEventSerieses\", EventSeries.class).getResultList(); } \/\/ [...] } ``` I also generate some test data, so there is something to output. As long as I annotated the field comments in AbstractCommentable with @XmlTransient instead of @XmlElement and @XmlElementWrapper and did not include it in EventSeries' @XmlType's propOrder, an ...\/all request with Accept application\/xml worked fine. Since I included the comments field as shown above, I get following NPE: ``` [#|2025-08-05T06:05:10.157290+02:00|WARNING|GF 7.0.25|jakarta.enterprise.web.vs.server|_ThreadID=56;_ThreadName=http-listener-1(2);_LevelValue=900;| StandardWrapperValve[de.gfrev.conmanager.application.webapi.WebApi]: Servlet.service() for servlet de.gfrev.conmanager.application.webapi.WebApi threw exception java.lang.NullPointerException: Cannot invoke \"org.glassfish.jaxb.runtime.v2.runtime.Transducer.useNamespace()\" because \"xducer\" is null at org.glassfish.jaxb.runtime.v2.runtime.reflect.TransducedAccessor.get(TransducedAccessor.java:139) at org.glassfish.jaxb.runtime.v2.runtime.property.AttributeProperty.<init>(AttributeProperty.java:59) at org.glassfish.jaxb.runtime.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:74) at org.glassfish.jaxb.runtime.v2.runtime.ClassBeanInfoImpl.<init>(ClassBeanInfoImpl.java:148) at org.glassfish.jaxb.runtime.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:464) at org.glassfish.jaxb.runtime.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:483) at org.glassfish.jaxb.runtime.v2.runtime.property.ArrayElementProperty.<init>(ArrayElementProperty.java:71) at org.glassfish.jaxb.runtime.v2.runtime.property.ArrayElementNodeProperty.<init>(ArrayElementNodeProperty.java:30) at java.base\/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62) at java.base\/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502) at java.base\/java.lang.reflect.Constructor.newInstance(Constructor.java:486) at org.glassfish.jaxb.runtime.v2.runtime.property.PropertyFactory.create(PropertyFactory.java:94) at org.glassfish.jaxb.runtime.v2.runtime.ClassBeanInfoImpl.<init>(ClassBeanInfoImpl.java:148) at org.glassfish.jaxb.runtime.v2.runtime.JAXBContextImpl.getOrCreate(JAXBContextImpl.java:464) at org.glassfish.jaxb.runtime.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:283) at org.glassfish.jaxb.runtime.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1115) at org.glassfish.jaxb.runtime.v2.ContextFactory.createContext(ContextFactory.java:144) at org.glassfish.jaxb.runtime.v2.JAXBContextFactory.createContext(JAXBContextFactory.java:44) at jakarta.xml.bind.ContextFinder.find(ContextFinder.java:373) at jakarta.xml.bind.JAXBContext.newInstance(JAXBContext.java:605) at jakarta.xml.bind.JAXBContext.newInstance(JAXBContext.java:546) at org.glassfish.jersey.jaxb.internal.AbstractJaxbProvider.getStoredJaxbContext(AbstractJaxbProvider.java:291) at org.glassfish.jersey.jaxb.internal.AbstractJaxbProvider.getJAXBContext(AbstractJaxbProvider.java:275) at org.glassfish.jersey.jaxb.internal.AbstractJaxbProvider.getMarshaller(AbstractJaxbProvider.java:242) at org.glassfish.jersey.jaxb.internal.AbstractJaxbProvider.getMarshaller(AbstractJaxbProvider.java:209) at org.glassfish.jersey.jaxb.internal.AbstractCollectionJaxbProvider.writeTo(AbstractCollectionJaxbProvider.java:246) at org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.invokeWriteTo(WriterInterceptorExecutor.java:242) at org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.aroundWriteTo(WriterInterceptorExecutor.java:227) at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139) at org.glassfish.jersey.server.internal.JsonWithPaddingInterceptor.aroundWriteTo(JsonWithPaddingInterceptor.java:85) at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139) at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundWriteTo(MappableExceptionWrapperInterceptor.java:61) at org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:139) at org.glassfish.jersey.message.internal.MessageBodyFactory.writeTo(MessageBodyFactory.java:1116) at org.glassfish.jersey.server.ServerRuntime$Responder.writeResponse(ServerRuntime.java:691) at org.glassfish.jersey.server.ServerRuntime$Responder.processResponse(ServerRuntime.java:398) at org.glassfish.jersey.server.ServerRuntime$Responder.process(ServerRuntime.java:388) at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244) at org.glassfish.jersey.internal.Errors.process(Errors.java:292) at org.glassfish.jersey.internal.Errors.process(Errors.java:274) at org.glassfish.jersey.internal.Errors.process(Errors.java:244) at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:266) at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:253) at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:696) at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:397) at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:349) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:312) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205) at org.apache.catalina.core.StandardWrapper.service(StandardWrapper.java:1368) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:200) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123) at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:563) at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:504) at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:71) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:121) at org.apache.catalina.connector.CoyoteAdapter.doService(CoyoteAdapter.java:293) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:186) at com.sun.enterprise.v3.services.impl.ContainerMapper$HttpHandlerCallable.call(ContainerMapper.java:429) at com.sun.enterprise.v3.services.impl.ContainerMapper.service(ContainerMapper.java:143) at org.glassfish.grizzly.http.server.HttpHandler.runService(HttpHandler.java:174) at org.glassfish.grizzly.http.server.HttpHandler.doHandle(HttpHandler.java:153) at org.glassfish.grizzly.http.server.HttpServerFilter.handleRead(HttpServerFilter.java:196) at org.glassfish.grizzly.filterchain.ExecutorResolver$9.execute(ExecutorResolver.java:88) at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:246) at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:178) at org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:118) at org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:96) at org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:51) at org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:510) at org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:82) at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:83) at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy$WorkerThreadRunnable.run(WorkerThreadIOStrategy.java:101) at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:535) at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:515) at java.base\/java.lang.Thread.run(Thread.java:1583) ``` I wrote also an plain text response (included above) method to check, if the problem is with the data, but that still returns the result as expected. So, what am I doing wrong here? Edit: I should maybe also mention that I use the jaxb2-maven-plugin when building the deployment package. ``` <plugin> <groupId>org.codehaus.mojo<\/groupId> <artifactId>jaxb2-maven-plugin<\/artifactId> <version>3.3.0<\/version> <executions> <execution> <id>schemagen<\/id> <goals> <goal>schemagen<\/goal> <\/goals> <\/execution> <\/executions> <configuration> <sources> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/package-info.java<\/source> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/AbstractPersistable.java<\/source> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/AbstractCommentable.java<\/source> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/EventSeries.java<\/source> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/Event.java<\/source> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/Comment.java<\/source> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/User.java<\/source> <source>src\/main\/java\/de\/gfrev\/conmanager\/entities\/infrastructure\/Location.java<\/source> <\/sources> <transformSchemas> <transformSchema> <uri>http:\/\/gfrev.de\/conmanager\/comment<\/uri> <toPrefix>cmcom<\/toPrefix> <toFile>conmanager-comment.xsd<\/toFile> <\/transformSchema> <transformSchema> <uri>http:\/\/gfrev.de\/conmanager\/eventseries<\/uri> <toPrefix>cmevs<\/toPrefix> <toFile>conmanager-eventseries.xsd<\/toFile> <\/transformSchema> <transformSchema> <uri>http:\/\/gfrev.de\/conmanager\/event<\/uri> <toPrefix>cmevt<\/toPrefix> <toFile>conmanager-event.xsd<\/toFile> <\/transformSchema> <transformSchema> <uri>http:\/\/gfrev.de\/conmanager\/user<\/uri> <toPrefix>cmusr<\/toPrefix> <toFile>conmanager-user.xsd<\/toFile> <\/transformSchema> <transformSchema> <uri>http:\/\/gfrev.de\/conmanager\/location<\/uri> <toPrefix>cmloc<\/toPrefix> <toFile>conmanager-location.xsd<\/toFile> <\/transformSchema> <\/transformSchemas> <verbose>true<\/verbose> <\/configuration> <\/plugin> <\/plugins> ``` Cheers",
    "author_id":5416,
    "publication_date":1754377077000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"TheOneAndOnlyMe",
    "author_reputation":3.0,
    "tags":"jakarta-ee, jaxb, glassfish",
    "text_length":18514,
    "title_length":142,
    "num_tags":3
  },
  {
    "id":5892,
    "title":"How to Configure Garnet Server to Use Installed Certificates for TLS Instead of PFX Files?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725621\/how-to-configure-garnet-server-to-use-installed-certificates-for-tls-instead-of",
    "text":"I have Garnet Server as my L2 Distributed Cache Layer. I am using TLS to secure the Data in transit. I have used PFX file to enable TLS. my configuration of garnet is as under: garnet.conf file content: ``` { \"Port\": 6379, \"LogDir\": \".\/data\/log\", \"CheckpointDir\": \".\/data\/checkpoint\", \"Recover\": true, \"EnableAOF\": true, \"LogLevel\": \"Debug\", \"FileLogger\": \".\/data\/log\/GarnetServer.log\", \"EnableStorageTier\": true, \"EnableTLS\": true, \"CertFileName\": \"my-certificate.pfx\", \"CertPassword\": \"our-secret-password\", \"ClientCertificateRequired\": false, \"CertificateRevocationCheckMode\": \"NoCheck\" } ``` I have tried using ``` \"CertSubjectName\" ``` instead of ``` \"CertFileName\" ``` , but no luck so far. I have also installed ``` .pfx ``` certificate with \"portable\" option on but still no luck. Can someone help step by step or direct me where can I find options so that I can use Garnet Server from installed CERT instead of using ``` .pfx ``` from repository. Please guide.",
    "author_id":5415,
    "publication_date":1754377527000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Creative Learner",
    "author_reputation":33.0,
    "tags":"ssl, pfx, distributed-cache, microsoft-garnet",
    "text_length":969,
    "title_length":90,
    "num_tags":4
  },
  {
    "id":5891,
    "title":"shoppingcontent.products.update is not updating the product",
    "link":"https:\/\/stackoverflow.com\/questions\/79725625\/shoppingcontent-products-update-is-not-updating-the-product",
    "text":"I am trying to update the ``` customLabel0 ``` of the products in my Merchant Center through Google ads script. I have enabled the Shopping Content in the advanced API section. When sending the request, I am getting the product as a response as expected, but the changes are not being reflected in the Merchant Center. I am using the following code to update the ``` customLabel0 ``` : ``` const resource = { 'customLabel0': label }; ShoppingContent.Products.update(resource, merchantId, id); ``` I am getting the following as response: ``` { \"kind\": \"content#product\", \"id\": \"local:fr:-:kar-240w3884-a-black-tu\", \"offerId\": \"kar-240w3884-a-black-tu\", \"contentLanguage\": \"fr\", \"feedLabel\": \"-\", \"channel\": \"local\" } ```",
    "author_id":5414,
    "publication_date":1754377884000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Hussnain Saeed",
    "author_reputation":1.0,
    "tags":"google-workspace, google-ads-script, google-content-api",
    "text_length":719,
    "title_length":59,
    "num_tags":3
  },
  {
    "id":5890,
    "title":"Does sqlparse.token.keyword recognize Top as a keyword?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725626\/does-sqlparse-token-keyword-recognize-top-as-a-keyword",
    "text":"I'm trying this code where I want to convert T-SQL to databricks-SQL and I'm trying to change only the uncommon by first defining the common keywords and check it against the parsed list. I want the code to go to next step of transformation when it encounters the keyword ``` top ``` as its not defined in databricks. but that is not happening. This is my code ``` parsed = sqlparse.parse(sql_query)[0] tokens = TokenList(parsed.tokens).flatten() common_keywords = [\"SELECT\",\"FROM\",\"DISTINCT\",\"WHERE\",\"GROUP BY\",\"ORDER BY\",\"AS\",\"JOIN\",\"VALUES\",\"INSERT INTO\",\"UPDATE\",\"DELETE FROM\",\"SET\",\"COUNT\",\"AVG\",\"MIN\",\"MAX\",\"SUM\"] flag=True for token in tokens: if token.ttype==Keyword: if token.value.upper() not in common_keywords: flag=False break if flag: return sql_query ``` later I'm writing the code for replacing top with limit at the end. but the o\/p of this is the query itself instead of break.",
    "author_id":5413,
    "publication_date":1754377927000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"SAMIBOIII",
    "author_reputation":11.0,
    "tags":"python, databricks, keyword, sql-parser",
    "text_length":895,
    "title_length":55,
    "num_tags":4
  },
  {
    "id":5889,
    "title":"How to specify custom link line\/link order in CMake?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725627\/how-to-specify-custom-link-line-link-order-in-cmake",
    "text":"TL;DR: Is there a way to specify custom link order when CMake managed dependencies seems not enough? I'm developing a toy OS for fun and having trouble linking in the GCC crt* files responsible for calling global constructors. I read that objects must be linked in this exact order : ``` crti.o crtbegin.o mykernel.o crtend.o crtn.o ``` , but don't know how to enforce that exact order using CMake. I'm aware that by ``` target_link_libraries() ``` , CMake automatically manages the link order, but it does not guarantee any specific order , namely put ``` crtbegin.o ``` before and ``` crtend.o ``` after. Currently I'm using the work-around below and it becomes infeasible if you try to link in more libraries. This is my temporary solution: ``` add_library(crti OBJECT path\/to\/crti.s) add_library(crtn OBJECT path\/to\/crtn.s) # the actual target add_executable(awesome_kernel myAwesomeKernel.cc ) add_dependencies(myAwesomeKernel crti crtn) set_property(TARGET myAwesomeKernel APPEND PROPERTY LINK_DEPENDS path\/to\/crti.s) set_property(TARGET myAwesomeKernel APPEND PROPERTY LINK_DEPENDS path\/to\/crtn.s) set_property(TARGET myAwesomeKernel APPEND PROPERTY LINK_DEPENDS path\/to\/linker_script) # custom link line set(CMAKE_C_LINK_EXECUTABLE \"${CMAKE_C_COMPILER} ${CMAKE_C_FLAGS} -T path\/to\/linker_script CMakeFiles\/crti.dir\/crti.s.obj ${CRTBEGIN_OBJ} <OBJECTS> ${CRTEND_OBJ} CMakeFiles\/crtn.dir\/crtn.s.obj -o myAwesomeKernel\") set(CMAKE_CXX_LINK_EXECUTABLE \"${CMAKE_C_COMPILER} ${CMAKE_C_FLAGS} -T path\/to\/linker_script CMakeFiles\/crti.dir\/crti.s.obj ${CRTBEGIN_OBJ} <OBJECTS> ${CRTEND_OBJ} CMakeFiles\/crtn.dir\/crtn.s.obj -o myAwesomeKernel\") ``` However, it's awkward and unable to link other libraries to ``` myAwesomeKernel ``` without compromising the desired link line. (Yes, you could use ``` target_link_libraries() ``` but it left you without the ability to put your programs between ``` crtbegin.o ``` and ``` crtend.o ``` ) Aside: something bothers me.. How to get rid of hard-coded ``` CMakeFiles\/crti.dir\/crti.s.obj ``` ? Generator expression (to get target file) is not supported inside ``` set ``` command. In the custom link line, how does ``` <OBJECTS> ``` work? Why does it get expanded to target executable?",
    "author_id":5412,
    "publication_date":1754378118000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"curbalman",
    "author_reputation":11.0,
    "tags":"embedded, gcc, cmake, linker",
    "text_length":2224,
    "title_length":52,
    "num_tags":4
  },
  {
    "id":5888,
    "title":"Get the method name for a route in an attribute, but without the specific text",
    "link":"https:\/\/stackoverflow.com\/questions\/79725628\/get-the-method-name-for-a-route-in-an-attribute-but-without-the-specific-text",
    "text":"I want to automatically get method names in the route without the async keyword. Can I do this in an attribute? I need a route like this ``` ...\/Read ``` : ``` [HttpPost(\"Read\")] public async Task<ActionResult> ReadAsync() ``` How to do it automatically from the method name by cutting async? ``` [HttpPost(?)] \/\/? public async Task<ActionResult> ReadAsync() ```",
    "author_id":5411,
    "publication_date":1754378223000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user31212464",
    "author_reputation":null,
    "tags":"c#, asp.net",
    "text_length":362,
    "title_length":78,
    "num_tags":2
  },
  {
    "id":5887,
    "title":"&#39;Just a moment...&#39; html page instead of JSON on Post request",
    "link":"https:\/\/stackoverflow.com\/questions\/79725632\/just-a-moment-html-page-instead-of-json-on-post-request",
    "text":"So basically what I am doing a post request to the wordpress endpoint to get user data for my application. And it was working fine for quite some time, but now i am getting 'Just a moment...' html page instead of JSON and this basically breaks my auth logic. From what I could find it has something to do with CloudFlares bot protection or something, but I have no idea how to get around it. My backend is C#, my front end is React. It would be nice if i could just do something in CloudFlare, but if not, what do I change in my code? My authcontroller: ``` public async Task\\<IActionResult\\> Login(LoginModel login) { var client = new HttpClient(); var request = new HttpRequestMessage(HttpMethod.Post, \"https:\/\/my-wpsite.com\/test\/auth.php\"); client.DefaultRequestHeaders.Add(\"User-Agent\", \"Mozilla\/5.0\"); client.DefaultRequestHeaders.Add(\"Accept\", \"*\/*\"); var content = new MultipartFormDataContent { { new StringContent(login.Username), \"user\" }, { new StringContent(login.Password), \"pass\" } }; request.Content = content; var response = await client.SendAsync(request); string responseString = await response.Content.ReadAsStringAsync(); var jsonResponse = System.Text.Json.JsonSerializer.Deserialize<WPAuthResponse>(responseString); if (response.IsSuccessStatusCode) { \/\/ my logic here } else { return StatusCode((int)response.StatusCode, jsonResponse); } } ``` Thank you",
    "author_id":5410,
    "publication_date":1754378467000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Anton K",
    "author_reputation":1.0,
    "tags":"c#, .net, wordpress, cloudflare, post",
    "text_length":1376,
    "title_length":68,
    "num_tags":5
  },
  {
    "id":5886,
    "title":"How to perform and track group_wise lme tests with metadata in trans_alpha$cal_diff? (R microeco)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725633\/how-to-perform-and-track-group-wise-lme-tests-with-metadata-in-trans-alphacal-d",
    "text":"I'm using the microeco package in R to analyse microbial community sequence data. I would like some advice on how to get metadata information into the $res_diff. I want to assess alpha diversity differences per Year, while still accounting for other grouping factors like Region and Management. Ideally, I’d like to get an output that performs the lme test on each year separately, and retains a Year identifier in the output (res_diff). I checked normality and homogeneity of variance, and since my data includes replicate samples per group (Region × Management × Year), and these are not independent — they're nested within farms. Using a linear mixed effects model allows me to include random effects (e.g. Region_Management) to account for this nested structure, which ANOVA cannot handle properly. Any insights into how the grouping logic works under the hood in trans_alpha or workarounds to extract properly grouped lme results would be really appreciated! This is how I created my trans_alpha object: ``` bac_a_lme <- trans_alpha$new( dataset = bac_cmt, group = \"Management\", by_group = \"Year\") ``` And this was to calculate the differences: ``` bac_a_lme$cal_diff( method = \"lme\", formula = \"Region * Management + (1 | Region_Management)\", measure = NULL) ``` The output that this gives me has the columns Method, Measure, Factors etc. The Factors do not include Year identifiers, and I will need to be able to discern the year for my analyses.",
    "author_id":5409,
    "publication_date":1754378502000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ivdL0526",
    "author_reputation":9.0,
    "tags":"r",
    "text_length":1453,
    "title_length":97,
    "num_tags":1
  },
  {
    "id":5885,
    "title":"dbutils.notebook.run() Masks Specific Errors with General WorkflowException Logs",
    "link":"https:\/\/stackoverflow.com\/questions\/79725636\/dbutils-notebook-run-masks-specific-errors-with-general-workflowexception-logs",
    "text":"I'm using ``` dbutils.notebook.run() ``` in a Databricks Python file to execute another notebook. When executed notebook fails, I'm only seeing a general error log instead of the real specific error (that I'm getting when running the notebook directly). I'm expecting to get the real specific error and not just a general error log. Code: ``` absolute_notebook_path = '\/Workspace\/company\/ETL-Processing\/packages\/example\/src\/scripts\/merge_to_gold.py' try: dbutils.notebook.run(absolute_notebook_path, 3600) print(f\"Successfully ran notebook: {absolute_notebook_path}\") except Exception as e: error_message = str(e) raise Exception(f\"Notebook '{absolute_notebook_path}' failed with error: {error_message}\") ``` general log: ``` Error: An error occurred while calling o560.run. : com.databricks.WorkflowException: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details at com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:99) at com.databricks.dbutils_v1.impl.NotebookUtilsImpl.run(NotebookUtilsImpl.scala:130) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397) at py4j.Gateway.invoke(Gateway.java:306) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199) at py4j.ClientServerConnection.run(ClientServerConnection.java:119) at java.lang.Thread.run(Thread.java:750) Caused by: com.databricks.NotebookExecutionException: FAILED: Workload failed, see run output for details at com.databricks.workflow.WorkflowDriver.run0(WorkflowDriver.scala:147) at com.databricks.workflow.WorkflowDriver.run(WorkflowDriver.scala:94) ... 13 more ``` real specific log: ``` FileNotFoundError: [Errno 2] No such file or directory: '\/Workspace\/company\/ETL-processing\/packages\/membership\/src\/tables\/silver\/memberships\/table.py' ```",
    "author_id":5408,
    "publication_date":1754378767000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Itai Sharon",
    "author_reputation":1.0,
    "tags":"databricks, databricks-notebook",
    "text_length":2284,
    "title_length":80,
    "num_tags":2
  },
  {
    "id":5884,
    "title":"Autocompleting TextField inside a TableView?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725637\/autocompleting-textfield-inside-a-tableview",
    "text":"I'm trying to add autocompletion to a TextField that acts as a data column in a TableView. In particular, I want to use an Array of String values as suggestions, but not require that the value of the TextField be one of those suggestions. I've searched a bunch online, but I haven't found anyone doing .textSuggestions for a TextField embedded in a TableView, which seems to complicate the text Binding and the use of .onChange(), so perhaps I'm on the bleeding edge here wrt pure SwiftUI solutions... Anyway, here's a simplified data model: ``` \/\/ A set of common location names, but not the only possible locations of a Thing. enum CommonLocation: String, CaseIterable, Identifiable { var id: Self { self } case Bathroom, Bedroom, DiningRoom, Foyer, Kitchen, LivingRoom } class Thing: Identifiable, Hashable, ObservableObject { static func == (lhs: Thing, rhs: Thing) -> Bool { lhs.id == rhs.id } @Published var name: String @Published var location: String \/\/ usually a CommonLocation, but not necessarily let id: UUID = UUID() public func hash(into hasher: inout Hasher) { hasher.combine(self.id) } public init(title: String, location: String) { self.name = title self.location = location } } ``` Using that model, as the first step, I built working autocompletion for a \"standalone\" TextField ( not part of a table), like this: ``` struct ThingView: View { @State var location: String = \"\" private func locationsMatching(substring: String) -> [Location] { do { let result = try Location.allCases.filter(#Predicate<Location> { loc in loc.rawValue.contains(substring) }) return result } catch { return [] } } var body: some View { VStack { Text(\"Location:\") TextField(\"\", text: $location) .textInputSuggestions() { ForEach(locationsMatching(substring: location)) { location in Text(location.rawValue) .textInputCompletion(location.rawValue) } } } .padding() } } ``` That works fine: the suggestions menu pops up in response to keystrokes, the suggestions are filtered to match the current text value, and hitting Return accepts the currently-highlighted suggestion. The real data model, though, is for an Array of ``` Things ``` displayed in a TableView: ``` class ThingSet: ObservableObject, Identifiable { @Published var things: Array<Thing> = [] let id = UUID() public init(things: Array<Thing>) { self.things = things } } ``` Note that the ``` ThingSet ``` is an ``` ObservableObject ``` and the Array of Things is ``` @Published ``` . Since the ``` Things ``` are now in an Array, I had to create custom Bindings to give to the name and location TextFields. I couldn't see any way of doing it without a custom Binding (e.g. some nice syntactic sugar). I then tried a few variations on a View to display\/edit them: ``` struct ThingSetView: View { @ObservedObject private var thingSet: ThingSet @State private var locationSuggestions: [Location] = Location.allCases private func locationsMatching(substring: String) -> [Location] { do { return try Location.allCases.filter(#Predicate<Location> { loc in loc.rawValue.contains(substring) }) } catch { return [] } } public init(thingSet: ThingSet) { self.thingSet = thingSet } var body: some View { Table(of: Thing.self) { TableColumn(\"Name\") { (thing: Thing) in TextField(\"\", text: Binding(get: { thing.name }, set: { newName in if let index = thingSet.things.firstIndex(where: { $0.id == thing.id }) { thingSet.things[index].name = newName } })) } TableColumn(\"Location\") { (thing: Thing) in TextField(\"\", text: Binding(get: { thing.location }, set: { newLocation in if let index = thingSet.things.firstIndex(where: { $0.id == thing.id }) { thingSet.things[index].location = newLocation } })) .textInputSuggestions() { ForEach(locationsMatching(substring: thing.location)) { location in Text(location.rawValue) .textInputCompletion(location.rawValue) } } } } rows: { ForEach(self.thingSet.things) { thing in TableRow(thing) } } } } ``` This doesn't work, since without an .onChange() modifier, location suggestions associated with the TextField don't update. I couldn't find a way to create a Binding that would work with a .onChange() View modifier, similar to . I tried wrapping the TextField in its own View struct so that it would have an @State instance var to use to create a Binding: ``` struct CompletingTextField: View { struct Choice: Identifiable { let value: String let id = UUID() public init(_ value: String) { self.value = value } } @Binding var value: String private var allChoices: [String] @State private var suggestions: [Choice] public init(value: Binding<String>, choices: [String]) { self._value = value self.allChoices = choices self.suggestions = choices.map({ Choice($0) }) } private func choicesMatching(substring: String) -> [Choice] { do { let result = try allChoices.filter(#Predicate<String> { choice in choice.contains(substring) }) return result.map({ Choice($0) }) } catch { return [] } } var body: some View { TextField(\"\", text: $value) .textInputSuggestions() { ForEach(suggestions) { c in Text(c.value).textInputCompletion(c.value) } } .onChange(of: value) { oldText, newText in suggestions = choicesMatching(substring: newText) } } } ``` As you can see, the TextField gets a Binding via ``` $value ``` , so I was able to add a .onChange() modifier to recompute the suggestions. The parent view with the TableView then looks like: ``` struct ThingSetView: View { @ObservedObject private var thingSet: ThingSet public init(thingSet: ThingSet) { self.thingSet = thingSet } var body: some View { Table(of: Thing.self) { TableColumn(\"Name\") { (thing: Thing) in TextField(\"\", text: Binding(get: { thing.name }, set: { newName in if let index = thingSet.things.firstIndex(where: { $0.id == thing.id }) { thingSet.things[index].name = newName } })) } TableColumn(\"Location\") { (thing: Thing) in let locationBinding = Binding(get: { thing.location }, set: { newLocation in if let index = thingSet.things.firstIndex(where: { $0.id == thing.id }) { thingSet.things[index].location = newLocation } }) CompletingTextField(value: locationBinding, choices: Location.allCases.map({ $0.rawValue })) } } rows: { ForEach(self.thingSet.things) { thing in TableRow(thing) } } } } ``` I would have thought the .onChange() would fire as the CompletingTextField's value changed, but that never happens. The onChange closure never gets called, even if I hit \"Return\" to accept a new text value. Any help would be greatly appreciated!",
    "author_id":5407,
    "publication_date":1754378786000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Robert Fuhrer",
    "author_reputation":11.0,
    "tags":"swiftui, binding, textfield, tableview",
    "text_length":6400,
    "title_length":44,
    "num_tags":4
  },
  {
    "id":5883,
    "title":"import sentence_transformers hangs on macOS with [mutex.cc] RAW: Lock blocking error",
    "link":"https:\/\/stackoverflow.com\/questions\/79725643\/import-sentence-transformers-hangs-on-macos-with-mutex-cc-raw-lock-blocking-e",
    "text":"I'm encountering a persistent issue where importing the ``` sentence_transformers ``` library on macOS causes my Python script to hang indefinitely. When the script hangs, the only output I see is a low-level mutex lock error. The script becomes unkillable ( ``` Ctrl+C ``` does not work), and I have to force-quit the terminal or process. The error message is: ``` [mutex.cc : 452] RAW: Lock blocking 0x11b3e1888 @ ``` Background This problem began after I was experimenting with running ``` sentence-transformers ``` tasks concurrently using Python's ``` asyncio ``` library. My code structure was similar to this: ``` import asyncio # from sentence_transformers import SentenceTransformer # This import is now problematic # model = SentenceTransformer('all-MiniLM-L6-v2') # Example model loading async def process_with_tracking(org_data): # Some processing logic that would eventually use the model # e.g., model.encode(org_data['text']) pass async def main(): orgs = [...] # A list of data objects tasks = [] # Create tasks for each org for org in orgs: task = asyncio.create_task(process_with_tracking(org)) tasks.append(task) # Wait for all tasks to complete if tasks: await asyncio.gather(*tasks, return_exceptions=True) # asyncio.run(main()) ``` My hypothesis is that a previous concurrent execution created a deadlock or corrupted a shared resource\/cache file used by ``` sentence-transformers ``` or one of its dependencies (like ``` Hugging Face Transformers ``` or ``` PyTorch ``` ). The Core Problem The issue is now system-wide and persists even when I am not using ``` asyncio ``` or any form of concurrency. A minimal script with just the import statement is enough to trigger the hang: ``` test_import.py ``` ``` print(\"Attempting to import sentence_transformers...\") import sentence_transformers print(\"Import successful!\") # This line is never reached ``` When I run ``` python test_import.py ``` , the script hangs after the first print statement, and I see the ``` [mutex.cc] ``` error. What I've Tried So Far Restarting my computer: The issue persists across reboots. Reinstalling the library: I have run ``` pip uninstall sentence-transformers ``` and ``` pip install sentence-transformers ``` multiple times. Running in a simple script: Confirmed the hang occurs with a single import line, ruling out my specific ``` asyncio ``` code as the immediate trigger. My Questions What could be causing this kind of persistent lock that survives reinstallation and reboots? Where does ``` sentence-transformers ``` or Hugging Face ``` transformers ``` store cache, configuration, or lock files on macOS that I could manually inspect or delete? (e.g., ``` ~\/.cache\/huggingface\/ ``` ) Are there any known issues with the underlying tokenizer parallelism ( ``` tokenizers ``` ) on macOS that could lead to this state? How can I reset it? Environment OS: macOS Python Version: 3.10",
    "author_id":5406,
    "publication_date":1754379495000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Akash Dubey",
    "author_reputation":1.0,
    "tags":"multithreading, locking, python-multithreading, python-3.10, sentence-transformers",
    "text_length":2893,
    "title_length":84,
    "num_tags":5
  },
  {
    "id":5882,
    "title":"Why is it not detecting and reading the face from my captured image?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725645\/why-is-it-not-detecting-and-reading-the-face-from-my-captured-image",
    "text":"I'm currently working on a face-comparison system that takes one loaded image and one captured image, extracts the faces from both, and compares them to see if they are the same person or not. I'm using OpenCV for it, and it's been successful at detecting and returning the face from the preloaded image. However, when I supplant the same code into an existing image capturing function, it fails to do so and says that there is no image to be compared. For the face detection on the preloaded image I have ``` def get_Face(image_path): img = cv2.imread(image_path) face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') color = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) faces = face_cascade.detectMultiScale(color, scaleFactor=1.1, minNeighbors=5) if len(faces) > 0: for (x, y, w, h) in faces: cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2) cv2.imshow(\"Detected Face\", img) cv2.waitKey(0) cv2.destroyAllWindows() x, y, w, h = faces[-1] face_img = img[y:y+h, x:x+w] return face_img else: print(\"No face detected.\") ``` and for the face detection on the live capture image I have ``` def get_Picture(): face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') cam = cv2.VideoCapture(0) ret, frame = cam.read() if ret: cv2.imshow(\"Captured\", frame) cv2.waitKey(0) cv2.destroyWindow(\"Captured\") cam.release() color = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) faces = face_cascade.detectMultiScale(color, scaleFactor=1.1, minNeighbors=5) if len(faces) > 0: for (x, y, w, h) in faces: cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2) cv2.imshow(\"Detected Face\", frame) cv2.waitKey(0) cv2.destroyAllWindows() x, y, w, h = faces[-1] face_img = frame[y:y+h, x:x+w] return face_img else: print(\"Failed to capture image\") cam.release() ``` The comparison function is as follows ``` def compare_Images(image_path): id_photo = get_Face(image_path) snapped_photo = get_Picture() id_photo_gray = cv2.cvtColor(id_photo, cv2.COLOR_RGB2GRAY) snapped_photo_gray = cv2.cvtColor(snapped_photo, cv2.COLOR_RGB2GRAY) score, diff = structural_similarity(id_photo_gray, snapped_photo_gray, full=True) print(\"Similarity Score: {:.3f}%\".format(score * 100)) diff = (diff * 255).astype(\"uint8\") thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1] contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) contours = contours[0] if len(contours) == 2 else contours[1] mask = np.zeros(id_photo.shape, dtype='uint8') filled = snapped_photo.copy() for c in contours: area = cv2.contourArea(c) if area > 100: x,y,w,h = cv2.boundingRect(c) cv2.rectangle(id_photo, (x, y), (x + w, y + h), (36,255,12), 2) cv2.rectangle(snapped_photo, (x, y), (x + w, y + h), (36,255,12), 2) cv2.drawContours(mask, [c], 0, (0,255,0), -1) cv2.drawContours(filled, [c], 0, (0,255,0), -1) cv2.imshow('id_photo', id_photo) cv2.imshow('snapped_photo', snapped_photo) cv2.imshow('diff', diff) cv2.imshow('mask', mask) cv2.imshow('filled', filled) cv2.waitKey() ``` and the error I'm getting is ``` error: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor' ``` I've tried fiddling with the BGR2GRAY in the original code by making is RGB2GRAY since the images are now in RGB color format but it's saying that there isn't any value in the snapped_photo_gray variable",
    "author_id":5405,
    "publication_date":1754379675000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"saksham shankar",
    "author_reputation":15.0,
    "tags":"python, image-processing, opencv, face-detection",
    "text_length":3461,
    "title_length":68,
    "num_tags":4
  },
  {
    "id":5881,
    "title":"GCC switch statements do not simplify on identical handling",
    "link":"https:\/\/stackoverflow.com\/questions\/79725647\/gcc-switch-statements-do-not-simplify-on-identical-handling",
    "text":"The switch statements in the following two functions ``` int foo(int value) { switch (value) { case 0: return 0; case 1: return 0; case 2: return 1; } } int bar(int value) { switch (value) { case 0: case 1: return 0; case 2: return 1; } } ``` compile to different assembly in the x86-64 and RISC-V editions of the GNU C Compiler, when not optimizing. Why? Is there any way to subvert this? I am using consteval lambdas in function templates to programmatically generate the return values of functions like this per output and they have a lot more assembly instructions than if I went in and hand-wrote all of the output handling and made all the identical output handlings into one case-case-case chain. Edit: There are no compile flags used.",
    "author_id":5404,
    "publication_date":1754379748000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"notgapriel",
    "author_reputation":123.0,
    "tags":"c++, gcc, g++, compiler-optimization, switch-statement",
    "text_length":742,
    "title_length":59,
    "num_tags":5
  },
  {
    "id":5880,
    "title":"Diff btw &quot;Invoke( lambda )&quot; and directly calling a lambda when we use Google Mock EXPECT_CALL",
    "link":"https:\/\/stackoverflow.com\/questions\/79725649\/diff-btw-invoke-lambda-and-directly-calling-a-lambda-when-we-use-google-moc",
    "text":"Since I learnt to use Google Test\/Mock framework, I allways used the form to setup an expectation as following: ``` EXPECT_CALL( mock_obj, mock_method ).WillOnce( Invoke( []() { do_things; } ) ); ``` Few days ago, I found that another form also can work as this: ``` EXPECT_CALL( mock_obj, mock_method ).WillOnce( []() { do_things; } ); ``` My question is: Is there any difference between those two forms? When should I use one over another? Thanks!",
    "author_id":5403,
    "publication_date":1754380129000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Leon",
    "author_reputation":2153.0,
    "tags":"unit-testing, lambda, googletest, invoke, googlemock",
    "text_length":449,
    "title_length":103,
    "num_tags":5
  },
  {
    "id":5879,
    "title":"Custom Smart Home skill with my own https backend",
    "link":"https:\/\/stackoverflow.com\/questions\/79725657\/custom-smart-home-skill-with-my-own-https-backend",
    "text":"Is it possible to create Custom \"Smart Home\" skill my own https backend without needing to use a lambda? What I am trying to achieve is, I created a custom skill with Invocation name as \"front room\" and intent utterance as \"light on\" and with a python backend service hosted at my home exposed to internet via https. This works with \"Alexa, ask front room light on\". I am trying to get rid of the \"ask\". When researched I found that using \"Smart Home\" skill, I can achieve this. However I am not seeing an option to use my own https backend in that. Any work around for my usecase?",
    "author_id":5402,
    "publication_date":1754380660000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Simon",
    "author_reputation":129.0,
    "tags":"alexa-smart-home-skill",
    "text_length":581,
    "title_length":49,
    "num_tags":1
  },
  {
    "id":5878,
    "title":"How to use a folder generated by an Xcode Aggregate Target as a resource in another target?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725658\/how-to-use-a-folder-generated-by-an-xcode-aggregate-target-as-a-resource-in-anot",
    "text":"I have a multi target project where every target relies on a built \"web bundle\" which is basically a collection of html, optimized images, and optimized javascript. Right now that web bundle is built in a pre build step by running a script. The script takes awhile to run and outputs to a folder that is referenced into the project via a PBXFileReference which is then referenced in the Copy Bundle Resources step. ``` 96516AC22BF928DD00576562 \/* build *\/ = {isa = PBXFileReference; lastKnownFileType = folder; name = build; path = \"..\/web-ui\/build\"; sourceTree = \"<group>\"; } .... 96516AC32BF928DD00576562 \/* build in Resources *\/ = {isa = PBXBuildFile; fileRef = 96516AC22BF928DD00576562 \/* build *\/; }; ``` As a step, I wrote an aggregate target that can also run this script. I specified its input and output files and turned off sandboxing. It does exactly what I need it to. Critically it is ran based on dependency analysis. If I modify any file in web-ui it rebuilds, if I dont I can repeatedly build the aggregate target and it will not re-run the script. This is perfect. ``` A97590172E419CBA00741928 \/* Build Web Bundle *\/ = { isa = PBXShellScriptBuildPhase; buildActionMask = 12; files = ( ); inputFileListPaths = ( ); inputPaths = ( \"$(SRCROOT)\/xcodescripts\/build-web-bundle.bash\", \"$(SRCROOT)\/web-ui\/index.html\", \"$(SRCROOT)\/web-ui\/vite-env.d.ts\", \"$(SRCROOT)\/web-ui\/vite.config.ts\", \"$(SRCROOT)\/web-ui\/tsconfig.json\", \"$(SRCROOT)\/web-ui\/stats.html\", \"$(SRCROOT)\/web-ui\/postcss.config.js\", \"$(SRCROOT)\/web-ui\/package.json\", \"$(SRCROOT)\/web-ui\/justfile\", \"$(SRCROOT)\/web-ui\/src\/\", ); name = \"Build Web Bundle\"; outputFileListPaths = ( ); outputPaths = ( \"$(SRCROOT)\/web-ui\/build\/\", ); runOnlyForDeploymentPostprocessing = 0; shellPath = \/bin\/sh; shellScript = \"exec \\\"${SCRIPT_INPUT_FILE_0}\\\"\\n\"; }; ``` You may notice I reference a src file. This is made possible via a flag ``` USE_RECURSIVE_SCRIPT_INPUTS_IN_SCRIPT_PHASES ``` which allows Xcode to check folder dependencies recursively. The problem is that my other targets do not automatically recognize that they need to run the \"WebBundle\" aggregate target in order to update a resource they copy in their \"Copy bundle resources\" phase. So I tried adding it as a Target Dependency. ``` A9DE685B2E41C9A8005EF4E0 \/* PBXTargetDependency *\/ = { isa = PBXTargetDependency; target = A97590132E419C1200741928 \/* WebBundle *\/; targetProxy = A9DE685A2E41C9A8005EF4E0 \/* PBXContainerItemProxy *\/; }; ``` Unfortunately this breaks whatever magic was allowing the script to be run only when there are web bundle changes. Every build it runs the \"Build Web Bundle\" script. I think what I am missing is a way to specify in these other targets that a resource they are used to copying from the Xcode PBXFileReference is produced by the aggregate target. This way they can start to reason about the dependencies. Other possibilities are that I should be building the web bundle to a separate location. Or that these references are somehow broken in another way. To be clear the folder format is as thus ``` project\/ iOS\/ client.xcodeproj web-ui\/ build\/ (web bundle build is output here and referenced relatively) src\/ index.html (and other things) ```",
    "author_id":4396,
    "publication_date":1754380661000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"CalebK",
    "author_reputation":663.0,
    "tags":"ios, swift, xcode, build",
    "text_length":3204,
    "title_length":91,
    "num_tags":4
  },
  {
    "id":5877,
    "title":"FVM how to point to latest stable version",
    "link":"https:\/\/stackoverflow.com\/questions\/79725662\/fvm-how-to-point-to-latest-stable-version",
    "text":"I use ``` fvm ``` to control flutter version in my project ``` fvm use 3.32.8 ``` Now my project using 3.32.8 How can I change my project to stable version? I tried ``` fvm use stable ``` , but it will point to 3.32.5 stable instead of 3.32.8 stable. ``` PS C:\\Users\\xxx\\VSCodeProjects\\xxx> fvm use stable ✓ Dependencies resolved. (35.9s) ✓ Project now uses Flutter SDK : Channel: Stable ┌────────────────────────────────────────────────────────────────────┐ │ ✓ Running on VsCode, please restart the terminal to apply changes. │ └────────────────────────────────────────────────────────────────────┘ You can then use \"flutter\" command within the VsCode terminal. PS C:\\Users\\xxx\\VSCodeProjects\\xxx> fvm flutter --version Flutter 3.32.5 • channel stable • https:\/\/github.com\/flutter\/flutter.git Framework • revision fcf2c11572 (6 weeks ago) • 2025-06-24 11:44:07 -0700 Engine • revision dd93de6fb1 (6 weeks ago) • 2025-06-24 07:39:37 -0700 Tools • Dart 3.8.1 • DevTools 2.45.1 ```",
    "author_id":5401,
    "publication_date":1754380813000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"John Joe",
    "author_reputation":12823.0,
    "tags":"flutter, dart, fvm",
    "text_length":980,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":5876,
    "title":"How to safely (re)start a pthread during runtime in a long-running main loop on embedded Linux?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725666\/how-to-safely-restart-a-pthread-during-runtime-in-a-long-running-main-loop-on",
    "text":"I'm developing an embedded Linux application in C for a touchscreen heating controller. The main() function is only called once at boot and then runs an infinite loop (forever). During runtime, based on certain conditions, I want to start a background thread (e.g., Modbus communication). However: If those conditions are met after boot, the thread is never started, because main() is not re-entered. I moved the logic to check conditions and potentially start the thread into the main loop. But now, when I try to start the thread during runtime, I sometimes get system crashes or instability, even though I check if the thread is already running. Here's what I'm doing: Logic of the thread: ``` static void* modbus_task(void* param) { while (!modbus_task_stop_requested) { if (LOCAL_eco_value_dirty == 1) { eco_write_in_progress = 1; writeECOValues(); eco_write_counter = 5; LOCAL_eco_value_dirty = 0; } if (eco_write_in_progress) { if (--eco_write_counter <= 0) { eco_write_in_progress = 0; } } else { readModbus(); } msleep(100); } modbus_task_running = 0; return 0; } ``` In my main() the thread is created: ``` if (LOCAL_wp_eco_vorhanden[0]) { pthread_create(&modbusThread, 0, modbus_task, 0); } ``` There are no race conditions in this setup because this is the only thread accessing these variables. The readModbus() function simply reads some registers, and if the user has modified anything, the changes are written back using the writeECOValues() function. The Problem: The issue occurs when the system is already running and the user enables wp_eco_vorhanden via the GUI (i.e. sets it to true at runtime). In this case, the Modbus thread should start and begin displaying values from Modbus. However, since main() is only executed once during startup, the thread only gets created if LOCAL_wp_eco_vorhanden[0] was already true at that time. If it becomes true later (at runtime), the thread is never started unless the system is restarted — which is not desirable. What I Need: I’m looking for a solution that allows me to check inside a loop (probably the main loop) whether the thread should be started, and then start it accordingly. I tried doing that, but ran into problems — probably due to my lack of experience in C, as I mostly work on the Java side (GUI). I hope my question is clearer now.",
    "author_id":5400,
    "publication_date":1754381125000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Lukas.lmst",
    "author_reputation":25.0,
    "tags":"c, pthreads, modbus",
    "text_length":2312,
    "title_length":95,
    "num_tags":3
  },
  {
    "id":5875,
    "title":"Bootstrap DataTables Arabic Search Not Working (English Search Works Fine)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725668\/bootstrap-datatables-arabic-search-not-working-english-search-works-fine",
    "text":"I'm currently using Bootstrap DataTables on the front end with JavaScript and AJAX to fetch data from a Spring Boot backend. The table loads correctly and searching in English works as expected. However, searching using Arabic text returns no results, even though the data clearly contains Arabic content. Arabic characters are rendered correctly in the table. The backend returns data encoded in UTF-8. I confirmed the data source contains Arabic values that should match the search.",
    "author_id":5399,
    "publication_date":1754381173000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Toufic Sleiman",
    "author_reputation":77.0,
    "tags":"ajax, utf-8, bootstrap-5, datatables, arabic",
    "text_length":484,
    "title_length":74,
    "num_tags":5
  },
  {
    "id":5874,
    "title":"How do I get multiple functions to work with onEdit trigger in a single sheet",
    "link":"https:\/\/stackoverflow.com\/questions\/79725679\/how-do-i-get-multiple-functions-to-work-with-onedit-trigger-in-a-single-sheet",
    "text":"I have multiple cells that I want to check for an onEdit trigger and then execute functions used on which cell was edited. I have used a mix of code from different sources unsucessfully I want to look for an on edit to cell A5 where the first function will clear a drop-down menu in B5 and highlight that cell I also want to look for on edit to E4 where the second function will clear drop-down menus in F4 & G4 and then highlight F4 I'm not getting any of the clearing of intended cells ``` function onEdit(e){ const sheetName = \"Wave(FREQ) MS Calc\"; const range = e.range; calc1(sheetName,range); calc2(sheetName,range); } function calc1(sheetName,range){ const sheet = range.getSheet(); const targetCell = range.getA1Notation(); const clearCell = sheet.getCell(5,2); if (sheet.getName() === sheetName && targetCell === \"A5\") clearCell.clearContent(); clearCell.activate();} function calc2(sheetName,range){ const sheet = range.getSheet(); const targetCell = range.getA1Notation(); const clearCell = sheet.getRange('F4:G4'); if (sheet.getName() === sheetName && targetCell === \"E4\") clearCell.clearContent(); clearCell.activate();} ```",
    "author_id":5398,
    "publication_date":1754381552000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"scjwolin57",
    "author_reputation":15.0,
    "tags":"google-apps-script, google-sheets",
    "text_length":1137,
    "title_length":77,
    "num_tags":2
  },
  {
    "id":5873,
    "title":"Module loading logistics failed: file logistics\/security\/ir.model.access.csv could not be processed:",
    "link":"https:\/\/stackoverflow.com\/questions\/79725680\/module-loading-logistics-failed-file-logistics-security-ir-model-access-csv-cou",
    "text":"``` Exception: Module loading logistics failed: file logistics\/security\/ir.model.access.csv could not be processed: No matching record found for external id 'model_logistics_order' in field 'Model' No matching record found for external id 'model_logistics_order_table' in field 'Model' No matching record found for external id 'model_logistics_vehicle' in field 'Model' No matching record found for external id 'model_logistics_route' in field 'Model' Missing required value for the field 'Model' (model_id) Missing required value for the field 'Model' (model_id) Missing required value for the field 'Model' (model_id) Missing required value for the field 'Model' (model_id) ``` my ``` ir.model.access.csv ``` ``` id,name,model_id:id,group_id:id,perm_read,perm_wite,perm_create,perm_unlink access_logistics_order,logistics_order,model_logistics_order,,1,1,1,1 access_logistics_order_table,logistics_order_table,model_logistics_order_table,,1,1,1,1 access_logistics_vehicle,logistics_vehicle,model_logistics_vehicle,,1,1,1,1 access_logistics_route,logistics_route,model_logistics_route,,1,1,1,1 ``` ``` __manifest__ ``` ``` { 'name': 'Logistics', 'depends': ['base', 'contacts', 'sale'], 'author': 'Anton', 'category': 'Services', 'description': \"aabababab\", 'data': [ 'security\/ir.model.access.csv', 'views\/partner_views.xml', 'views\/product_views.xml', 'views\/order_views.xml', 'views\/vehicle_views.xml', 'views\/route_views.xml', 'views\/menu.xml', ], 'installable': True, 'application': True } ``` ``` logistics\/__init__ ``` ``` from . import models ``` ``` models\/__init__ ``` ``` from . import res_partner from . import product from . import orders from . import vehicle from . import route ``` model names ``` class Order(models.Model): _name = 'logistics.order' class OrderTable(models.Model): _name = 'logistics.order.table' class Route(models.Model): _name = 'logistics.route' class Vehicle(models.Model): _name = 'logistics.vehicle' ``` I tried a lot of solutions like this or this but nothing works for me.",
    "author_id":5397,
    "publication_date":1754381606000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Anton",
    "author_reputation":105.0,
    "tags":"python, exception, odoo, odoo-17",
    "text_length":2016,
    "title_length":100,
    "num_tags":4
  },
  {
    "id":5872,
    "title":"Move entire rows when PowerQuery data change",
    "link":"https:\/\/stackoverflow.com\/questions\/79725685\/move-entire-rows-when-powerquery-data-change",
    "text":"I'm working on a file with two different tables on two different Queries, with this structure: Table 1 GROUP MEMBER ID GROUP SCORE 1 SCORE 2 SCORE 3 Member 1 01 A1 1 1 4 Member 2 02 A1 3 3 5 Member 3 03 A2 4 1 2 Member 4 04 A2 5 5 5 Table 2 GROUP MEMBER ID GROUP SCORE 1 SCORE 2 SCORE 3 Member 5 05 B1 1 1 4 Member 6 06 B1 3 3 5 Member 7 07 B2 4 1 2 Member 8 08 B2 5 5 5 Now, the first 3 columns from the left ( ``` Group Member ``` , ``` ID ``` and ``` Group ``` ) are created with the Queries, but all the other columns are just simple columns added by me in the excel sheet, so they're not connected to any Query. If I change in the root file for the Query (is the same for both) for example the group of a member, all the tables correctly update, but only for the first 3 column. For example, if I change Member 2 group from A1 to B2, the result: Table 1 GROUP MEMBER ID GROUP SCORE 1 SCORE 2 SCORE 3 Member 1 01 A1 1 1 4 Member 3 03 A2 3 3 5 Member 4 04 A2 4 1 2 Table 2 GROUP MEMBER ID GROUP SCORE 1 SCORE 2 SCORE 3 Member 5 05 B1 1 1 4 Member 6 06 B1 3 3 5 Member 2 02 B2 4 1 2 Member 7 07 B2 5 5 5 Member 8 08 B2 So the Member is correctly moved to the GROUP B table, but all the scores are now wrong, because they were not shifted with the correct member. How can I fix that? Is basically the most important part of this file since I should manage something around 200 members divided in 4\/5 groups and would be the best to have those sorted automatically with just a single change in the main file.",
    "author_id":5396,
    "publication_date":1754382183000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MrBoop",
    "author_reputation":57.0,
    "tags":"powerquery, excel, excel-tables",
    "text_length":1508,
    "title_length":44,
    "num_tags":3
  },
  {
    "id":5871,
    "title":"Refresh token being passed in response body in registration but not login in Django",
    "link":"https:\/\/stackoverflow.com\/questions\/79725686\/refresh-token-being-passed-in-response-body-in-registration-but-not-login-in-dja",
    "text":"I am using the Django REST Framework with ``` dj-rest-auth ``` and ``` django-allauth ``` for authentication and authorisation with JWTs. From what I have heard, using HTTP-only cookies to store the tokens is a secure approach where the access token is passed in the response body but the refresh token is passed to the browser as a cookie. The default (straight from dj_rest_auth.registration.views) login endpoint works this way, returning the access token in the response body and returning the access and refresh tokens as HTTP-only cookies as well as a csrftoken and sessionid. However, the default signup endpoint doesn't exhibit the same behaviour, it returns both access and refresh tokens in the response body and returns neither as a cookie, but does return these cookies: messages, csrftoken, sessionid. Does anyone know why this might be happening and how I could get registration to behave the same way as login? Here are the relevant parts of my settings.py: ``` REST_AUTH = { \"USE_JWT\": True, \"JWT_AUTH_COOKIE\": \"access-tkk\", \"JWT_AUTH_REFRESH_COOKIE\": \"refresh-tkk\", # \"JWT_AUTH_SECURE\": True, # Use secure cookies in production for HTTPS only \"JWT_AUTH_HTTPONLY\": True, # Secure HTTP-only cookies \"REGISTER_SERIALIZER\": \"authentication.serializers.CustomRegisterSerializer\", \"USER_DETAILS_SERIALIZER\": \"authentication.serializers.CustomUserDetailsSerializer\", } SIMPLE_JWT = { \"ACCESS_TOKEN_LIFETIME\": timedelta(minutes=15), # Short-lived access token \"REFRESH_TOKEN_LIFETIME\": timedelta(days=14), # Longer-lived refresh token \"ROTATE_REFRESH_TOKENS\": True, # Issue new refresh token on refresh \"BLACKLIST_AFTER_ROTATION\": True, # Blacklist old refresh tokens \"UPDATE_LAST_LOGIN\": True, \"ALGORITHM\": \"HS256\", \"AUTH_HEADER_TYPES\": (\"Bearer\",), \"AUTH_HEADER_NAME\": \"HTTP_AUTHORIZATION\", \"AUTH_TOKEN_CLASSES\": (\"rest_framework_simplejwt.tokens.AccessToken\",), } REST_FRAMEWORK = { \"DEFAULT_AUTHENTICATION_CLASSES\": ( \"dj_rest_auth.jwt_auth.JWTCookieAuthentication\", \"rest_framework.authentication.TokenAuthentication\", ), \"DEFAULT_PERMISSION_CLASSES\": [ \"rest_framework.permissions.IsAuthenticated\", ], } ```",
    "author_id":5395,
    "publication_date":1754382247000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sebastian Power",
    "author_reputation":37.0,
    "tags":"python, django-rest-framework, django, jwt, dj-rest-auth",
    "text_length":2122,
    "title_length":83,
    "num_tags":5
  },
  {
    "id":5870,
    "title":"Convert an LDIF file to a tabular DataFrame with Python Polars",
    "link":"https:\/\/stackoverflow.com\/questions\/79725691\/convert-an-ldif-file-to-a-tabular-dataframe-with-python-polars",
    "text":"I have several LDIF files that look like this: ``` dn: uid=jdoe,ou=People,dc=example,dc=com changetype: add objectClass: inetOrgPerson uid: jdoe cn: John Doe sn: Doe mail: jdoe@example.com dn: uid=asmith,ou=People,dc=example,dc=com changetype: add objectClass: inetOrgPerson uid: asmith cn: Alice Smith sn: Smith mail: asmith@example.com ``` In R I would normally use the unnest() function to normalize the data in an LDIF file and convert it into tabular format: dn changetype objectClass uid cn sn mail uid=jdoe,ou=People,dc=example,dc=com add inetOrgPerson jdoe John Doe Doe jdoe@example.com uid=asmith,ou=People,dc=example,dc=com add inetOrgPerson asmith Alice Smith Smith asmith@example.com How do achieve this using Python Polars? Is there a way to do it",
    "author_id":5394,
    "publication_date":1754382537000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"CT_369",
    "author_reputation":131.0,
    "tags":"python, python-polars",
    "text_length":760,
    "title_length":62,
    "num_tags":2
  },
  {
    "id":5869,
    "title":"CTRL + C not working in android studio flutter project",
    "link":"https:\/\/stackoverflow.com\/questions\/79725695\/ctrl-c-not-working-in-android-studio-flutter-project",
    "text":"When I am opening any Flutter project <CTRL + C> not working in my Windows Android studio or anywhere. It's taking empty space. <CTRL+V> is pasting and empty space after copying any thing. But when I am closing the project or opening native project, <CTRL + C> working fine. I have uninstall the Flutter plugin, the issue resolved but when I reinstall it persist. When I use <CTRL + C> it's rebuilding the Flutter project everytime.",
    "author_id":5393,
    "publication_date":1754382873000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mir Injamamul",
    "author_reputation":167.0,
    "tags":"flutter, android-studio, plugins",
    "text_length":432,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":5868,
    "title":"espressif ide error : FileNotFoundError: [Errno 2] right after installation",
    "link":"https:\/\/stackoverflow.com\/questions\/79725701\/espressif-ide-error-filenotfounderror-errno-2-right-after-installation",
    "text":"I am installing -I have installed- the Espressif-IDE to have as many posible features as I can instead of the Visual Studio Code. I installed it all with the online installer without any problem: Ok, then I am going to create the very first espressif IDF project: First pop up with errors: Ok, I will solve this error. By pressing the button Add ESP_IDF. How can I solve this problem?",
    "author_id":5392,
    "publication_date":1754383784000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Carles Lloret Salvo",
    "author_reputation":55.0,
    "tags":"esp32, esp-idf, espressif-idf",
    "text_length":384,
    "title_length":75,
    "num_tags":3
  },
  {
    "id":5867,
    "title":"Line chart&gt; (swimlane\/ngx-charts) In zoom-in mode, the chart marker is displaying the value of the next data point instead of the selected one",
    "link":"https:\/\/stackoverflow.com\/questions\/79725709\/line-chart-swimlane-ngx-charts-in-zoom-in-mode-the-chart-marker-is-displayin",
    "text":"1. Marker Display Issue in Zoom-In Mode When zooming into the line chart, the marker\/tooltip is showing the value of the next data point instead of the currently selected one. 2. Tooltip Position Misalignment Applying the following zoom styles transform: scale(1.39); transform-origin: center top; the tooltip position to become misaligned — it no longer correctly aligns with the corresponding data point on the chart with the above styles Example Link Stackblitz Link",
    "author_id":5132,
    "publication_date":1754384044000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sathya",
    "author_reputation":49.0,
    "tags":"angular, charts, transform, zooming, scale",
    "text_length":469,
    "title_length":145,
    "num_tags":5
  },
  {
    "id":5866,
    "title":"Why does not sympy simplify &quot;Abs(cos(inc))\/sqrt(cos(inc)**2)&quot; as 1?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725715\/why-does-not-sympy-simplify-abscosinc-sqrtcosinc2-as-1",
    "text":"I am trying to use sympy help me simplify some equations and check whether two formula are equivelant. But in one case I find it quite confusing when handling Abs(cos(inc))\/sqrt(cos(inc)**2). I expect the output should be 1 but the original expression is given back. ``` from sympy import simplify, parse_expr,Symbol inc=Symbol('inc',positive=True,real=True) exp_str='1*Abs(cos(inc))\/sqrt(cos(inc)**2)' exp=parse_expr(exp_str) tmp=exp.subs(\"Abs(cos(inc))\/sqrt(cos(inc)**2)\",\"1\") ``` which gives the following output: ``` Abs(cos(inc))\/sqrt(cos(inc)**2) ``` instead of simply 1. Note that I already restrict the inc to be positive real number, I wonder what to do to make it work and understand why the formula is not simplified. Some reference about the difference of expression, function is also welcomed.",
    "author_id":5391,
    "publication_date":1754384454000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"jp1527",
    "author_reputation":88.0,
    "tags":"python, sympy",
    "text_length":806,
    "title_length":77,
    "num_tags":2
  },
  {
    "id":5865,
    "title":"Input audio from microphone not collected when audio is reproduced",
    "link":"https:\/\/stackoverflow.com\/questions\/79725721\/input-audio-from-microphone-not-collected-when-audio-is-reproduced",
    "text":"I'm developing a simple real-time voice bot using the OpenAI real-time API, specifically integrating with Semantic Kernel. The code is written in an async manner, and it initially works well. However, I'm facing issues with task synchronization and the event loop, preventing me from interrupting the bot or audio playback until it finishes completely. When the application starts, everything works as expected: I ask a question, and the bot responds. But after this initial interaction, I can only pose another question once the bot has finished responding. Upon debugging, I noticed that writing the microphone's input stream to a file captures perfect audio initially. However, once the bot begins responding, the recorded audio becomes chopped and incomplete, only resuming full recording once the bot finishes playback. This leads me to believe there is a problem with my generators and their synchronization. When the OpenAI service sends events, the receive generator processes them, but the input generator stops working, failing to collect data, while the other generator plays back the response. Occasionally, the event loop returns to the microphone generator briefly, but it only captures incomplete audio. Ideally, I want the input audio stream to continuously record in parallel with the model's output being played through the speakers. I suspect multithreading might be necessary, but Python's Global Interpreter Lock (GIL) complicates this approach. Could anyone suggest solutions or workarounds to ensure uninterrupted audio recording while handling the model's output? Are there specific techniques or libraries that can help manage concurrency effectively in this scenario? Here's some code (it's not complete for full reproducibility, but should be enough to give an idea): VoiceBotService: ``` import asyncio import numpy as np from semantic_kernel.contents.realtime_events import RealtimeAudioEvent, RealtimeEvents from acev_realtime_voice_bot.service.ports import ( AIStreamingServicePort, AudioInputPort, AudioOutputPort, ) from tests.events import CallInterruptedEvent class VoiceBotService: def __init__( self, audio_in: AudioInputPort, audio_out: AudioOutputPort, ai_service: AIStreamingServicePort, ) -> None: self.audio_in = audio_in self.audio_out = audio_out self.ai_service = ai_service async def run(self): async def receive_task(): async for event in self.ai_service.receive(): await asyncio.sleep(0.01) if ( isinstance(event, RealtimeAudioEvent) or isinstance(event, np.ndarray) # Can convert this into a response done of OpenAI and eliminate this # Gonna have coupling with openAI SDK, but that's ok for the moment. ): await self.audio_out.send_audio_output(event) if isinstance(event, CallInterruptedEvent): break else: await self.event_handler(event) receive_task_future = asyncio.create_task(receive_task()) # the problem is definitely with the async generator in sending the audio. That's the only difference i found between my code and the others. # By saving the audio frames i get as input, it looks like the input microphone gets blocked in collecting audio! The first time or when the bot is done answeringf # It saves all the audio and sends it, but when the bot is answering, the audio is weirdly chopped and missing! # Some locking of the audio interface? Sync problems? async for audio_frame in self.audio_in.get_input_audio_frames(): print(\"Sending audio\") await self.ai_service.send(audio_frame) await receive_task_future async def event_handler(self, event: RealtimeEvents): print(event.service_type) ``` LocalAudioRecorder (used to record the microphone): ``` class LocalAudioRecorder(AudioInputPort): def __init__(self, device, sample_rate, channels, dtype, frame_size) -> None: super().__init__() self.device = device self.sample_rate = sample_rate self.channels = channels self.dtype = dtype self.frame_size = frame_size async def get_input_audio_frames(self) -> AsyncGenerator[np.ndarray, None]: \"\"\"Generator function to yield audio data chunks and save them to a WAV file.\"\"\" try: with InputStream( samplerate=self.sample_rate, channels=self.channels, device=self.device, dtype=np.int16, ) as stream: # Open a WAV file to write the audio data with wave.open('recorded_audio.wav', 'wb') as wav_file: wav_file.setnchannels(self.channels) wav_file.setsampwidth(np.dtype(np.int16).itemsize) wav_file.setframerate(self.sample_rate) while True: if self._is_key_pressed(): input() # Clear the input buffer print(\"Stopping recording...\") break if stream.read_available < self.frame_size: await asyncio.sleep(0) continue audio_chunk, _ = stream.read(self.frame_size) print(\"Read audio chunk.\") print(f\"Content of audio: {audio_chunk}\") # Write the audio chunk to the WAV file wav_file.writeframes(audio_chunk.tobytes()) await asyncio.sleep(0) yield audio_chunk # Check for Enter keypress to stop recording except Exception as e: print(f\"An error occurred: {e}\") def _is_key_pressed(self): return select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], []) ``` LocalAudioPlayer (used to play the audio): ``` class LocalAudioPlayer(AudioOutputPort): def __init__(self, channels, sample_rate) -> None: self.channels = channels self.sample_rate = sample_rate self.stream = None async def send_audio_output( self, audio_frame: ndarray | RealtimeAudioEvent ) -> None: if isinstance(audio_frame, RealtimeAudioEvent): audio_frame = np.frombuffer(audio_frame.audio.data, dtype=np.int16) if self.stream is None: # Initialize the stream self.stream = OutputStream( channels=self.channels, samplerate=self.sample_rate, dtype=\"float32\" ) self.stream.start() # Convert int16 audio chunk to float32 and normalize to [-1.0, 1.0] audio_chunk = audio_frame.astype(np.float32) \/ np.iinfo(np.int16).max self.stream.write(audio_chunk) ``` And this is the adapter for the openAI realtime connection: ``` import base64 from collections.abc import AsyncGenerator, Callable, Coroutine from typing import Any, ClassVar, Final, cast import numpy as np from numpy import ndarray from semantic_kernel.connectors.ai import FunctionChoiceBehavior from semantic_kernel.connectors.ai.open_ai import ( AzureRealtimeExecutionSettings, AzureRealtimeWebsocket, ) from semantic_kernel.contents.audio_content import AudioContent from semantic_kernel.contents.realtime_events import RealtimeAudioEvent, RealtimeEvents from acev_realtime_voice_bot.service.ports import AIStreamingServicePort class OpenAIRealtimeAdapter(AIStreamingServicePort): def __init__( self, system_prompt: str, endpoint: str | None = None, api_version: str | None = None, deployment_name: str | None = None, ) -> None: super().__init__() self.client = AzureRealtimeWebsocket( endpoint=endpoint, api_version=api_version, deployment_name=deployment_name ) self.settings = AzureRealtimeExecutionSettings( instructions=system_prompt, turn_detection={\"type\": \"server_vad\"}, voice=\"shimmer\", input_audio_format=\"pcm16\", output_audio_format=\"pcm16\", input_audio_transcription={\"model\": \"whisper-1\"}, function_choice_behavior=FunctionChoiceBehavior.Auto(), ) async def create_session(self): await self.client.create_session( settings=self.settings ) # TODO: Add chathistory if needed. async def close_session(self): await self.client.close_session() async def send(self, event: RealtimeEvents | np.ndarray) -> None: if isinstance(event, np.ndarray): event = self._cast_input_audio_to_event(event) print(\"Sending event to openAI\") await self.client.send(event=event) async def receive( self, audio_output_callback: Callable[[ndarray], Coroutine[Any, Any, None]] | None = None, ) -> AsyncGenerator[RealtimeEvents, None]: async for event in self.client.receive( audio_output_callback=audio_output_callback ): yield event # TODO: Dont know if this is a smell of bad code, probably. I should unify somehow the interfaces and the data types. def _cast_input_audio_to_event(self, audio_frame) -> RealtimeAudioEvent: return RealtimeAudioEvent( audio=AudioContent( data=base64.b64encode(cast(Any, audio_frame)).decode(\"utf-8\") ) ) ``` (I would like to add an audio sample but I can't figure out how to do it on stackoverflow, if you know how, please let me know and I will also add the audio sample)",
    "author_id":5390,
    "publication_date":1754384874000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mattia Surricchio",
    "author_reputation":1618.0,
    "tags":"python, python-asyncio, audio, real-time, python-sounddevice",
    "text_length":8168,
    "title_length":66,
    "num_tags":5
  },
  {
    "id":5864,
    "title":"How to send Live Activity push token to server (like Airship or OneSignal) if app hasn&#39;t been opened for days?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725723\/how-to-send-live-activity-push-token-to-server-like-airship-or-onesignal-if-ap",
    "text":"What I Know So Far: You must call Activity.request(..., pushType: .token) inside the app to create a Live Activity and receive its push token. The token can be captured using for await activity.pushTokenUpdates. Once you send that token to your backend (or Airship \/ OneSignal), the server can send updates using the apns-push-type: liveactivity payload — even while the app is terminated. strong text My Questions: How can the app send the Live Activity push token to the server if the user hasn’t opened the app for days? Is it mandatory that the app must be opened at least once before the match to start a Live Activity and send the token? Can third-party services like Airship or OneSignal start a Live Activity without the app running (i.e., via silent push or background fetch)? in the FotMob app, I’ve noticed they display a stopwatch-style placeholder Live Activity about 1 hour before the match, and then begin updating it with real scores at kickoff. Are they using something like Background App Refresh or silent push notifications to wake the app in between and update the activity — or is the entire logic handled solely by APNs liveactivity pushes without reopening the app? My Goal: I want to implement Live Activities in my app that work like these professional apps — reliably updating even when the user hasn't opened the app close to the event time. Any insight or experience from those who've built large-scale Live Activity flows would be much appreciated!",
    "author_id":5389,
    "publication_date":1754385092000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"shanth kumar",
    "author_reputation":25.0,
    "tags":"swift, push-notification, urbanairship.com, widgetkit, activitykit",
    "text_length":1478,
    "title_length":114,
    "num_tags":5
  },
  {
    "id":5863,
    "title":"Kamal hangs on healthcheck when deploying",
    "link":"https:\/\/stackoverflow.com\/questions\/79725724\/kamal-hangs-on-healthcheck-when-deploying",
    "text":"I'm trying to deploy my Rails 8 app to Digital Ocean with Kamal. But it seems to hang here: Running docker exec kamal-proxy kamal-proxy deploy store-web --target=\"f41e7b74c98a:80\" --host=\"my-ip-address\" --deploy-timeout=\"30s\" --drain-timeout=\"30s\" --buffer-requests --buffer-responses --log-request-header=\"Cache-Control\" --log-request-header=\"Last-Modified\" --log-request-header=\"User-Agent\" on my-ip-address I get this error: docker stderr: Error: target failed to become healthy within configured timeout (30s) Does anyone has experience with this error when deploying a Rails 8 app? Thanks for your advice",
    "author_id":5388,
    "publication_date":1754385137000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Anthony Candaele",
    "author_reputation":3.0,
    "tags":"digital-ocean, ruby-on-rails-8, kamal",
    "text_length":609,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":5862,
    "title":"Create event not showing event in attendee&#39;s calendar",
    "link":"https:\/\/stackoverflow.com\/questions\/79725727\/create-event-not-showing-event-in-attendees-calendar",
    "text":"I have implemented MS Graph API in my Laravel project for create event, it's creating event successfully. After event creation orgnaizer calender (Teams) showing event but attendee's calendar (Teams) not showing event, strange. AUTH TOKEN CODE ``` $oauthClient = new \\League\\OAuth2\\Client\\Provider\\GenericProvider([ 'clientId' => config('azure.appId'), 'clientSecret' => config('azure.appSecret'), 'redirectUri' => config('azure.redirectUri'), 'urlAuthorize' => config('azure.authority') . config('azure.authorizeEndpoint'), 'urlAccessToken' => config('azure.authority') . config('azure.tokenEndpoint'), 'urlResourceOwnerDetails' => '', 'scopes' => config('azure.scopes') ]); \/\/ Make the token request $accessToken = $oauthClient->getAccessToken('authorization_code', [ 'code' => $authCode ]); $graph = new Graph(); $graph->setAccessToken($accessToken->getToken()); $user = $graph->createRequest('GET', '\/me?$select=displayName,mail,mailboxSettings,userPrincipalName') ->setReturnType(Model\\User::class) ->execute(); $tokenCache = new TokenCache(); $tokenCache->storeTokens($accessToken, $user); return redirect('\/'); ``` CREATE EVENT CODE (where i have called \/me\/events api for create event) ``` public function createNewEvent(Request $request) { \/\/ Validate required fields $request->validate([ 'eventSubject' => 'nullable|string', 'eventAttendees' => 'nullable|string', 'eventStart' => 'required|date', 'eventEnd' => 'required|date', 'eventBody' => 'nullable|string' ]); $viewData = $this->loadViewData(); $graph = $this->getGraph(); \/\/ Attendees from form are a semi-colon delimited list of \/\/ email addresses $attendeeAddresses = explode(';', $request->eventAttendees); \/\/ The Attendee object in Graph is complex, so build the structure $attendees = []; foreach($attendeeAddresses as $attendeeAddress) { array_push($attendees, [ \/\/ Add the email address in the emailAddress property 'emailAddress' => [ 'address' => $attendeeAddress ], \/\/ Set the attendee type to required 'type' => 'required' ]); } \/\/ Build the event $newEvent = [ 'subject' => $request->eventSubject, 'attendees' => $attendees, 'start' => [ 'dateTime' => $request->eventStart, 'timeZone' => $viewData['userTimeZone'] ], 'end' => [ 'dateTime' => $request->eventEnd, 'timeZone' => $viewData['userTimeZone'] ], 'body' => [ 'content' => $request->eventBody, 'contentType' => 'text' ] ]; \/\/ POST \/me\/events $response = $graph->createRequest('POST', '\/me\/events') ->attachBody($newEvent) ->setReturnType(Model\\Event::class) ->execute(); return redirect('\/calendar'); } ``` .ENV where I have defined all required keys. ``` OAUTH_APP_ID= {$client_id} OAUTH_APP_SECRET={$client_secrect} OAUTH_REDIRECT_URI=http:\/\/localhost:8000\/callback OAUTH_SCOPES='openid profile offline_access user.read mailboxsettings.read calendars.readwrite' OAUTH_AUTHORITY=https:\/\/login.microsoftonline.com\/common OAUTH_AUTHORIZE_ENDPOINT=\/oauth2\/v2.0\/authorize OAUTH_TOKEN_ENDPOINT=\/oauth2\/v2.0\/token ``` Note: I have used MS Graph free subscription, so I am not sure that attendee's calendar update is available in free version.",
    "author_id":5387,
    "publication_date":1754385292000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"shazim ali",
    "author_reputation":439.0,
    "tags":"azure, microsoft-graph-api, microsoft-graph-teams",
    "text_length":3072,
    "title_length":57,
    "num_tags":3
  },
  {
    "id":5861,
    "title":"How to suppress &quot;Restricted method called: java.lang.System::load&quot; warning when using native-platform in Gradle on JDK 22+",
    "link":"https:\/\/stackoverflow.com\/questions\/79725728\/how-to-suppress-restricted-method-called-java-lang-systemload-warning-when",
    "text":"I'm working with Gradle 8.12 and Java 22, and I'm encountering the following warning during build: ``` WARNING: A restricted method in java.lang.System has been called WARNING: java.lang.System::load has been called by net.rubygrapefruit.platform.internal.NativeLibraryLoader... WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning... WARNING: Restricted methods will be blocked in a future release unless native access is enabled ``` I understand this relates to Java's native access restrictions introduced in newer JDKs. I'm trying to silence the warning by setting: ``` --enable-native-access=ALL-UNNAMED ``` However, I'm unsure of the best way to add this JVM argument to Gradle. Should it go in gradle.properties, as an environment variable, or somewhere else? Also, is this the recommended way for builds using native-platform? Any guidance on where exactly to configure this in a Gradle project (and whether this approach is safe\/portable) would be appreciated!",
    "author_id":5386,
    "publication_date":1754385412000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Akhil Antony",
    "author_reputation":3.0,
    "tags":"java, android, flutter, gradle, build",
    "text_length":983,
    "title_length":132,
    "num_tags":5
  },
  {
    "id":5860,
    "title":"How to compile a Vulkan app with Mingw-w64 using CMake from Linux to target Windows?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725739\/how-to-compile-a-vulkan-app-with-mingw-w64-using-cmake-from-linux-to-target-wind",
    "text":"I'm trying to compile an executable link with Vulkan using Mingw-w64 and CMake and come up with this: ``` if(CMAKE_SYSTEM_NAME STREQUAL \"Windows\" AND CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") # using mingw find_package(VulkanHeaders REQUIRED) add_executable(myExe) target_sources(myExe PRIVATE main.cpp) target_link_libraries(myExe PRIVATE Vulkan::Headers) target_link_libraries(myExe PRIVATE \/home\/adem\/VulkanRT-X64-1.4.321.0-Components\/x64\/vulkan-1.dll) endif() ``` The code needs to link with Vulkan but Vulkan SDK distributed as .exe file and can't install it on Linux but SDK download page also provides runtime files which has the shared libraries vulkan-1.dll file but not the header files (naturally, that's the runtime files). Another repo Vulkan-Headers provides Vulkan header files. Vulkan headers repo has tags to match with SDK version. I compiled and installed it using Mingw-w64. The code above works but that's 2 step solution, getting the components of the same library from 2 different place doesn't feel safe, and it might confuse the anyone use the script above. One can pass a variable (with ``` -D ``` ) and use a path on ``` target_link_directories() ``` but then there might be a missing symbol when the Vulkan Headers and the passed SDK version mismatch. I wonder is there a better way to use Vulkan when cross compiling from Linux to generate Windows binaries using Mingw-w64. Thanks in advance...",
    "author_id":5385,
    "publication_date":1754386048000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Adem Budak",
    "author_reputation":79.0,
    "tags":"c++, cmake, cross-compiling, vulkan, mingw-w64",
    "text_length":1419,
    "title_length":84,
    "num_tags":5
  },
  {
    "id":5859,
    "title":"Session variables are not set on mock request Fat Free Framework",
    "link":"https:\/\/stackoverflow.com\/questions\/79725740\/session-variables-are-not-set-on-mock-request-fat-free-framework",
    "text":"I am trying to write Tests for my App. In the controller I set a message in the session: ``` $f3->set('SESSION.error_message', 'Some error'); ``` I my test I mock a request to that route and try to check the Session variable afterwards ``` $f3->mock('POST \/someRoute'); $test->expect(!empty($f3->get('SESSION.error_message')), 'An error occured!'); ``` However, this test always fails, i.e. the error message is always empty. How comes?",
    "author_id":5384,
    "publication_date":1754386080000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MaximeW",
    "author_reputation":440.0,
    "tags":"php, unit-testing, session, fat-free-framework",
    "text_length":436,
    "title_length":64,
    "num_tags":4
  },
  {
    "id":5858,
    "title":"JSON.DEL applied to an array with filter on its elements returns no more than 10 elements",
    "link":"https:\/\/stackoverflow.com\/questions\/79725743\/json-del-applied-to-an-array-with-filter-on-its-elements-returns-no-more-than-10",
    "text":"I have this json document ``` $.workspace.localIndex[] ``` where array elements have this schema ``` { \"propertyName\": \"the-name\", \"referencedItem\": \"e2e34852-0577-4fc1-9eb8-e68acd2a548c\", \"hit\": 1, \"ID\": \"5fdca267-0fbd-4b30-80cb-748133d58b01\", \"ownerId\": \"7416f9ed-d165-4817-ad0e-9caa259ad0ff\" } ``` a call to ``` JSON.DEL mf\/application\/workspace-manager\/workspace(IVA74BIS:04.00.00-0) \"$.workspace.localIndex[?(@.ownerId == '<value>')]\" ``` delete no more than 10 elements every time. Is this the desidered behaviour? Am I missing something? ``` JSON.DEL test2 JSON.SET test2 \"$\" '{\"localIndex\":[]}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"1\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"14\"}' JSON.ARRAPPEND test2 \"$.localIndex\" '{\"index\": \"15\"}' JSON.DEL test2 \"$.localIndex[?(@.index == '1')]\" JSON.GET test2 \"$.localIndex[?(@.index == '1')]\" ``` Last ``` JSON.DEL ``` returns ``` 10 ``` as result and next ``` JSON.GET ``` returns an array with 3 elements where the expectation is an empty one.",
    "author_id":5383,
    "publication_date":1754386124000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Luca Basso Ricci",
    "author_reputation":18465.0,
    "tags":"redis, redisjson",
    "text_length":1650,
    "title_length":89,
    "num_tags":2
  },
  {
    "id":5857,
    "title":"Placeholder Text Color Not Persisting When Focus Changes on UITextField",
    "link":"https:\/\/stackoverflow.com\/questions\/79725747\/placeholder-text-color-not-persisting-when-focus-changes-on-uitextfield",
    "text":"I'm facing an issue in tvOS related to setting the placeholder text color of a UITextField. I programmatically change the placeholder color using an attributed string like this: ``` textField.attributedPlaceholder = NSAttributedString( string: \"Enter text\", attributes: [.foregroundColor: UIColor.gray] ) ``` The color sets correctly initially, but when the focus changes (i.e., the text field becomes focused or unfocused), the placeholder color resets automatically, and the custom color I set is lost. This issue seems specific to tvOS, as it doesn't happen the same way in iOS. What I've tried: Setting the placeholder color again in didUpdateFocus(in:with:), but it's not a reliable solution. Observing focus updates and re-applying the placeholder styling, but it's not smooth and still inconsistent. How can I ensure that the placeholder text color stays consistent in a UITextField across focus changes in tvOS? Is there a proper way to override or hook into the focus behavior to preserve the placeholder styling? Code i am using on didUpdateFocusOn to update the textField UI like text colour and placeholder colour: ``` private func setTextFieldUI(button : UIButton ,isNextView : Bool) { guard let textField = button == btnEmail ? tfEmail : tfPassword else { print(\"Ui update failed\") return } let placeColor = isNextView ? UIColor.black.withAlphaComponent(0.5) : UIColor.white.withAlphaComponent(0.5) let placeHolder = textField == tfEmail ? \"Username\" : \"Password\" textField.attributedPlaceholder = NSAttributedString( string: placeHolder, attributes: [ .foregroundColor: placeColor ] ) textField.backgroundColor = isNextView ? UIColor.white : #colorLiteral(red: 0.2789022923, green: 0.2789022923, blue: 0.2789022923, alpha: 0.8) textField.textColor = isNextView ? UIColor.black : UIColor.white } ```",
    "author_id":5382,
    "publication_date":1754386430000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Number One",
    "author_reputation":49.0,
    "tags":"ios, xcode, placeholder, tvos, onfocus",
    "text_length":1813,
    "title_length":71,
    "num_tags":5
  },
  {
    "id":5856,
    "title":"Evaluate expression in Helm deployment file from values file",
    "link":"https:\/\/stackoverflow.com\/questions\/79725750\/evaluate-expression-in-helm-deployment-file-from-values-file",
    "text":"I have the following code block in my Helm chart ( ``` deployment.yml ``` ) ``` {{`{{- with secret \"my\/path\/kv\/key\" }} {{ .Data.data.key | indent 10 }} {{- end }} `}} ``` I would like to replace the second line ``` {{ .Data.data.key | indent 10 }} ``` with a variable from Values file. For instance, instead of having ``` .Data.data.key ``` hardcoded, I would like to have a variable, which can take for instance ``` {{ .Data.data.key | indent 10 }} {{ .Data.key | indent 10 }} {{ .Data.data.mykey | indent 10 }} etc ``` I went to add this to my Values file: ``` vault: data_path: .Data.data.key ``` ``` vault: data_path: .Data.key ``` ``` vault: data_path: .Data.data.mykey ``` And this to my deployment file: ``` {{`{{- with secret \"my\/path\/kv\/key\" }} {{ {{ .Values.vault.data_path }} | indent 10 }} {{- end }} `}} ``` I was hoping this would evaluate and take the value from the Values file. However, it is not working. The result would be just: ``` {{`{{- with secret \"my\/path\/kv\/key\" }} {{ {{ .Values.vault.data_path }} | indent 10 }} {{- end }} `}} ``` Instead of ``` {{`{{- with secret \"my\/path\/kv\/key\" }} {{ .Data.data.key | indent 10 }} {{- end }} `}} ``` What is the correct syntax to evaluate the second line?",
    "author_id":5381,
    "publication_date":1754386491000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"PatPanda",
    "author_reputation":5220.0,
    "tags":"kubernetes-helm",
    "text_length":1220,
    "title_length":60,
    "num_tags":1
  },
  {
    "id":5855,
    "title":"Circumvent Gatekeeper for macOS package released outside of the macOS App Store",
    "link":"https:\/\/stackoverflow.com\/questions\/79725752\/circumvent-gatekeeper-for-macos-package-released-outside-of-the-macos-app-store",
    "text":"As I did not get any answer related to this issue so far neither from Apple Support nor from the Apple Developer Support Forum, I wanted to give it a try here too. I also aim to ask this question because there are posts saying that the notarization process was completed but the issue persists (e.g. here ), so I want to make sure to make things right right from the start. I am responsible for the mobile app and thus also of the apple developer and app store connect accounts of a company. An external freelancer developed a software package for us which we aim to offer for installation and use on macOS systems of our customers; distributed exclusively outside of the Apple App Store. The software package has nothing to do with the mobile app. MacOS' Gatekeeper currently warns or even prevents our customers regarding the installation of the package on their device; pretty much as described here: https:\/\/developer.apple.com\/developer-id\/ . According to a previous talk with Apple's Support, the software package (.app) the Freelancer developed must be signed (and optionally notarized) with one of our own certificates. As we cannot grant selective app store connect access to third persons (only for the concerned certificates), we prefer to not provide access to our entire apple developer account to the freelancer, for the sole reason of the certificate & signing process. According to previous attempts with Apples' support regarding the most feasible solution in this case, they recommended me to manage the signing of the package of the freelancer, and simply request the package from the freelancer. I've thus generated an according Developer ID Certificate, but regarding the signing process, I'm confused. I know how signing works with mobile apps in XCode, but regarding software that is not distributed throughout the App Store on macOS, I'm unsure about the process. Also, as far as I know, the entitlements of the application are involved in the signing process. So my concern is that simply having the software package (.app) from the freelancer is not really enough to complete the signing + notarization process? Won't I need further information about the app's entitlements etc.?",
    "author_id":5380,
    "publication_date":1754386589000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"DevelJoe",
    "author_reputation":1476.0,
    "tags":"ios, macos, code-signing-certificate, notarize",
    "text_length":2205,
    "title_length":79,
    "num_tags":4
  },
  {
    "id":5854,
    "title":"Vue application cache busting",
    "link":"https:\/\/stackoverflow.com\/questions\/79725755\/vue-application-cache-busting",
    "text":"I have updated an application and it's accessible to user. How do I make the user get the latest application rather the previous cached one? The user can see the cached previous deployed application normal css enforces this with the version tag like below ``` <link rel=\"stylesheet\" href=\"css\/styledynamicm.css?version=2\" type=\"text\/css\"> ``` I saw a solution in Vue WebApp - Load latest build The cache busting method would suit me, but my scripts are embedded in single page. Any help for single pages? TIA",
    "author_id":5379,
    "publication_date":1754386856000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"joseph son",
    "author_reputation":21.0,
    "tags":"vue.js, browser-cache",
    "text_length":508,
    "title_length":29,
    "num_tags":2
  },
  {
    "id":5853,
    "title":"How to avoid double checks with asyncio.Lock",
    "link":"https:\/\/stackoverflow.com\/questions\/79725756\/how-to-avoid-double-checks-with-asyncio-lock",
    "text":"I have a piece of code, that checks if the redis db has data updates and loads them to the memory. I want only one corouitine to execute this load. ``` class MyService: def __init__(self): self.lock = asyncio.Lock() self.latest_timestamp = None async def _check_latest_timestamp_from_db(self): # go to db async def ensure_update_loaded(): latest_timestamp_from_db = await self._check_latest_timestamp_from_db() if self.timestamp == latest_timestamp_from_db: return if self.lock.locked(): return # the load is in progress, we're okay to use the old data for now async with self.lock: # do the load self.timestamp = latest_timestamp_from_db ``` From my understanding multiple coroutines can go simultaneously to this line: ``` async with lock: ``` , after all the checks are passed. Yes, they will execute consequently, but the load will happen more than once. I could double check within the lock: ``` async with self.lock: if self.timestamp == latest_timestamp_from_db: return # do the load self.timestamp = latest_timestamp_from_db ``` But I think there should be a clearer solution to my problem.",
    "author_id":5378,
    "publication_date":1754386908000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Artem Ilin",
    "author_reputation":417.0,
    "tags":"python, python-asyncio, locking, primitive",
    "text_length":1098,
    "title_length":44,
    "num_tags":4
  },
  {
    "id":5852,
    "title":"dev mode with tsup and turborepo",
    "link":"https:\/\/stackoverflow.com\/questions\/79725757\/dev-mode-with-tsup-and-turborepo",
    "text":"In typescript turborepo, this is turbo.json ``` { \"$schema\": \"https:\/\/turborepo.com\/schema.json\", \"ui\": \"tui\", \"tasks\": { \"build\": { \"dependsOn\": [\"^build\"], \"inputs\": [\"$TURBO_DEFAULT$\", \".env*\"], \"outputs\": [\"dist\/**\"] }, \"dev\": { \"dependsOn\": [\"^build\"], \"cache\": false, \"persistent\": true } } } ``` I have an app and internal package that the app depends on. the app package.json have this ``` { \"name\": \"sample-node-app\", \"packageManager\": \"yarn@4.9.2\", \"type\": \"module\", \"scripts\": { \"build\": \"tsup\", \"dev\": \"tsup --watch --onSuccess \\\"node dist\/index.js\\\"\" }, \"dependencies\": { \"@org\/be-constants\": \"workspace:\", \"express\": \"^5.1.0\" }, \"devDependencies\": { \"@eslint\/js\": \"^9.32.0\", \"@types\/express\": \"^5.0.3\", \"@types\/node\": \"^24.2.0\", \"@org\/eslint\": \"workspace:\", \"@org\/tsup\": \"workspace:\", \"@org\/typescript\": \"workspace:*\", \"eslint\": \"^9.32.0\", \"tsup\": \"^8.5.0\", \"tsx\": \"^4.20.3\", \"typescript\": \"^5.9.2\" } } ``` The internal package has this package.json ``` { \"name\": \"@org\/be-constants\", \"version\": \"1.0.0\", \"description\": \"The constants mainly used in the backend\", \"license\": \"ISC\", \"author\": \"\", \"type\": \"module\", \"main\": \".\/dist\/index.js\", \"files\": [ \"dist\" ], \"exports\": { \".\": { \"types\": \".\/src\/index.ts\", \"import\": \".\/dist\/index.js\", \"default\": \".\/dist\/index.js\" }, \".\/package.json\": \".\/package.json\" }, \"scripts\": { \"build\": \"tsup\" }, \"devDependencies\": { \"@types\/node\": \"^24.2.0\", \"@org\/eslint\": \"workspace:*\", \"@org\/tsup\": \"workspace:*\", \"@org\/typescript\": \"workspace:*\", \"eslint\": \"^9.32.0\", \"tsup\": \"^8.5.0\", \"typescript\": \"^5.9.2\" } } ``` My tsup config for both app and package is the same. ``` export const nodeConfig: Options = { entry: [\"src\/**\/*.ts\"], clean: true, outDir: \".\/dist\", dts: false, bundle: false, format: [\"esm\"], target: \"node20\", }; ``` So when I run yarn turbo run dev --filter sample-node-app, the build for the package keeps repeating, output a file called ``` tsup.config.bundled_....mjs ``` over and over again. How can I fix this? I will provide any other required information. The dev process smoothly run and all its dependencies rebuild if something related changed",
    "author_id":5377,
    "publication_date":1754386913000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"polar",
    "author_reputation":1.0,
    "tags":"typescript, turborepo, monorepo, tsup",
    "text_length":2118,
    "title_length":32,
    "num_tags":4
  },
  {
    "id":5851,
    "title":"How to export all data from a Milvus collection?(~3 million records)",
    "link":"https:\/\/stackoverflow.com\/questions\/79725763\/how-to-export-all-data-from-a-milvus-collection3-million-records",
    "text":"As Milvus does not support adding fields by default, I need to export all data, move it to the new collection, and set it in the collection of dynamic fields. My plan is to export all data and then insert it, but how do I export it? I use query, but I keep encountering the 16384 limit. My primary key is in string format, so I cannot use the interval method.",
    "author_id":5376,
    "publication_date":1754387160000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jade Roy",
    "author_reputation":15.0,
    "tags":"python, database, milvus",
    "text_length":359,
    "title_length":68,
    "num_tags":3
  },
  {
    "id":5850,
    "title":"Why can&#39;t I draw on HTML canvas",
    "link":"https:\/\/stackoverflow.com\/questions\/79725765\/why-cant-i-draw-on-html-canvas",
    "text":"I'm trying to recreate a classic snake minigame I had made in the past but I've run into an issue with drawing on a canvas using JavaScript. Why does it not draw a rectangle on the canvas? First snippet is current state and second snippet is the old game, which still works fine. Whats the difference between these two? ``` function game() { let canvasSize = 500 let canvas = document.getElementById('game-canvas') function getGameCanvas() { let canvas = document.createElement(\"canvas\") canvas.id = \"game-canvas\" canvas.width = canvasSize canvas.height = canvasSize return canvas } function getGameDiv() { let gameDiv = document.createElement(\"div\") gameDiv.id = \"game-div\" gameDiv.appendChild(getGameCanvas()) let ctx = canvas.getContext(\"2d\") \/\/ set background ctx.fillStyle = \"#000\" ctx.fillRect(0, 0, canvasSize, canvasSize) return gameDiv } document.body.replaceChildren(getGameDiv()) } window.onload = () => { game() } ``` ``` <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Game<\/title> <link rel=\"stylesheet\" href=\"styles.css\"> <\/head> <body> <canvas id=\"game-canvas\"><\/canvas> <script src=\"this.js\"><\/script> <\/body> <\/html> ``` ``` function game() { let canvasSize = 500 let blockSize = 10 let gridSize = canvasSize\/blockSize let tickInterval = 100 let score = { value: -1, increment: () => { score.value++ \/\/ 1% speed increase for every food eaten tickInterval *= 0.8 }, reset: () => { score.value = -1 tickInterval = 100 } }; let food = { x: 25, y: 25, generateNew: () => { food.x = Math.round(Math.random() * (gridSize - 1)) food.y = Math.round(Math.random() * (gridSize - 1)) let flag = false; if (food.x === snake.head.x || food.y === snake.head.y) { flag = true } for (const bodyElement of snake.body) { if (food.x === snake.body.x || food.y === snake.body.y) { flag = true break } } if (flag) { food.generateNew() } score.increment() } } function getSnake() { return { head: { x: 3, y: 0 }, direction: { x: 1, y: 0 }, body: [ {x: 2, y: 0}, {x: 1, y: 0}, {x: 0, y: 0}, ], colors: { head: \"#00ff00\", body: \"#aaff00\" }, cachedDirection: { x: 1, y: 0 }, changeDirection: (newDirectionX, newDirectionY) => { let newDirection = { isDiagonal: newDirectionX * newDirectionY !== 0, isOpposing: newDirectionX + snake.cachedDirection.x === 0 || newDirectionX + snake.cachedDirection.x === 0, } \/\/ make sure one of the two is always zero, \/\/ so we don't have diagonal direction if (!newDirection.isDiagonal && !newDirection.isOpposing) { snake.direction.x = newDirectionX snake.direction.y = newDirectionY } }, move: () => { let predecessor = {x: snake.head.x, y: snake.head.y} \/\/ move forward based on current direction snake.head.x += snake.direction.x snake.head.y += snake.direction.y \/\/ artificially forcing positive only values, \/\/ by virtually moving our snake one grid length to the positive axis' snake.head.x += gridSize snake.head.y += gridSize \/\/ stay within actual grid snake.head.x %= gridSize snake.head.y %= gridSize \/\/ check if ate food if (food.x === snake.head.x && food.y === snake.head.y) { \/\/grow body without moving snake.body.unshift(predecessor) food.generateNew() } else { \/\/ make body follow for (let i = 0; i < snake.body.length; i++) { let temp = snake.body[i] snake.body[i] = predecessor predecessor = temp } } \/\/ cache current direction \/\/ we do it this way because javascript would use pass by reference by default \/\/ I hate this language Object.assign(snake.cachedDirection, snake.direction) \/\/ check for collision for (const bodyElement of snake.body) { if (snake.head.x === bodyElement.x && snake.head.y === bodyElement.y) { alert(\"Game over!\") restart() } } } } } let snake = getSnake() function restart() { score.reset() snake = getSnake() food.generateNew() } document.addEventListener(\"keydown\", (ev) => { switch (ev.key) { case \"w\": snake.changeDirection(0, -1) break case \"s\": snake.changeDirection(0, 1) break case \"a\": snake.changeDirection(-1, 0) break case \"d\": snake.changeDirection(1, 0) break } }) function draw() { let canvas = document.getElementById(\"game-canvas\") let ctx = canvas.getContext(\"2d\") \/\/ background ctx.fillStyle = \"#000000\" ctx.fillRect(0, 0, canvasSize, canvasSize) \/\/ snake \/\/ head ctx.fillStyle = snake.colors.head ctx.fillRect( snake.head.x*blockSize, snake.head.y*blockSize, blockSize, blockSize ) \/\/ body ctx.fillStyle = snake.colors.body for (const bodypart of snake.body) { ctx.fillRect( bodypart.x*blockSize, bodypart.y*blockSize, blockSize, blockSize ) } \/\/ food ctx.fillStyle = \"#ff0000\" ctx.fillRect(food.x*blockSize, food.y*blockSize, blockSize, blockSize) } function updateScoreboard() { document.getElementById(\"scoreboard\").innerHTML = score.value.toString() } function tick() { snake.move() draw() updateScoreboard() window.setTimeout(tick, tickInterval) } function getGameCanvas() { let canvas = document.createElement(\"canvas\") canvas.id = \"game-canvas\" canvas.width = canvasSize canvas.height = canvasSize return canvas } function getScoreBoard() { let div = document.createElement(\"div\") div.id = \"scoreboard\" return div } function getTickInterval() { } function getGameDiv() { let gameDiv = document.createElement(\"div\") gameDiv.id = \"game-div\" gameDiv.appendChild(getScoreBoard()) gameDiv.appendChild(getGameCanvas()) window.setTimeout(tick, tickInterval) return gameDiv } function getOuterWrapper() { let outerWrapper = document.createElement(\"div\") outerWrapper.id = \"outer-wrapper\" outerWrapper.appendChild(getGameDiv()) return outerWrapper } document.body.replaceChildren(getOuterWrapper()) food.generateNew() } window.onload = () => { game() } ``` ``` body { display: flex; align-items: center; justify-content: center; height: 100vh; width: 100vw; overflow: hidden; background-color: darkslategray; } .noactive { display: none; } #outer-wrapper { } #game-canvas { height: 500px; width: 500px; } #scoreboard { display: flex; align-items: center; justify-content: center; color: white; font-weight: bold; font-size: 32px; } ``` ``` <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Snake<\/title> <script type=\"text\/javascript\" src=\"snake.js\"><\/script> <link rel=\"stylesheet\" href=\"style.css\"> <\/head> <body> <\/body> <\/html> ``` DOM not being loaded is not the issue, since I do everything after ``` window.onload. ``` I've tried moving the ``` <script> ``` to before the ``` canvas ``` or inside the ``` head ``` but no result. I've also tried setting the initial color of the canvas through CSS to make sure it is visible and has the correct size, this is true. One thing I noticed while fiddling around with the old script is that it doesn't draw the first 2 frames it is told to. I gave it an array of color codes to draw, and the first 2 colors didn't show, only the 3rd onwards, what am I missing here? Solved: A \"dark mode\" browser extension was at fault, messing with colors, drawing etc.",
    "author_id":5375,
    "publication_date":1754387226000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Leonardo Hysesani",
    "author_reputation":89.0,
    "tags":"javascript, html5-canvas",
    "text_length":6836,
    "title_length":35,
    "num_tags":2
  },
  {
    "id":5849,
    "title":"IIS sometimes replaces 413 response from NGINX with 502 response",
    "link":"https:\/\/stackoverflow.com\/questions\/79725772\/iis-sometimes-replaces-413-response-from-nginx-with-502-response",
    "text":"I have a situation where IIS sometimes converts a 413 response from NGINX into a 502 response. However, sometimes it also correctly passes on the 413 response. I haven't been able to deduce a clear pattern, but I get the feeling that whenever I wait for long enough and try again, I get a 413 at first, followed by a 502 whenever I re-try. My network architecture looks like this: Proxy Server IIS Proxy Service Cloud Container NGINX Backend Service The happy path is: Request arrives at the Proxy Server, the IIS routes it to the Proxy Service, which routes it to the cloud container, where NGINX sends it to the backend service, which then returns a response that is returned in the reverse way. In the happy path that works. However, when uploading a file that is larger than the NGINX limit, the NGINX returns the following 413 response: ``` <html> <head><title>413 Request Entity Too Large<\/title><\/head> <body> <center><h1>413 Request Entity Too Large<\/h1><\/center> <hr><center>nginx<\/center> <\/body> <\/html> ``` I've confirmed that when I send my requests directly against the Proxy Service, that is what is returned in 100% of the cases. However, as soon as I go over IIS, I get above mentioned behavior, where often the 413 response is replaced with a 502 resposne with this body: ``` <!DOCTYPE html PUBLIC \"-\/\/W3C\/\/DTD XHTML 1.0 Strict\/\/EN\" \"http:\/\/www.w3.org\/TR\/xhtml1\/DTD\/xhtml1-strict.dtd\"> <html xmlns=\"http:\/\/www.w3.org\/1999\/xhtml\"> <head> <meta http-equiv=\"Content-Type\" content=\"text\/html; charset=iso-8859-1\"\/> <title>502 - Web server received an invalid response while acting as a gateway or proxy server.<\/title> <style type=\"text\/css\"> <!-- body{margin:0;font-size:.7em;font-family:Verdana, Arial, Helvetica, sans-serif;background:#EEEEEE;} fieldset{padding:0 15px 10px 15px;} h1{font-size:2.4em;margin:0;color:#FFF;} h2{font-size:1.7em;margin:0;color:#CC0000;} h3{font-size:1.2em;margin:10px 0 0 0;color:#000000;} #header{width:96%;margin:0 0 0 0;padding:6px 2% 6px 2%;font-family:\"trebuchet MS\", Verdana, sans-serif;color:#FFF; background-color:#555555;} #content{margin:0 0 0 2%;position:relative;} .content-container{background:#FFF;width:96%;margin-top:8px;padding:10px;position:relative;} --> <\/style> <\/head> <body> <div id=\"header\"><h1>Server Error<\/h1><\/div> <div id=\"content\"> <div class=\"content-container\"><fieldset> <h2>502 - Web server received an invalid response while acting as a gateway or proxy server.<\/h2> <h3>There is a problem with the page you are looking for, and it cannot be displayed. When the Web server (while acting as a gateway or proxy) contacted the upstream content server, it received an invalid response from the content server.<\/h3> <\/fieldset><\/div> <\/div> <\/body> <\/html> ``` My IIS configuration for the Proxy Service is as follows: ``` <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <system.web> <httpCookies httpOnlyCookies=\"true\" requireSSL=\"true\" \/> <\/system.web> <system.webServer> <httpErrors existingResponse=\"Replace\" \/> <rewrite> <rules> <rule name=\"ReverseProxyInboundRule1\" stopProcessing=\"true\"> <match url=\"(.*)\" \/> <action type=\"Rewrite\" url=\"http:\/\/localhost:1234\/{R:1}\" \/> <\/rule> <\/rules> <\/rewrite> <\/system.webServer> <\/configuration> ``` The ``` <httpErrors existingResponse=\"Replace\" \/> ``` is for preventing potentially revealing error messages (such as the NGINX error) to be returned to the outside. And this works sometimes. Every once and so often, when I try to upload a file that is too large going over IIS, I get a response with Status code 413 and the sterilized body \"The page was not displayed because the request entity is too large.\", which is the intended behavior. However, if I try it again afterwards I get the 502 BAD GATEWAY response again. Does anyone know why this happens and what I can do about that?",
    "author_id":5374,
    "publication_date":1754387451000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Kira Resari",
    "author_reputation":2526.0,
    "tags":"nginx, proxy, iis",
    "text_length":3815,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":5848,
    "title":"Flex Gateway-MULE: Replace Authorization Header After OAuth Authentication for Backend Compatibility",
    "link":"https:\/\/stackoverflow.com\/questions\/79725777\/flex-gateway-mule-replace-authorization-header-after-oauth-authentication-for-b",
    "text":"Good morning, I have a question regarding the use of Flex Gateway in MuleSoft. I would like to know if it's possible to configure, at the level of an experience API published on Flex Gateway, a way to modify the Authorization header after the user has been successfully authenticated against MuleSoft. Specifically, I need the Authorization value to be updated or replaced with another value that is required by the backend in order to process the request correctly. Is there a way to implement this logic—perhaps through custom policies, transformations, or any native functionality provided by Flex Gateway? I appreciate any guidance you can provide.",
    "author_id":5373,
    "publication_date":1754387720000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"claudia2014",
    "author_reputation":1.0,
    "tags":"authorization, mulesoft, flex-gateway",
    "text_length":652,
    "title_length":100,
    "num_tags":3
  },
  {
    "id":5847,
    "title":"Automapper. ForAllMembers except some members",
    "link":"https:\/\/stackoverflow.com\/questions\/79725781\/automapper-forallmembers-except-some-members",
    "text":"I'm using automapper 13.0.1 to map one object to another: ``` CreateMap<CandidateInfoContactDataSendEvent, TechUser>() .ForMember(dest => dest.PhoneNumber, opt => opt.MapFrom(src => src.NewPhoneNumber)) .ForMember(dest => dest.EmailHash, opt => opt.MapFrom<TechUserEmailHashResolver>()) .ForMember(dest => dest.BirthDate, opt => opt.Condition((src, dest) => dest.DestroyDate == null)) .ForMember(dest => dest.AdditionalPhoneNumber, opt => opt.Condition((src, dest) => dest.DestroyDate == null)) .ForMember(dest => dest.FirstName, opt => opt.Condition((src, dest) => dest.DestroyDate == null)) .ForMember(dest => dest.LastName, opt => opt.Condition((src, dest) => dest.DestroyDate == null)) .ForMember(dest => dest.FatherName, opt => opt.Condition((src, dest) => dest.DestroyDate == null)) .ForMember(dest => dest.Gender, opt => opt.Condition((src, dest) => dest.DestroyDate == null)) .ForMember(dest => dest.Email, opt => opt.Condition((src, dest) => dest.DestroyDate == null)); ``` As you can see some properties maps with the same condition. And I want to combine this conditions in one. I cant not use ForAllMembers for PhoneNumber and EmailHash because they do not depend on DestroyDate. But I need something like ForAllMembers.I need ForAllMembers condition except PhoneNumber and EmailHash. Thanks for help",
    "author_id":5372,
    "publication_date":1754387922000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Natalia",
    "author_reputation":313.0,
    "tags":"c#, automapper",
    "text_length":1312,
    "title_length":45,
    "num_tags":2
  },
  {
    "id":5846,
    "title":"Restrict results by a sublinked table that it&#39;s parent has multiple results on EF Core",
    "link":"https:\/\/stackoverflow.com\/questions\/79725785\/restrict-results-by-a-sublinked-table-that-its-parent-has-multiple-results-on-e",
    "text":"I'm sorry as the title may not be descriptive of the question. I'm trying to build an EF Core query and I don't know how the required where clause can be built. The equivalent SQL query may be: ``` SELECT * FROM Authors A LEFT JOIN Books B ON B.AuthorId = A.Id LEFT JOIN BookReviews R ON R.BookId=B.Id AND R.Edition=B.Edition WHERE A.Id = @AuthorId AND R.Approved='Y' ``` The point here is that there are multiple books per author, and I need to place a restriction in the where clause by a column on a child table that also has multiple results. ``` var results = await context.Authors .Include(i => i.Books) .ThenInclude(i => i.BookReviews) .FirstOrDefaultAsync(x => x.Id == author && x.Books..... ``` And there I can't continue with ``` .BookReview.Approved == \"Y\" ``` because Books is a collection. The navigation property for those two tables is created at the Books entity. (Don't ask me) ``` builder.HasOne(s => s.BookReview) .WithOne(d => d.Book) .HasForeignKey<BookReviews>(d => new { d.BookId, d.Edition }) .HasPrincipalKey<Books>(s => new { s.Id, s.Edition }) .IsRequired(false); ``` Note: There is always only one \"Approved\" result.",
    "author_id":5371,
    "publication_date":1754388021000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sergio",
    "author_reputation":1455.0,
    "tags":"c#, entity-framework-core",
    "text_length":1144,
    "title_length":90,
    "num_tags":2
  },
  {
    "id":5845,
    "title":"JavaFX ListView layout incorrect until scroll or click",
    "link":"https:\/\/stackoverflow.com\/questions\/79725789\/javafx-listview-layout-incorrect-until-scroll-or-click",
    "text":"I'm building a chat app in JavaFX using a ListView with custom cells to display messages. The issue I'm facing is this: When a new message arrives, its message bubble and its children are initially vertically stretched (extra vertical space), until I click on the cell or scroll the list. Then it resizes to the correct height. ChatController.java: ``` private final ObservableList<Message> messages = FXCollections.observableArrayList(); private String name; @FXML public void initialize() { messageListView.setItems(messages); messageListView.setCellFactory(listView -> new ListCell<>() { private FXMLLoader loader; private MessageCellController cellController; private Node root; @Override protected void updateItem(Message item, boolean empty) { super.updateItem(item, empty); if (empty || item == null) { setGraphic(null); setText(null); } else { String fxml = item.getUser().getName().equals(name) ? \"message_outgoing.fxml\" : \"message_incoming.fxml\"; if (loader == null || !loader.getLocation().toExternalForm().endsWith(fxml)) { try { loader = new FXMLLoader(getClass().getResource(fxml)); root = loader.load(); cellController = loader.getController(); } catch (Exception e) { e.printStackTrace(); } } if (cellController != null) { cellController.setData(item.getUser().getName(), item.getText()); setGraphic(root); } else { setText(item.getUser().getName() + \": \" + item.getText()); } } } }); \/\/ Handling new messages clientSocket = new ClientSocket(\"localhost\", 5000, name); clientSocket.addListener(new MessageListener() { @Override public void onMessageReceived(Message message) { Platform.runLater(() -> { messages.add(message); messageListView.scrollTo(messages.size() - 1); }); } @Override public void onError(Exception ex) {} @Override public void onConnectionClosed() {} }); } ``` MessageCellController.java: ``` @FXML public Label usernameLabel; @FXML public Label messageLabel; @FXML private VBox bubbleVBox; @FXML public void initialize() { messageLabel.setWrapText(true); \/\/ Limit width of the bubble to 60% of parent width bubbleVBox.maxWidthProperty().bind( ((Region) bubbleVBox.getParent()).widthProperty().multiply(0.6) ); } public void setData(String username, String message) { usernameLabel.setText(username); messageLabel.setText(message); } ``` Notes: Once I click the cell or scroll, the layout corrects itself. I’ve tried layout(), applyCss(), and even requestLayout() inside Platform.runLater() with no success. My question: Is there a reliable way to force the correct layout immediately after a message is added? Is this a known issue with JavaFX ListView reusing cells and not updating layout properly? Any advice or workarounds would be much appreciated. Thank you! Edit: The problem was in MessageCellController.java where I was binding width of one container to its parent container. After removing this the problem was fixed.",
    "author_id":5370,
    "publication_date":1754388375000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Richard Penč&#237;k",
    "author_reputation":11.0,
    "tags":"java, javafx, listview",
    "text_length":2864,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":5844,
    "title":"Xcode folder reference to an aggregate target&#39;s output folder breaks between Debug\/Release builds. BUILT_PRODUCTS_DIR not relative to the aggregate",
    "link":"https:\/\/stackoverflow.com\/questions\/79725792\/xcode-folder-reference-to-an-aggregate-targets-output-folder-breaks-between-deb",
    "text":"In my project I have an aggregate target that runs a script that generates a \"web bundle\" in a folder ``` build\/ ``` with all the html\/css\/resources needed for the website. These resources are generated via a script. I use these resources in multiple targets so I have created an aggregate target to make this simple. However, I have not found this simple. If I have the aggregate target output to ``` $(DERIVED_FILE_DIR)\/ ``` then it outputs to a path that looks like this ``` DerivedData\/BrowserApp-bla\/Build\/Intermediates.noindex\/BrowserApp.build\/Debug-iphonesimulator\/WebBundle.build\/DerivedSources\/build\/index.html ``` If I then drag that folder into Xcode and create a folder that references it. Then take that reference and make it relative to build products I get this ``` A9DE6A502E41E397005EF4E0 \/* build *\/ = { isa = PBXFileSystemSynchronizedRootGroup; name = build; path = \"..\/..\/Intermediates.noindex\/BrowserApp.build\/Debug-iphonesimulator\/WebBundle.build\/DerivedSources\/build\"; sourceTree = BUILT_PRODUCTS_DIR; }; ``` This is because to the main BrowserApp target the ``` BUILT_PRODUCTS_DIR ``` is ``` DerivedData\/BrowserApp-bla\/Build\/Products\/Debug-iphonesimulator ``` . You can see that the reference essentially has to maneuver out of the built products folder for the main BrowserApp target and into the built products directory for WebBundle. This is ok for a debug build I suppose you can imagine that this is going to break down the moment you have a release build. Now if we ignore that problem then comes the problem of getting it in Copy Bundle Resources. Xcode 16s ``` PBXFileSystemSynchronizedRootGroup ``` is a great improvement but does not help here. The only way I have found to get the folder copied is to hop into an earlier Xcode or make the following modifications. Change the ``` PBXFileSystemSynchronizedRootGroup ``` into a ``` PBXBuildFile ``` (In PBXFileReference section) ``` A9DE6A502E41E397005EF4E0 \/* ..\/..\/Intermediates.noindex\/client.build\/Debug\/WebBundle.build\/DerivedSources\/build *\/ = {isa = PBXFileReference; lastKnownFileType = text; path = ..\/..\/Intermediates.noindex\/BrowserApp.build\/Debug-iphonesimulator\/WebBundle.build\/DerivedSources\/build; sourceTree = BUILT_PRODUCTS_DIR; }; ``` (In the mainGroup PBXGroupSection) ``` A9DE6A502E41E397005EF4E0 \/* ..\/..\/Intermediates.noindex\/client.build\/Debug\/WebBundle.build\/DerivedSources\/build *\/, ``` Then remove any references for A9DE6A502E41E397005EF4E0 as a synchronized group Create a PBXBuildFile using the previous PBXBuildFile as its fileRef then add it to the copy resources stage (In PBXBuildFile section) ``` 96516AC32BF928DD00576562 \/* ..\/..\/Intermediates.noindex\/BrowserApp.build\/Debug-iphonesimulator\/WebBundle.build\/DerivedSources\/build in Resources *\/ = {isa = PBXBuildFile; fileRef = A9DE6A502E41E397005EF4E0 \/* ..\/..\/Intermediates.noindex\/BrowserApp.build\/Debug-iphonesimulator\/WebBundle.build\/DerivedSources\/build *\/; }; ``` (In the PBXBuildResources phase for the resources section of the main target) ``` 96516AC32BF928DD00576562 \/* ..\/..\/Intermediates.noindex\/BrowserApp.build\/Debug-iphonesimulator\/WebBundle.build\/ ``` I do not know of a way to do this with a PBXFileSystemSynchronizedRootGroup so these manual changes seem necessary. After these changes I have the following: And you can confirm the entire folder is accessible to the app. However if you try to build for release\/Archive you get ``` lstat(\/Users\/calebkierum\/Library\/Developer\/Xcode\/DerivedData\/BrowserApp-bla\/Build\/Intermediates.noindex\/ArchiveIntermediates\/BrowserApp\/Intermediates.noindex\/BrowserApp.build\/Debug-iphonesimulator\/WebBundle.build\/DerivedSources\/build): No such file or directory (2) ``` Which makes sense. The file reference has a relative path that explicitly mentions \"debug\" however because this path is based on an aggregate target I am not sure how you really update this reference for release.",
    "author_id":4396,
    "publication_date":1754388672000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"CalebK",
    "author_reputation":663.0,
    "tags":"ios, xcode",
    "text_length":3900,
    "title_length":151,
    "num_tags":2
  },
  {
    "id":5843,
    "title":"Superset Dynamic Pivot Table building: returns totals, but does not show splitted data for columns. I use Superset, Jinja and PostgreSQL",
    "link":"https:\/\/stackoverflow.com\/questions\/79725793\/superset-dynamic-pivot-table-building-returns-totals-but-does-not-show-splitte",
    "text":"Have trouble incorporating jinja template that I found on the internet https:\/\/habr.com\/ru\/companies\/magnit\/articles\/869924\/ Basically what they are doing there is using Jinja and filters to make Excell-like table. What I get is that my table builds up but it doesn't return anything on dimentions, only showing totals. I recieve this error when I try to watch data template: ``` LINE 1: SELECT goals AS goals, tasks AS tasks, fo AS fo, itogo_rub A... ``` This is query code: ``` --filter dictionary {% set dimension_value = { 'программа': \"programm\", 'иерархия': \"ierarchy\", 'цели': \"goals\", 'задачи': \"tasks\", 'ФО': \"fo\" } %} --measure and dimentions setup {% set FV_Measure = filter_values('measure') or ['итого_руб'] %} {% set FV_Dimension = filter_values('dimension') or ['программа', 'иерархия', 'цели', 'задачи', 'ФО'] %} {% set FV_Dimension1 = filter_values('dimension1') or ['программа', 'иерархия', 'цели', 'задачи', 'ФО'] %} --the main query SELECT {% for value in FV_Dimension %} {{ \",\" if not loop.first else \"\" }} {{ dimension_value.get(value) }} as \"{{value}}_{{ loop.index0 }}\" {% endfor %} {% for value in FV_Dimension1 %} ,{{ dimension_value.get(value) }} as \"{{ value }}_{{ loop.index0 + 10 }}\" {% if loop.last %}, {% endif %} {% endfor %} -- metrics {% for value in FV_Measure %} {% if value == 'итого_руб' %} Sum(itogo_rub) AS itogo_rub {% if not loop.last %}, {% endif %} {% else %} NULL {% if not loop.last %}, {% endif %} {% endif %} {% endfor %} --db table FROM (select tasks, fo, itogo_rub, programm, ierarchy, goals FROM gpbi.monitoring_gosprogramm_table) tab_base {% if FV_Measure %} GROUP BY {% for value in FV_Dimension %} {{ dimension_value.get(value) }} {{ \",\" if not loop.last else \"\" }} {% endfor %} {% for value in FV_Dimension1 %} , {{ dimension_value.get(value) }} {% endfor %} {% endif %} ``` this is typical dimension codes: dimention1 : ``` \"{{filter_values('dimension')[0] }}_0\" {% else %} coalesce(NULL, '') {% endif %} ``` dimention2: ``` {% if filter_values('dimension1')[10] %} \"{{filter_values('dimension1')[10] }}_0\" {% else %} coalesce(NULL, '') {% endif %} ``` measure: ``` {% if 'итого_руб' in filter_values('measure') %} Sum(itogo_rub) {% else %} NULL {% endif %} ``` This is generated query: ``` SELECT coalesce(NULL, '') AS \"1\", coalesce(NULL, '') AS \"2\", coalesce(NULL, '') AS \"3\", coalesce(NULL, '') AS \"4\", coalesce(NULL, '') AS \"5\", coalesce(NULL, '') AS \"10\", coalesce(NULL, '') AS \"20\", coalesce(NULL, '') AS \"30\", coalesce(NULL, '') AS \"40\", coalesce(NULL, '') AS \"50\", Sum(itogo_rub) AS \"итого_руб\" FROM ( SELECT programm as \"программа_0\", ierarchy as \"иерархия_1\", goals as \"цели_2\", tasks as \"задачи_3\", fo as \"ФО_4\", programm as \"программа_10\" , ierarchy as \"иерархия_11\" , goals as \"цели_12\" , tasks as \"задачи_13\" , fo as \"ФО_14\" , Sum(itogo_rub) AS itogo_rub FROM (select tasks, fo, itogo_rub, programm, ierarchy, goals FROM gpbi.monitoring_gosprogramm_table) tab_base GROUP BY programm, ierarchy, goals, tasks, fo, programm, ierarchy, goals, tasks, fo ) AS virtual_table GROUP BY coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') , coalesce(NULL, '') ORDER BY \"итого_руб\" DESC LIMIT 10000; ``` ``` All in all all seems fine but for some reason does not work. It brings up totals, but does not do what it is intended to do: splitted data and I receive an error which does not recognize my fields although they all are in the database. ``` LINE 1: SELECT goals AS goals, tasks AS tasks, fo AS fo, itogo_rub A... ``` Also field names in database are correct, checked 100 times. Looking forward for your replies. what it looks like what should it look like (example from another table)",
    "author_id":5369,
    "publication_date":1754388672000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Александр Нисилевич",
    "author_reputation":1.0,
    "tags":"postgresql, jinja2, apache-superset",
    "text_length":3786,
    "title_length":136,
    "num_tags":3
  },
  {
    "id":5842,
    "title":"How to make gRPC PostgreSQL function to increment count everytime a button is tapped?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725796\/how-to-make-grpc-postgresql-function-to-increment-count-everytime-a-button-is-ta",
    "text":"I tried to make a function to increment count for likes everytime a button is tapped using Flutter that trigger a Supabase PostgreSQL gRPC function. grpc function: ``` create or replace function increment_likes(post_id uuid) returns text language plpgsql as $$ begin update posts SET reactions = reactions + 1 where post_id = post_id; return 'Likes incremented successfully'; end; $$; ``` database snippet: ``` post_id UUID PRIMARY KEY, reactions INTEGER NOT NULL DEFAULT 0 CHECK (reactions >= 0), is_reacted BOOLEAN NOT NULL DEFAULT false ``` on initial setup its trigger successfully but when i tried to include a When condition that ensure the function trigger the correct ID its failed. I define the ID at the function declaration. i also tried to include condition when changed happens to is_reacted is true then increase the count, then if changes to false the count is decrease",
    "author_id":5368,
    "publication_date":1754388859000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"shotmeinthehead",
    "author_reputation":1345.0,
    "tags":"postgresql, flutter, grpc, supabase",
    "text_length":884,
    "title_length":85,
    "num_tags":4
  },
  {
    "id":5841,
    "title":"Eclipse: java.lang.ClassNotFoundException: com.library.Class cannot be found by com.example.Plugin Despite Explicit Wiring",
    "link":"https:\/\/stackoverflow.com\/questions\/79725799\/eclipse-java-lang-classnotfoundexception-com-library-class-cannot-be-found-by",
    "text":"First off: I know my XY is stupid, but 1) I've tried everything else and 2) I want to answer this question about Eclipse and OSGi for my general understanding. So please don't answer only to say that Xalan is deprecated or whatever, I already know that. I am trying to use Xalan for XPath queries in Jaspersoft Studio 7.0.1 , because I have XML data sources with namespaces that won't work, and I heard that it used to work back when JSS still used Xalan instead of Jaxen. I set ``` net.sf.jasperreports.xpath.executer.factory=net.sf.jasperreports.xalan.util.XalanXPathExecuterFactory ``` on my project to use Xalan instead of Jaxen, and now I am getting the exception: ``` java.lang.NoClassDefFoundError: org\/apache\/xpath\/CachedXPathAPI at net.sf.jasperreports.xalan.util.XalanXPathExecuter.<init>(XalanXPathExecuter.java:47) at net.sf.jasperreports.xalan.util.XalanXPathExecuterFactory.getXPathExecuter(XalanXPathExecuterFactory.java:40) at net.sf.jasperreports.engine.util.xml.JRXPathExecuterUtils.getXPathExecuter(JRXPathExecuterUtils.java:98) at com.jaspersoft.studio.data.querydesigner.xpath.XMLDocumentManager.getXPathQueryExecuter(XMLDocumentManager.java:183) at com.jaspersoft.studio.data.querydesigner.xpath.XMLDocumentManager.selectNodeList(XMLDocumentManager.java:211) at com.jaspersoft.studio.data.querydesigner.xpath.XMLDocumentManager.getSelectableNodes(XMLDocumentManager.java:233) at com.jaspersoft.studio.data.querydesigner.xpath.XPathQueryDesigner$DecorateTreeViewerJob.runInUIThread(XPathQueryDesigner.java:367) at org.eclipse.ui.progress.UIJob.lambda$0(UIJob.java:148) ... Caused by: java.lang.ClassNotFoundException: org.apache.xpath.CachedXPathAPI cannot be found by net.sf.jasperreports_7.0.1.final ``` ``` net.sf.jasperreports_7.0.0.final ``` is an Eclipse plugin, and it seemingly cannot find a package. I know that Eclipse plugins are OSGi bundles and cannot pull packages from a general classpath, instead they have to be explicitly \"wired\" to another bundle that exports the dependency they want, cf. this post . Now I went and unzipped ``` net.sf.jasperreports_7.0.0.final.jar ``` , edited its ``` MANIFEST.MF ``` , and added the entry ``` org.apache.xalan;bundle-version=\"2.7.2\";visibility:=reexport ``` to its ``` Require-Bundle ``` . This means \"import all classes from that bundle\", right? I also took an OSGi-wrapped Xalan plugin from an older Jaspersoft Studio, version 6.20.3, and put it in my ``` plugins ``` folder. This contains the file ``` org\/apache\/xpath\/CachedXPathAPI.class ``` and its ``` MANIFEST.MF ``` has ``` org.apache.xpath ``` in its ``` Export-Package ``` . So what else could be going wrong here? ``` org.apache.xalan ``` explictly exports the class ``` org.apache.xpath.CachedXPathAPI ``` , ``` net.sf.jasperreports ``` explicitly imports it, how can it still be that the bundle can't find the class?",
    "author_id":5367,
    "publication_date":1754389095000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"LemongrabThree",
    "author_reputation":121.0,
    "tags":"eclipse, osgi, eclipse-rcp",
    "text_length":2857,
    "title_length":122,
    "num_tags":3
  },
  {
    "id":5840,
    "title":"PostgreSQL Github Database File Structure Template",
    "link":"https:\/\/stackoverflow.com\/questions\/79725806\/postgresql-github-database-file-structure-template",
    "text":"I am creating a Github project for PostgreSQL database files (tables, stored procedures). Is there a Github template, and folder template I should follow? What should be my gitignore template also? Didn't see anything in official documentation. Any guidance would help.",
    "author_id":4453,
    "publication_date":1754389557000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"mattsmith5",
    "author_reputation":1289.0,
    "tags":"postgresql",
    "text_length":269,
    "title_length":50,
    "num_tags":1
  },
  {
    "id":5839,
    "title":"piexif can&#39;t use comma , in python XPKeywords how to fix it",
    "link":"https:\/\/stackoverflow.com\/questions\/79725809\/piexif-cant-use-comma-in-python-xpkeywords-how-to-fix-it",
    "text":"CLOSE THX... exif_dict[\"0th\"][piexif.ImageIFD.XPKeywords] = keyword_str.encode(\"utf-16le\") + b'\\x00\\x00' When I use cat, pet, animal encode ``` utf-16le ``` , I get cat; pet; animal. Is there a way to fix this? I've tried everything. I wanted to add cat, pet, animal. But I ended up with cat; pet; animal. Or is there another way? Because the website doesn't usually accept cat tags; I want it to be like in the picture. Is it possible without filling it in yourself? https:\/\/ibb.co\/VWs8VWQs CLOSE THX...",
    "author_id":5366,
    "publication_date":1754389649000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ifudth",
    "author_reputation":19.0,
    "tags":"python, python-3.x, tags, keyword, piexif",
    "text_length":504,
    "title_length":63,
    "num_tags":5
  },
  {
    "id":5838,
    "title":"Prefix all colons and semi-colons with non-breaking space, except when within an HTML attribute",
    "link":"https:\/\/stackoverflow.com\/questions\/79725813\/prefix-all-colons-and-semi-colons-with-non-breaking-space-except-when-within-an",
    "text":"We generate some text, for French language, and we need to clean-up the output. Here is an example: ``` <p style=\"color: red; text-align: center\"> In French, semi-colons and colons should be prefixed: by a non-breaking space; That's the rule. <\/p> ``` We used to have a replacement regex which worked: ``` $out = preg_replace('\/(\\S)(\\s*)([:;])([^\\\/])\/', '$1&nbsp;$2$3', $in); ``` Historically, we did not have to handle ``` style ``` attributes but now we do, and that regex is breaking the HTML style attribute. Expected: ``` <p style=\"color: red; text-align: center\"> In French, semi-colons and colons should be prefixed&nbsp;: by a non-breaking space&nbsp;; That's the rule. <\/p> ``` Current: ``` <p style=\"color&nbsp;: red&nbsp;; text-align&nbsp;: center\"> In French, semi-colons and colons should be prefixed&nbsp;: by a non-breaking space&nbsp;; That's the rule. <\/p> ``` I fail to get a proper result. Best I could get was to make multiple passes, to undo changes between double-quotes. But event that fails to clean-up multiple matches within double-quotes. ``` $out = preg_replace( ['\/(\\S)(\\s*)([:;])([^\\\/])\/', '', ] ['$1&nbsp;$2$3', '', ] $in ); ``` Link to failed experiment",
    "author_id":5365,
    "publication_date":1754389820000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Vincent Mimoun-Prat",
    "author_reputation":28643.0,
    "tags":"php, regex",
    "text_length":1185,
    "title_length":95,
    "num_tags":2
  },
  {
    "id":5837,
    "title":"Required attribute is not working for the body request at the method of API endpoint",
    "link":"https:\/\/stackoverflow.com\/questions\/79725814\/required-attribute-is-not-working-for-the-body-request-at-the-method-of-api-endp",
    "text":"I have simple API methods. I need to check whether the body request is null. For example: ``` [HttpPost] public ActionResult<Product> CreateProduct([FromBody] Product product) { if (product == null) { return BadRequest(\"request is empty\"); } _someService.Create(product); } ``` But I would like to avoid this repeating piece of code in all my API methods: ``` if (product == null) { return BadRequest(\"request is empty\"); } ``` So I tried to use attribute ``` [Required] ``` and send an empty body in the request: ``` [HttpPost] public ActionResult<Product> CreateProduct([FromBody] [Required] Product product) { \/\/ this piece of code should not be run because \"product\" is null _someService.Create(product); } ``` However, no errors are thrown. Is it possible to use ``` [Required] ``` in API methods as I have shown here?",
    "author_id":4982,
    "publication_date":1754389881000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Learner",
    "author_reputation":669.0,
    "tags":"c#, asp.net-core, asp.net-core-webapi",
    "text_length":823,
    "title_length":84,
    "num_tags":3
  },
  {
    "id":5836,
    "title":"How to implement a type-safe zip function that accepts multiple lists of different types?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725821\/how-to-implement-a-type-safe-zip-function-that-accepts-multiple-lists-of-differe",
    "text":"I'm trying to find a clean and type-safe way, in Java, to iterate over three or more lists of equal length in one go. By type-safe, I mean that the result type is accurately reflects the types of the input arguments. Example: So if I have such lists: ``` List<String> names = List.of(\"Alice\", \"Bob\", \"Charlie\"); List<Integer> ages = List.of(25, 30, 28); List<Boolean> active = List.of(true, false, true); ``` What I want is something concise and readable like this (pseudo code): ``` zip(names, ages, active).forEach((name, age, active) -> { \/\/ Types of name, age and active are inferred from the input arguments, \/\/ so not need to cast them to the initial types \/\/ \/\/ do something }) ```",
    "author_id":4924,
    "publication_date":1754390396000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"M S",
    "author_reputation":51.0,
    "tags":"java, indexing, foreach, java-stream",
    "text_length":688,
    "title_length":89,
    "num_tags":4
  },
  {
    "id":5835,
    "title":"Reconnect to CORBA server",
    "link":"https:\/\/stackoverflow.com\/questions\/79725823\/reconnect-to-corba-server",
    "text":"CORBA server sends events to the client using AMI by calling the sendc_EventItem method. Initially, the client calls SubscribeItem and passes a pointer to the Subscriber interface. Then, the client starts calling WriteItem to save or modify elements depending on whether they exist. When an element is modified, it triggers sendc_EventItem. Additionally, the client pings this chain by sending a validation element and expecting to receive it within a timeout. If the validation element is not received within the timeout, it shuts down the ORB and POA using shutdown and destroy, then initiates a reconnection procedure. After reconnecting and restoring all subscriptions, all sendc_EventItem calls fail with: ``` TIMEOUT info exception: system exception, ID 'IDL:omg.org\/CORBA\/TIMEOUT:1.0' TAO exception, minor code = 3e (timeout during recv; low 7 bits of errno: 62 Timer expired), completed = MAYBE ``` However, if I call the synchronous EventItem method, all subsequent sendc_EventItem calls stop returning errors and work correctly. How can I fix this? I'm using ACE 7.1.1 and TAO 3.1.1. Server Policies: ``` CORBA::Object_var manager_object = orb_->resolve_initial_references(\"ORBPolicyManager\"); CORBA::PolicyManager_var policy_manager = CORBA::PolicyManager::_narrow(manager_object.in()); if (CORBA::is_nil(policy_manager.in())) ACE_ERROR_RETURN((LM_ERROR, \" (%P|%t) Panic: nil PolicyManager\\n\"), 1); CORBA::Any policy_value; policy_value <<= Messaging::SYNC_WITH_SERVER; CORBA::PolicyList policies(2); policies.length(2); policies[0] = orb_->create_policy(Messaging::SYNC_SCOPE_POLICY_TYPE, policy_value); TimeBase::TimeT relative_rt_timeout = TIMEOUT_ASYNC_RESULT * MS; \/\/ 10000 = 1 millisecond CORBA::Any relative_rt_timeout_as_any; relative_rt_timeout_as_any <<= relative_rt_timeout; policies[1] = orb_->create_policy(Messaging::RELATIVE_RT_TIMEOUT_POLICY_TYPE, relative_rt_timeout_as_any); policy_manager->set_policy_overrides(policies, CORBA::ADD_OVERRIDE); ```",
    "author_id":5364,
    "publication_date":1754390410000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Александр Гордиенко",
    "author_reputation":1.0,
    "tags":"c++, corba, tao",
    "text_length":1976,
    "title_length":25,
    "num_tags":3
  },
  {
    "id":5834,
    "title":"Getting &quot;Cannot apply unknown utility class&quot; in the main CSS file when trying to use standard, not custom utilities",
    "link":"https:\/\/stackoverflow.com\/questions\/79725830\/getting-cannot-apply-unknown-utility-class-in-the-main-css-file-when-trying-to",
    "text":"The issue can occur when using the Vite plugin, the PostCSS plugin, or even the CLI. If I import TailwindCSS into my main CSS file ( ``` .\/src\/index.css ``` ) and then try to use the ``` @apply ``` directive with any standard utility class, I get an error message. ``` @import url(\"tailwindcss\"); @layer base { *, ::after, ::before, ::backdrop, ::file-selector-button { border-color: var(--color-gray-200, currentColor); } div { scrollbar-color: var(--color-neutral-400) transparent; } body { margin: 0; display: flex; flex-direction: column; @apply bg-slate-50; \/* Cannot apply unknown utility class `bg-slate-50`. *\/ \/* And at this point, the build fails as a critical error. *\/ @apply text-neutral-700; } a { @apply text-purple-900 hover:text-purple-700 hover:underline; } button { cursor: pointer; } } ``` And at this point, along with the error message, I also get an automatic suggestion saying that the ``` @reference ``` directive is required when using it outside of the main CSS file (e.g., in CSS modules). But I don't think that's the solution in this case. [plugin:@tailwindcss\/vite:generate:serve] Cannot apply unknown utility class ``` bg-slate-50 ``` . Are you using CSS modules or similar and missing ``` @reference ``` ? https:\/\/tailwindcss.com\/docs\/functions-and-directives#reference-directive <path_to>\/src\/index.css",
    "author_id":5363,
    "publication_date":1754390590000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"rozsazoltan",
    "author_reputation":15428.0,
    "tags":"tailwind-css, tailwind-css-4",
    "text_length":1336,
    "title_length":125,
    "num_tags":2
  },
  {
    "id":5833,
    "title":"Is L[a:b]=L[c:d] optimized in Python to avoid creating a new list?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725843\/is-lab-lcd-optimized-in-python-to-avoid-creating-a-new-list",
    "text":"I am unable to find anything on the official Python documentation whether ``` L[a:b] = L[c:d] ``` creates a new (temporary) list for ``` L[c:d] ``` before the in-place modification of ``` L ``` . The tutorial says that: All slice operations return a new list containing the requested elements. This does not say whether there is any optimization for slice assignment to another slice. So is there an officially specified behaviour, and what is it? Note that this question cannot be answered by claiming that ``` L[c:d] ``` is evaluated before ``` L[a:b] ``` (even if the claim is true, which is doubtful since many programming languages have exceptional cases that violate the usual rules). This is because even such a claim might only specify what must appear to happen, and does not specify what must actually happen! Since any decent programmer would use an appropriate for-loop (in either forward or reverse direction) to modify ``` L ``` in-place and not create any new list, there is also no reason to assume that the language designers did not also officially require this special case of list assignment of a slice to be done that way.",
    "author_id":5362,
    "publication_date":1754391566000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user29120650",
    "author_reputation":143.0,
    "tags":"python, python-internals",
    "text_length":1143,
    "title_length":66,
    "num_tags":2
  },
  {
    "id":5832,
    "title":"Angle Brackets in Notepad++ regex",
    "link":"https:\/\/stackoverflow.com\/questions\/79725846\/angle-brackets-in-notepad-regex",
    "text":"A Regex in Notepad++ is not behaving as I expect, and I'm wondering if it's a bug in Notepad++ or something else. Here is my file: ``` <File Name=\"NAME.EXT\">0123456789ABCDEF<\/File> ``` Here is my Regex: ``` (\\<File .+?\\>) ``` Here are all search settings: When I pressed Replace All, here is what I got: ``` <File Name =\"NAME.EXT\">0123456789ABCDEF<\/File> ``` Here is what I expected instead: ``` <File Name=\"NAME.EXT\"> 0123456789ABCDEF<\/File> ``` I switched to the Find tab, and even there, it is only matching ``` <File Name ``` When I load it into regex101.com and selected the \"PCRE\" flavor (I read that this is what Notepad++ used), it behaves as I expected, not as Notepad++ behaves.",
    "author_id":5361,
    "publication_date":1754391682000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"hypehuman",
    "author_reputation":1397.0,
    "tags":"regex, notepad++, pcre",
    "text_length":688,
    "title_length":33,
    "num_tags":3
  },
  {
    "id":5831,
    "title":"Validation failed Invalid Executable The executable .app\/Frameworks\/hermes.framework\/hermes",
    "link":"https:\/\/stackoverflow.com\/questions\/79725847\/validation-failed-invalid-executable-the-executable-app-frameworks-hermes-frame",
    "text":"Validation failed: Invalid Executable. The executable cam_app.app\/Frameworks\/hermes.framework\/hermes contains bitcode. (ID: ) I have tried so many times, but none of the solutions work for me. Why is that?",
    "author_id":5360,
    "publication_date":1754391731000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"kavindu",
    "author_reputation":50.0,
    "tags":"ios, react-native",
    "text_length":205,
    "title_length":91,
    "num_tags":2
  },
  {
    "id":5830,
    "title":"arm-none-eabi-gcc stack frame usage issue",
    "link":"https:\/\/stackoverflow.com\/questions\/79725849\/arm-none-eabi-gcc-stack-frame-usage-issue",
    "text":"I am using below gcc compiler to compile codebase for ARM cortex M33 with optimization -Os. ``` arm-none-eabi-gcc.exe (Arm GNU Toolchain 14.3.Rel1 (Build arm-14.174)) 14.3.1 20250623 Copyright (C) 2024 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. ``` In some functions prologue it is pushing r0,r1,r2 as well in stack, this is not done for all functions. ``` 23d8: e92d 41ff stmdb sp!, {r0, r1, r2, r3, r4, r5, r6, r7, r8, lr} 23dc: 460c mov r4, r1 23de: 4605 mov r5, r0 ``` When it wants that functions argument to be passed to another function, it tries to pop it from stack, when it does, it pops to a wrong location due to incorrect sp manipulation in generated code, ``` 24f2: b004 add sp, #16 24f4: e8bd 81f0 ldmia.w sp!, {r4, r5, r6, r7, r8, pc} ``` This is causing the pointer address to be in location not existing on device causing a harfault Tarmac from ARM Core: ``` Push: 245840 clk ES (000023d8:e92d41ff) T thrd: PUSH {r0-r8,lr} ST 200046e0 00004a23 00000010 00000009 2000018c 00000000200046e0 NM NSH 200046d0 20001910 00000000 000023d9 a0000000 00000000200046d0 NM NSH 200046c0 20002350 20001910 ........ ........ 00000000200046c0 NM NSH R MSP 200046c8 Pop: 246163 clk ES (000024f4:e8bd81f0) T thrd: POP {r4-r8,pc} LD 200046e0 00004a23 00000010 00000009 2000018c 00000000200046e0 NM NSH 200046d0 20001910 00000100 ........ ........ 00000000200046d0 NM NSH R R4 00000100 R R5 20001910 R R6 2000018c R R7 00000009 R R8 00000010 R MSP 200046f0 R XPSR 69000000 BR (00004a22) T ``` R0 On Push is 20001910 and on pop is 00000100, why? [here R4 on pop is used with MLA with another register to get the actual pointer location to be passed to function, expecting R4 to have 20001910] When I mark all those functions which does push the registers (r0,r1,r2) into stack and retrieves from it as -O2, I am not seeing hardfault. I tried to justify maybe the stack frame is big or stack depth in the call is big, none of it seems to be correlating theme among all the functions where this is done. Any suggestions on how to mitigate this problem still keeping a global -Os flag? Note there no naked functions in codebase or any manual manipulation of MSP in code. There are some inline assembly or ARM gcc intrinsics from CMSIS. On -O2: ``` 000023f0 <func>: 23f0: e92d 43f0 stmdb sp!, {r4, r5, r6, r7, r8, r9, lr} 23f4: 460d mov r5, r1 23f6: 4604 mov r4, r0 23f8: b085 sub sp, #20 23fa: 2202 movs r2, #2 23fc: 2100 movs r1, #0 23fe: f205 201b addw r0, r5, #539 @ 0x21b 2402: f02a f8e3 bl 2c5cc <memset> 2406: f894 3068 ldrb.w r3, [r4, #104] @ 0x68 240a: f894 01c8 ldrb.w r0, [r4, #456] @ 0x1c8 240e: f3c3 0202 ubfx r2, r3, #0, #3 2412: f003 0307 and.w r3, r3, #7 2416: f88d 2008 strb.w r2, [sp, #8] 241a: 2b02 cmp r3, #2 241c: f3c0 0202 ubfx r2, r0, #0, #3 2420: f88d 2009 strb.w r2, [sp, #9] 2424: d103 bne.n 242e <func+0x3e> 2426: f000 0007 and.w r0, r0, #7 242a: 2801 cmp r0, #1 242c: d000 beq.n 2430 <func+0x40> 242e: 2000 movs r0, #0 2430: f025 fe18 bl 28064 <func3> 2434: 2204 movs r2, #4 2436: 2100 movs r1, #0 2438: a803 add r0, sp, #12 243a: f02a f8c7 bl 2c5cc <memset> 243e: 2202 movs r2, #2 2440: 2100 movs r1, #0 2442: a802 add r0, sp, #8 2444: f02a f8c2 bl 2c5cc <memset> 2448: 2201 movs r2, #1 244a: f04f 0800 mov.w r8, #0 244e: f894 3068 ldrb.w r3, [r4, #104] @ 0x68 2452: f894 11c8 ldrb.w r1, [r4, #456] @ 0x1c8 2456: f013 0307 ands.w r3, r3, #7 245a: bf1c itt ne 245c: f103 33ff addne.w r3, r3, #4294967295 2460: b2db uxtbne r3, r3 2462: eb0d 0403 add.w r4, sp, r3 2466: 18e8 adds r0, r5, r3 2468: eb0d 0343 add.w r3, sp, r3, lsl #1 246c: 731a strb r2, [r3, #12] 246e: 7a23 ldrb r3, [r4, #8] 2470: f011 0107 ands.w r1, r1, #7 2474: 4413 add r3, r2 2476: 7223 strb r3, [r4, #8] 2478: f890 321b ldrb.w r3, [r0, #539] @ 0x21b 247c: f04f 0901 mov.w r9, #1 2480: ea43 0302 orr.w r3, r3, r2 2484: f880 321b strb.w r3, [r0, #539] @ 0x21b 2488: bf08 it eq 248a: 4613 moveq r3, r2 248c: 4647 mov r7, r8 248e: bf1c itt ne 2490: f101 31ff addne.w r1, r1, #4294967295 2494: b2cb uxtbne r3, r1 2496: 441d add r5, r3 2498: eb0d 0243 add.w r2, sp, r3, lsl #1 249c: 446b add r3, sp 249e: f882 900d strb.w r9, [r2, #13] 24a2: 7a1a ldrb r2, [r3, #8] 24a4: ae02 add r6, sp, #8 24a6: 444a add r2, r9 24a8: 721a strb r2, [r3, #8] 24aa: f895 321b ldrb.w r3, [r5, #539] @ 0x21b 24ae: ac03 add r4, sp, #12 24b0: f043 0302 orr.w r3, r3, #2 24b4: f885 321b strb.w r3, [r5, #539] @ 0x21b 24b8: f816 3b01 ldrb.w r3, [r6], #1 24bc: 2b02 cmp r3, #2 24be: d812 bhi.n 24e6 <func+0xf6> 24c0: d02c beq.n 251c <func+0x12c> 24c2: 2300 movs r3, #0 24c4: 4638 mov r0, r7 24c6: 461a mov r2, r3 24c8: 4619 mov r1, r3 24ca: f8cd 9000 str.w r9, [sp] 24ce: f024 fdc1 bl 27054 <func4> 24d2: 1c7b adds r3, r7, #1 24d4: 2b02 cmp r3, #2 24d6: f04f 0701 mov.w r7, #1 24da: f104 0402 add.w r4, r4, #2 24de: d1eb bne.n 24b8 <func+0xc8> 24e0: b005 add sp, #20 24e2: e8bd 83f0 ldmia.w sp!, {r4, r5, r6, r7, r8, r9, pc} 24e6: 7823 ldrb r3, [r4, #0] 24e8: b1b3 cbz r3, 2518 <func+0x128> 24ea: 2100 movs r1, #0 24ec: 4640 mov r0, r8 24ee: f025 fad9 bl 27aa4 <func2> 24f2: 7863 ldrb r3, [r4, #1] 24f4: f108 0001 add.w r0, r8, #1 24f8: b2c0 uxtb r0, r0 24fa: b1e3 cbz r3, 2536 <func+0x146> 24fc: 2101 movs r1, #1 24fe: f025 fad1 bl 27aa4 <func2> 2502: 2301 movs r3, #1 2504: 2103 movs r1, #3 2506: 9300 str r3, [sp, #0] 2508: 4638 mov r0, r7 250a: 2300 movs r3, #0 250c: f108 0802 add.w r8, r8, #2 2510: fa5f f888 uxtb.w r8, r8 2514: 461a mov r2, r3 2516: e7da b.n 24ce <func+0xde> 2518: 2107 movs r1, #7 251a: e7e7 b.n 24ec <func+0xfc> 251c: 7823 ldrb r3, [r4, #0] 251e: b9db cbnz r3, 2558 <func+0x168> 2520: 7863 ldrb r3, [r4, #1] 2522: b983 cbnz r3, 2546 <func+0x156> 2524: f1b8 0f02 cmp.w r8, #2 2528: d007 beq.n 253a <func+0x14a> 252a: 2301 movs r3, #1 252c: 2102 movs r1, #2 252e: 9300 str r3, [sp, #0] 2530: 4638 mov r0, r7 2532: 2300 movs r3, #0 2534: e7ee b.n 2514 <func+0x124> 2536: 2107 movs r1, #7 2538: e7e1 b.n 24fe <func+0x10e> 253a: 2101 movs r1, #1 253c: 2300 movs r3, #0 253e: 4638 mov r0, r7 2540: 461a mov r2, r3 2542: 9100 str r1, [sp, #0] 2544: e7c3 b.n 24ce <func+0xde> 2546: 4640 mov r0, r8 2548: 2101 movs r1, #1 254a: f108 0801 add.w r8, r8, #1 254e: f025 faa9 bl 27aa4 <func2> 2552: fa5f f888 uxtb.w r8, r8 2556: e7e5 b.n 2524 <func+0x134> 2558: 4640 mov r0, r8 255a: 2100 movs r1, #0 255c: f108 0801 add.w r8, r8, #1 2560: f025 faa0 bl 27aa4 <func2> 2564: fa5f f888 uxtb.w r8, r8 2568: e7da b.n 2520 <func+0x130> ``` On -Os: ``` 000023f0 <func>: 23f0: e92d 41ff stmdb sp!, {r0, r1, r2, r3, r4, r5, r6, r7, r8, lr} 23f4: 460c mov r4, r1 23f6: 4605 mov r5, r0 23f8: 2202 movs r2, #2 23fa: 2100 movs r1, #0 23fc: f204 201b addw r0, r4, #539 @ 0x21b 2400: f02a f8dc bl 2c5bc <memset> 2404: f895 3068 ldrb.w r3, [r5, #104] @ 0x68 2408: f895 01c8 ldrb.w r0, [r5, #456] @ 0x1c8 240c: f3c3 0202 ubfx r2, r3, #0, #3 2410: f003 0307 and.w r3, r3, #7 2414: f88d 2008 strb.w r2, [sp, #8] 2418: 2b02 cmp r3, #2 241a: f3c0 0202 ubfx r2, r0, #0, #3 241e: f88d 2009 strb.w r2, [sp, #9] 2422: f000 0007 and.w r0, r0, #7 2426: d173 bne.n 2510 <func+0x120> 2428: 2801 cmp r0, #1 242a: d171 bne.n 2510 <func+0x120> 242c: f025 fe12 bl 28054 <func3> 2430: 2204 movs r2, #4 2432: 2100 movs r1, #0 2434: a803 add r0, sp, #12 2436: f02a f8c1 bl 2c5bc <memset> 243a: 2202 movs r2, #2 243c: 2100 movs r1, #0 243e: a802 add r0, sp, #8 2440: f02a f8bc bl 2c5bc <memset> 2444: f895 3068 ldrb.w r3, [r5, #104] @ 0x68 2448: aa04 add r2, sp, #16 244a: f013 0307 ands.w r3, r3, #7 244e: bf1c itt ne 2450: f103 33ff addne.w r3, r3, #4294967295 2454: b2db uxtbne r3, r3 2456: eb02 0143 add.w r1, r2, r3, lsl #1 245a: 2201 movs r2, #1 245c: f801 2c04 strb.w r2, [r1, #-4] 2460: f103 0110 add.w r1, r3, #16 2464: eb0d 0001 add.w r0, sp, r1 2468: f810 1c08 ldrb.w r1, [r0, #-8] 246c: 4423 add r3, r4 246e: 4411 add r1, r2 2470: f800 1c08 strb.w r1, [r0, #-8] 2474: f893 121b ldrb.w r1, [r3, #539] @ 0x21b 2478: f04f 0801 mov.w r8, #1 247c: 4311 orrs r1, r2 247e: f883 121b strb.w r1, [r3, #539] @ 0x21b 2482: f895 31c8 ldrb.w r3, [r5, #456] @ 0x1c8 2486: af02 add r7, sp, #8 2488: f013 0307 ands.w r3, r3, #7 248c: bf0e itee eq 248e: 4613 moveq r3, r2 2490: f103 33ff addne.w r3, r3, #4294967295 2494: b2db uxtbne r3, r3 2496: aa04 add r2, sp, #16 2498: eb02 0243 add.w r2, r2, r3, lsl #1 249c: 441c add r4, r3 249e: f802 8c03 strb.w r8, [r2, #-3] 24a2: f103 0210 add.w r2, r3, #16 24a6: f894 321b ldrb.w r3, [r4, #539] @ 0x21b 24aa: eb0d 0102 add.w r1, sp, r2 24ae: f043 0302 orr.w r3, r3, #2 24b2: f884 321b strb.w r3, [r4, #539] @ 0x21b 24b6: 2400 movs r4, #0 24b8: 4626 mov r6, r4 24ba: f811 2c08 ldrb.w r2, [r1, #-8] 24be: ad03 add r5, sp, #12 24c0: 4442 add r2, r8 24c2: f801 2c08 strb.w r2, [r1, #-8] 24c6: f817 3b01 ldrb.w r3, [r7], #1 24ca: 2b02 cmp r3, #2 24cc: d926 bls.n 251c <func+0x12c> 24ce: 782b ldrb r3, [r5, #0] 24d0: b303 cbz r3, 2514 <func+0x124> 24d2: 2100 movs r1, #0 24d4: 4620 mov r0, r4 24d6: f025 fadd bl 27a94 <func2> 24da: 786b ldrb r3, [r5, #1] 24dc: 1c60 adds r0, r4, #1 24de: b2c0 uxtb r0, r0 24e0: b1d3 cbz r3, 2518 <func+0x128> 24e2: 2101 movs r1, #1 24e4: f025 fad6 bl 27a94 <func2> 24e8: 2301 movs r3, #1 24ea: 9300 str r3, [sp, #0] 24ec: 2300 movs r3, #0 24ee: 2103 movs r1, #3 24f0: 461a mov r2, r3 24f2: 3402 adds r4, #2 24f4: b2e4 uxtb r4, r4 24f6: 4630 mov r0, r6 24f8: f024 fda4 bl 27044 <func4> 24fc: 1c73 adds r3, r6, #1 24fe: 2b02 cmp r3, #2 2500: f04f 0601 mov.w r6, #1 2504: f105 0502 add.w r5, r5, #2 2508: d1dd bne.n 24c6 <func+0xd6> 250a: b004 add sp, #16 250c: e8bd 81f0 ldmia.w sp!, {r4, r5, r6, r7, r8, pc} 2510: 2000 movs r0, #0 2512: e78b b.n 242c <func+0x3c> 2514: 2107 movs r1, #7 2516: e7dd b.n 24d4 <func+0xe4> 2518: 2107 movs r1, #7 251a: e7e3 b.n 24e4 <func+0xf4> 251c: d117 bne.n 254e <func+0x15e> 251e: 782b ldrb r3, [r5, #0] 2520: b12b cbz r3, 252e <func+0x13e> 2522: 4620 mov r0, r4 2524: 2100 movs r1, #0 2526: f025 fab5 bl 27a94 <func2> 252a: 3401 adds r4, #1 252c: b2e4 uxtb r4, r4 252e: 786b ldrb r3, [r5, #1] 2530: b12b cbz r3, 253e <func+0x14e> 2532: 4620 mov r0, r4 2534: 2101 movs r1, #1 2536: f025 faad bl 27a94 <func2> 253a: 3401 adds r4, #1 253c: b2e4 uxtb r4, r4 253e: 2101 movs r1, #1 2540: 2300 movs r3, #0 2542: 2c02 cmp r4, #2 2544: 9100 str r1, [sp, #0] 2546: 461a mov r2, r3 2548: bf18 it ne 254a: 2102 movne r1, #2 254c: e7d3 b.n 24f6 <func+0x106> 254e: 2300 movs r3, #0 2550: f8cd 8000 str.w r8, [sp] 2554: 461a mov r2, r3 2556: 4619 mov r1, r3 2558: e7cd b.n 24f6 <func+0x106> ``` Trace for -Os: ``` 246161 clk ES (000024f0:d1dd) T thrd: CCFAIL BNE 0x24ae 246162 clk ES (000024f2:b004) T thrd: ADD sp,sp,#0x10 R MSP 200046d8 246163 clk ES (000024f4:e8bd81f0) T thrd: POP {r4-r8,pc} LD 200046e0 00004a23 00000010 00000009 2000018c 00000000200046e0 NM NSH 200046d0 20001910 00000100 ........ ........ 00000000200046d0 NM NSH R R4 00000100 R R5 20001910 R R6 2000018c R R7 00000009 R R8 00000010 R MSP 200046f0 R XPSR 69000000 BR (00004a22) T 246173 clk ES (00004a22:464b) T thrd: MOV r3,r9 R R3 20002350 246174 clk ES (00004a24:462a) T thrd: MOV r2,r5 R R2 20001910 246175 clk ES (00004a26:4620) T thrd: MOV r0,r4 R R0 00000100 246176 clk ES (00004a28:f8d67770) T thrd: LDR r7,[r6,#0x770] LD 200008f0 000039fd ........ ........ ........ 00000000200008f0 NM NSH R R7 000039fd 246178 clk ES (00004a2c:2102) T thrd: MOVS r1,#2 R R1 00000002 R XPSR 29000000 246179 clk ES (00004a2e:47b8) T thrd: BLX r7 R LR 00004a31 R XPSR 29000000 BR (000039fc) T 246181 clk ES (000039fc:e92d4ff0) T thrd: PUSH {r4-r11,lr} ST 200046e0 00004a31 000000b5 000000b5 20002350 00000000200046e0 NM NSH 200046d0 00000010 000039fd 2000018c 20001910 00000000200046d0 NM NSH 200046c0 00000100 ........ ........ ........ 00000000200046c0 NM NSH R MSP 200046cc 246191 clk ES (00003a00:4698) T thrd: MOV r8,r3 R R8 20002350 246192 clk ES (00003a02:2300) T thrd: MOVS r3,#0 R R3 00000000 R XPSR 69000000 246193 clk ES (00003a04:b08d) T thrd: SUB sp,sp,#0x34 R MSP 20004698 246194 clk ES (00003a06:4605) T thrd: MOV r5,r0 R R5 00000100 246195 clk ES (00003a08:4617) T thrd: MOV r7,r2 R R7 20001910 246196 clk ES (00003a0a:9109) T thrd: STR r1,[sp,#0x24] ST 200046b0 00000002 ........ ........ ........ 00000000200046b0 NM NSH 246198 clk ES (00003a0c:930b) T thrd: STR r3,[sp,#0x2c] ST 200046c0 ........ ........ 00000000 ........ 00000000200046c0 NM NSH 246199 clk ES (00003a0e:f88d302b) T thrd: STRB r3,[sp,#0x2b] ST 200046c0 ........ ........ ........ 00...... 00000000200046c0 NM NSH 246200 clk ES (00003a12:2906) T thrd: CMP r1,#6 R XPSR 89000000 246201 clk ES (00003a14:f2008105) T thrd: CCFAIL BHI 0x3c22 246202 clk ES (00003a18:e8dff011) T thrd: TBH [pc,r1,LSL #1] LD 00003a20 ........ ........ ........ ....008b 0000000000003a20 NM NSH BR (00003b32) T 246207 clk ES (00003b32:f44f73b0) T thrd: MOV.W r3,#0x160 R R3 00000160 246208 clk ES (00003b36:f04f0b0f) T thrd: MOV.W r11,#0xf R R11 0000000f 246209 clk ES (00003b3a:f04f0a0c) T thrd: MOV.W r10,#0xc R R10 0000000c 246210 clk ES (00003b3e:fb032300) T thrd: MLA r3,r3,r0,r2 R R3 20017910 246212 clk ES (00003b42:f8932058) T thrd: LDRB r2,[r3,#0x58] R CFSR 00008200 R HFSR 40000000 246226 clk ES EXC [3] HardFault ST 20004640 89000000 00003b42 00004a31 200046d8 0000000020004640 NM NSH 20004630 20017910 20001910 00000002 00000100 0000000020004630 NM NSH R MSP 20004630 R LR ffffffa8 R XPSR 89000003 R CONTROL 00000000 R FPCCR c0000019 R FPCCR 00000000 BR (0001ea00) T ``` R4 -> 100, Movs to R0, MLA to derive next pointer gets wrong adddress Working case for -O2: ``` 239512 clk ES (000024e0:b005) T thrd: ADD sp,sp,#0x14 R MSP 200046d4 239513 clk ES (000024e2:e8bd83f0) T thrd: POP {r4-r9,pc} LD 200046e0 00004a5f 20002350 00000010 00000009 00000000200046e0 NM NSH 200046d0 2000018c 20001910 00000000 ........ 00000000200046d0 NM NSH R R4 00000000 R R5 20001910 R R6 2000018c R R7 00000009 R R8 00000010 R R9 20002350 R MSP 200046f0 R XPSR 69000000 BR (00004a5e) T 239524 clk ES (00004a5e:464b) T thrd: MOV r3,r9 R R3 20002350 239525 clk ES (00004a60:462a) T thrd: MOV r2,r5 R R2 20001910 239526 clk ES (00004a62:4620) T thrd: MOV r0,r4 R R0 00000000 239527 clk ES (00004a64:f8d67770) T thrd: LDR r7,[r6,#0x770] LD 200008f0 00003a39 ........ ........ ........ 00000000200008f0 NM NSH R R7 00003a39 239529 clk ES (00004a68:2102) T thrd: MOVS r1,#2 R R1 00000002 R XPSR 29000000 239530 clk ES (00004a6a:47b8) T thrd: BLX r7 R LR 00004a6d R XPSR 29000000 BR (00003a38) T 239532 clk ES (00003a38:e92d4ff0) T thrd: PUSH {r4-r11,lr} ST 200046e0 00004a6d 000001b5 000001b5 20002350 00000000200046e0 NM NSH 200046d0 00000010 00003a39 2000018c 20001910 00000000200046d0 NM NSH 200046c0 00000000 ........ ........ ........ 00000000200046c0 NM NSH R MSP 200046cc 239542 clk ES (00003a3c:4698) T thrd: MOV r8,r3 R R8 20002350 239543 clk ES (00003a3e:2300) T thrd: MOVS r3,#0 R R3 00000000 R XPSR 69000000 239544 clk ES (00003a40:b08d) T thrd: SUB sp,sp,#0x34 R MSP 20004698 239545 clk ES (00003a42:4605) T thrd: MOV r5,r0 R R5 00000000 239546 clk ES (00003a44:4617) T thrd: MOV r7,r2 R R7 20001910 239547 clk ES (00003a46:9109) T thrd: STR r1,[sp,#0x24] ST 200046b0 00000002 ........ ........ ........ 00000000200046b0 NM NSH 239549 clk ES (00003a48:930b) T thrd: STR r3,[sp,#0x2c] ST 200046c0 ........ ........ 00000000 ........ 00000000200046c0 NM NSH 239550 clk ES (00003a4a:f88d302b) T thrd: STRB r3,[sp,#0x2b] ST 200046c0 ........ ........ ........ 00...... 00000000200046c0 NM NSH 239551 clk ES (00003a4e:2906) T thrd: CMP r1,#6 R XPSR 89000000 239552 clk ES (00003a50:f2008105) T thrd: CCFAIL BHI 0x3c5e 239553 clk ES (00003a54:e8dff011) T thrd: TBH [pc,r1,LSL #1] LD 00003a50 ....008b ........ ........ ........ 0000000000003a50 NM NSH BR (00003b6e) T 239558 clk ES (00003b6e:f44f73b0) T thrd: MOV.W r3,#0x160 R R3 00000160 239559 clk ES (00003b72:f04f0b0f) T thrd: MOV.W r11,#0xf R R11 0000000f 239560 clk ES (00003b76:f04f0a0c) T thrd: MOV.W r10,#0xc R R10 0000000c 239561 clk ES (00003b7a:fb032300) T thrd: MLA r3,r3,r0,r2 R R3 20001910 239563 clk ES (00003b7e:f8932058) T thrd: LDRB r2,[r3,#0x58] LD 20001960 ........ ......86 ........ ........ 0000000020001960 NM NSH R R2 00000086 ```",
    "author_id":5359,
    "publication_date":1754391793000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Hari",
    "author_reputation":21.0,
    "tags":"arm, stack, assembly, gcc, optimization",
    "text_length":16182,
    "title_length":41,
    "num_tags":5
  },
  {
    "id":5829,
    "title":"How to bind a std::string stored in a std::any vector to an ODBC SQL procedure call",
    "link":"https:\/\/stackoverflow.com\/questions\/79725858\/how-to-bind-a-stdstring-stored-in-a-stdany-vector-to-an-odbc-sql-procedure-c",
    "text":"I want to create a (MSVC) C++ generic class that handles interaction with an SQL Server database over ODBC. ``` std::wstring ``` s are bound correctly but ``` std::string ``` s are not, even if the logic is the same. This is how I handle the ``` std::string ``` and ``` std::wstring ``` types. ``` std::wstring ``` reaches the database correctly, but ``` std::string ``` does not. ``` \/\/ allocate statement \/\/ prepare statement \/\/ bind return code template<typename T> inline void StoredProc::bindInputParameter(const T& param) { lengths.push_back(0); \/\/ member of StoredProc class SQLLEN* len = &lengths.back(); bound_parameters.push_back(param); \/\/ member of StoredProc class auto& stored_param = std::any_cast<T&>(bound_parameters.back()); if constexpr (std::is_same_v<std::decay_t<T>, std::string>) { strings.push_back(param); std::string& stored_param = strings.back(); errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_INPUT, SQL_C_CHAR, SQL_VARCHAR, 80, 0, (SQLPOINTER)(stored_param.c_str()), stored_param.size() + 1, NULL ); } else if constexpr (std::is_same_v<std::decay_t<T>, std::wstring>) { errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_INPUT, SQL_C_WCHAR, SQL_WVARCHAR, 80, 0, (SQLPOINTER)(stored_param.c_str()), (stored_param.size() + 1) * sizeof(wchar_t), NULL ); } \/\/ other types ... if (!SQL_SUCCEEDED(errorCode)) { \/\/ no errors cleanup(); throw std::exception(Errors::SQL_STATEMENT_BIND_INPUT_ERROR.data()); } } \/\/... bind output parameters \/\/ SQLExecute(statementHandle); \/\/ return code == 0 and no errors registered ``` The left column is a ``` VARCHAR(100) ``` , corresponds to a ``` std::string ``` , should be ``` \"123\" ``` , the right column is an ``` NVARCHAR(max) ``` , corresponds to a ``` std::wstring ``` , is correct: IMAGE I added a new member field ``` std::vector<std::string> ``` , used it to handle ``` std::string ``` parameters binding and it worked. However, the logic is cluttered in this approach. ``` \/\/ this works if constexpr (std::is_same_v<std::decay_t<T>, std::string>) { strings.push_back(param); \/\/member in StoredProc std::string& stored_param = strings.back(); errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_INPUT, SQL_C_CHAR, SQL_VARCHAR, 80, 0, (SQLPOINTER)(stored_param.c_str()), stored_param.size() + 1, NULL ); } ``` To me, it seems that converting ``` std::string ``` to ``` std::any ``` and back to a ``` std::string ``` causes some issues that I am unaware of. What is wrong in the first snippet? It looks like the database does not know that the parameter is a null terminated string. Can you give me any advice? P.S. Other design ideas for the StoredProc class are welcome. Currently, the same issue appears if I use ``` std::variant ``` and ``` std::visit ``` . The entire code of the class: ``` \/\/SoredProc.h #pragma once #include \"Errors.h\" #include <string> #include <tuple> #include <vector> #include <type_traits> #include <any> #include <windows.h> #include <sql.h> #include <sqlext.h> #include <sqltypes.h> using procedureName = std::string; class StoredProc { public: StoredProc(const SQLHDBC dbH) : connectionHandle{ dbH } {} ~StoredProc() = default; template<typename... OutputTypes, typename... InputTypes> std::tuple<OutputTypes...> execute(const procedureName& p, const InputTypes&... parameters); protected: inline static size_t MAX_SIZE_BUFFER{ 255 }; static void handleDiagnosticRecord(SQLHANDLE hHandle, SQLSMALLINT hType, RETCODE RetCode); void allocate(); void prepare(const procedureName& p, const size_t totalNumberOfParameters); void bindReturnCode(); template<typename... OutputTypes> void bindOutputs(std::tuple<OutputTypes...>& results); template<typename... InputTypes> void bindInputs(const InputTypes&... parameters); template<typename... OutputTypes, typename... InputTypes> void call(const procedureName& p, const size_t totalNumberOfParameters, std::tuple<OutputTypes...>& results, const InputTypes&... parameters); template<typename... OutputTypes> void fetchOutputs(const size_t totalNumberOfParameters, std::tuple<OutputTypes...>& results); void cleanup(); template<typename T> void bindInputParameter(const T& param); template<typename T> void bindOutputParameter(const T& param); template<typename T> void fetchOneOutput(T& param, size_t index); SQLHDBC connectionHandle{}; SQLHSTMT statementHandle{}; SQLRETURN errorCode{ 0 }; SQLUSMALLINT paramNumber{ 1 }; SQLINTEGER returnCode{ -1 }; std::vector<std::any> bound_parameters{}; std::vector<SQLLEN> lengths{}; }; template<typename... OutputTypes, typename... InputTypes> inline std::tuple<OutputTypes...> StoredProc::execute(const procedureName& p, const InputTypes&... parameters) { std::tuple<OutputTypes...> results{}; size_t totalNumberOfParameters = std::tuple_size<std::decay_t<decltype(results)>>::value + sizeof...(parameters); allocate(); prepare(p, totalNumberOfParameters); bindReturnCode(); bindInputs(parameters...); bindOutputs(results); call(p, totalNumberOfParameters, results, parameters...); fetchOutputs(totalNumberOfParameters, results); cleanup(); return results; } template<typename ...InputTypes> inline void StoredProc::bindInputs(const InputTypes&... parameters) { (bindInputParameter(parameters), ...); } template<typename... OutputTypes> inline void StoredProc::bindOutputs(std::tuple<OutputTypes...>& results) { std::apply([this](auto&... params) { (this->bindOutputParameter(params), ...); }, results); } template<typename... OutputTypes, typename... InputTypes> inline void StoredProc::call(const procedureName& p, const size_t totalNumberOfParameters, std::tuple<OutputTypes...>& results, const InputTypes&... parameters) { errorCode = SQLExecute(statementHandle); if (!SQL_SUCCEEDED(errorCode)) { cleanup(); throw std::exception(Errors::SQL_EXECUTE_ERROR.data()); } if (returnCode) { cleanup(); throw std::exception(Errors::SQL_RETURN_CODE_ERROR.data()); } } template<typename ...OutputTypes> inline void StoredProc::fetchOutputs(const size_t totalNumberOfParameters, std::tuple<OutputTypes...>& results) { } template<typename T> inline void StoredProc::bindInputParameter(const T& param) { lengths.push_back(0); SQLLEN* len = &lengths.back(); bound_parameters.push_back(param); \/\/ store value auto& stored_param = std::any_cast<T&>(bound_parameters.back()); if constexpr (std::is_same_v<std::decay_t<T>, int>) { *len = sizeof(int); errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_INPUT, SQL_C_LONG, SQL_INTEGER, 10, 0, (SQLPOINTER)std::addressof(stored_param), 0, NULL ); } else if constexpr (std::is_same_v<std::decay_t<T>, bool>) { *len = sizeof(bool); errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_INPUT, SQL_C_BIT, SQL_BIT, 1, 0, (SQLPOINTER)std::addressof(stored_param), 0, NULL ); } else if constexpr (std::is_same_v<std::decay_t<T>, std::string>) { \/\/ here wrong errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_INPUT, SQL_C_CHAR, SQL_VARCHAR, 80, 0, (SQLPOINTER)(stored_param.c_str()), stored_param.size() + 1, NULL ); } else if constexpr (std::is_same_v<std::decay_t<T>, std::wstring>) { \/\/ here correct errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_INPUT, SQL_C_WCHAR, SQL_WVARCHAR, 80, 0, (SQLPOINTER)(stored_param.c_str()), (stored_param.size() + 1) * sizeof(wchar_t), NULL ); } if (!SQL_SUCCEEDED(errorCode)) { cleanup(); throw std::exception(Errors::SQL_STATEMENT_BIND_INPUT_ERROR.data()); } } template<typename T> inline void StoredProc::bindOutputParameter(const T& param) { lengths.push_back(0); SQLLEN* len = &lengths.back(); bound_parameters.push_back(param); auto& stored_param = std::any_cast<T&>(bound_parameters.back()); if constexpr (std::is_same_v<std::decay_t<T>, int>) { errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_OUTPUT, SQL_C_LONG, SQL_INTEGER, sizeof(stored_param), 0, (SQLPOINTER)std::addressof(stored_param), sizeof(stored_param), len ); } else if constexpr (std::is_same_v<std::decay_t<T>, bool>) { errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_OUTPUT, SQL_C_TINYINT, SQL_TINYINT, 1, 0, (SQLPOINTER)std::addressof(stored_param), 0, len ); } else if constexpr (std::is_same_v<std::decay_t<T>, std::string>) { *len = MAX_SIZE_BUFFER; stored_param.resize(*len); errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_OUTPUT, SQL_C_CHAR, SQL_VARCHAR, 80, 0, (SQLPOINTER)stored_param.data(), static_cast<SQLLEN>(stored_param.size()), len ); } else if constexpr (std::is_same_v<std::decay_t<T>, std::wstring>) { *len = MAX_SIZE_BUFFER * sizeof(wchar_t); stored_param.resize(MAX_SIZE_BUFFER); errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_OUTPUT, SQL_C_WCHAR, SQL_WVARCHAR, 80, 0, (SQLPOINTER)stored_param.data(), static_cast<SQLLEN>(stored_param.size()) * sizeof(wchar_t), len ); } if (!SQL_SUCCEEDED(errorCode)) { cleanup(); throw std::exception(Errors::SQL_STATEMENT_BIND_OUTPUT_ERROR.data()); } } template<typename T> inline void StoredProc::fetchOneOutput(T& param, size_t any_index) { } \/\/StoredProc.cpp #include \"StoredProc.h\" void StoredProc::handleDiagnosticRecord(SQLHANDLE hHandle, SQLSMALLINT hType, RETCODE RetCode) { SQLSMALLINT iRec = 0; SQLINTEGER iError; WCHAR wszMessage[1000]; WCHAR wszState[SQL_SQLSTATE_SIZE + 1]; if (RetCode == SQL_INVALID_HANDLE) { fwprintf(stderr, L\"Invalid handle!\\n\"); return; } while (SQLGetDiagRecW( hType, hHandle, ++iRec, wszState, &iError, wszMessage, (SQLSMALLINT)(sizeof(wszMessage) \/ sizeof(WCHAR)), (SQLSMALLINT*)NULL) == SQL_SUCCESS) if (wcsncmp(wszState, L\"01004\", 5)) fwprintf(stderr, L\"[%5.5s] %s (%d)\\n\", wszState, wszMessage, iError); } void StoredProc::allocate() { errorCode = SQLAllocHandle(SQL_HANDLE_STMT, connectionHandle, &statementHandle); if (!SQL_SUCCEEDED(errorCode)) throw std::exception(Errors::SQL_STATEMENT_ALLOC_ERROR.data()); } void StoredProc::prepare(const procedureName& p, const size_t totalNumberOfParameters) { std::string procedureCall{ \"{ ? = CALL \" + p + \"(\" }; for (size_t i = 0; i < totalNumberOfParameters; ++i) procedureCall += \"?, \"; if (totalNumberOfParameters > 0) procedureCall.erase(procedureCall.end() - 2, procedureCall.end()); procedureCall += \") }\"; errorCode = SQLPrepare(statementHandle, (SQLCHAR*)(procedureCall.c_str()), SQL_NTS); if (!SQL_SUCCEEDED(errorCode)) { cleanup(); throw std::exception(Errors::SQL_STATEMENT_PREP_ERROR.data()); } } void StoredProc::bindReturnCode() { \/\/ must be first lengths.push_back(0); SQLLEN* len = &lengths.back(); errorCode = SQLBindParameter( statementHandle, paramNumber++, SQL_PARAM_OUTPUT, SQL_C_LONG, SQL_INTEGER, sizeof(returnCode), 0, reinterpret_cast<SQLPOINTER>(&returnCode), sizeof(returnCode), len ); if (!SQL_SUCCEEDED(errorCode)) { cleanup(); throw std::exception(Errors::SQL_STATEMENT_BIND_RETCODE_ERROR.data()); } } void StoredProc::cleanup() { handleDiagnosticRecord(statementHandle, SQL_HANDLE_STMT, errorCode); errorCode = SQLFreeHandle(SQL_HANDLE_STMT, statementHandle); if (!SQL_SUCCEEDED(errorCode)) { handleDiagnosticRecord(statementHandle, SQL_HANDLE_STMT, errorCode); throw std::exception(Errors::SQL_STATEMENT_CLEANUP_ERROR.data()); } } ``` You should be able to call any stored procedure as: ``` #include \"StoredProc.h\" int main() { \/\/ connect to database StoredProc proc{ connectionHandle }; \/\/ define parameters; std::string in_param1 = \"param1\"; std::wstring in_param2 = L\"something\"; int in_param3 = 6; bool in_param4 = true; \/\/ order of types matters: <procedure_name> returns an int and a bool as OUTPUT parameters auto t = proc.execute<int, bool>(\"[dbo][<procedure_name>]\", in_param1, in_param2, in_param3, in_param4); return 0; } ``` See this for how to connect to SQL Server database.",
    "author_id":5358,
    "publication_date":1754392217000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Laurentiu",
    "author_reputation":11.0,
    "tags":"c++, sql-server, stdany",
    "text_length":11731,
    "title_length":83,
    "num_tags":3
  },
  {
    "id":5828,
    "title":"GStreamer rtpbin DTMF pad linking fails with &quot;caps are incompatible&quot; in Kurento media server",
    "link":"https:\/\/stackoverflow.com\/questions\/79725862\/gstreamer-rtpbin-dtmf-pad-linking-fails-with-caps-are-incompatible-in-kurento",
    "text":"I'm trying to handle DTMF (telephone-event) pads from GStreamer's rtpbin element in a Kurento media server application. Both direct linking and capsfilter approaches fail with \"caps are incompatible\" errors, even after converting encoding-name from lowercase to uppercase. Environment: GStreamer Version: 1.x (Kurento media server) Platform: Linux Language: C Framework: Kurento Media Server ``` 0:00:19.635646312 [35m790342[00m 0x79d0d8001150 [36mINFO [00m [00;01;37;41m GST_ELEMENT_PADS gstelement.c:1016:gst_element_get_static_pad:[00m found pad dtmf_capsfilter:sink 0:00:19.635664077 [35m790342[00m 0x79d0d8001150 [36mINFO [00m [00;01;31;44m GST_PADS gstpad.c:2440:gst_pad_link_prepare:[00m trying to link rtpbin1:recv_rtp_src_0_1693280758_101 and dtmf_capsfilter:sink 0:00:19.635688821 [35m790342[00m 0x79d0d8001150 [36mINFO [00m [00;01;31;44m GST_PADS gstpad.c:2453:gst_pad_link_prepare:[00m caps are incompatible 0:00:19.635700218 [35m790342[00m 0x79d0d8001150 [36mINFO [00m [00;01;31;44m GST_PADS gstpad.c:2563:gst_pad_link_full:[00m link between rtpbin1:recv_rtp_src_0_1693280758_101 and dtmf_capsfilter:sink failed: no common format Pankaj Capsfilter link also failed (-4) ``` Implementation: ``` GstElement *dtmfdepay = kms_utils_element_factory_make (\"rtpdtmfdepay\", \"dtmfdepay\"); GstElement *fake = kms_utils_element_factory_make (\"fakesink\", PLUGIN_NAME); if (!dtmfdepay || !fake) { GST_ERROR_OBJECT (self, \"Failed to create dtmfdepay or fakesink\"); goto end; } \/\/ Configure fakesink g_object_set (fake, \"async\", FALSE, \"sync\", FALSE, \"silent\", TRUE, NULL); \/\/ Add elements to bin BEFORE linking gst_bin_add_many (GST_BIN (self), dtmfdepay, fake, NULL); \/\/ Set elements to READY state to allow pad queries gst_element_set_state (dtmfdepay, GST_STATE_READY); gst_element_set_state (fake, GST_STATE_READY); \/\/ Get the sink pad from dtmfdepay GstPad *dtmf_sinkpad = gst_element_get_static_pad (dtmfdepay, \"sink\"); if (!dtmf_sinkpad) { GST_ERROR_OBJECT (self, \"Failed to get dtmfdepay sink pad\"); gst_bin_remove_many (GST_BIN (self), dtmfdepay, fake, NULL); goto end; } \/\/ Check what caps dtmfdepay can accept GstCaps *dtmf_caps = gst_pad_query_caps (dtmf_sinkpad, NULL); gchar *dtmf_caps_str = gst_caps_to_string (dtmf_caps); g_print (\"Pankaj DTMF depay accepts: %s\\n\", dtmf_caps_str); g_free (dtmf_caps_str); gst_caps_unref (dtmf_caps); \/\/ Try to link rtpbin pad directly to dtmfdepay GstPadLinkReturn link_result = gst_pad_link (pad, dtmf_sinkpad); gst_object_unref (dtmf_sinkpad); if (link_result != GST_PAD_LINK_OK) { g_print (\"Pankaj Direct link failed (%d), trying with capsfilter\\n\", link_result); \/\/ Remove elements and try with capsfilter gst_bin_remove_many (GST_BIN (self), dtmfdepay, fake, NULL); \/\/ Create capsfilter approach GstElement *capsfilter = gst_element_factory_make (\"capsfilter\", \"dtmf_capsfilter\"); dtmfdepay = kms_utils_element_factory_make (\"rtpdtmfdepay\", \"dtmfdepay\"); fake = kms_utils_element_factory_make (\"fakesink\", PLUGIN_NAME); if (!capsfilter || !dtmfdepay || !fake) { GST_ERROR_OBJECT (self, \"Failed to create elements for capsfilter approach\"); goto end; } g_object_set (fake, \"async\", FALSE, \"sync\", FALSE, \"silent\", TRUE, NULL); \/\/ Try different caps variants that might work GstCaps *filter_caps; \/\/ Option 1: Keep original caps but fix encoding-name if (g_strrstr (caps_str, \"clock-rate=(int)8000\")) { filter_caps = gst_caps_from_string ( \"application\/x-rtp, media=audio, encoding-name=TELEPHONE-EVENT, payload=101, clock-rate=8000\"); } else { \/\/ Option 2: More flexible caps filter_caps = gst_caps_from_string ( \"application\/x-rtp, encoding-name=TELEPHONE-EVENT, payload=101\"); } g_object_set (capsfilter, \"caps\", filter_caps, NULL); gst_caps_unref (filter_caps); \/\/ Add elements to bin gst_bin_add_many (GST_BIN (self), capsfilter, dtmfdepay, fake, NULL); \/\/ Set to READY state gst_element_set_state (capsfilter, GST_STATE_READY); gst_element_set_state (dtmfdepay, GST_STATE_READY); gst_element_set_state (fake, GST_STATE_READY); \/\/ Link capsfilter -> dtmfdepay -> fakesink first if (!gst_element_link_many (capsfilter, dtmfdepay, fake, NULL)) { GST_ERROR_OBJECT (self, \"Failed to link capsfilter -> dtmfdepay -> fakesink\"); gst_bin_remove_many (GST_BIN (self), capsfilter, dtmfdepay, fake, NULL); goto end; } \/\/ Now link rtpbin pad to capsfilter GstPad *capsfilter_sinkpad = gst_element_get_static_pad (capsfilter, \"sink\"); link_result = gst_pad_link (pad, capsfilter_sinkpad); gst_object_unref (capsfilter_sinkpad); if (link_result != GST_PAD_LINK_OK) { g_print (\"Pankaj Capsfilter link also failed (%d)\\n\", link_result); GST_ERROR_OBJECT (self, \"Failed to link rtpbin pad to capsfilter\"); gst_bin_remove_many (GST_BIN (self), capsfilter, dtmfdepay, fake, NULL); goto end; } \/\/ Sync state with parent gst_element_sync_state_with_parent (capsfilter); gst_element_sync_state_with_parent (dtmfdepay); gst_element_sync_state_with_parent (fake); } else { g_print (\"Pankaj Direct link successful!\\n\"); \/\/ Link dtmfdepay to fakesink if (!gst_element_link (dtmfdepay, fake)) { GST_ERROR_OBJECT (self, \"Failed to link dtmfdepay to fakesink\"); gst_bin_remove_many (GST_BIN (self), dtmfdepay, fake, NULL); goto end; } \/\/ Sync state with parent gst_element_sync_state_with_parent (dtmfdepay); gst_element_sync_state_with_parent (fake); } ``` All linking attempts fail with: ``` caps are incompatible link between rtpbin1:recv_rtp_src_0_1147121133_101 and dtmfdepay_rtpdtmfdepay0:sink failed: no common format ``` Debug Output: ``` rtpbin pad caps: Pankaj Pad caps: application\/x-rtp, media=(string)audio, encoding-name=(string)telephone-event, payload=(int)101, clock-rate=(int)8000 After case conversion for capsfilter: Pankaj Converting encoding-name from 'telephone-event' to 'TELEPHONE-EVENT' Pankaj Using filter caps: application\/x-rtp, media=(string)audio, encoding-name=(string)TELEPHONE-EVENT, payload=(int)101, clock-rate=(int)8000 ``` Error logs: ``` INFO GST_PADS gstpad.c:2440:gst_pad_link_prepare: trying to link rtpbin1:recv_rtp_src_0_1147121133_101 and dtmfdepay_rtpdtmfdepay0:sink INFO GST_PADS gstpad.c:2453:gst_pad_link_prepare: caps are incompatible INFO GST_PADS gstpad.c:2563:gst_pad_link_full: link between rtpbin1:recv_rtp_src_0_1147121133_101 and dtmfdepay_rtpdtmfdepay0:sink failed: no common format ```",
    "author_id":5357,
    "publication_date":1754392455000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Pankaj Patel",
    "author_reputation":11.0,
    "tags":"c, linux, gstreamer, kurento, kurento-media-server",
    "text_length":6273,
    "title_length":102,
    "num_tags":5
  },
  {
    "id":5827,
    "title":"Conjunction vs. null for optional predicates",
    "link":"https:\/\/stackoverflow.com\/questions\/79725867\/conjunction-vs-null-for-optional-predicates",
    "text":"I'm writing a helper method for a Spring Data JPA Specification and want to handle the case where a filter value is not provided. My goal is for the method to return a \"no-op\" specification that doesn't add a predicate to the ``` WHERE ``` clause. I'm considering two options for my helper method ``` hasProductNumber ``` : ``` \/\/ Option 1: Return criteriaBuilder.conjunction() public static Specification<ProductEntity> hasProductNumber(int productNumber) { return (root, query, criteriaBuilder) -> productNumber == 0 ? criteriaBuilder.conjunction() : criteriaBuilder.equal(root.get(\"productNumber\"), productNumber); } \/\/ Option 2: Return null public static Specification<ProductEntity> hasProductNumber(int productNumber) { return (root, query, criteriaBuilder) -> productNumber == 0 ? null : criteriaBuilder.equal(root.get(\"productNumber\"), productNumber); } ``` Both options seem to work correctly when a specification is chained, for example: ``` Specification.where(someOtherSpec).and(hasProductNumber(0)) ``` . My question is not about the functional difference, but about community best practices and preference. Which of these two approaches is considered the cleaner, more robust, or idiomatic way to handle an optional Specification?",
    "author_id":5356,
    "publication_date":1754392793000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Nico S.",
    "author_reputation":172.0,
    "tags":"java, jpa, spring-data-jpa, criteria-api",
    "text_length":1244,
    "title_length":44,
    "num_tags":4
  },
  {
    "id":5826,
    "title":"export data table to excel starting data from fourth row",
    "link":"https:\/\/stackoverflow.com\/questions\/79725868\/export-data-table-to-excel-starting-data-from-fourth-row",
    "text":"I have exported the data table data to excel file. I have added features like borders and exporting all the data to excel sheet. Now, I need to export data to the excel leaving first three rows empty because clients will add their own data. How can I do that? I searched but can't found any relevant information regarding this. ``` $('#example2').DataTable({ responsive: true, dom: 'lBfrtip', \/\/ 'l' for length menu, 'B' for buttons, 'f' for filter, 'r' for processing display, 't' for table, 'i' for information, 'p' for pagination buttons: [ { extend: 'excelHtml5', title: '', filename: 'report', exportOptions: { modifier: { selected: null } }, customize: function(xlsx) { var sheet = xlsx.xl.worksheets['sheet1.xml']; $('row c', sheet).attr('s', '25'); } } ] }); ```",
    "author_id":5355,
    "publication_date":1754392832000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user4221591",
    "author_reputation":2238.0,
    "tags":"javascript, jquery, datatables",
    "text_length":770,
    "title_length":56,
    "num_tags":3
  },
  {
    "id":5825,
    "title":"How to use AppIntents from an XCFramework in a Widget?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725870\/how-to-use-appintents-from-an-xcframework-in-a-widget",
    "text":"I'm building an XCFramework that contains a Widget and an AppIntent . When I integrate the framework into an app, the widget UI works perfectly , but the AppIntent is never called when I try to use it with a button like this: ``` Button(intent: MyIntent()) { someImage() } ``` From what I understand, Xcode registers AppIntents at runtime, so I tried explicitly referencing the intent class in a configuration method: ``` public class MySDKModal { @available(iOS 17.0, *) public static func configure() { _ = MyIntent.self } } ``` and call it from the ``` init ``` of WidgetBundle but unfortunately, this didn’t help — the AppIntent is still not invoked. How can I ensure that AppIntents defined in an XCFramework are properly registered and called in a Widget? Any help or insights would be greatly appreciated!",
    "author_id":5354,
    "publication_date":1754392903000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"sapir1126",
    "author_reputation":39.0,
    "tags":"ios, swift, widget, xcframework",
    "text_length":812,
    "title_length":54,
    "num_tags":4
  },
  {
    "id":5824,
    "title":"How do I smoothly animate falling elements after removing items from a CSS grid-like layout?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725873\/how-do-i-smoothly-animate-falling-elements-after-removing-items-from-a-css-grid",
    "text":"I'm building a slot game where symbols (divs) are laid out in columns inside a flex container. When a match is detected, I remove the matching elements from the DOM and then new ones fall in from the top. After the removal, I want the remaining symbols above them to fall smoothly into the empty space (Not the newly generated ones, since they fall in fine). However, since removing elements causes reflow, a transition doesn't apply and the elements just shift downwards to fill the empty spaces that were left by removed elements. And then after that newly generated ones fill whatever is left with a nice transition. I tried adding a transition through CSS to the elements that are supposed to fall down. I tried setting the height of the removed element to 0 and adding a transition to the height, before it gets removed. How can I make the symbols fall down with a smooth animation when I remove symbols below it? This is the logic for the slot, without setting the height to 0, since it looks way worse that way. ``` const symbols = [\"🍒\", \"🍋\", \"🍇\", \"🔔\"]; const columns = document.querySelectorAll(\".col\"); const spinButton = document.querySelector(\"#spin-button\"); const wait = (ms) => new Promise((resolve) => setTimeout(resolve, ms)); const getRandomSymbol = () => { const randomIndex = Math.floor(Math.random() * symbols.length); return symbols[randomIndex]; }; const generateColumns = () => { [...columns].forEach((col) => { while (col.children.length < 4) { const symbol = getRandomSymbol(); const symbolElement = document.createElement(\"div\"); symbolElement.textContent = symbol; symbolElement.dataset.symbolType = symbol; symbolElement.classList.add(\"symbol\", \"above-reel\", \"falling\"); setTimeout(() => { symbolElement.style.transform = \"translateY(0%)\"; }, 300); col.appendChild(symbolElement); } }); }; const startSpin = async () => { let currentSymbols = document.querySelectorAll(\".symbol\"); currentSymbols.forEach((symbol) => { symbol.remove(); }); generateColumns(); await wait(400); checkWins(); }; const checkWins = async () => { let winningSymbols = []; symbols.forEach((symbol) => { \/\/ Check for each symbol let consecutiveCount = 0; for (let col = 0; col < columns.length; col++) { \/\/ Check if each column has the symbol const columnSymbols = [...columns[col].children].map( (s) => s.dataset.symbolType ); if (columnSymbols.includes(symbol)) { consecutiveCount++; } else { break; \/\/ Stop checking if a column does not have the symbol } } if (consecutiveCount >= 3) { let lastColIndex = consecutiveCount - 1; \/\/ Get the last column index where the symbol was found winningSymbols.push([symbol, lastColIndex]); \/\/ if there are 3 or more consecutive columns with the same symbol store the winning symbol } }); if (winningSymbols.length !== 0) { await wait(1000); tumble(winningSymbols); } else { console.log(\"gg\"); } }; const tumble = async (winningSymbols) => { const allMatches = []; for (let i = 0; i < winningSymbols.length; i++) { for (let j = 0; j < winningSymbols[i][1] + 1; j++) { const matches = columns[j].querySelectorAll( `div[data-symbol-type=\"${winningSymbols[i][0]}\"]` ); allMatches.push(...matches); } } allMatches.map(async (match) => { match.classList.add(\"removing\"); await wait(850); match.remove(); }); await wait(200); generateColumns(); await wait(350); \/\/ wait for symbols to drop down before checking checkWins(); }; spinButton.addEventListener(\"click\", () => { startSpin(); }); ``` ``` body { font-family: Arial, sans-serif; background: radial-gradient(circle, #000000, #250136); color: white; text-align: center; padding: 20px; } .slots-container { display: flex; justify-content: center; gap: 10px; padding: 20px; background: #222; border-radius: 10px; } .col { display: flex; flex-direction: column-reverse; overflow-y: hidden; gap: 5px; min-height: 215px; width: 60px; background: rgba(255, 255, 255, 0.1); border-radius: 5px; padding: 10px; } .symbol { font-size: 30px; text-align: center; background: white; border-radius: 5px; padding: 5px 0; transition: transform 0.4s cubic-bezier(0.4, 0, 0.2, 1); user-select: none; } .symbol.above-reel { transform: translateY(-500%); } .symbol.removing { animation: fadeOut 0.5s forwards; } @keyframes fadeOut { to { opacity: 0; transform: scale(0); } } \/* Controls *\/ .controls { display: flex; justify-content: center; gap: 20px; margin-top: 20px; } #spin-button { padding: 10px 25px; font-weight: bold; background: #f44336; color: white; border: none; border-radius: 5px; cursor: pointer; } #spin-button.spinning { opacity: 0.5; cursor: not-allowed; pointer-events: none; } .box { background: #333; color: white; padding: 10px; border-radius: 5px; min-width: 100px; text-align: center; } ``` ``` <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\" \/> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" \/> <link rel=\"stylesheet\" href=\"style.css\" \/> <title>Document<\/title> <script type=\"module\" src=\".\/js\/main.js\"><\/script> <\/head> <body> <main> <div class=\"slots-container\"> <div id=\"col1\" class=\"col\"><\/div> <div id=\"col2\" class=\"col\"><\/div> <div id=\"col3\" class=\"col\"><\/div> <div id=\"col4\" class=\"col\"><\/div> <div id=\"col5\" class=\"col\"><\/div> <div id=\"col6\" class=\"col\"><\/div> <\/div> <div class=\"controls\"> <div id=\"last-win\" class=\"box\">LAST WIN:<\/div> <button id=\"spin-button\">SPIN<\/button> <div id=\"balance\" class=\"box\"><\/div> <\/div> <\/main> <div id=\"explosions\"><\/div> <\/body> <\/html> ```",
    "author_id":5353,
    "publication_date":1754393037000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Domas",
    "author_reputation":31.0,
    "tags":"javascript, css, flexbox",
    "text_length":5418,
    "title_length":92,
    "num_tags":3
  },
  {
    "id":5823,
    "title":"Immediately write single-line progress to Azure Devops log",
    "link":"https:\/\/stackoverflow.com\/questions\/79725882\/immediately-write-single-line-progress-to-azure-devops-log",
    "text":"In Azure Devops, the log seems to be buffered until a newline character is printed. Some programs use single-line progress bars, like Cucumber's progress or cucumber-js's progress-bar outputs. This is visible on a local machine, but remains invisible in the ADO pipeline, until the line is completed. How can I see the incomplete line being filled in (near) realtime? I have already tried using stdbuf like this: ``` stdbuf -o0 -e0 npm run tests ``` or unbuffer like this: ``` unbuffer npm run tests ``` Both approaches did not work.",
    "author_id":5352,
    "publication_date":1754393489000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Corbie",
    "author_reputation":1148.0,
    "tags":"azure-devops, cucumber, cucumberjs",
    "text_length":533,
    "title_length":58,
    "num_tags":3
  },
  {
    "id":5822,
    "title":"Neo4J (5.14.0): add labels based on values in CSV file",
    "link":"https:\/\/stackoverflow.com\/questions\/79725896\/neo4j-5-14-0-add-labels-based-on-values-in-csv-file",
    "text":"I'm trying to create nodes with one set label (:object) and two additional labels that are based on values in the following CSV file: Object_ID Classification Hierarchy_name Object_type O0010 visual_works sculpture sculpture O0011 components book_jackets book I thought it would be possible to add dynamic labels based on the values in the spreadsheet using the $ expression, but I cannot find information about how to use this in relation to loading information from a CSV file. Currently, my faulty script looks like this: ``` LOAD CSV WITH HEADERS FROM 'file:\/\/\/classification.csv' AS row CREATE (o:object:$(row.Hierarchy_name):$(row.Classification){ID: row.Object_ID, Type: row.Type}); ```",
    "author_id":5351,
    "publication_date":1754393938000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jaap",
    "author_reputation":101.0,
    "tags":"csv, neo4j, cypher",
    "text_length":693,
    "title_length":54,
    "num_tags":3
  },
  {
    "id":5821,
    "title":"The function doesn&#39;t replace inputs with spaces",
    "link":"https:\/\/stackoverflow.com\/questions\/79725899\/the-function-doesnt-replace-inputs-with-spaces",
    "text":"I tried making a program which takes a template from a file ``` letter.txt ``` and replaces required placeholders with user entered inputs and stores it in file ``` bill.txt ``` . But the program stops working if inputs with spaces are entered. The program stops after taking all inputs even if only one input with spaces is entered. The problem starts after the ``` name ``` , ``` item ``` and ``` outlet ``` variables are given values. ``` char **text = (char **)malloc(lines * sizeof(char *)); \/\/ making a big continuous block of memory for better speed char *text2 = (char *)malloc(140 * lines * sizeof(char)); int lines = 3; FILE *f = fopen(\"letter.txt\", \"r\"); printf(\"The template is\\n\"); for (int i = 0; i < lines; i++) { text[i] = text2 + i * 140; fgets(text[i], 140, f); printf(\"%s\", text[i]); } printf(\"\\n\"); char *name, *item, *outlet; name = makevar(\"name\"); item = makevar(\"item\"); outlet = makevar(\"outlet\"); \/\/The problem starts here. char **output = (char **)malloc(lines * sizeof(char *)); \/\/ making a big continuous block of memory for better speed char *output2 = (char *)malloc(200 * lines * sizeof(char)); for (int i = 0; i < lines; i++) { output[i] = output2 + i * 200; } \/\/replacing placeholders replword(text, output, \"{{name}}\", name, lines); replword(output, output, \"{{item}}\", item, lines); replword(output, output, \"{{outlet}}\", outlet, lines); printf(\"The output is\\n\"); for (int i = 0; i < lines; i++) { printf(\"%s\", output[i]); } printf(\"\\n\"); return 0; } char *makevar(char string[]) { char *val; printf(\"Enter %s\\n\", string); val = (char *)malloc(32 * sizeof(char)); fgets(val, 32, stdin); val[strcspn(val, \"\\n\")] = '\\0'; return val; } void replword(char **text, char **output, char oldword[], char newword[], int lines) { int oldsize = strlen(oldword); int newsize = strlen(newword); for (int i = 0; i < lines; i++) { char *src = text[i]; char *dest = output[i]; while (*src != '\\0') { if (strstr(src, oldword) == src) { strcpy(dest, newword); dest += newsize; src += oldsize; } else { *dest = *src; dest += 1; src += 1; } } *dest = '\\0'; } } ``` Output ``` PS D:\\PROGRAM\\C PROG> cd \"d:\\PROGRAM\\C PROG\\\" ; if ($?) { gcc filebillreceipt.c -o filebillreceipt } ; if ($?) { .\\filebillreceipt } The template is Thanks {{name}} for buying {{item}} from the outlet {{outlet}}. Please visit our outlet {{outlet}} for any issues. We hope to serve you again. Enter name Khorne Enter item Blood Enter outlet Skull Throne PS D:\\PROGRAM\\C PROG> ``` It works as intended for inputs without spaces.",
    "author_id":5350,
    "publication_date":1754394007000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Dragzterz",
    "author_reputation":53.0,
    "tags":"c, string, file",
    "text_length":2519,
    "title_length":51,
    "num_tags":3
  },
  {
    "id":5820,
    "title":"WhatsApp Embedded Signup &quot;Sorry, something went wrong&quot; Error in Meta Dashboard",
    "link":"https:\/\/stackoverflow.com\/questions\/79725900\/whatsapp-embedded-signup-sorry-something-went-wrong-error-in-meta-dashboard",
    "text":"I'm trying to integrate WhatsApp Embedded Signup into my web application, but I'm encountering an error when using the official Meta Developer Dashboard's Embedded Signup Builder. Current setup Facebook App: Created and verified by Meta Business Verification: Completed App Review & Access Verification: Completed Integrity Verification: Completed OAuth Redirect URIs: Configured in Facebook App settings All required settings: Enabled for Facebook Login App Mode: Development mode (WhatsApp login restricted to developer account only) Permissions Granted by Meta: ``` whatsapp_business_messaging ``` ``` whatsapp_business_management ``` ``` business_management ``` The Embedded Signup Builder should open a popup allowing users to connect their WhatsApp Business account to my application. When I navigate to WhatsApp → Embedded Signup Builder in the Meta Developer Dashboard and click \"Launch Embedded Signup\", it generates this OAuth URL: ``` https:\/\/www.facebook.com\/v23.0\/dialog\/oauth?display=popup&client_id=9915250391888243&redirect_uri=https%3A%2F%2Fdevelopers.facebook.com%2Fes%2Foauth%2Fcallback%2F%3Fuse_case_enum%3DWHATSAPP_BUSINESS_MESSAGING%26selected_tab%3Dwa-dev-quickstart%26product_route%3Dwhatsapp-business%26business_id%3D1221054146106644%26nonce%3DgY8GfXBgFKVLSg2tQg9FiFGC18miPqqi&config_id=1760584467867569&response_type=code&auth_type&fallback_redirect_uri=https%3A%2F%2Fdevelopers.facebook.com%2Fes%2Foauth%2Fcallback%2F%3Fuse_case_enum%3DWHATSAPP_BUSINESS_MESSAGING%26selected_tab%3Dwa-dev-quickstart%26product_route%3Dwhatsapp-business%26business_id%3D1221054146106644%26nonce%3DgY8GfXBgFKVLSg2tQg9FiFGC18miPqqi&override_default_response_type=true&extras=%7B%22sessionInfoVersion%22%3A%223%22%2C%22version%22%3A%22v3%22%7D ``` However, this URL redirects to a page showing: Sorry, something went wrong. We're working on getting this fixed as soon as we can. So far I've: Double-checked all Facebook App configurations Verified all OAuth redirect URIs are properly set Confirmed business verification status Verified all required permissions are granted and active: ``` whatsapp_business_messaging ``` ``` whatsapp_business_management ``` ``` business_management ``` Confirmed app is in Development mode with WhatsApp login restricted to developer account Tested login using the developer Facebook account that created the app Tested with different browsers Cleared cache and cookies Waited several days thinking it might be temporary Questions: Has anyone encountered this specific error with WhatsApp Embedded Signup Builder? Is there a known issue with Meta's developer platform currently? Could the Development mode restriction be causing this issue, even when testing with the developer account? Should I implement the OAuth flow manually instead of using the builder? Are there additional permissions or settings I might be missing? Environment: Meta Graph API Version: v23.0 App Type: Web Application Testing Environment: Both localhost and production domain Browser: Tested on Chrome, Firefox, Safari The error occurs even when testing directly from the Meta Developer Dashboard, which suggests it might not be related to my specific app configuration but rather a platform-wide issue.",
    "author_id":5349,
    "publication_date":1754394135000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Rayyan Maqsood",
    "author_reputation":32.0,
    "tags":"facebook, whatsapp, facebook-oauth",
    "text_length":3218,
    "title_length":88,
    "num_tags":3
  },
  {
    "id":5819,
    "title":"PdfEncryption exception in iText",
    "link":"https:\/\/stackoverflow.com\/questions\/79725904\/pdfencryption-exception-in-itext",
    "text":"I hit pdfencrytion exception while trying to set password for my created pdf, below is my code snippet: ``` String userPassword = code != null ? code : \"default\"; WriterProperties writerProperties = new WriterProperties() .setStandardEncryption( userPassword.getBytes(StandardCharsets.UTF_8), userPassword.getBytes(StandardCharsets.UTF_8), EncryptionConstants.ALLOW_PRINTING, EncryptionConstants.ENCRYPTION_AES_128 ); PdfWriter writer = new PdfWriter(new FileOutputStream(destination), writerProperties); \/\/ initialize pdf document PdfDocument pdf = new PdfDocument(writer); pdf.setDefaultPageSize(PageSize.A4.rotate()); ... ``` The error was: ``` > SEVERE: Either com.itextpdf:bouncy-castle-adapter or > com.itextpdf:bouncy-castle-fips-adapter dependency must be added in > order to use BouncyCastleFactoryCreator > com.itextpdf.kernel.exceptions.PdfException: PdfEncryption exception. > at > com.itextpdf.kernel.crypto.securityhandler.SecurityHandler.safeInitMessageDigest(SecurityHandler.java:105) > at > com.itextpdf.kernel.crypto.securityhandler.SecurityHandler.<init>(SecurityHandler.java:67) > at > com.itextpdf.kernel.crypto.securityhandler.StandardSecurityHandler.<init>(StandardSecurityHandler.java:34) > at > com.itextpdf.kernel.crypto.securityhandler.StandardHandlerUsingStandard40.<init>(StandardHandlerUsingStandard40.java:55) > at > com.itextpdf.kernel.crypto.securityhandler.StandardHandlerUsingStandard128.<init>(StandardHandlerUsingStandard128.java:35) > at > com.itextpdf.kernel.crypto.securityhandler.StandardHandlerUsingAes128.<init>(StandardHandlerUsingAes128.java:41) > at > com.itextpdf.kernel.pdf.PdfEncryption.<init>(PdfEncryption.java:129) > Caused by: java.lang.UnsupportedOperationException: Either com.itextpdf:bouncy-castle-adapter or > com.itextpdf:bouncy-castle-fips-adapter dependency must be added in > order to use BouncyCastleFactoryCreator at > com.itextpdf.bouncycastleconnector.BouncyCastleDefaultFactory.isInApprovedOnlyMode(BouncyCastleDefaultFactory.java:904) > at > com.itextpdf.kernel.crypto.securityhandler.SecurityHandler.safeInitMessageDigest(SecurityHandler.java:101) > Caused by: java.lang.UnsupportedOperationException: Either > com.itextpdf:bouncy-castle-adapter or > com.itextpdf:bouncy-castle-fips-adapter dependency must be added in > order to use BouncyCastleFactoryCreator my build.gradle: > compile 'com.itextpdf:itext7-core:8.0.1' compile 'com.itextpdf:kernel:8.0.1' compile 'com.itextpdf:io:8.0.1' compile 'com.itextpdf:layout:8.0.1' compile ('com.itextpdf.licensing:licensing-base:4.1.1') { exclude group: 'com.fasterxml.jackson.core', module: 'jackson-databind' } compile ('com.itextpdf:bouncy-castle-adapter:8.0.1') { exclude group: 'org.bouncycastle', module: 'bcutil-jdk18on' exclude group: 'org.bouncycastle', module: 'bcprov-jdk18on' exclude group: 'org.bouncycastle', module: 'bcpkix-jdk18on' } ``` Appreciate if anyone has idea is it because of dependency issue? any solution i can change the code without modifying the dependency",
    "author_id":4698,
    "publication_date":1754394293000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tracy",
    "author_reputation":3.0,
    "tags":"passwords, pdf, itext, bouncycastle",
    "text_length":2999,
    "title_length":32,
    "num_tags":4
  },
  {
    "id":5818,
    "title":"How to create two divs with diagonal inner edges, all corners rounded, and a matching gap between them?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725909\/how-to-create-two-divs-with-diagonal-inner-edges-all-corners-rounded-and-a-mat",
    "text":"I want to recreate a layout where: Two divs (sections) are stacked vertically. Both divs have all four corners rounded (top-left, top-right, bottom-left, bottom-right). The inner edge of each div is cut at a matching diagonal, creating a gap (with the background showing through) that runs diagonally between them (see attached screenshot). The diagonal cut is in the same direction for both divs. The outer corners remain rounded; only the inner edges are diagonal. There’s a small gap between the two shapes, following the same diagonal line. The background is visible through the gap. The top section is a solid colour, the bottom a light colour or gradient. I’ve tried using clip-path, but that removes the border-radius on the diagonal side, which isn’t what I want. What is this shape called? Each section is basically a rounded rectangle with one side sliced diagonally. How can I achieve this in HTML\/CSS, so the outer corners stay rounded and the gap between the divs stays diagonal and matches the angle of the cuts? (See screenshot)",
    "author_id":5348,
    "publication_date":1754394419000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Jake McAllister",
    "author_reputation":1085.0,
    "tags":"html, css, css-shapes, clip-path, border-radius",
    "text_length":1043,
    "title_length":103,
    "num_tags":5
  },
  {
    "id":5817,
    "title":"How to treat string as content in Typst?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725911\/how-to-treat-string-as-content-in-typst",
    "text":"In normal text\/content, Typst will automatically convert certain characters to their typographically correct counterparts (e.g. apostrophes, quotation marks). I would like to get the same behavior for text represented as strings (for example from reading in a csv\/toml file). Is this possible? This is what I tried, without success. The goal would be to automatically get typographically correct apostrophes from the character string, similar to the first line ``` \/\/ what I want China's Flag \/\/ what I have #let s = \"China's Flag\" #s #[#s] ``` Output:",
    "author_id":5347,
    "publication_date":1754394467000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dufei",
    "author_reputation":3489.0,
    "tags":"typst",
    "text_length":552,
    "title_length":40,
    "num_tags":1
  },
  {
    "id":5816,
    "title":"How to export all the Databases Explorer Folders &amp; Databases",
    "link":"https:\/\/stackoverflow.com\/questions\/79725912\/how-to-export-all-the-databases-explorer-folders-databases",
    "text":"I have IBExpert installed on 2 PC's. On 1 PC I have many Database Folders containing many Registered Databases. How can I export or port they entire Database Explorer Structure and then import into another Database Explorer which is on another PC. Hope that makes sense.",
    "author_id":5346,
    "publication_date":1754394572000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Yuppski",
    "author_reputation":189.0,
    "tags":"ibexpert",
    "text_length":270,
    "title_length":64,
    "num_tags":1
  },
  {
    "id":5815,
    "title":"How to write elasticsearch Query DSL with filter the nested objects",
    "link":"https:\/\/stackoverflow.com\/questions\/79725916\/how-to-write-elasticsearch-query-dsl-with-filter-the-nested-objects",
    "text":"There have a doc with the day_count like : ``` \"day_count\": [ { \"user_count\": 2, \"count\": 9, \"start\": \"2025-08-04 00:00:00\", \"end\": \"2025-08-04 23:59:59\", \"day\": \"2025-08-04\" }, { \"user_count\": 2, \"count\": 10, \"start\": \"2025-08-04 00:00:00\", \"end\": \"2025-08-05 23:59:59\", \"day\": \"2025-08-042025-08-05\" }, { \"user_count\": 0, \"count\": 1, \"start\": \"2025-08-05 00:00:00\", \"end\": \"2025-08-05 23:59:59\", \"day\": \"2025-08-05\" } ] ``` then I want filter like sql : day_count.user_count>=1 and day_count.day=\"2025-08-05\" will not return this doc ,because day_count.day=\"2025-08-05\" is the third item ,but user_count is equal 0, not >=1 and the query dsl like below with will return this doc: ``` \"filter\": [ { \"bool\": { \"must\": [ { \"range\": { \"day_count.user_count\": { \"gte\": 1 } } }, { \"term\": { \"day_count.day\": \"2025-08-05\" } } ] } } ] ``` so how to write this filter statement to exclude this doc",
    "author_id":5345,
    "publication_date":1754394690000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"J T",
    "author_reputation":3.0,
    "tags":"elasticsearch, filter",
    "text_length":890,
    "title_length":67,
    "num_tags":2
  },
  {
    "id":5814,
    "title":"Angular compiles but should throw an error",
    "link":"https:\/\/stackoverflow.com\/questions\/79725918\/angular-compiles-but-should-throw-an-error",
    "text":"I'm using angular 16 and I need to configure my app that throws an error when, for example, I have on function in the html file but this function doesn't exists in the ts file. Any idea how can I do it? Currently only fails when you go to the specific page, but angular compiles in jit or aot mode. Thanks.",
    "author_id":5344,
    "publication_date":1754394706000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"colau",
    "author_reputation":29.0,
    "tags":"html, angular, compiler-errors, angular-aot, angular-jit",
    "text_length":306,
    "title_length":42,
    "num_tags":5
  },
  {
    "id":5813,
    "title":"How to dynamically identify and run only the relevant test cases during your CI\/CD pipeline, based on which files were changed?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725920\/how-to-dynamically-identify-and-run-only-the-relevant-test-cases-during-your-ci",
    "text":"I’m building a CI\/CD setup in Azure DevOps where my goal is to dynamically identify and run only the relevant test cases during the pipeline, based on which source files were changed. Instead of running all tests, I want the pipeline to: Check for changed files in a commit. Compare them semantically against my existing test cases using OpenAI embeddings. Run only the matching test cases. Below is the flow chart for ease of understanding- code diff -> embedding -> code embedding -> cosine similarity -> identify the matched function -> identify files where the matched function is there -> identify the matched test case So far I've: Prepared the embeddings generation script I created a Python script ``` generate_embeddings.py ``` which: Reads ``` test-map.json ``` (a mapping of test case names to their code content). Calls OpenAI API to generate embeddings for each test case. Stores the results in ``` test-embeddings.json ``` . My ``` test-map.json ``` ``` func init . --worker-runtime python --python func new --name SelectTestCases --template \"HTTP trigger\" --authlevel \"anonymous\" ``` Created an Azure Function App for running embeddings logic This Function will later act as a REST API endpoint to accept the content of ``` test-map.json ``` in the request body and return the generated embeddings as JSON. Created ``` check_and_generate_embeddings.py ``` which compares the current ``` test-map.json ``` with the last committed one. If changes are detected, calls the embeddings generation logic. Initially, it directly ran generate_embeddings.py locally, but I plan to replace it with an API call to my Azure Function so the embeddings generation runs in the cloud. Instead of the pipeline running ``` generate_embeddings.py ``` directly, I’m implementing an HTTP POST endpoint in the Azure Function that: Takes ``` test-map.json ``` as input. Returns generated embeddings JSON. ``` check_and_generate_embeddings.py ``` will send the POST request with ``` test-map.json ``` content. Save the API response into ``` test-embeddings.json ``` . I have: Local scripts ( ``` generate_embeddings.py ``` , ``` check_and_generate_embeddings.py ``` ). Azure Function App initialized. Azure DevOps repo and pipeline configured. But I need help in: Finalizing the Azure Function code for embeddings generation. Updating ``` check_and_generate_embeddings.py ``` to make the REST API call to the Function App and save the output to ``` test-embeddings.json ``` . Making sure the DevOps pipeline runs this flow automatically when ``` test-map.json ``` changes. My current ``` generate_embeddings.py ``` code. ``` import openai import json # Azure OpenAI config openai.api_type = \"azure\" openai.api_base = \"https:\/\/arkoaiopenai.openai.azure.com\/\" openai.api_key = \"abcd\" openai.api_version = \"2023-05-15\" # Load your test mapping JSON with open(\"test-map.json\", \"r\") as f: test_map = json.load(f) # This will store the generated embeddings embeddings = {} # Loop through test files and generate embeddings for test_name, test_code in test_map.items(): response = openai.Embedding.create( input=test_code, deployment_id=\"text-embedding-ada-002\" ) embedding = response[\"data\"][0][\"embedding\"] embeddings[test_name] = embedding # Save all embeddings to file with open(\"test-embeddings.json\", \"w\") as f: json.dump(embeddings, f) ``` My ``` check_and_generate_embeddings.py ``` code. ``` import json import hashlib import os import requests # File paths TEST_MAP_FILE = \"test-map.json\" TEST_EMBEDDINGS_FILE = \"test-embeddings.json\" HASH_FILE = \".test_map_hash\" FUNCTION_URL = \"https:\/\/arkofunctionapp.azurewebsites.net\/api\/generate-embeddings\" FUNCTION_KEY = \"abcd\" def get_file_hash(file_path): \"\"\"Returns MD5 hash of a file.\"\"\" with open(file_path, \"rb\") as f: return hashlib.md5(f.read()).hexdigest() def read_stored_hash(): \"\"\"Reads previously stored hash from file.\"\"\" if os.path.exists(HASH_FILE): with open(HASH_FILE, \"r\") as f: return f.read().strip() return None def write_stored_hash(hash_value): \"\"\"Writes hash to file.\"\"\" with open(HASH_FILE, \"w\") as f: f.write(hash_value) def regenerate_embeddings(): \"\"\"Calls Azure Function API and writes embeddings to file.\"\"\" with open(TEST_MAP_FILE, \"r\") as f: test_map_data = json.load(f) headers = { \"Content-Type\": \"application\/json\", \"x-functions-key\": FUNCTION_KEY } print(f\"Calling Azure Function at {FUNCTION_URL}...\") response = requests.post(FUNCTION_URL, headers=headers, json=test_map_data) if response.status_code == 200: embeddings_data = response.json() with open(TEST_EMBEDDINGS_FILE, \"w\") as f: json.dump(embeddings_data, f, indent=2) print(f\" Embeddings saved to {TEST_EMBEDDINGS_FILE}\") else: print(f\" Failed to call Azure Function: {response.status_code}\") print(response.text) if __name__ == \"__main__\": current_hash = get_file_hash(TEST_MAP_FILE) stored_hash = read_stored_hash() if current_hash != stored_hash: print(\" test-map.json has changed. Regenerating embeddings via Azure Function...\") regenerate_embeddings() write_stored_hash(current_hash) else: print(\"test-map.json has not changed. No regeneration needed.\") ```",
    "author_id":5343,
    "publication_date":1754394806000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Arko",
    "author_reputation":4015.0,
    "tags":"azure, azure-pipelines, azure-functions, azure-openai, openaiembeddings",
    "text_length":5093,
    "title_length":127,
    "num_tags":5
  },
  {
    "id":5812,
    "title":"Main class in an eclipse plugin",
    "link":"https:\/\/stackoverflow.com\/questions\/79725923\/main-class-in-an-eclipse-plugin",
    "text":"I have made an Eclipse plugin. Now I want it possible to also run it as a standalone app using ``` java -jar ``` . However the classes are packed in the ``` bin\/ ``` folder inside the jar, and I get a ``` ClassNotFoundException ``` . I tried to specify ``` Class-Path: bin\/ ``` in ``` MANIFEST.MF ``` , but this did not help. I am aware that I will have to take care of class loading and context in the main class, my question is about how to make ``` java -jar ``` just find the main class. My MANIFEST.MF: ``` Manifest-Version: 1.0 Bundle-ManifestVersion: 2 Main-Class: io.github.magwas.inez.Main Bundle-Name: inez-model Bundle-SymbolicName: inez-model;singleton:=true Spring-Context: *;create-asynchronously:=false Bundle-Activator: io.github.magwas.inez.osgi.SpringBootBundleActivator Bundle-Version: 0.4.1 Bundle-Vendor: Árpád Magosányi Export-Package: io.github.magwas.inez, io.github.magwas.inez.element Require-Bundle: org.eclipse.core.runtime;bundle-version=\"3.33.0\" Bundle-ClassPath: bin\/, target\/dependency\/antlr4-runtime-4.13.2.jar, [omitted a lot of jars here] target\/dependency\/konveyor-base-0.2.7-testing.jar Class-Path: bin\/ Bundle-RequiredExecutionEnvironment: JavaSE-21 Automatic-Module-Name: io.github.magwas.inez.model ``` the pom.xml of the project: ``` <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/xsd\/maven-4.0.0.xsd\"> <modelVersion>4.0.0<\/modelVersion> <parent> <groupId>io.github.magwas<\/groupId> <artifactId>inez<\/artifactId> <version>0.0.1<\/version> <relativePath>..\/pom.xml<\/relativePath> <\/parent> <packaging>eclipse-plugin<\/packaging> <artifactId>inez-model<\/artifactId> <name>inez-model<\/name> <description> model layer for Inez <\/description> <properties> <osgi.version>6.0.0<\/osgi.version> <\/properties> <dependencies> <dependency> <groupId>org.springframework<\/groupId> <artifactId>spring-core<\/artifactId> <version>${spring-version}<\/version> <\/dependency> <dependency> <groupId>org.springframework.boot<\/groupId> <artifactId>spring-boot<\/artifactId> <version>${spring-boot-version}<\/version> <\/dependency> <dependency> <groupId>org.springframework.boot<\/groupId> <artifactId>spring-boot-autoconfigure<\/artifactId> <version>${spring-boot-version}<\/version> <\/dependency> <dependency> <groupId>org.springframework.data<\/groupId> <artifactId>spring-data-keyvalue<\/artifactId> <version>3.5.1<\/version> <\/dependency> <dependency> <groupId>io.github.magwas<\/groupId> <artifactId>inez-parser<\/artifactId> <version>${project.version}<\/version> <\/dependency> <dependency> <groupId>io.github.magwas<\/groupId> <artifactId>inez-parser<\/artifactId> <version>${project.version}<\/version> <classifier>testing<\/classifier> <scope>test<\/scope> <\/dependency> <dependency> <groupId>io.github.magwas<\/groupId> <artifactId>konveyor-base<\/artifactId> <version>${konveyor-version}<\/version> <classifier>runtime<\/classifier> <\/dependency> <dependency> <groupId>io.github.magwas<\/groupId> <artifactId>konveyor-base<\/artifactId> <version>${konveyor-version}<\/version> <classifier>testing<\/classifier> <scope>test<\/scope> <\/dependency> <dependency> <groupId>org.osgi<\/groupId> <artifactId>org.osgi.core<\/artifactId> <version>${osgi.version}<\/version> <scope>provided<\/scope> <\/dependency> <\/dependencies> <build> <outputDirectory>bin<\/outputDirectory> <plugins> <plugin> <groupId>org.eclipse.tycho<\/groupId> <artifactId>tycho-packaging-plugin<\/artifactId> <\/plugin> <plugin> <groupId>org.eclipse.tycho<\/groupId> <artifactId>tycho-maven-plugin<\/artifactId> <\/plugin> <plugin> <groupId>org.apache.maven.plugins<\/groupId> <artifactId>maven-clean-plugin<\/artifactId> <version>3.5.0<\/version> <configuration> <filesets> <fileset> <directory>bin<\/directory> <followSymlinks>false<\/followSymlinks> <\/fileset> <\/filesets> <\/configuration> <\/plugin> <plugin> <groupId>org.apache.maven.plugins<\/groupId> <artifactId>maven-dependency-plugin<\/artifactId> <executions> <execution> <phase>initialize<\/phase> <goals> <goal>copy-dependencies<\/goal> <\/goals> <configuration> <\/configuration> <\/execution> <\/executions> <\/plugin> <\/plugins> <\/build> <\/project> ``` Should I package the bundle such that the classes are not in ``` bin\/ ``` but in the root of the jar (and how?), or put something into MANIFEST.MF which makes it work?",
    "author_id":5327,
    "publication_date":1754394901000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"&#193;rp&#225;d Magos&#225;nyi",
    "author_reputation":1476.0,
    "tags":"eclipse, maven, osgi, jar, tycho",
    "text_length":4400,
    "title_length":31,
    "num_tags":5
  },
  {
    "id":5811,
    "title":"Apache NiFi memory usage keeps growing and crashes after clustering – GC not freeing Old Gen",
    "link":"https:\/\/stackoverflow.com\/questions\/79725926\/apache-nifi-memory-usage-keeps-growing-and-crashes-after-clustering-gc-not-fre",
    "text":"I’m running a Dockerized Apache NiFi 2.5.0 setup on a machine with 40 GB RAM. I initially used a single-node setup without issues. But after I added more nodes (2-node and then 3-node cluster), the memory usage started increasing continuously over time, eventually leading to NiFi slowing down problem with UI and finally crashes and restarts. Even though I see GC logs like: ``` [info][gc,start] GC(102) Pause Young (Normal) ``` the memory usage shown by docker stats, Prometheus, and Grafana doesn't decrease — it keeps growing until it reaches the maximum heap and crashes. I suspect Old Generation GC is not being triggered effectively in the clustered setup. My current heap settings: ``` NIFI_JVM_HEAP_INIT: 8g NIFI_JVM_HEAP_MAX: 16g ``` The total container memory usage keeps growing to 24GB+ with no drops over time. Tuned JVM options, but it didn’t help: ``` java.arg.2=-Xms16g java.arg.3=-Xmx16g java.arg.7=-XX:MaxMetaspaceSize=512m java.arg.8=-XX:MaxDirectMemorySize=2g java.arg.9=-XX:+UseG1GC java.arg.10=-XX:MaxGCPauseMillis=200 java.arg.12=-Xlog:gc*,gc+age=trace:file=\/opt\/nifi\/nifi-current\/logs\/gcdetals.log:time,uptime,level,tags:filecount=5,filesize=10M java.arg.13=-XX:+HeapDumpOnOutOfMemoryError java.arg.14=-XX:HeapDumpPath=\/opt\/nifi\/nifi-current\/logs\/heapdumps.log java.arg.15=-XX:InitiatingHeapOccupancyPercent=30 java.arg.16=-XX:+ExplicitGCInvokesConcurrent java.arg.17=-XX:+ParallelRefProcEnabled java.arg.18=-XX:MaxTenuringThreshold=8 ``` Upgrade NiFi from 2.0.0 to 2.5.0 Reduced heap from 20g\/24g to 8g\/16g to observe behavior, but memory still grows and doesn’t drop. Checked for any queue buildup, but there doesn’t seem to be significant data in flowfile queues or backpressure. Monitored GC logs — see frequent Young GCs but almost no Old Gen collection or Full GC. Enabled Prometheus\/Grafana to monitor memory metrics — confirmed memory keeps increasing over hours.",
    "author_id":5342,
    "publication_date":1754394981000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Omid",
    "author_reputation":35.0,
    "tags":"java, jvm, apache-nifi",
    "text_length":1896,
    "title_length":92,
    "num_tags":3
  },
  {
    "id":5810,
    "title":"how to resolve &quot;binding.node is not a valid win-32 application error&quot; installing node-sass",
    "link":"https:\/\/stackoverflow.com\/questions\/79725929\/how-to-resolve-binding-node-is-not-a-valid-win-32-application-error-installing",
    "text":"I have a vuejs application, and when I npm install, it produces the following error: ``` npm error code 1 npm error path D:\\Dev\\Asm.Helios\\Web\\Aeos\\ASM.Helios.Website.Winform-Component-Library\\node_modules\\node-sass npm error command failed npm error command C:\\WINDOWS\\system32\\cmd.exe \/d \/s \/c node scripts\/build.js npm error Binary found at D:\\Dev\\Asm.Helios\\Web\\Aeos\\ASM.Helios.Website.Winform-Component-Library\\node_modules\\node-sass\\vendor\\win32-x64-137\\binding.node npm error Testing binary npm error Binary has a problem: Error: \\\\?\\D:\\Dev\\Asm.Helios\\Web\\Aeos\\ASM.Helios.Website.Winform-Component-Library\\node_modules\\node-sass\\vendor\\win32-x64-137\\**binding.node is not a valid Win32 application**. npm error \\\\?\\D:\\Dev\\Asm.Helios\\Web\\Aeos\\ASM.Helios.Website.Winform-Component-Library\\node_modules\\node-sass\\vendor\\win32-x64-137\\binding.node npm error at Object..node (node:internal\/modules\/cjs\/loader:1898:18) npm error at Module.load (node:internal\/modules\/cjs\/loader:1470:32) npm error at Module._load (node:internal\/modules\/cjs\/loader:1290:12) npm error at TracingChannel.traceSync (node:diagnostics_channel:322:14) npm error at wrapModuleLoad (node:internal\/modules\/cjs\/loader:238:24) npm error at Module.require (node:internal\/modules\/cjs\/loader:1493:12) npm error at require (node:internal\/modules\/helpers:152:16) npm error at module.exports (D:\\Dev\\Asm.Helios\\Web\\Aeos\\ASM.Helios.Website.Winform-Component-Library\\node_modules\\node-sass\\lib\\binding.js:19:10) npm error at Object.<anonymous> (D:\\Dev\\Asm.Helios\\Web\\Aeos\\ASM.Helios.Website.Winform-Component-Library\\node_modules\\node-sass\\lib\\index.js:13:35) ``` My node version is 24.4.1 npm version is 10.2 Other devs in the team with the same setup, it works perfectly for, so it seems to be some kind of configuration problem In the package.json it is trying to install: \"node-sass\": \"^9.0.0\", I have tried: clearing down node_modules and reinstalling npm uninstall node-sass npm i node-sass npm rebuild node-sass npm i sass npm cache clean --force",
    "author_id":5341,
    "publication_date":1754395078000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"jazza1000",
    "author_reputation":4277.0,
    "tags":"node.js, npm, node-sass",
    "text_length":2017,
    "title_length":100,
    "num_tags":3
  },
  {
    "id":5809,
    "title":"ItextSharp C# : combine multiple PDFs with AcroFields specified",
    "link":"https:\/\/stackoverflow.com\/questions\/79725932\/itextsharp-c-combine-multiple-pdfs-with-acrofields-specified",
    "text":"I have multiples PDF created with the iTextsharp library in C#. Those PDFs are created and are also using ``` AcroFields.Item ``` to create fields and field them with the users info. Once I have created all these PDFs, we want to merge them in one new single PDF. But when I try to join them, the final result is all the PDF fields empty except the last PDF. Only the last PDF have their fields filled. We have a template with the empty PDF and the fields are already specified with the name. In such a way that you only take the template and write the data searching by name. For example, this is how I write the data in the fields for each PDF: ``` PdfReader pdfReader = new PdfReader(TemplatePdfRoute); \/\/the pdf template with their fields and name to acces to each one MemoryStream ms = new MemoryStream(); PdfStamper pdfStamper = new PdfStamper(pdfReader, ms); AcroFields pdfFormFields = pdfStamper.AcroFields; BaseFont baseFont = BaseFont.CreateFont(BaseFont.HELVETICA, BaseFont.WINANSI, BaseFont.EMBEDDED); IDictionary<string, AcroFields.Item> fields = pdfFormFields.Fields; foreach (KeyValuePair<string, AcroFields.Item> kvp in fields) { pdfFormFields.SetFieldProperty(kvp.Key, \"textfont\", baseFont, null); pdfFormFields.SetFieldProperty(kvp.Key, \"textcolor\", BaseColor.BLACK, null); pdfFormFields.SetFieldProperty(kvp.Key, \"textsize\", 10f, null); } pdfFormFields.SetField(\"Document\", documentData); pdfFormFields.SetField(\"Name\", Namedata); pdfFormFields.SetField(\"Age\", Agedata); ``` This is the code that try to join the PDFs: ``` Document document = new Document(); using (FileStream newFileStream = new FileStream(TempRute + \"joined.pdf\", FileMode.Create)) { PdfCopy writer = new PdfCopy(document, newFileStream); document.Open(); foreach (var pdf in listofPDF) { PdfReader reader = new PdfReader(pdf); reader.ConsolidateNamedDestinations(); for (int j = 1; j <= reader.NumberOfPages; j++) { PdfImportedPage page = writer.GetImportedPage(reader, j); writer.AddPage(page); } PRAcroForm form = reader.AcroForm; if (form != null) { writer.CopyAcroForm(reader); } reader.Close(); } writer.Close(); document.Close(); } ``` As I said, that code join the PDF but all empty except the last page. How can I join then correctly?",
    "author_id":5340,
    "publication_date":1754395190000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Keras-JOB",
    "author_reputation":125.0,
    "tags":"c#, asp.net-mvc, pdf, pdf-generation, itext",
    "text_length":2231,
    "title_length":63,
    "num_tags":5
  },
  {
    "id":5808,
    "title":"How to verify button click in Selenium with proper exception handling using TestNG?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725934\/how-to-verify-button-click-in-selenium-with-proper-exception-handling-using-test",
    "text":"I am working on a Selenium automation project using Java + TestNG. I want to verify that a button click works as expected, and I need to ensure: The button click is attempted with proper try-catch error handling. The test is structured using TestNG annotations like @BeforeClass, @Test, @AfterClass. I want an industry-standard way to confirm that the button was clicked. ``` public class ButtonClickTest { WebDriver driver; @BeforeClass public void setup() { driver = new ChromeDriver(); driver.get(\"https:\/\/example.com\"); } @Test public void testButtonClick() { try { WebElement button = driver.findElement(By.id(\"submitBtn\")); button.click(); ***\/\/ Not sure how to verify it was actually clicked*** } catch (Exception e) { e.printStackTrace(); } } @AfterClass public void teardown() { driver.quit(); } } ``` This clicks the button, but I am unsure how to verify that the click actually worked.",
    "author_id":5339,
    "publication_date":1754395216000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Elegant Student",
    "author_reputation":305.0,
    "tags":"selenium-webdriver, testng",
    "text_length":896,
    "title_length":83,
    "num_tags":2
  },
  {
    "id":5807,
    "title":"How to call an API at exactly 00 and 30 minutes every hour?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725937\/how-to-call-an-api-at-exactly-00-and-30-minutes-every-hour",
    "text":"I am trying to implement a repeating API call in my iOS app (Swift), and I want the call to happen at exactly 00 and 30 minutes of every hour (e.g., 1:00, 1:30, 2:00, etc.). On launch (when app is in foreground), I want to make the first API call to check if a slot is available. Then, I want to schedule the API to run at every hour and half-hour mark. This should be accurate to the clock (not just every 1800 seconds), so the API should fire exactly at hh:00 and hh:30. Using this approach, the API is called 30–40 times per minute within a single view controller. The API call is triggered from a custom navigation bar (ENavigationBar), which is subclass of UIView. ``` func scheduleFunctionCall() { if UIApplication.shared.applicationState == .background { return } \/\/ We check the doctor appointment the first time to see if it's available. self.drTimer?.invalidate() self.drTimer = nil if !GlobalConstant.appCONSTANT.isDrAppointmentCheckedFirstTime { checkDrAppointmentAvailable() } else { if GlobalConstant.appCONSTANT.roomName != \"\" { showNeedHelpDrButton() } else { hideNeedHelpDrView() } } \/\/ Get the current time let calendar = Calendar.current let now = Date() let minute = calendar.component(.minute, from: now) let second = calendar.component(.second, from: now) var fireInSeconds: TimeInterval = 0 if minute < 1 || (minute >= 31 && minute < 32) { \/\/ If it's already at :00 or :30, just wait for the next second alignment fireInSeconds = TimeInterval(60 - second) } else if minute < 31 { fireInSeconds = TimeInterval((31 - minute) * 60 - second) } else { fireInSeconds = TimeInterval((61 - minute) * 60 - second) } Timer.scheduledTimer(withTimeInterval: fireInSeconds, repeats: false) { _ in self.checkDrAppointmentAvailable() \/\/ Now set up the repeating timer every 30 minutes self.drTimer = Timer.scheduledTimer(withTimeInterval: 1800, repeats: true) { _ in self.checkDrAppointmentAvailable() let nextMinute = minute < 31 ? \"00\" : \"30\" print(\"Function called at \\(calendar.component(.hour, from: Date())):\\(nextMinute)\") } } } ```",
    "author_id":5338,
    "publication_date":1754395386000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Niki Mehta",
    "author_reputation":19.0,
    "tags":"ios, swift",
    "text_length":2047,
    "title_length":59,
    "num_tags":2
  },
  {
    "id":5806,
    "title":"SageMaker PyTorch MME ignores entry_point and falls back to default handler, causing ModelLoadError",
    "link":"https:\/\/stackoverflow.com\/questions\/79725939\/sagemaker-pytorch-mme-ignores-entry-point-and-falls-back-to-default-handler-cau",
    "text":"I'm trying to deploy a custom PyTorch model to a SageMaker Multi-Model Endpoint (MME). My model is saved as a state_dict using torch.save(), so it requires a custom inference.py script to load the model architecture before loading the weights. The problem is that the endpoint fails to load any model. The CloudWatch logs show that SageMaker is ignoring my custom inference.py and falling back to its default handler. This default handler expects a TorchScript model, which mine is not, leading to a ModelLoadError (\"Please ensure model is saved using torchscript\"). This happens even when I replace my inference.py with a simple \"hello world\" script. This makes me believe the issue is in the deployment configuration, not the inference code itself. My Setup: Models: My model artifacts (e.g., model_A.tar.gz, model_B.tar.gz) are located in an S3 prefix. Each .tar.gz file contains only the model weights (model.pt) and other data artifacts (.json, .csv), with no Python code inside. Deployment: I am using the SageMaker Python SDK. File Structure: ``` \/project_root ├── deploy.py └── code\/ ├── __init__.py ├── inference.py ├── model.py └── requirements.txt ``` Deployment Script (deploy.py): ``` import sagemaker from sagemaker.pytorch.model import PyTorchModel from sagemaker.multidatamodel import MultiDataModel import time def main(): # Generic configuration s3_model_repository = \"s3:\/\/my-bucket\/my-mme-models\/\" endpoint_name = f\"my-mme-endpoint-{time.strftime('%Y%m%d-%H%M%S')}\" role_arn = \"arn:aws:iam::123456789012:role\/MySageMakerRole\" instance_type = 'ml.g4dn.xlarge' sagemaker_session = sagemaker.Session() # 1. Define the container and the code to be used for inference pytorch_model_container = PyTorchModel( entry_point='inference.py', source_dir='.\/code', role=role_arn, sagemaker_session=sagemaker_session, model_data=None, # This is critical for MME framework_version='2.0', py_version='py310', ) # 2. Create the MultiDataModel object pointing to the S3 model repository multi_data_model = MultiDataModel( name=f\"my-mme-container-def-{time.strftime('%Y%m%d-%H%M%S')}\", model_data_prefix=s3_model_repository, model=pytorch_model_container ) # 3. Deploy the endpoint try: multi_data_model.deploy( initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name, ) print(f\"Deployment successful. Endpoint Name: {endpoint_name}\") except Exception as e: print(f\"Deployment failed: {e}\") if __name__ == \"__main__\": main() ``` Inference Script (code\/inference.py): ``` # code\/inference.py import logging import traceback logging.basicConfig(level=logging.INFO, format='%(asctime)s - [CUSTOM_HANDLER] - %(levelname)s - %(message)s') logger = logging.getLogger(__name__) try: logger.info(\"Attempting to import MyCustomModel from model.py...\") from model import MyCustomModel logger.info(\"Successfully imported MyCustomModel.\") except Exception as e: logger.critical(f\"CRITICAL IMPORT FAILED: {e}\") raise def initialize(context): \"\"\" Loads the model artifacts and instantiates the model. \"\"\" try: properties = context.system_properties model_dir = properties.get(\"model_dir\") logger.info(f\"--- [initialize] Initializing model from '{model_dir}' ---\") # ... logic to load artifacts and model ... # model = MyCustomModel(...) # model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt'))) logger.info(f\"--- [initialize] Model loaded successfully. ---\") except Exception as e: logger.error(\"!!!!!! CRITICAL ERROR DURING initialize() !!!!!!\") logger.error(f\"Stack Trace:\\n{traceback.format_exc()}\") raise ``` CloudWatch Logs: Regardless of the configuration, I never see my [CUSTOM_HANDLER] log messages. The worker process dies almost instantly, and the logs always show the same error from the SageMaker default handler: Generated code ``` ... [INFO] W-9000-...-stdout MODEL_LOG - Torch worker started. ... [INFO] W-9000-...-stdout MODEL_LOG - Backend worker process died. ... [INFO] W-9000-...-stdout MODEL_LOG - Traceback (most recent call last): ... [INFO] W-9000-...-stdout MODEL_LOG - File \"\/opt\/conda\/lib\/python3.10\/site-packages\/sagemaker_pytorch_serving_container\/default_pytorch_inference_handler.py\", line 80, in default_model_fn ... [INFO] W-9000-...-stdout MODEL_LOG - raise ModelLoadError( sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load \/opt\/ml\/models\/...\/model.pt. Please ensure model is saved using torchscript. ``` Question: What could be causing SageMaker to completely ignore my source_dir and entry_point configuration and fall back to its default handler? I have followed the official examples for PyTorch MME, but my custom inference code is never executed What I tried: I am attempting to deploy a PyTorch Multi-Model Endpoint using the standard PyTorchModel class with a custom entry_point and a source_dir containing my inference code. Initial Attempt: I configured PyTorchModel with entry_point='inference.py', source_dir='.\/code', and model_data=None, which is the standard pattern for MME. Isolation Test: To rule out errors within my script, I replaced my full inference.py with a minimal \"hello world\" script that only contained logging messages and no complex imports or logic. Configuration Variants: I have also tried using the lower-level sagemaker.model.Model class with explicit environment variables like SAGEMAKER_PROGRAM and MMS_DEFAULT_HANDLER to force the container to use my script. What actually resulted: In every attempt, my custom script is completely ignored. I never see any of my custom log messages. The container worker dies almost instantly, and the CloudWatch logs always show the same error from SageMaker's default handler: sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: ... Please ensure model is saved using torchscript. This proves that SageMaker is not executing my code and is falling back to its default model loading mechanism, which is incompatible with my torch.save() model artifacts.",
    "author_id":5337,
    "publication_date":1754395445000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Miguel Oliveira",
    "author_reputation":1.0,
    "tags":"pytorch, amazon-sagemaker, torchserve",
    "text_length":5976,
    "title_length":99,
    "num_tags":3
  },
  {
    "id":5805,
    "title":"How to disable some tooltip dialogs in VSCode",
    "link":"https:\/\/stackoverflow.com\/questions\/79725941\/how-to-disable-some-tooltip-dialogs-in-vscode",
    "text":"This question has been asked before, but none of the answers I have found so far have worked. I'm using VSCode 1.101.2 with Pylance 2025.7.1, but I have had this problem for some time. I want to disable most tooltip hover dialogs, except when I'm debugging. I'd like the attribute values to stay up. In this example, there are actually two dialogs on top of each other. I'd like them both to disappear and preferably the yellow highlighting that has appeared at the end of each line. This dialog is acceptable, as it tells me something about a variable in scope. This is also fine - the same info as the image above, but from the sidebar. Here is the editor part of my current user settings.json and there are no overriding settings in the current repo settings.json. If anyone could even tell me what these dialogs are called, that would help. I've been searching settings for \"hover\", \"delay\", \"dialog\", and \"tooltip\", but I've yet to find the right setting to turn them off. And as stated above, I don't want to turn off ALL dialogs, I would like to keep the attribute values popups that appear when debugging.",
    "author_id":5336,
    "publication_date":1754395514000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"jon_two",
    "author_reputation":1258.0,
    "tags":"python, visual-studio-code, vscode-debugger",
    "text_length":1113,
    "title_length":45,
    "num_tags":3
  },
  {
    "id":5804,
    "title":"How to prevent autocomplete and extra props in exact types?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725942\/how-to-prevent-autocomplete-and-extra-props-in-exact-types",
    "text":"With Typescript, while trying to define an exact object built from a known list of keys, I was unable to restrict two things: Extraneous properties in the exact definition Autocomplete suggesting extraneous properties of type ``` never ``` . This is the code ( TS Playground ): ``` type Items = \"apple\" | \"banana\" | \"cherry\" type ItemsTemplate = { [K in Items]: unknown } type ExactItems<T extends ItemsTemplate> = T & { [K in keyof T]: K extends Items ? T[K] : never } type ItemPriceUnit = ExactItems<{apple: \"unit\", banana: \"unit\", cherry: \"weight\"}>; type ItemPackaging = ExactItems<{apple: \"box\" | \"bag\", banana: \"box\", cherry: \"bag\", orangeJuice: \"bottle\"}>; \/\/ No error with orangeJuice const packaging: ItemPackaging = { \/\/ orangeJu \/\/ <-- Autocomplete suggest \"orangeJuice\" } ``` Is it possible to prevent having extra properties in the restricted type, and therefore avoid them being shown in with autocomplete? It's a little fragile because removing a supported item causes causes type errors on typed values with the removed item, but doesn't produce type errors on the generic types (and autocomplete still suggests the removed type).",
    "author_id":5335,
    "publication_date":1754395531000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"aryzing",
    "author_reputation":5981.0,
    "tags":"typescript",
    "text_length":1146,
    "title_length":59,
    "num_tags":1
  },
  {
    "id":5803,
    "title":"Azure DevOps language metrics endpoint not working",
    "link":"https:\/\/stackoverflow.com\/questions\/79725943\/azure-devops-language-metrics-endpoint-not-working",
    "text":"We have an Azure DevOps server 2022.1, and I am trying to call the endpoint ``` _apis\/projectanalysis\/languagemetrics ``` to understand the main language used in a given repository (I learnt about it here: Extract languages from repo in Azure Devops ). However, it does not seem to work. Here it is the response I am getting: ``` { \"url\": \"https:\/\/my-server\/my-org\/project-uid\/_apis\/projectanalysis\/languagemetrics\", \"resultPhase\": \"preliminary\", \"languageBreakdown\": [], \"repositoryLanguageAnalytics\": [], \"id\": \"project-uid\" } ``` The repository has plenty of files, and has existed for years; so, I am not sure how to understand the \"preliminary\" result phase. I have also tried: ``` GET https:\/\/my-server\/my-org\/_apis\/repositories\/project-uid\/projectanalysis\/languagemetrics ``` but I got a 404. Is there anything that I should do to get the endpoint to work?",
    "author_id":5334,
    "publication_date":1754395541000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Pampa Nello",
    "author_reputation":242.0,
    "tags":"azure-devops, cicd",
    "text_length":863,
    "title_length":50,
    "num_tags":2
  },
  {
    "id":5802,
    "title":"Cannot expire snapshot with retain last properies",
    "link":"https:\/\/stackoverflow.com\/questions\/79725944\/cannot-expire-snapshot-with-retain-last-properies",
    "text":"I have 67 snapshot in a single table but when i use CALL ``` iceberg_catalog.system.expire_snapshots( table => 'iceberg_catalog.default.test_7', retain_last => 5 ); ``` It doesn't delete any snapshot. My table use 2 properties which is : write.metadata.delete-after-commit.enabled true write.metadata.previous-versions-max 5 can someone explain, tks very much",
    "author_id":5333,
    "publication_date":1754395587000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sơn B&#249;i",
    "author_reputation":1.0,
    "tags":"apache-spark, hadoop, apache-iceberg",
    "text_length":359,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":5801,
    "title":"What are the consequences of using Action&lt;T&gt; vs Func&lt;Task&lt;T&gt;&gt; in Subscribe method?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725946\/what-are-the-consequences-of-using-actiont-vs-functaskt-in-subscribe-metho",
    "text":"What is the difference between and consequences of using: ``` IObservable<Trade> bigTrades = ... bigTrades.Subsribe(async t => await bigTradeStore.LogTradeAsync(t)); \/\/async void ``` vs. ``` IAsyncObservable<Trade> bigTrades = ... bigTrades.Subscribe(async t => await bigTradeStore.LogTradeAsync(t)); \/\/async task ``` I'm specifically interested in exception handling. Does it terminate subscription? Is OnError called? Is the exception propagated anywhere?",
    "author_id":5332,
    "publication_date":1754395638000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Liero",
    "author_reputation":27655.0,
    "tags":".net, async-await, system.reactive, asyncrx.net",
    "text_length":457,
    "title_length":100,
    "num_tags":4
  },
  {
    "id":5800,
    "title":"How to change the default executor of Python in VSCode Code Runner? The `executorMap` does not work",
    "link":"https:\/\/stackoverflow.com\/questions\/79725950\/how-to-change-the-default-executor-of-python-in-vscode-code-runner-the-executo",
    "text":"Is it possible to change the default command executed by the play button for Python files? Instead of running ``` conda run ... ``` , I'd like it to directly use ``` <path-to-current-env>\\python.exe ... ``` (as shown in the second image of this post). Background According to the official documentation from VS Code , When you click the Run button, it should directly use the python.exe from the corresponding virtual environment to execute the code. That’s how it used to work on my computer as well. However, today I noticed that the default command in Code Runner (by clicking the Run button) has changed to: I copied the text from this image below: ``` (base) PS C:\\Users\\Shwan\\Desktop\\1> & conda run --live-stream --name mybase python c:\/Users\/Shwan\/Desktop\/1\/1.py ``` ``` usage: conda-script.py [-h] [-v] [--no-plugins] [-V] COMMAND ... ``` ``` conda-script.py: error: argument COMMAND: invalid choice: '' (choose from activate, clean, commands, compare, config, create, deactivate, env, export, info, init, install, list, notices, package, doctor, repoquery, remove, uninstall, rename, run, search, update, upgrade) ``` I have Miniforge installed, and its conda\/mamba commands usually only work properly in their own dedicated terminal window. Unless I set the default terminal to Miniforge's exclusive one, this Run button is basically useless. Even when it does work, I really dislike using ``` conda run ``` - I much prefer switching to the virtual environment first and then directly running ``` python <path>.py ``` , or alternatively executing the ``` python.exe ``` file from the virtual environment like I used to do. This error appears only because the default PowerShell terminal in VS Code cannot execute Conda\/Mamba commands properly. If I change the default terminal in VS Code to the Miniforge Prompt, it works correctly. The command ``` conda run ... ``` no longer leaves this output in the terminal, so I don’t have an image or text to share here. I also tried modifying the ``` executorMap ``` setting for Python, but even after I changed it, clicking the play button still executed the ``` conda run ... ``` command. This makes me suspect the issue isn’t related to Code Runner, though the play button’s behavior is tied to that plugin. Does anyone know how to adjust this? I can't seem to find any relevant settings in the Code Runner plugin's configuration to tweak this behavior. My question appears very similar to How do you change your default Code Runner execution command in VS Code? , however, while their issue was resolved by modifying the ``` executorMap ``` setting, this solution didn't work in my case. It seems Code Runner's behavior is being overridden by Miniforge?",
    "author_id":5331,
    "publication_date":1754396077000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Zhang Xuan",
    "author_reputation":63.0,
    "tags":"python, visual-studio-code, coderunner",
    "text_length":2708,
    "title_length":99,
    "num_tags":3
  },
  {
    "id":5799,
    "title":"How can I render a custom legend?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725952\/how-can-i-render-a-custom-legend",
    "text":"I am trying to render a custom legend for a React project. I am using React-ChartJS2. I have 2 legends, both are rectangular which I want to keep. However, one dataset is simply a single point used to represent a \"goal weight\". I want this dataset to have a circular dot Legend, as it always has a single data point in the entire set. Despite reading the documentation I cannot seem to mix normal legends with a single customized one. This is the best I have so far: ``` import { Line } from 'react-chartjs-2'; import { Chart, LineElement, PointElement, LinearScale, CategoryScale, Legend, Tooltip, } from 'chart.js'; \/\/ Register required components Chart.register(LineElement, PointElement, LinearScale, CategoryScale, Legend, Tooltip); \/\/ Plugin to change \"Goal Weight\" legend item to a circle const goalWeightLegendPlugin = { id: 'goalWeightLegendPlugin', beforeInit(chart) { const original = chart.options.plugins.legend.labels.generateLabels; chart.options.plugins.legend.labels.generateLabels = function (chartInstance) { const labels = original(chartInstance); return labels.map((label) => label.text === 'Goal Weight' ? { ...label, usePointStyle: true, pointStyle: 'circle' } : { ...label, usePointStyle: false }, ); }; }, }; Chart.register(goalWeightLegendPlugin); const options = { responsive: true, plugins: { legend: { display: true, labels: { boxWidth: 30, boxHeight: 12, \/\/ usePointStyle: false \/\/ Don't enable globally }, }, }, }; const data = { labels: ['A', 'B', 'C'], datasets: [ { label: 'User Weight', data: [65, 66, 67], borderColor: 'blue', backgroundColor: 'lightblue', }, { label: 'Goal Prediction', data: [68, 68, 68], borderColor: 'gray', backgroundColor: 'lightgray', }, { label: 'Goal Weight', data: [70, null, null], borderColor: 'green', backgroundColor: 'green', pointStyle: 'circle', pointRadius: 6, showLine: false, }, ], }; function WeightTrackingLineGraph() { return <Line options={options} data={data} \/>; } export default WeightTrackingLineGraph; ``` How can I make this a green circle dot for the \"Goal Weight\" Legend?",
    "author_id":5330,
    "publication_date":1754396185000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"memelord23",
    "author_reputation":134.0,
    "tags":"react-chartjs-2",
    "text_length":2056,
    "title_length":33,
    "num_tags":1
  },
  {
    "id":5798,
    "title":"Snapshot for visual testing is not capturing entire component",
    "link":"https:\/\/stackoverflow.com\/questions\/79725953\/snapshot-for-visual-testing-is-not-capturing-entire-component",
    "text":"I am using ``` vitest ``` with ``` vitest-plugin-vis ``` and ``` playwright ``` to perform visual testing in my Vite project. The issue I am facing is that the snapshots that are generated are only capturing part of my components. Here is an example that tests my error component. As you can see, it is not very helpful as most of the error component is outside of the captured image. Is there a way to make sure the entire component is captured and\/or that you can set the width of the viewport? Here is my vitest.config.js: ``` import defineConfig from \".\/vitest.base.config\"; import { mergeConfig } from \"vitest\/config\"; import { resolve } from \"node:path\"; import { vis } from 'vitest-plugin-vis\/config'; const vitestBaseConfig = await defineConfig; export default mergeConfig(vitestBaseConfig, { plugins: [vis( { preset: 'auto' } )], test: { exclude: [ \"**\/__tests__\/*\"], browser: { provider: 'playwright', enabled: true, headless: true, instances: [ { browser: 'chromium' } ], }, mockReset: true, setupFiles: [\".\/src\/test-setup.ts\"], alias: {} }, resolve: { alias: { \"@mendix\/extensions-api\": resolve(__dirname, \".\/__mocks__\/@mendix\/extensions-api.ts\") } } }); ``` And here is my test: ``` import { afterEach, beforeEach, describe, expect, test, vi } from \"vitest\"; import { render } from \"vitest-browser-react\"; import ErrorBoundary from \"..\/ErrorBoundary\"; import { screen } from \"@testing-library\/react\"; import { setAutoSnapshotOptions } from \"vitest-plugin-vis\"; describe(\"ErrorBoundary\", () => { let consoleErrorSpy: ReturnType<typeof vi.spyOn>; beforeEach(() => { consoleErrorSpy = vi.spyOn(console, \"trace\").mockImplementation(() => {}); }); afterEach(() => { consoleErrorSpy.mockRestore(); }); test(\"renders outer error boundary\", () => { setAutoSnapshotOptions({ enable: true }); renderOuterErrorBoundary(); const errorBoundary = screen.getByRole(\"alert\"); expect(errorBoundary).toMatchImageSnapshot(); }); }); const renderOuterErrorBoundary = () => render( <ErrorBoundary isOuterErrorBoundary={true}> <ErrorThrowingComponent \/> <\/ErrorBoundary> ); const ErrorThrowingComponent = () => { const error = new Error(\"Oops\"); error.stack = \"Test stack\"; \/\/ Stack can contain random ids. Therefore, we overwrite it with something constant, so it is constant during testing error.cause = \"Test cause\"; \/\/ Cause can contain random ids. Therefore, we overwrite it with something constant, so it is constant during testing throw error; }; ```",
    "author_id":5329,
    "publication_date":1754396202000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"MWB",
    "author_reputation":1889.0,
    "tags":"reactjs, playwright, automated-tests, vitest",
    "text_length":2448,
    "title_length":61,
    "num_tags":4
  },
  {
    "id":5797,
    "title":"How can I draw a dotted underline and a solid strikethrough over some text?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725955\/how-can-i-draw-a-dotted-underline-and-a-solid-strikethrough-over-some-text",
    "text":"I've got text in a Flutter app that needs to have both a dotted underline and a solid strikethrough effect applied. I can do them separately, but the ``` decorationStyle ``` attribute applies to both decorations. I could draw a box around the text, but the border only supports solid styles. And that seems awkward. Is there a simpler way? Example text:",
    "author_id":5328,
    "publication_date":1754396273000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Chris B.",
    "author_reputation":90839.0,
    "tags":"flutter, dart",
    "text_length":353,
    "title_length":75,
    "num_tags":2
  },
  {
    "id":5796,
    "title":"Spring repository instantiation in an OSGI bundle?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725960\/spring-repository-instantiation-in-an-osgi-bundle",
    "text":"I am trying to convert a spring-based code to OSGI bundle. My activator class looks like this: ``` package io.github.magwas.inez.osgi; import java.util.Dictionary; import java.util.Hashtable; import org.osgi.framework.BundleActivator; import org.osgi.framework.BundleContext; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.ConfigurableApplicationContext; import org.springframework.context.annotation.AnnotationConfigApplicationContext; import org.springframework.context.annotation.ComponentScan; import org.springframework.data.map.repository.config.EnableMapRepositories; import org.springframework.util.Assert; @SpringBootApplication @EnableMapRepositories @ComponentScan(basePackages = { \"io.github.magwas\" }) public class SpringBootBundleActivator implements BundleActivator { ConfigurableApplicationContext appContext; @Autowired BridiElementService bridiElementService; @Override public void start(BundleContext bundleContext) { Thread.currentThread() .setContextClassLoader(this.getClass().getClassLoader()); appContext = new AnnotationConfigApplicationContext( SpringBootBundleActivator.class); Dictionary<String, String> props = new Hashtable<String, String>(); bridiElementService.initialize(); bundleContext.registerService(BridiElementService.class, bridiElementService, props); System.err.println(\"registered service\"); } @Override public void stop(BundleContext bundleContext) { SpringApplication.exit(appContext, () -> 0); } public static void main(String[] args) { SpringApplication.run(SpringBootBundleActivator.class); } } ``` And I got this error message for the line which tries to establish the Spring app context (inserted line breaks for easier read): ``` Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'springBootBundleActivator': Unsatisfied dependency expressed through field 'bridiElementService': Error creating bean with name 'bridiElementService': Unsatisfied dependency expressed through field 'bridiElementSystemInitialization': Error creating bean with name 'bridiElementSystemInitializationService': Unsatisfied dependency expressed through field 'inez': Error creating bean with name 'inezImpl': Unsatisfied dependency expressed through field 'queryProcessor': Error creating bean with name 'queryProcessorService': Unsatisfied dependency expressed through field 'findAllByRepresentation': Error creating bean with name 'findAllByRepresentationService': Unsatisfied dependency expressed through field 'sumtiRepository': No qualifying bean of type 'io.github.magwas.inez.storage.repository.SumtiRepository' available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)} ``` So it seems all services are resolved, but the repository is not. SumtiRepository looks like this: ``` package io.github.magwas.inez.storage.repository; import java.util.Set; import org.springframework.data.repository.CrudRepository; import org.springframework.stereotype.Repository; import io.github.magwas.inez.storage.model.Sumti; @Repository public interface SumtiRepository extends CrudRepository<Sumti, String> { Set<Sumti> findAllByRepresentation(String representation); } ``` As I understand, the ``` @EnableMapRepositories ``` annotation should have taken care of this in a non-osgi environment, but somehow the code which creates an instance based on the interface does not run now. What am I missing? my MANIFEST.MF: ``` Manifest-Version: 1.0 Bundle-ManifestVersion: 2 Main-Class: io.github.magwas.inez.Main Bundle-Name: inez-model Bundle-SymbolicName: inez-model;singleton:=true Spring-Context: *;create-asynchronously:=false Bundle-Activator: io.github.magwas.inez.osgi.SpringBootBundleActivator Bundle-Version: 0.4.1 Bundle-Vendor: Árpád Magosányi Export-Package: io.github.magwas.inez, io.github.magwas.inez.element Require-Bundle: org.eclipse.core.runtime;bundle-version=\"3.33.0\" Bundle-ClassPath: bin\/, target\/dependency\/antlr4-runtime-4.13.2.jar, target\/dependency\/apiguardian-api-1.1.2.jar, target\/dependency\/asm-9.8.jar, target\/dependency\/byte-buddy-1.17.5.jar, target\/dependency\/byte-buddy-agent-1.17.5.jar, target\/dependency\/checker-qual-3.49.3.jar, target\/dependency\/commons-codec-1.15.jar, target\/dependency\/commons-lang3-3.17.0.jar, target\/dependency\/error_prone_annotations-2.38.0.jar, target\/dependency\/gson-2.13.1.jar, target\/dependency\/inez-parser-0.4.1.jar, target\/dependency\/inez-parser-0.4.1-testing.jar, target\/dependency\/javax.annotation-api-1.3.2.jar, target\/dependency\/jul-to-slf4j-1.7.36.jar, target\/dependency\/konveyor-base-0.2.7-annotations.jar, target\/dependency\/micrometer-commons-1.14.9.jar, target\/dependency\/micrometer-observation-1.14.9.jar, target\/dependency\/nice-xml-messages-3.1.jar, target\/dependency\/objenesis-3.3.jar, target\/dependency\/opentest4j-1.3.0.jar, target\/dependency\/pcollections-4.0.2.jar, target\/dependency\/pmd-core-7.14.0.jar, target\/dependency\/pmd-java-7.14.0.jar, target\/dependency\/Saxon-HE-12.5.jar, target\/dependency\/slf4j-api-2.0.2.jar, target\/dependency\/spring-aop-6.2.9.jar, target\/dependency\/spring-beans-6.2.9.jar, target\/dependency\/spring-boot-3.5.3.jar, target\/dependency\/spring-context-6.2.9.jar, target\/dependency\/spring-core-6.2.9.jar, target\/dependency\/spring-data-commons-3.5.2.jar, target\/dependency\/spring-data-keyvalue-3.5.1.jar, target\/dependency\/spring-expression-6.2.9.jar, target\/dependency\/spring-jcl-6.2.9.jar, target\/dependency\/spring-test-6.2.9.jar, target\/dependency\/spring-tx-6.2.8.jar, target\/dependency\/xmlresolver-5.2.2.jar, target\/dependency\/xmlresolver-5.2.2-data.jar, target\/dependency\/spring-boot-autoconfigure-3.5.3.jar, target\/dependency\/konveyor-base-0.2.7-runtime.jar, target\/dependency\/mockito-core-5.18.0.jar, target\/dependency\/junit-jupiter-api-5.12.1.jar, target\/dependency\/junit-jupiter-engine-5.12.1.jar, target\/dependency\/junit-platform-commons-1.12.1.jar, target\/dependency\/junit-platform-engine-1.12.1.jar, target\/dependency\/junit-platform-launcher-1.12.1.jar, target\/dependency\/konveyor-base-0.2.7-testing.jar, target\/dependency\/httpclient5-5.1.3.jar, target\/dependency\/httpcore5-5.1.3.jar, target\/dependency\/httpcore5-h2-5.1.3.jar, target\/dependency\/konveyor-base-tooling.jar, target\/dependency\/org.eclipse.core.contenttype-3.9.600.v20241001-1711.jar, target\/dependency\/org.eclipse.core.jobs-3.15.500.v20250204-0817.jar, target\/dependency\/org.eclipse.core.runtime-3.33.0.v20250206-0919.jar, target\/dependency\/org.eclipse.equinox.app-1.7.300.v20250130-0528.jar, target\/dependency\/org.eclipse.equinox.common-3.20.0.v20250129-1348.jar, target\/dependency\/org.eclipse.equinox.preferences-3.11.300.v20250130-0533.jar, target\/dependency\/org.eclipse.equinox.registry-3.12.300.v20250129-1129.jar, target\/dependency\/org.eclipse.osgi-3.23.0.v20250228-0640.jar, target\/dependency\/org.osgi.core-6.0.0.jar, target\/dependency\/org.osgi.service.prefs-1.1.2.jar, target\/dependency\/osgi.annotation-8.0.1.jar Class-Path: bin\/ Bundle-RequiredExecutionEnvironment: JavaSE-21 Automatic-Module-Name: io.github.magwas.inez.model ```",
    "author_id":5327,
    "publication_date":1754396431000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"&#193;rp&#225;d Magos&#225;nyi",
    "author_reputation":1476.0,
    "tags":"eclipse, spring-boot, osgi, spring-data-keyvalue",
    "text_length":7271,
    "title_length":50,
    "num_tags":4
  },
  {
    "id":5795,
    "title":"Rate limits to follow users",
    "link":"https:\/\/stackoverflow.com\/questions\/79725967\/rate-limits-to-follow-users",
    "text":"I'm getting conflicting information for X API. It clearly says you can follow someone every 15 minutes in the documentation : Endpoint Pro Limit Basic Limit Free Limit POST \/2\/users\/:id\/following ... ... 1 requests \/ 15 mins PER USER I assume that's wrong as I'm getting too many requests no matter what.",
    "author_id":5326,
    "publication_date":1754396712000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user237462",
    "author_reputation":421.0,
    "tags":"twitter",
    "text_length":304,
    "title_length":27,
    "num_tags":1
  },
  {
    "id":5794,
    "title":"How do I programmatically launch an application on a remote machine with PowerShell?",
    "link":"https:\/\/stackoverflow.com\/questions\/79725971\/how-do-i-programmatically-launch-an-application-on-a-remote-machine-with-powersh",
    "text":"The following code will launch ``` notepad.exe ``` which can be seen in the processes. But it does not launch the notepad on the screen. ``` $wmi = [WMIClass]\"\\\\REMOTE_MACHINE_NAME\\root\\cimv2:Win32_Process\" $result = $wmi.Create(\"notepad.exe\", $null, $null) ``` The following code also launches the notepad on the remote machine for a specified duration but it is not visible. ``` Invoke-Command -ComputerName \"REMOTE_MACHINE\" -Credential $cred -ScriptBlock { Start-Process \"C:\\Windows\\System32\\Notepad.exe\" Start-Sleep -Seconds 15 Get-Process -Name \"Notepad\" #-ErrorAction SilentlyContinue Get-Process | Where-Object {$_.Name -Like 'Note*'} } ``` How to achieve this? I do not want to use PSEXEC for security reasons.",
    "author_id":5325,
    "publication_date":1754396994000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Abhimanyu",
    "author_reputation":85.0,
    "tags":"powershell, powershell-remoting",
    "text_length":718,
    "title_length":84,
    "num_tags":2
  },
  {
    "id":5793,
    "title":"Quarkus test reactive failure",
    "link":"https:\/\/stackoverflow.com\/questions\/79725974\/quarkus-test-reactive-failure",
    "text":"I have the following resource: ``` @Path(\"\/config\") @RequiredArgsConstructor public class Resource { private final Service service; @PUT @Consumes(MediaType.APPLICATION_JSON) @Produces(MediaType.APPLICATION_JSON) public Uni<Response> putConfig(List<Config> configs) { return service.updateConfig(configs) .map(ignored -> Response.noContent().build()) .onFailure().recoverWithItem(ignored -> Response.serverError().build()); } } ``` and the corresponding tests: ``` @QuarkusTest class ResourceTest { @InjectMock Service service; @Test void putConfigOk() { final var request = mockConfig(); when(service.updateConfig(request)).thenReturn(Uni.createFrom().voidItem()); given() .body(request) .contentType(MediaType.APPLICATION_JSON) .when().put(\"\/config\") .then() .statusCode(204); } @Test void putConfigFails() { final var request = mockConfig(); when(service.updateConfig(request)).thenReturn(Uni.createFrom().failure(RuntimeException::new)); given() .body(request) .contentType(MediaType.APPLICATION_JSON) .when().put(\"\/config\") .then() .statusCode(500); } private List<Config> mockConfig() { return List.of(new Config()); } } ``` First test runs perfectly, returns 204. However, second test still returns 204. I don't know what I'm doing wrong. In fact, running test with coverage shows failure part is never covered... Any ideas?",
    "author_id":5324,
    "publication_date":1754397053000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"thmasker",
    "author_reputation":660.0,
    "tags":"java, rest-assured, quarkus, reactive",
    "text_length":1331,
    "title_length":29,
    "num_tags":4
  },
  {
    "id":5792,
    "title":"Group multiple checkbox responses to a single question",
    "link":"https:\/\/stackoverflow.com\/questions\/79725980\/group-multiple-checkbox-responses-to-a-single-question",
    "text":"I am able to extract individual checkbox value using Azure Document Intelligence: Is there any way I can define a field (e.g. Languages) and get selected checkbox values (e.g. in this case English and Latin)? My form contains multiple questions in this format so I would like to directly assign values for them to a key instead of manually extracting each data points. Example of current form:",
    "author_id":5323,
    "publication_date":1754397149000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"developer",
    "author_reputation":1621.0,
    "tags":"azure-form-recognizer, azure-document-intelligence",
    "text_length":393,
    "title_length":54,
    "num_tags":2
  },
  {
    "id":5791,
    "title":"Creating new empty branch for major refactor",
    "link":"https:\/\/stackoverflow.com\/questions\/79725981\/creating-new-empty-branch-for-major-refactor",
    "text":"I have a Git repository with a Python project that performs some data transformation. This repo also includes files that I don't want Git to track, like: files where I just want to try out stuff, like Jupyter notebooks logs and other by-products of the pipeline These files are included in the ``` .gitignore ``` patterns in the ``` dev ``` branch. The thing is, I want to do a major rewrite of this project, taking what I like and changing what I don't. My idea is to create a new branch from scratch, called ``` refactor ``` . This branch should be totally empty. However when I check out again to the ``` dev ``` branch, I want all of these untracked files to remain there. I've tried doing ``` git checkout --orphan refactor ``` with dummy small repos but the untracked files (e.g. ``` workspace.ipynb ``` ) remain in the ``` refactor ``` branch. So: Is it a good idea to start a brand new branch in the same repo if I want to do a major rewrite? If so, how can I do it?",
    "author_id":5322,
    "publication_date":1754397240000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"zest16",
    "author_reputation":679.0,
    "tags":"git",
    "text_length":974,
    "title_length":44,
    "num_tags":1
  },
  {
    "id":5790,
    "title":"Spin inside ros2 lifecycle node as a separate thread",
    "link":"https:\/\/stackoverflow.com\/questions\/79726007\/spin-inside-ros2-lifecycle-node-as-a-separate-thread",
    "text":"I’m working inside a ``` rclcpp_lifecycle::LifecycleNode ``` and I’d like to spin up a background thread for loop‐closure detection: ``` loop_thread_ = std::thread([this]() { LoopDetection(); }); ``` However, the “spin_some” approach below is giving me compile\/run errors. ``` what(): Node '\/session_map_optimizer' has already been added to an executor. [ERROR] [component_container_mt-1]: process has died [pid 78859, exit code -6, cmd '\/opt\/ros\/jazzy\/lib\/rclcpp_components\/component_container_mt --ros-args --log-level ``` Is there another way to do this? Option 1: Spin on the node’s base interface ``` void NodeV::LoopDetection() { \/\/ Run at loopClosureFrequency Hz rclcpp::Rate rate(loopClosureFrequency); while (!stopRequested.load() && rclcpp::ok()) { \/\/ Spin any ready callbacks for this node rclcpp::spin_some(this->get_node_base_interface()); PerformRSLoopClosure(); rate.sleep(); } } ``` Option 2: Don’t spin at all in the loop thread ``` void NodeV::LoopDetection() { \/\/ Run at loopClosureFrequency Hz rclcpp::Rate rate(loopClosureFrequency); while (!stopRequested.load() && rclcpp::ok()) { PerformRSLoopClosure(); rate.sleep(); } } ```",
    "author_id":5321,
    "publication_date":1754398538000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"GPrathap",
    "author_reputation":7860.0,
    "tags":"c++, multithreading, c++17, ros2",
    "text_length":1148,
    "title_length":52,
    "num_tags":4
  },
  {
    "id":5789,
    "title":"Combining Windows&#39; GetMessage Event Loop with a UI Animation Loop for Low CPU Usage",
    "link":"https:\/\/stackoverflow.com\/questions\/79726008\/combining-windows-getmessage-event-loop-with-a-ui-animation-loop-for-low-cpu-us",
    "text":"I am developing an app on Windows using the WinAPI. This app has a UI built using my own framework. Some views of this UI are animated and need to be refreshed over time. Prior to implementing animated views, I was simply doing the following: ``` while (GetMessage(&msg, NULL, 0, 0)) { \/\/ if (msg.message == WM_QUIT) { \/\/ break; \/\/ } TranslateMessage(&msg); DispatchMessage(&msg); } ``` I’ve been using ``` GetMessage ``` instead of ``` PeekMessage ``` primarily for CPU resource savings. Redrawing of the UI and its elements is done in the ``` WndProc ``` function. When I receive, say, a ``` WM_MOUSEMOVE ``` event, I update the UI and trigger a redraw (all GPU-based). Now, with animated views, I need something like the following (pseudo-code): ``` for (;;) { for (animation in app->timeline) { animation->tick(current_time, ...); } } ``` In this naive version, I have to iterate over all animations in the timeline to ensure they run, even if the timeline contains no active animations. This keeps the main thread running continuously, even when there’s no animation, which is not ideal. Consequently, I’d need to switch to using ``` PeekMessage ``` : ``` while (!done) { for (animation in app->timeline) { animation->tick(current_time, ...); } if (PeekMessage(&msg, nullptr, 0, 0, PM_REMOVE)) { if (msg.message == WM_QUIT) { done = true; } else { TranslateMessage(&msg); DispatchMessage(&msg); } } } ``` If I want to keep using ``` GetMessage ``` , what’s a good way to handle UI animations? Should I post a custom message like ``` MSG_START_ANIMATION ``` when an animation starts, which I would then process in ``` WndProc ``` ? This could trigger a timer (via ``` SetTimer ``` ) that would generate a ``` WM_TIMER ``` message every 1\/60th of a second. Or is it better to run the UI animation in a separate thread and implement a similar scheduling system to ``` GetMessage ``` , where the thread sleeps when there’s no work to do? I should just stick to ``` PeekMessage ``` and process the UI animation event on every iteration and not care about CPU usage? Make my own queue messaging system like Chromium does which would include both my animation UI events + the native Windows messages? I’m curious how others have approached this in the past and whether there are alternatives to the two methods I’m considering. Edit: A special kudos to the person who replied to that post with a lengthy, detailed explanation. I thought I’d provide an update on the approach I’ve chosen, which is similar to the Chromium approach. The idea is to move the infinite loop into a ``` DoRunLoop ``` function (the function name isn't particularly important). Essentially, it’s a ``` for (;;) ``` loop. Inside this loop, we do three main things: Process native events if any are pending ( ``` WM_ ``` events). Process delegate tasks — the delegate essentially represents your app and what it needs to do beyond just responding to Windows events. Finally, it calls a ``` WaitForWork ``` method. This is where the thread sleeps using ``` MsgWaitForMultipleObjectsEx ``` . That function can wake the thread when it receives a native event, when an event handler is triggered, or after a specified delay. Your delegate can define what that delay is, based on what animation it needs to render and when it needs to render it. It’s a bit intricate, but it seems logical, flexible, and reasonably robust.",
    "author_id":5320,
    "publication_date":1754398547000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user18490",
    "author_reputation":3893.0,
    "tags":"winapi, events, design-patterns",
    "text_length":3388,
    "title_length":87,
    "num_tags":3
  },
  {
    "id":5788,
    "title":"Could there be any reason because of which retrofit adds ellipsize characters in request body",
    "link":"https:\/\/stackoverflow.com\/questions\/79726010\/could-there-be-any-reason-because-of-which-retrofit-adds-ellipsize-characters-in",
    "text":"I am facing this issue in 1 out of 1000 api calls. I checked the size of data it was only 8KB. For one of the request even for 125Kb data, whole request body was being sent to backend, so I am sure its not the issue of data size. As it is an event api, so its a critical pipeline for us. Has anyone faced this issue till now.",
    "author_id":5319,
    "publication_date":1754398597000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Md Anas Shikoh",
    "author_reputation":1.0,
    "tags":"android, kotlin, retrofit",
    "text_length":325,
    "title_length":93,
    "num_tags":3
  },
  {
    "id":5787,
    "title":"Console not connecting to Cluster",
    "link":"https:\/\/stackoverflow.com\/questions\/79726012\/console-not-connecting-to-cluster",
    "text":"I'm operating a Hazelcast v5.5 cluster running on OpenShift as a StatefulSet. Alongside, a Mancenter is deployed and connected to the cluster. Stats are displayed, it is possible to clear Maps, etc, all good. But the integrated console is unable to connect to the cluster, it will not go past this: ``` Hazelcast CLC MC-v5.5.2 (c) 2024 Hazelcast Inc. * Type 'help' for help information. Prefix non-SQL commands with \\ MC-CLC> \\multimap key-set --verbose Connecting to the cluster (Ctrl+C to cancel) .. ``` I followed https:\/\/docs.hazelcast.com\/management-center\/5.5\/tools\/console and understand this is not a production feature. No logs, no errors. What is missing to make this work? This is the config of the cluster: ``` <hazelcast> <cluster-name>cluster<\/cluster-name> <properties> <property name=\"hazelcast.discovery.enabled\">true<\/property> <property name=\"hazelcast.socket.bind.any\">false<\/property> <\/properties> <network> <port port-count=\"1\" auto-increment=\"false\">5701<\/port> <reuse-address>true<\/reuse-address> <join> <multicast enabled=\"false\"\/> <kubernetes enabled=\"true\"> <service-dns>some-discovery.svc.cluster.local<\/service-dns> <\/kubernetes> <\/join> <rest-api enabled=\"true\"> <endpoint-group name=\"HEALTH_CHECK\" enabled=\"true\"\/> <\/rest-api> <\/network> <management-center console-enabled=\"true\" \/> <\/hazelcast> ```",
    "author_id":5318,
    "publication_date":1754398624000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"saimonsez",
    "author_reputation":344.0,
    "tags":"hazelcast",
    "text_length":1331,
    "title_length":33,
    "num_tags":1
  },
  {
    "id":5786,
    "title":"Will targetSDK 35 and compileSDK34 meet new Google Play requirements?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726014\/will-targetsdk-35-and-compilesdk34-meet-new-google-play-requirements",
    "text":"I am not able build my old Flutter app when compileSDK is set to 35. However, I am still able to build and run when targetSDK is set to 35, but compileSDK is set to 34. Will Google Play accept this build? TargetSDK meets requirements and compileSDK is not mentioned anywhere. From other side as far as I know, I should not use compileSDK < targetSDK, but that's not directly forbidden. In general, is it enough to bump targetSDK to meet new requirements, or I will be forced to bump compileSDK too?",
    "author_id":5317,
    "publication_date":1754398684000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"user1209216",
    "author_reputation":8024.0,
    "tags":"android, flutter, google-play, target-sdk",
    "text_length":498,
    "title_length":69,
    "num_tags":4
  },
  {
    "id":5785,
    "title":"ChatHistory strategy in multi-turn chatbots with Semantic Kernel : Full history vs. reducer",
    "link":"https:\/\/stackoverflow.com\/questions\/79726019\/chathistory-strategy-in-multi-turn-chatbots-with-semantic-kernel-full-history",
    "text":"I'm working with Semantic Kernel (C#) to build a multi-turn conversational chatbot using OpenAI function calling and tool integrations (GPT4o). Naturally, this framework relies heavily on function calls (tools) which are logged as part of the chat history. Here’s the dilemma I’m facing: As the conversation progresses, the chat history keeps growing — including user messages, assistant replies, and tool outputs. This leads to a significant token cost for each subsequent prompt. To reduce cost and token usage, Semantic Kernel provides a history reducer strategy, like ChatHistorySummarizationReducer, to summarize or truncate older parts of the conversation. However, by using summarization or truncation, I notice the quality of responses tends to degrade over time, especially when tool calls like RAG or function results are compressed into vague summaries. The bot sometimes forgets details or repeats itself, possibly due to lost context. This raises a fundamental question: What is the recommended strategy for handling history in a production-grade Semantic Kernel chatbot? Is it common to store only user\/assistant messages, or also tool\/function outputs? Should I always use summarization, even if it slightly reduces the fidelity of context? Are there any patterns or guidelines from Microsoft or the community to balance cost vs. quality? I find it hard to believe that real-world SK-based bots simply pass the entire chat history to the LLM at each turn — that would be very costly. Looking for practical insights from those who have deployed Semantic Kernel chatbots at scale. Thanks!",
    "author_id":5316,
    "publication_date":1754398871000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Marc Alves",
    "author_reputation":51.0,
    "tags":"artificial-intelligence, large-language-model, chatbot, agent, semantic-kernel",
    "text_length":1601,
    "title_length":91,
    "num_tags":5
  },
  {
    "id":5784,
    "title":"React Testing Library : TypeError: Cannot read properties of undefined (reading &#39;enhanceEndpoints&#39;)",
    "link":"https:\/\/stackoverflow.com\/questions\/79726020\/react-testing-library-typeerror-cannot-read-properties-of-undefined-reading",
    "text":"I am using redux toolkit query and trying to run yarn test and after that i got the message below: ``` TypeError: Cannot read properties of undefined (reading 'enhanceEndpoints') ❯ src\/store\/features\/serviceRequest\/serviceRequestApi.ts:66:4 64| 65| export const serviceRequestApi = apiSlice 66| .enhanceEndpoints({ ``` here is my apiSlice: ``` export const apiSlice = createApi({ reducerPath: \"api\", baseQuery, endpoints: () => ({}), keepUnusedDataFor: 60, refetchOnMountOrArgChange: false, refetchOnFocus: false, refetchOnReconnect: true, }); ``` finally here is my store.ts ``` export const store = configureStore({ reducer: { [apiSlice.reducerPath]: apiSlice.reducer, auth: authReducer, }, middleware: (getDefaultMiddleware) => { return getDefaultMiddleware() .concat(errorHandlingMiddleware) .concat(apiSlice.middleware); }, \/\/ Enable Redux DevTools in development devTools: import.meta.env.DEV, }); setupListeners(store.dispatch); export type RootState = ReturnType<typeof store.getState>; export type AppDispatch = typeof store.dispatch; ``` and the actual usage of enhanceEndpoints is here: ``` export const serviceRequestApi = apiSlice .enhanceEndpoints({ addTagTypes: [ \"ServiceRequest\", \"ServiceRequestDetails\", \"ApplicationComments\", \"CustomerRequests\", \"ServiceRequestQuotes\", \"Attachments\", \"ServiceRequestTeeData\", ], }) ``` and my baseQuery function below that imported from @reduxjs\/toolkit\/query\/react ``` const baseQuery = fetchBaseQuery({ baseUrl: import.meta.env.VITE_API_URL || \"http:\/\/localhost:5000\/api\", prepareHeaders: async (headers) => { headers.set(\"x-client-name\", \"csm-web\"); headers.set(\"x-client-version\", APP_VERSION); headers.set(\"accept-language\", \"el\"); headers.set(\"x-request-id\", uuidv4()); try { let { idToken, accessToken } = getAuthTokens(); if (!idToken || !accessToken || isAccessTokenExpired()) { const res = await msalInstance.acquireTokenSilent(loginRequest); idToken = res.idToken; accessToken = res.accessToken; setAuthTokens({ idToken, accessToken }); } \/\/ Set authorization headers if (idToken && !headers.has(\"Authorization\")) { headers.set(\"Authorization\", `Bearer ${idToken}`); } \/\/ Handle Graph API token replacement if ( accessToken && headers.has(\"User-Access-Token\") && import.meta.env.VITE_ENVIRONMENT === \"dev\" ) { headers.set(\"Authorization\", `Bearer ${accessToken}`); headers.delete(\"User-Access-Token\"); } } catch (error) { console.error(\"Token acquisition failed:\", error); await msalInstance.logoutRedirect(); throw error; } return headers; }, }); ``` that's all my files for redux store configuration, apart from slices.",
    "author_id":5315,
    "publication_date":1754398912000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Manfre",
    "author_reputation":684.0,
    "tags":"reactjs, javascript, redux, react-testing-library, redux-toolkit",
    "text_length":2585,
    "title_length":107,
    "num_tags":5
  },
  {
    "id":5783,
    "title":"Prefab not aligning with grid quads when rotated in grid-based building system",
    "link":"https:\/\/stackoverflow.com\/questions\/79726021\/prefab-not-aligning-with-grid-quads-when-rotated-in-grid-based-building-system",
    "text":"I'm building a grid-based placement\/building system in Unity. I have prefabs with different shapes and sizes—some are long, some are wide, and each prefab describes its own grid mask. I use a highlight system to show where the prefab would be placed, with “quad” overlays that match the occupied grid cells. My problem: When I rotate the prefab (in 90° steps), the prefab's mesh does not align correctly with the highlighted grid cells (quads). The alignment is correct at 0°, but as soon as I rotate the prefab, the mesh and the highlight quads become misaligned—especially for asymmetrical\/rectangular prefabs (for example, 1x3 or 2x4 shapes). What I want: No matter the rotation, I want the prefab to always be centered perfectly on the highlighted grid quads (mask center), with its pivot in the correct place. The highlight\/overlay system and grid logic already work. I only want to solve the prefab alignment when rotated. How can I reliably center any rotated prefab (no matter its original shape, size, or pivot) so that it perfectly matches the highlighted grid mask, for all 0°, 90°, 180°, and 270° rotations? Is there a robust way to do this with pure math so the mesh and quads always match up, without per-rotation hacks? - GridManager : Handles conversion between grid and world positions, and placement rules for building ``` public class GridManager : MonoBehaviour { public int width = 20; public int height = 20; public float cellSize = 1f; private bool[,] occupied; void Awake() { occupied = new bool[width, height]; } public Vector2Int WorldToGrid(Vector3 pos) { int x = Mathf.FloorToInt(pos.x \/ cellSize); int y = Mathf.FloorToInt(pos.z \/ cellSize); return new Vector2Int(x, y); } public Vector3 GridToWorld(int x, int y) { \/\/ Sol-alt köşe return new Vector3(x * cellSize, 0, y * cellSize); } public bool IsInBounds(int x, int y) { return x >= 0 && x < width && y >= 0 && y < height; } public bool CanPlaceObject(int x, int y) { return IsInBounds(x, y) && !occupied[x, y]; } public void OccupyCell(int x, int y) { if (IsInBounds(x, y)) occupied[x, y] = true; } #if UNITY_EDITOR private void OnDrawGizmos() { Gizmos.color = Color.gray; for (int x = 0; x < width; x++) for (int y = 0; y < height; y++) { Vector3 pos = GridToWorld(x, y); Gizmos.DrawWireCube(pos + new Vector3(cellSize \/ 2, 0, cellSize \/ 2), new Vector3(cellSize, 0.02f, cellSize)); } } #endif } ``` PrefabGridData : Describes which grid cells the prefab occupies (its shape\/mask), and the local grid offset for pivot alignment ``` public class PrefabGridData : MonoBehaviour { [Tooltip(\"Griddeki işgal maskesi. Her Vector2Int = modelin sol-alt köşesinden kaç kare ötede.\")] public List<Vector2Int> occupiedCells = new List<Vector2Int>() { new Vector2Int(0,0) }; public Vector3 gridOffset; public int maskWidth = 3; public int maskHeight = 3; #if UNITY_EDITOR private void OnDrawGizmosSelected() { float cellSize = 1f; Vector3 basePos = transform.position + gridOffset; Gizmos.color = new Color(0.2f, 0.8f, 1f, 0.2f); foreach (var cell in occupiedCells) { Vector3 cellCenter = basePos + new Vector3((cell.x + 0.5f) * cellSize, 0.1f, (cell.y + 0.5f) * cellSize); Gizmos.DrawCube(cellCenter, new Vector3(cellSize, 0.1f, cellSize)); Gizmos.color = Color.blue; Gizmos.DrawWireCube(cellCenter, new Vector3(cellSize, 0.1f, cellSize)); Gizmos.color = new Color(0.2f, 0.8f, 1f, 0.2f); } } [CustomEditor(typeof(PrefabGridData))] public class MaskEditor : Editor { public override void OnInspectorGUI() { DrawDefaultInspector(); PrefabGridData data = (PrefabGridData)target; EditorGUILayout.Space(); EditorGUILayout.LabelField(\"Visual Grid Mask Editor\", EditorStyles.boldLabel); \/\/ Temp mask buffer bool[,] tempMask = new bool[data.maskWidth, data.maskHeight]; foreach (var cell in data.occupiedCells) if (cell.x >= 0 && cell.x < data.maskWidth && cell.y >= 0 && cell.y < data.maskHeight) tempMask[cell.x, cell.y] = true; bool changed = false; for (int y = data.maskHeight - 1; y >= 0; y--) { EditorGUILayout.BeginHorizontal(); for (int x = 0; x < data.maskWidth; x++) { bool occupied = tempMask[x, y]; GUIStyle style = new GUIStyle(GUI.skin.button); style.normal.textColor = occupied ? Color.green : Color.red; style.fontStyle = occupied ? FontStyle.Bold : FontStyle.Normal; string label = occupied ? \"■\" : \"□\"; if (GUILayout.Button(label, style, GUILayout.Width(30), GUILayout.Height(30))) { Undo.RecordObject(data, \"Grid Mask Change\"); tempMask[x, y] = !tempMask[x, y]; changed = true; } } EditorGUILayout.EndHorizontal(); } \/\/ CANLI güncelleme: Her tıklamada maskeyi direkt güncelle if (changed) { data.occupiedCells.Clear(); for (int x = 0; x < data.maskWidth; x++) for (int y = 0; y < data.maskHeight; y++) if (tempMask[x, y]) data.occupiedCells.Add(new Vector2Int(x, y)); EditorUtility.SetDirty(data); } } } #endif } ``` BuildingSystem : Handles previewing and placing prefabs on a grid, including visual highlights and rotation logic. Prefabs of any shape\/size (with a PrefabGridData mask) can be placed, and the highlight system shows which grid cells will be occupied. ``` using UnityEngine; using UnityEngine.InputSystem; using System.Collections.Generic; \/\/\/ <summary> \/\/\/ BuildingSystem handles previewing and placing prefabs on a grid with arbitrary masks, \/\/\/ rotating both the prefab mesh and highlights for 0°, 90°, 180°, 270° steps, \/\/\/ with correct grid-aligned placement without hard-coded offset hacks. \/\/\/ <\/summary> public class BuildingSystem : MonoBehaviour { public GridManager grid; public GameObject buildingPrefab; public Material highlightValidMat; public Material highlightInvalidMat; private GameObject previewContainer; private GameObject previewMesh; private PrefabGridData gridData; private List<GameObject> highlightQuads = new List<GameObject>(); private int rotation = 0; \/\/ 0..3 => *90° void Update() { \/\/ Rotate input if (Keyboard.current.rKey.wasPressedThisFrame) rotation = (rotation + 1) % 4; if (Keyboard.current.qKey.wasPressedThisFrame) rotation = (rotation + 3) % 4; \/\/ Raycast to grid Vector2 mp = Mouse.current.position.ReadValue(); Ray ray = Camera.main.ScreenPointToRay(mp); if (!Physics.Raycast(ray, out RaycastHit hit)) { if (previewContainer != null) previewContainer.SetActive(false); HideHighlights(); return; } \/\/ Get target cell Vector2Int gridPos = grid.WorldToGrid(hit.point); Vector3 basePos = grid.GridToWorld(gridPos.x, gridPos.y); float cs = grid.cellSize; \/\/ Create preview if needed if (previewContainer == null) { previewContainer = new GameObject(\"PreviewPivot\"); previewMesh = Instantiate(buildingPrefab, previewContainer.transform); gridData = previewMesh.GetComponent<PrefabGridData>(); \/\/ Move mesh so its gridOffset aligns with pivot previewMesh.transform.localPosition = -gridData.gridOffset; SetAlpha(previewMesh, 0.5f); } previewContainer.SetActive(true); \/\/ Set container position (no rotation here) previewContainer.transform.position = basePos; previewContainer.transform.rotation = Quaternion.identity; \/\/ Apply mesh rotation about its gridOffset pivot Quaternion rotQuat = Quaternion.Euler(0, rotation * 90f, 0); previewMesh.transform.localRotation = rotQuat; \/\/ Compute rotated mask cells var rotatedCells = GetRotatedCells( gridData.occupiedCells, rotation, gridData.maskWidth, gridData.maskHeight ); \/\/ Show highlights on each rotated cell ShowHighlights(rotatedCells, cs); \/\/ Check if building can be placed bool canBuild = true; foreach (var c in rotatedCells) { Vector3 localPos = new Vector3( c.x * cs + cs \/ 2f, 0, c.y * cs + cs \/ 2f ); Vector3 worldPos = previewContainer.transform.TransformPoint(localPos); Vector2Int gp = grid.WorldToGrid(worldPos); if (!grid.CanPlaceObject(gp.x, gp.y)) { canBuild = false; break; } } \/\/ Place on click if (Mouse.current.leftButton.wasPressedThisFrame && canBuild) { \/\/ Instantiate at mesh pivot world position Vector3 placePos = previewMesh.transform.position; Instantiate(buildingPrefab, placePos, rotQuat); \/\/ Mark occupied cells foreach (var c in rotatedCells) { Vector3 localPos = new Vector3( c.x * cs + cs \/ 2f, 0, c.y * cs + cs \/ 2f ); Vector3 worldPos = previewContainer.transform.TransformPoint(localPos); Vector2Int gp = grid.WorldToGrid(worldPos); grid.OccupyCell(gp.x, gp.y); } } } \/\/\/ <summary> \/\/\/ Rotates mask cell coordinates clockwise by the given rotation step (0..3). \/\/\/ <\/summary> List<Vector2Int> GetRotatedCells( List<Vector2Int> cells, int rot, int width, int height ) { var result = new List<Vector2Int>(cells.Count); foreach (var cell in cells) { int x = cell.x; int y = cell.y; Vector2Int rc; switch (rot) { case 1: \/\/ 90° rc = new Vector2Int(height - 1 - y, x); break; case 2: \/\/ 180° rc = new Vector2Int(width - 1 - x, height - 1 - y); break; case 3: \/\/ 270° rc = new Vector2Int(y, width - 1 - x); break; default: \/\/ 0° rc = new Vector2Int(x, y); break; } result.Add(rc); } return result; } void ShowHighlights(List<Vector2Int> mask, float cellSize) { while (highlightQuads.Count < mask.Count) { var quad = GameObject.CreatePrimitive(PrimitiveType.Quad); quad.transform.parent = previewContainer.transform; quad.transform.localRotation = Quaternion.Euler(90, 0, 0); quad.transform.localScale = new Vector3(cellSize, cellSize, 1); quad.GetComponent<Collider>().enabled = false; highlightQuads.Add(quad); } for (int i = 0; i < highlightQuads.Count; i++) { var quad = highlightQuads[i]; if (i < mask.Count) { quad.SetActive(true); var c = mask[i]; Vector3 localPos = new Vector3( c.x * cellSize + cellSize \/ 2f, 0.02f, c.y * cellSize + cellSize \/ 2f ); quad.transform.localPosition = localPos; Vector3 worldPos = previewContainer.transform.TransformPoint(localPos); Vector2Int gp = grid.WorldToGrid(worldPos); bool valid = grid.CanPlaceObject(gp.x, gp.y); quad.GetComponent<Renderer>().material = valid ? highlightValidMat : highlightInvalidMat; } else quad.SetActive(false); } } void HideHighlights() { foreach (var q in highlightQuads) q.SetActive(false); } void SetAlpha(GameObject obj, float alpha) { foreach (var r in obj.GetComponentsInChildren<Renderer>()) { var mat = r.material; Color c = mat.color; c.a = alpha; mat.color = c; mat.SetFloat(\"_Mode\", 3); mat.SetInt(\"_SrcBlend\", (int)UnityEngine.Rendering.BlendMode.SrcAlpha); mat.SetInt(\"_DstBlend\", (int)UnityEngine.Rendering.BlendMode.OneMinusSrcAlpha); mat.SetInt(\"_ZWrite\", 0); mat.DisableKeyword(\"_ALPHATEST_ON\"); mat.EnableKeyword(\"_ALPHABLEND_ON\"); mat.DisableKeyword(\"_ALPHAPREMULTIPLY_ON\"); mat.renderQueue = 3000; } } } ```",
    "author_id":5314,
    "publication_date":1754398980000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"G&#246;ktuğ Ozleyen",
    "author_reputation":121.0,
    "tags":"c#, unity-game-engine",
    "text_length":10419,
    "title_length":78,
    "num_tags":2
  },
  {
    "id":5782,
    "title":"Hostinger subdomain for a web",
    "link":"https:\/\/stackoverflow.com\/questions\/79726023\/hostinger-subdomain-for-a-web",
    "text":"I am Not able to run my site through a subdomain on hostinger as some secutrity issue it can be or may be DSN and SSL even tried many ways but not able to track it would you mind to help. I have attached the error snapshot as well ``` enter code here as the issue is not related to the code ```",
    "author_id":5313,
    "publication_date":1754399536000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Harsh",
    "author_reputation":165.0,
    "tags":"html, deployment",
    "text_length":294,
    "title_length":29,
    "num_tags":2
  },
  {
    "id":5781,
    "title":".NET 8 background service not running post RestAPI deployment in IIS",
    "link":"https:\/\/stackoverflow.com\/questions\/79726029\/net-8-background-service-not-running-post-restapi-deployment-in-iis",
    "text":"I have created a ``` BackgroundService ``` in my ASP.NET Core 8 Web API. I want to run ``` BackgroundService ``` along with my API. ``` public class RunQueueMessage: BackgroundService ``` Issue is whenever my code deployed to IIS, then background service not triggering automatically. But when any Web API endpoint is triggered for the first time post deployment, the background service was working as expected. I tried by installing Application Initialisation module in IIS and also updated the Web API's ``` web.config ``` to hit some dummy endpoint. I have also set Preload Enabled property to true in IIS App Advanced Settings. Also startup mode set to \"AlwaysRunning\" and also Idle Timeout set to 0. Now I want to run my ``` BackgroundService ``` continuously without any interruptions. It should start immediately whenever app pool or IIS starts and should not wait for any first request. Please guide.",
    "author_id":5312,
    "publication_date":1754399907000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Sravani Yadav",
    "author_reputation":35.0,
    "tags":"c#, asp.net-core-webapi, .net-8.0, iis",
    "text_length":908,
    "title_length":68,
    "num_tags":4
  },
  {
    "id":5780,
    "title":"Invert matrix in PostgreSQL",
    "link":"https:\/\/stackoverflow.com\/questions\/79726031\/invert-matrix-in-postgresql",
    "text":"What is the best and easiest way to invert square matrix (about 1000 * 1000) in PostgreSQL? Use some extension (OneSparse, PgEigen)? Write function for Gauss-Jordan in PL\/pgSQL? I have used UTL_NLA.LAPACK_GELS in Oracle and it was very simple.",
    "author_id":5311,
    "publication_date":1754399989000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"hinotf",
    "author_reputation":1144.0,
    "tags":"postgresql, linear-algebra",
    "text_length":243,
    "title_length":27,
    "num_tags":2
  },
  {
    "id":5779,
    "title":"Multiple resource API calls in Angular",
    "link":"https:\/\/stackoverflow.com\/questions\/79726034\/multiple-resource-api-calls-in-angular",
    "text":"I am trying to understand the new Angular resource API. One typical problem is to fetch several HTTP endpoints, combine the data and use it. My current approach looks like this: ``` name = rxResource({ stream: () => this.nameService.getNameId('test') }); build = rxResource({ stream: () => this.buildService.buildGet() });}); \/\/ Update computed signal to use resource values tableData = computed(() => { const nameValue = this.name.value(); const buildValue = this.build.value(); if (!nameValue || !buildValue) { return null; } return { data: this.getTable(nameValue , buildValue ), }; }); ``` Is this the proper way to use it? Currently the computed signal is executed when either one of the resources return a value. I manually test if the value is set and only if both values are not null, the signal returns a proper value. Is there a better way to do this? Something like ``` combineLatest ``` in RxJS? I would really like to stick to the signal\/resource approach.",
    "author_id":5310,
    "publication_date":1754400258000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mo3bius",
    "author_reputation":672.0,
    "tags":"angular, angular-signals, rxjs",
    "text_length":969,
    "title_length":38,
    "num_tags":3
  },
  {
    "id":5778,
    "title":"Gollum 6.1 Wiki: NoMethodError at \/gollum\/commit\/* (Windows)",
    "link":"https:\/\/stackoverflow.com\/questions\/79726037\/gollum-6-1-wiki-nomethoderror-at-gollum-commit-windows",
    "text":"I have Gollum 6.1 wiki working on Windows Server 2022 and Windows 11 Pro, but when I try to access any URL starting with ``` \/gollum\/commit\/ ``` , it produces the following error: ``` NoMethodError at \/gollum\/commit\/908017e5b790a3dfded06aaaeece48ff91e64abb undefined method 'escaped_url_path' for nil file: has_page.rb location: nil line: 8 ``` How to fix this error?",
    "author_id":5309,
    "publication_date":1754400372000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Tommi Gustafsson",
    "author_reputation":141.0,
    "tags":"gollum-wiki",
    "text_length":367,
    "title_length":60,
    "num_tags":1
  },
  {
    "id":5777,
    "title":"Stimulus useDirtyFormTracking - how to update data-detect-dirty-load-value after form submission",
    "link":"https:\/\/stackoverflow.com\/questions\/79726041\/stimulus-usedirtyformtracking-how-to-update-data-detect-dirty-load-value-after",
    "text":"Reference Link: https:\/\/sub-xaero.github.io\/stimulus-library\/docs form_controller.js ``` import { Controller } from '@hotwired\/stimulus'; import { useDirtyFormTracking } from \"stimulus-library\"; export default class extends Controller { connect() { useDirtyFormTracking(this, this.element); this.element.addEventListener(\"form-dirtied\", this.#onFormDirtied); this.element.addEventListener(\"form-cleaned\", this.#onFormCleaned); } #onFormDirtied = (event) => { \/\/ show save button } #onFormCleaned = () => { \/\/ hide save button } \/\/ called after form submission to reset the form \/\/ & pass the new saved values to every field inside the form (should update data-detect-dirty-load-value) reset() { this.#onFormCleaned(); \/\/ i don't know if there's a better way to reset the form and input fields after form submission this.element.removeAttribute(\"data-dirty\"); const dirtyInputs = this.element.querySelectorAll(\"[data-dirty]\"); dirtyInputs.forEach((el) => el.removeAttribute(\"data-dirty\")); } } ``` My problem is in reset() method, because the initial load values stay the same after form submission. Please help, thanks in advance.",
    "author_id":5308,
    "publication_date":1754400544000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"aldrien.h",
    "author_reputation":3663.0,
    "tags":"stimulusjs, stimulus-rails",
    "text_length":1130,
    "title_length":96,
    "num_tags":2
  },
  {
    "id":5776,
    "title":"Problems at running application with Java21, Geotools and Javafx",
    "link":"https:\/\/stackoverflow.com\/questions\/79726043\/problems-at-running-application-with-java21-geotools-and-javafx",
    "text":"My target is to create an application, which uses ``` Java 21.0.7 ``` (OpenJDK), ``` JavaFx 23.0.2 ``` and ``` GeoTools 34-SNAPSHOT ``` to show S57 sea charts. As a prerequirement I have downloaded ``` javafx-sdk-23.0.2 ``` as well and set the environment variable ``` FX-PATH=C:\\Git\\openjfx-23.0.2_windows-x64_bin-sdk\\javafx-sdk-23.0.2\\lib ``` . The application contains the ``` pom.xml ``` and two classes: ``` <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/xsd\/maven-4.0.0.xsd\"> <modelVersion>4.0.0<\/modelVersion> <groupId>org.example<\/groupId> <artifactId>GeoToolsTest<\/artifactId> <version>1.0-SNAPSHOT<\/version> <properties> <version.java>21<\/version.java> <maven.compiler.source>${version.java}<\/maven.compiler.source> <maven.compiler.target>${version.java}<\/maven.compiler.target> <project.build.sourceEncoding>UTF-8<\/project.build.sourceEncoding> <version.javafx>23.0.2<\/version.javafx> <version.geotools>34-SNAPSHOT<\/version.geotools> <maven.deploy.skip>true<\/maven.deploy.skip> <\/properties> <dependencyManagement> <dependencies> <!-- Import the GeoTools BOM --> <dependency> <groupId>org.geotools<\/groupId> <artifactId>gt-bom<\/artifactId> <version>${version.geotools}<\/version> <type>pom<\/type> <scope>import<\/scope> <\/dependency> <\/dependencies> <\/dependencyManagement> <repositories> <repository> <id>osgeo<\/id> <name>OSGeo Release Repository<\/name> <url>https:\/\/repo.osgeo.org\/repository\/release\/<\/url> <snapshots><enabled>false<\/enabled><\/snapshots> <releases><enabled>true<\/enabled><\/releases> <\/repository> <repository> <id>osgeo-snapshot<\/id> <name>OSGeo Snapshot Repository<\/name> <url>https:\/\/repo.osgeo.org\/repository\/snapshot\/<\/url> <snapshots><enabled>true<\/enabled><\/snapshots> <releases><enabled>false<\/enabled><\/releases> <\/repository> <\/repositories> <build> <resources> <resource> <directory>src\/main\/config<\/directory> <targetPath>${project.build.directory}<\/targetPath> <filtering>true<\/filtering> <\/resource> <\/resources> <plugins> <plugin> <groupId>org.apache.maven.plugins<\/groupId> <artifactId>maven-compiler-plugin<\/artifactId> <version>3.14.0<\/version> <configuration><release>${version.java}<\/release><\/configuration> <\/plugin> <plugin> <groupId>org.openjfx<\/groupId> <artifactId>javafx-maven-plugin<\/artifactId> <version>0.0.8<\/version> <configuration><mainClass>org.example.Main<\/mainClass><\/configuration> <\/plugin> <plugin> <!-- Build an executable JAR --> <groupId>org.apache.maven.plugins<\/groupId> <artifactId>maven-jar-plugin<\/artifactId> <version>3.4.2<\/version> <configuration> <archive> <manifest> <addClasspath>true<\/addClasspath> <classpathPrefix>lib\/<\/classpathPrefix> <mainClass>org.example.Starter<\/mainClass> <\/manifest> <\/archive> <\/configuration> <\/plugin> <\/plugins> <\/build> <dependencies> <dependency> <groupId>org.openjfx<\/groupId> <artifactId>javafx<\/artifactId> <version>${version.javafx}<\/version> <type>pom<\/type> <\/dependency> <dependency> <groupId>org.openjfx<\/groupId> <artifactId>javafx-controls<\/artifactId> <version>${version.javafx}<\/version> <\/dependency> <dependency> <groupId>org.openjfx<\/groupId> <artifactId>javafx-swing<\/artifactId> <version>${version.javafx}<\/version> <\/dependency> <dependency> <groupId>org.geotools<\/groupId> <artifactId>gt-shapefile<\/artifactId> <\/dependency> <dependency> <groupId>org.geotools<\/groupId> <artifactId>gt-swing<\/artifactId> <\/dependency> <dependency> <groupId>org.geotools<\/groupId> <artifactId>gt-api<\/artifactId> <\/dependency> <\/dependencies> <\/project> ``` The class ``` org.example.Starter ``` (which acts as a wrapper class): ``` package org.example; public class Starter { \/\/Wrapper class public static void main(String[] args) { Main.main(args); } } ``` And the file: ``` org.example.Main ``` : ``` package org.example; import javafx.application.Application; import javafx.embed.swing.SwingNode; import javafx.scene.Scene; import javafx.scene.layout.BorderPane; import javafx.stage.Stage; import org.geotools.api.data.DataStore; import org.geotools.api.data.DataStoreFinder; import org.geotools.map.Layer; import org.geotools.map.MapContent; import org.geotools.swing.JMapPane; import java.io.File; import java.util.HashMap; import java.util.Map; public class Main extends Application { @Override public void start(Stage primaryStage) { primaryStage.setTitle(\"S-57 seachart with GeoTools\"); SwingNode swingNode = new SwingNode(); createAndSetMapPane(swingNode); BorderPane root = new BorderPane(); root.setCenter(swingNode); Scene scene = new Scene(root, 800, 600); primaryStage.setScene(scene); primaryStage.show(); } private void createAndSetMapPane(SwingNode swingNode) { javafx.application.Platform.runLater(() -> { MapContent mapContent = new MapContent(); mapContent.setTitle(\"S-57 Sea chart\"); try { File s57File = new File(\"PathToS57Files\\\\file.000\"); Map<String, Object> params = new HashMap<>(); params.put(\"url\", s57File.toURI().toURL()); DataStore dataStore = DataStoreFinder.getDataStore(params); String typeName = dataStore.getTypeNames()[0]; Layer layer = new org.geotools.map.FeatureLayer(dataStore.getFeatureSource(typeName), null); mapContent.addLayer(layer); JMapPane mapPane = new JMapPane(); mapPane.setMapContent(mapContent); swingNode.setContent(mapPane); } catch (Exception e) { e.printStackTrace(); } }); } public static void main(String[] args) { launch(args); } } ``` I start my application with the following command: ``` java --module-path %FX-PATH% --add-modules javafx.controls,javafx.swing -jar GeoToolsTest-1.0-SNAPSHOT.jar Main ``` but it ends up with the following exception: ``` Exception in thread \"main\" java.lang.NoClassDefFoundError: org\/geotools\/api\/data\/FeatureSource at org.example.Starter.main(Starter.java:7) Caused by: java.lang.ClassNotFoundException: org.geotools.api.data.FeatureSource at java.base\/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) at java.base\/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) at java.base\/java.lang.ClassLoader.loadClass(ClassLoader.java:526) ... 1 more ``` Could someone explain, what is missing or wrong? Some notices: As far as i can see the class ``` FeatureSource ``` is part of ``` org.geotools.gt-api ``` and this dependency is included. Even if i remove the ``` gt-bom ``` -pattern (which is introduced by version ``` 34-SNAPSHOT ``` ) and set all ``` geotools ``` -dependencies to the last stable version ``` 33.2 ``` this exception appears.",
    "author_id":5307,
    "publication_date":1754400802000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Alex",
    "author_reputation":1.0,
    "tags":"java, maven, javafx, geotools",
    "text_length":6590,
    "title_length":64,
    "num_tags":4
  },
  {
    "id":5775,
    "title":"Why is my code only assigning a certain variable sometimes, rather than everytime the program runs?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726046\/why-is-my-code-only-assigning-a-certain-variable-sometimes-rather-than-everytim",
    "text":"I am working on a small hobby project where I try to create a government for a fictional world. So starting out, I have an assembly of 250 people, with many different details that are being generated for each one of them and one of these things being generated is the role of said person. Most of the roles should just be MPs, but there are a few roles that should only be assigned to a certain amount of people, for example the role of the President should only be assigned to one person. Now, I already have all of my code working without errors, but I notice that when running the code, sometimes it generates the role of president for 1 person and other times it doesn't generate the role at all! I want to know what in my code is causing this to happen. So in short, I want the roll of president to always be generated and assigned to a random person in the assembly, but my current code is preventing that from happening sometimes and doesn't assign the role at all. Here is my current code. main.py: ``` import random import character_details as charDetails def legislative(): legislative_size = 250 return legislative_size def createCharacter(): legislative_size = legislative() president_index = random.randint(0, legislative_size - 1) print(f\"President will be assigned to Member #{president_index + 1}\") # Debug line for i in range(legislative_size): # Assign role if i == president_index: charRoll = \"President\" else: charRoll = random.choice(charDetails.roleInGov) # Gender and Age group gender = random.choice(charDetails.genders) if i == president_index: # Assign age group with 15% chance of being young if random.randrange(100) < 15: ageChoice = \"Young\" else: ageChoice = random.choice([\"Middle-Aged\", \"Old\"]) else: ageChoice = random.choice(charDetails.ages) if ageChoice == \"Young\": age = random.randrange(21, 39) elif ageChoice == \"Middle-Aged\": age = random.randrange(40, 64) elif ageChoice == \"Old\": age = random.randrange(65, 75) else: print(\"Age is not found\") age = \"Unknown\" # Name generation if gender == \"Male\": personName = random.choice(charDetails.male_names) elif gender == \"Female\": personName = random.choice(charDetails.female_names) else: personName = \"Unknown\" personLastName = random.choice(charDetails.last_names) character_traits = [\"Ambitious\", \"Brave\", \"Deceitful\", \"Just\", \"Lustful\", \"Greedy\"] characterTraits = random.choice(character_traits) print(\"Member Number: #{}\".format(i + 1)) print(\"Role: {}\".format(charRoll)) print(\"Name: {} {}\".format(personName, personLastName)) print(\"Gender: {}\".format(gender)) print(\"Age: {}\".format(age)) print(\"Traits: {}\".format(characterTraits)) print(\"\") createCharacter() ``` character_details.py: ``` roleInGov = [\"MP\", \"Minister\", \"Speaker of the Assembly\", \"Party Leader\", \"Leader of Party Faction\"] male_names = [\"Thomas\", \"Tony\", \"Alec\", \"Steven\", \"James\", \"Alexander\", \"Marcus\", \"Zaine\", \"Jacob\", \"Paul\", \"Liam\", \"Donald\", \"Herbert\", \"Petr\", \"David\", \"Anton\", \"Ronald\", \"Hadron\", \"Gilligan\", \"Omar\"] female_names = [\"Alexis\", \"Jamie\", \"Sarah\", \"Ciara\", \"Monica\", \"Justine\", \"Marla\", \"Madalaine\", \"Janine\", \"Petra\", \"Hannah\", \"Fiemke\", \"Olga\", \"Jillian\", \"Georgia\", \"Atlanta\", \"Morigan\", \"Gina\", \"Anna\", \"Lee\"] last_names = [\"Margus\", \"Neeglin\", \"Alethis\", \"Saratoga\", \"Ramin\", \"Larmin\", \"Jahigan\", \"Mastrul\", \"Prima\", \"Questra\", \"Felstra\", \"Ulgan\", \"Cancea\", \"Stermenta\", \"Polkumta\", \"Indrah\", \"Ylvin\", \"Klarinitin\", \"Hartcin\", \"Komboto\"] genders = [\"Male\", \"Female\"] ages = [\"Young\", \"Middle-Aged\", \"Old\"] ```",
    "author_id":5306,
    "publication_date":1754400860000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Keenonthedaywalker",
    "author_reputation":85.0,
    "tags":"python, python-3.x, random",
    "text_length":3500,
    "title_length":99,
    "num_tags":3
  },
  {
    "id":5774,
    "title":"pydantic model validator raise ValueError to field",
    "link":"https:\/\/stackoverflow.com\/questions\/79726058\/pydantic-model-validator-raise-valueerror-to-field",
    "text":"I am trying to do a validation on passwords where if they dont match, return an error. But I want to assign the error to field. ``` class RequestFile( BaseModel ): password: str = Field(..., min_length=8, max_length=128) confirm_password: str = Field(..., min_length=8, max_length=128) @field_validator(\"password\") def password_validate(cls, v: str) -> str: if not v.strip(): raise ValueError(\"Password required\") return v @field_validator(\"confirm_password\") def confirm_password_validate(cls, v: str) -> str: if not v.strip(): raise ValueError(\"Confirm Password Required\") return v @model_validator(mode=\"before\") def check_passwords_match(cls, values): password = values.get(\"password\") confirm_password = values.get(\"confirm_password\") if password != confirm_password: raise ValueError(\"Passwords does not match\") return values ``` In the code above, when either password or confirm_password failed validation (char length), it pushes the ValueError to field password and confirm_password respectively. However, when they dont match, it returns the ValueError but not on any field. How to push the ValueError to either password or confirm_password? Preferrably in confirm_password",
    "author_id":5305,
    "publication_date":1754401606000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Mr. Kenneth",
    "author_reputation":992.0,
    "tags":"python, pydantic, pydantic-v2",
    "text_length":1184,
    "title_length":50,
    "num_tags":3
  },
  {
    "id":5773,
    "title":"make inline-flex shrink when items are wrapped",
    "link":"https:\/\/stackoverflow.com\/questions\/79726059\/make-inline-flex-shrink-when-items-are-wrapped",
    "text":"flex-yellow takes the min-width which is the width of the 2 flex-items ``` .screen { width: 300px; background-color: pink; } .flex-yellow { padding: 4px; display: inline-flex; flex-wrap: wrap; gap: 4px; background-color: yellow; } .item-black { height: 50px; min-width: 100px; background-color: black; } ``` ``` <div class=\"screen\"> <div class=\"flex-yellow\"> <div class=\"item-black\"><\/div> <div class=\"item-black\"><\/div> <\/div> <\/div> ``` but when the items are wrapped, how do i make it so, flex-yellow takes the min-width which in this case is the width of 1 flex-item ``` .screen { width: 200px; background-color: pink; } .flex-yellow { padding: 4px; display: inline-flex; flex-wrap: wrap; gap: 4px; background-color: yellow; } .item-black { height: 50px; min-width: 100px; background-color: black; } ``` ``` <div class=\"screen\"> <div class=\"flex-yellow\"> <div class=\"item-black\"><\/div> <div class=\"item-black\"><\/div> <\/div> <\/div> ```",
    "author_id":5304,
    "publication_date":1754401610000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Ibrahim Ali",
    "author_reputation":2541.0,
    "tags":"html, css",
    "text_length":938,
    "title_length":46,
    "num_tags":2
  },
  {
    "id":5772,
    "title":"Required package &#39;ibxpress&#39; not found error in Delphi 12 CE when compiling",
    "link":"https:\/\/stackoverflow.com\/questions\/79726061\/required-package-ibxpress-not-found-error-in-delphi-12-ce-when-compiling",
    "text":"Sometimes when I compile my project in Delphi 12 CE, then I get the following error: [dcc32 Fatal Error] E2202 Required package 'ibxpress' not found The weird thing is that I'm not using ibx or anything related to it in my project. And when I search for \"ibx\" through my whole project, then there's no references for it anywhere as can be seen here when searching through all files in my project folder within VSC: There's nothing about \"ibx\" in my project at all. I'm 100% certain that I'm not using ibx in my project. So why is it complaining about missing ibx packages? I also looked through runtime packages in my project options and couldn't find anything in here: However when compiling using Delphi 12 Enterprise, then I don't get these errors at all. As an example, yesterday I worked on the project in Delphi CE the whole day without any issues, and then suddenly out of nowhere it started giving me this error. I don't know what is causing it, but any help to stop getting the error would be very much appreciated.",
    "author_id":5303,
    "publication_date":1754401647000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"mark w",
    "author_reputation":21.0,
    "tags":"delphi, delphi-12-athens, delphi-ide",
    "text_length":1024,
    "title_length":82,
    "num_tags":3
  },
  {
    "id":5771,
    "title":"Convert an integer to IBM PIC S9(09) COMP. type in python",
    "link":"https:\/\/stackoverflow.com\/questions\/79726067\/convert-an-integer-to-ibm-pic-s909-comp-type-in-python",
    "text":"I have the following three values in a CSV file: ``` 1683199814 2087175640 1348771152 ``` I need to write them into a flat file using a python program, which will be loaded into Mainframe DB2 using a COBOL program. The copybook says that it is of type ``` PIC S9(09) COMP ``` . This will be loaded into a DB2 table of type INTEGER of DB2 length 4. Below is the python code logic that I used to convert this: ``` record = bytearray() record.extend(b'1') member_id= int(memb_num) record.extend(struct.pack('>i', member_id)) record.extend(pad_right(pr_cat, 3)) record.extend(pad_right(cm_ts, 26)) record.extend(pad_right(cr_id, 8)) record.extend(b' ' * 38) return record ``` Where ``` member_id ``` has the values from the CSV file through which I am iterating. I then write this record out into the file which they load into the DB2 table. However, after loading, the mainframe engineer showed me that the values are completely different and sometimes negative. He mentioned that mainframe is using Big Endian for that value. The values are showing differently as below: ``` -2065547322 1334273920 -679209769 ``` I don't know what I am doing wrong! EDIT: Below is how the copybook looks: ``` 01 CCS001-DETAIL-RECORD. 05 CCS001-REC-TYPE PIC X(01). Should be '1' 05 CCS001-MEMB-NUM PIC S9(09) COMP. 05 CCS001-PR-CAT PIC X(03). 05 CCS001-CM-TS PIC X(26). 05 CCS001-CR-ID PIC X(08). 05 FILLER PIC X(38). ``` I got a sample file that COBOL used and checked the hex value from it. I got ``` 07C2AEC2 ``` . But ideally its actual integer value ``` 800006829 ``` has the hex of ``` 2faf22ad ``` . Does this have something to do with how my Windows laptop(x64) treats the endian values in my Python IDE?",
    "author_id":4872,
    "publication_date":1754401900000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"dijeah",
    "author_reputation":323.0,
    "tags":"python, db2, endianness, cobol",
    "text_length":1692,
    "title_length":57,
    "num_tags":4
  },
  {
    "id":5770,
    "title":"Example for setting up Docusaurus for multiple projects?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726071\/example-for-setting-up-docusaurus-for-multiple-projects",
    "text":"At my company we're trying to decentralize our architecture. One of the steps we've decide upon is to have each project capture its designs in their git repository so the ADRs, etc., stay close to the code. That way we avoid having to maintain things in two places and have one go stale. One of the things we're considering is using Docusaurus to front an internal site for developers to reference those documents and designs from a single place. It seems that it should be possible to have a single Docusaurus repo where the main site's code lives and then have each project, as a part of its CI pipeline, spit out that project's documents and have them pushed to the Docusaurus server and be hosted. Is there an example of how to do something like this? Does each project simply need to have a folder with markdown pages that get pushed to the server, or do we need to do a bit more per project to have pages be ready for hosting?",
    "author_id":5302,
    "publication_date":1754402249000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"mcpierce",
    "author_reputation":316.0,
    "tags":"architecture, documentation, docusaurus",
    "text_length":932,
    "title_length":56,
    "num_tags":3
  },
  {
    "id":5769,
    "title":"How to set a primary key that is inside a struct?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726072\/how-to-set-a-primary-key-that-is-inside-a-struct",
    "text":"I'm trying to get a JSON file into kuzu but I'm facing some issues. This is what I'm trying: ``` import kuzu def put_into_kuzu(): db = kuzu.Database(\"rag.kuzu\") conn = kuzu.Connection(db) cex = conn.execute cex(\"LOAD json;\") cex(\"CREATE NODE TABLE Chunks(embedding_dim INT, data STRUCT(__id__ STRING, __created_at__ INT, content STRING, full_doc_id STRING, file_path STRING), PRIMARY KEY (__id__));\") cex('COPY Chunks FROM \".\/rag_storage\/vdb_chunks.json\"') response = conn.execute( \"\"\" RETURN Chunks; \"\"\" ) for row in response: print(row) if __name__ == \"__main__\": put_into_kuzu() ``` JSON file structure: ``` { \"embedding_dim\": 4096, \"data\": [ { \"__id__\": \"chunk-ed1b1b28b5cf4df59ca4bf1031e4cc27\", \"__created_at__\": 1753946350, \"content\": \"content of the text document\", \"full_doc_id\": \"doc-ed1b1b28b5cf4df59ca4bf1031e4cc27\", \"file_path\": \"test.txt\" }, { a lot more of this }, ], \"matrix\" : \"bunch of random characters, but they are unneeded\" } ``` As can be seen from the code I want ``` __id__ ``` as my primary key, but can't achieve it. I even tried ``` data.`__id__` ``` but that didn't work either. Error messages: when trying ``` __id__ ``` : ``` RuntimeError: Binder exception: Primary key __id__ does not match any of the predefined node properties. ``` when trying ``` data.`__id__` ``` : ``` RuntimeError: Parser exception: Invalid input < TABLE>: expected rule kU_CreateNodeTable (line: 1, offset: 54) \"CREATE NODE TABLE Chunks(embedding_dim INT, data NODE TABLE Chunks(__id__ STRING, __created_at__ INT, content STRING, full_doc_id STRING, file_path STRING), PRIMARY KEY (__id__);\" ```",
    "author_id":5301,
    "publication_date":1754402315000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Detryx",
    "author_reputation":5.0,
    "tags":"python, json, opencypher",
    "text_length":1600,
    "title_length":49,
    "num_tags":3
  },
  {
    "id":5768,
    "title":"mat1 and mat2 shapes cannot be multiplied (64x121 and 7744x64)",
    "link":"https:\/\/stackoverflow.com\/questions\/79726076\/mat1-and-mat2-shapes-cannot-be-multiplied-64x121-and-7744x64",
    "text":"I've recently been trying to make a Pong agent using reinforcement learning, but I keep getting errors no matter what I do. The PPO implementation I’m using was originally designed for CartPole, and I modified it to work with Pong. The problem likely comes from this part of model: ``` with torch.no_grad(): dummy = torch.zeros(1, 1, 96, 96) # batch=1, grayscale, 96x96 conv_out = self.shared_layers(dummy) flattened_size = conv_out.view(1, -1).shape[1] # Inside ActorCriticNetwork.__init__() with torch.no_grad(): dummy = torch.zeros(1, 1, 96, 96) # simulate one preprocessed image conv_out = self.shared_layers(dummy) flattened_size = conv_out.view(1, -1).shape[1] ``` and how it connects to the Linear layer: ``` self.policy_layers = nn.Sequential( nn.Linear(flattened_size, 64), # ← uses flattened_size nn.ReLU(), nn.Linear(64, action_space_size) ) ``` Full code: https:\/\/github.com\/twhighschooler\/pong",
    "author_id":5300,
    "publication_date":1754402586000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"just a tw highschooler",
    "author_reputation":3.0,
    "tags":"python, pytorch, reinforcement-learning",
    "text_length":906,
    "title_length":62,
    "num_tags":3
  },
  {
    "id":5767,
    "title":"If one storage account is accessed with private endpoint in a vnet, now all other storage accounts have to be accessed with pe, how to avoid this?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726077\/if-one-storage-account-is-accessed-with-private-endpoint-in-a-vnet-now-all-othe",
    "text":"We whitelisted ips of some storage accounts in our vnet and were using those storage accounts, at some point we needed to create a private endpoint to access new storage account. Now initial storage accounts ips are not getting resolved as all storage accounts traffic is going from newly created private dns zone which has 'a record' of new storage account only. How can this be handled without creating private endpoints for initial storage accounts ?",
    "author_id":5299,
    "publication_date":1754402607000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"arya.s",
    "author_reputation":121.0,
    "tags":"azure, private-endpoint, dns, azure-storage-account, vnet",
    "text_length":453,
    "title_length":146,
    "num_tags":5
  },
  {
    "id":5766,
    "title":"Can someone explain why in Batch Script IF 08 GEQ 4 is FALSE?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726086\/can-someone-explain-why-in-batch-script-if-08-geq-4-is-false",
    "text":"In Batch Script, when comparing a 2 digit number to a 1 digit number 08 & 09 are treated as low numbers but everything else works as expected. So if you run the following: ``` IF 01 LEQ 1 ECHO 01 IF 02 LEQ 1 ECHO 02 IF 07 LEQ 1 ECHO 07 IF 08 LEQ 1 ECHO 08 IF 09 LEQ 1 ECHO 09 IF 10 LEQ 1 ECHO 10 ``` You get the output: ``` 01 08 09 ``` The GEQ operator has similar results, run the following: ``` IF 01 GEQ 1 ECHO 01 IF 02 GEQ 1 ECHO 02 IF 07 GEQ 1 ECHO 07 IF 08 GEQ 1 ECHO 08 IF 09 GEQ 1 ECHO 09 IF 10 GEQ 1 ECHO 10 ``` You get the output: ``` 01 02 07 10 ``` If you swap out the 1 for other numbers, like in the title 4, then it always treats 08 & 09 as low numbers between 0 & 1. Comparing with another 2 digit number, padded with a zero provides the correct results. e.g. 04 works as expected, it's less than 08 & 09. The following script can be used to test this: ``` @ECHO OFF SET COMP=%1 ECHO. ECHO LEQ %COMP% ECHO. IF 00 LEQ %COMP% ECHO 00 IF 01 LEQ %COMP% ECHO 01 IF 02 LEQ %COMP% ECHO 02 IF 03 LEQ %COMP% ECHO 03 IF 04 LEQ %COMP% ECHO 04 IF 05 LEQ %COMP% ECHO 05 IF 06 LEQ %COMP% ECHO 06 IF 07 LEQ %COMP% ECHO 07 IF 08 LEQ %COMP% ECHO 08 IF 09 LEQ %COMP% ECHO 09 IF 10 LEQ %COMP% ECHO 10 IF 11 LEQ %COMP% ECHO 11 IF 12 LEQ %COMP% ECHO 12 IF 13 LEQ %COMP% ECHO 13 IF 14 LEQ %COMP% ECHO 14 IF 15 LEQ %COMP% ECHO 15 IF 16 LEQ %COMP% ECHO 16 IF 17 LEQ %COMP% ECHO 17 IF 18 LEQ %COMP% ECHO 18 IF 19 LEQ %COMP% ECHO 19 IF 20 LEQ %COMP% ECHO 20 ECHO. ECHO. ECHO GEQ %COMP% ECHO. IF 00 GEQ %COMP% ECHO 00 IF 01 GEQ %COMP% ECHO 01 IF 02 GEQ %COMP% ECHO 02 IF 03 GEQ %COMP% ECHO 03 IF 04 GEQ %COMP% ECHO 04 IF 05 GEQ %COMP% ECHO 05 IF 06 GEQ %COMP% ECHO 06 IF 07 GEQ %COMP% ECHO 07 IF 08 GEQ %COMP% ECHO 08 IF 09 GEQ %COMP% ECHO 09 IF 10 GEQ %COMP% ECHO 10 IF 11 GEQ %COMP% ECHO 11 IF 12 GEQ %COMP% ECHO 12 IF 13 GEQ %COMP% ECHO 13 IF 14 GEQ %COMP% ECHO 14 IF 15 GEQ %COMP% ECHO 15 IF 16 GEQ %COMP% ECHO 16 IF 17 GEQ %COMP% ECHO 17 IF 18 GEQ %COMP% ECHO 18 IF 19 GEQ %COMP% ECHO 19 IF 20 GEQ %COMP% ECHO 20 ECHO. ECHO. ECHO EQU %COMP% ECHO. IF 00 EQU %COMP% ECHO 00 IF 01 EQU %COMP% ECHO 01 IF 02 EQU %COMP% ECHO 02 IF 03 EQU %COMP% ECHO 03 IF 04 EQU %COMP% ECHO 04 IF 05 EQU %COMP% ECHO 05 IF 06 EQU %COMP% ECHO 06 IF 07 EQU %COMP% ECHO 07 IF 08 EQU %COMP% ECHO 08 IF 09 EQU %COMP% ECHO 09 IF 10 EQU %COMP% ECHO 10 IF 11 EQU %COMP% ECHO 11 IF 12 EQU %COMP% ECHO 12 IF 13 EQU %COMP% ECHO 13 IF 14 EQU %COMP% ECHO 14 IF 15 EQU %COMP% ECHO 15 IF 16 EQU %COMP% ECHO 16 IF 17 EQU %COMP% ECHO 17 IF 18 EQU %COMP% ECHO 18 IF 19 EQU %COMP% ECHO 19 IF 20 EQU %COMP% ECHO 20 ``` It would appear that 08 & 09 are being compared like string but why would 07 not be compared as a string if that is the case.",
    "author_id":5298,
    "publication_date":1754403327000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"Paul Birtle",
    "author_reputation":161.0,
    "tags":"batch-file",
    "text_length":2696,
    "title_length":61,
    "num_tags":1
  },
  {
    "id":5765,
    "title":"Pre-running C++ project parsing from command line for VS Code development",
    "link":"https:\/\/stackoverflow.com\/questions\/79726087\/pre-running-c-project-parsing-from-command-line-for-vs-code-development",
    "text":"I have a c++ project that I am working on on a Linux remote machine. I would like to use VS Code to connect to the machine and do my work. The problem is that this is a shared server, where load-intensive tasks are meant to be run inside compute sessions that can be requested via a load-sharing facillity. Without any additional configuration, VS Code is doing the IntelliSense\/Code-navigation parsing on the login node, which is meant to be shared among all users. I would like to pre-run that parsing in a special compute session from command line in a compute session. Alternatively, I would like to start the vscode-server manually, inside such a compute session, and have my vscode client connect to it.",
    "author_id":5297,
    "publication_date":1754403346000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"ianos",
    "author_reputation":153.0,
    "tags":"intellisense, vscode-remote",
    "text_length":709,
    "title_length":73,
    "num_tags":2
  },
  {
    "id":5764,
    "title":"Is it expected that vmapping over different input sizes for the same function impacts the accuracy of the result?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726091\/is-it-expected-that-vmapping-over-different-input-sizes-for-the-same-function-im",
    "text":"I was suprised to see that depending on the size of an input matrix, which is vmapped over inside of a function, the output of the function changes slightly. That is, not only does the size of the output change (which is what I would expect from vmapping) but also the numerics changed slightly. (Note that this only occurs in ``` float32 ``` and only on the GPU) I wrote a minimally reproducible example to illustrate the behaviour: ``` import jax import jax.numpy as jnp import equinox as eqx def equinox_vmap(x, mlp): out = eqx.filter_vmap(mlp.__call__)(x) return out key = jax.random.PRNGKey(0) key, network_key = jax.random.split(key, 2) mlp = eqx.nn.MLP(2, 2, 10, 2, key=network_key) key, key_x = jax.random.split(key, 2) x = jax.random.normal(key_x, (10000, 2)) error_eqx = equinox_vmap(x[:10], mlp) - equinox_vmap(x, mlp)[:10] print(\"eqx error:\", error_eqx) ``` When running this example I get the output: ``` eqx error: [[-1.4442205e-04 1.0999292e-04] [-5.9515238e-05 -9.1716647e-06] [ 1.4841557e-05 5.6132674e-05] [ 0.0000000e+00 0.0000000e+00] [-9.1642141e-06 -2.5466084e-05] [ 3.8832426e-05 -3.3110380e-05] [ 3.3825636e-05 -2.4946406e-05] [ 4.0918589e-05 -3.2216311e-05] [ 1.3601780e-04 8.7693334e-06] [ 0.0000000e+00 0.0000000e+00]] ``` I understand that the numerics of ``` float32 ``` are not fully accurate and some error is to be expected. However, I was suprised that the result changes depending on how much of the input array is put into the function. I was expecting that the first row of the ``` x ``` array, i.e., ``` x[0,:] ``` would still be filled with the same values and therefore the first row in the output would be the same. Further notes: I enabled the use of ``` float64 ``` ( ``` jax.config.update(\"jax_enable_x64\", False) ``` ) which completely removed this from occuring. I understand that this is a numerical problem, but I am a little bit confused how the vmapping interacts with the example. When I run the same example on the CPU (using ``` jax.config.update(\"jax_platform_name\", \"cpu\") ``` ) this problem also disappears which I also find difficult to understand. Questions: Is this to be expected? Where does this \"inconsistency\" come from? Why does it not occur on the CPU and only on the GPU? Setup: GPU: NVIDIA RTX 6000 Ada Generation 48 GB Python 3.11.11 with ``` equinox 0.13.0 jax 0.7.0 jax-cuda12-pjrt 0.7.0 jax-cuda12-plugin 0.7.0 jaxlib 0.7.0 jaxtyping 0.3.2 ml_dtypes 0.5.3 numpy 2.3.2 nvidia-cublas-cu12 12.9.1.4 nvidia-cuda-cupti-cu12 12.9.79 nvidia-cuda-nvcc-cu12 12.9.86 nvidia-cuda-nvrtc-cu12 12.9.86 nvidia-cuda-runtime-cu12 12.9.79 nvidia-cudnn-cu12 9.11.0.98 nvidia-cufft-cu12 11.4.1.4 nvidia-cusolver-cu12 11.7.5.82 nvidia-cusparse-cu12 12.5.10.65 nvidia-nccl-cu12 2.27.6 nvidia-nvjitlink-cu12 12.9.86 nvidia-nvshmem-cu12 3.3.9 opt_einsum 3.4.0 pip 24.0 scipy 1.16.1 setuptools 65.5.0 typing_extensions 4.14.1 wadler_lindig 0.1.7 ``` Any explanations are greatly appreachiated.",
    "author_id":5296,
    "publication_date":1754403437000,
    "scraped_at":1754660259000,
    "scrape_method":"api",
    "author_name":"hvater",
    "author_reputation":55.0,
    "tags":"python, gpu, jax, floating-point",
    "text_length":2938,
    "title_length":113,
    "num_tags":4
  },
  {
    "id":5763,
    "title":"Gradle\/Java\/Kotlin version conflict during EAS Build with React Native 0.76.9 + Expo SDK 52",
    "link":"https:\/\/stackoverflow.com\/questions\/79726095\/gradle-java-kotlin-version-conflict-during-eas-build-with-react-native-0-76-9",
    "text":"I'm working on a React Native 0.76.9 application using Expo SDK 52 with a custom development client due to the need for LiveKit integration (which requires native modules). However, I'm encountering a Gradle + JDK compatibility issue during an EAS Android build using: ``` eas build --profile production --platform android ``` Issue Summary: The EAS build fails with the following error during :app:assembleDebug: ``` Invalid Java installation found at '\/usr\/lib\/jvm\/openjdk-17'. Could not determine the dependencies of null. > Could not resolve all dependencies for configuration 'classpath'. > The new Java toolchain feature cannot be used at the project level in combination with source and\/or target compatibility ``` What I’ve Tried: Verified java -version locally: I'm using JDK 17 (Temurin). Cleaned build cache (cd android && .\/gradlew clean). Downgraded Kotlin from 1.9.25 → 1.6.10 to avoid conflict with Mapbox SDK (v10.1.39). Ensured Gradle wrapper is pinned to 7.6. android\/build.gradle (partial) ``` buildscript { ext { buildToolsVersion = findProperty('android.buildToolsVersion') ?: '35.0.0' minSdkVersion = Integer.parseInt(findProperty('android.minSdkVersion') ?: '24') compileSdkVersion = Integer.parseInt(findProperty('android.compileSdkVersion') ?: '35') targetSdkVersion = Integer.parseInt(findProperty('android.targetSdkVersion') ?: '34') kotlinVersion = findProperty('android.kotlinVersion') ?: '1.6.10' reactNativeCodegenPath = \"..\/node_modules\/react-native-codegen\" ndkVersion = \"26.1.10909125\" } repositories { google() mavenCentral() } dependencies { classpath 'com.google.gms:google-services:4.4.1' classpath(\"com.android.tools.build:gradle:7.3.1\") classpath('com.facebook.react:react-native-gradle-plugin') classpath('org.jetbrains.kotlin:kotlin-gradle-plugin') } } apply plugin: \"com.facebook.react.rootproject\" allprojects { repositories { maven { \/\/ All of React Native (JS, Obj-C sources, Android binaries) is installed from npm url(new File(['node', '--print', \"require.resolve('react-native\/package.json')\"].execute(null, rootDir).text.trim(), '..\/android')) } maven { \/\/ Android JSC is installed from npm url(new File(['node', '--print', \"require.resolve('jsc-android\/package.json', { paths: [require.resolve('react-native\/package.json')] })\"].execute(null, rootDir).text.trim(), '..\/dist')) } maven { url 'https:\/\/api.mapbox.com\/downloads\/v2\/releases\/maven' credentials { username = \"mapbox\" password = project.properties[\"MAPBOX_DOWNLOAD_TOKEN\"] ?: \"\" } } google() mavenCentral() maven { url 'https:\/\/www.jitpack.io' } } } ``` gradle-wrapper.properties ``` distributionBase=GRADLE_USER_HOME distributionPath=wrapper\/dists distributionUrl=https\\:\/\/services.gradle.org\/distributions\/gradle-7.6-all.zip networkTimeout=10000 validateDistributionUrl=true zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper\/dists ``` Tools & Versions: React Native: 0.76.9 Expo SDK: 52.0.47 EAS CLI: 6.14.0 JDK: Temurin OpenJDK 17.0.16 AGP: 7.3.1 Kotlin: 1.6.10 Question: How can I resolve the Gradle + Java toolchain error during EAS build? Is this caused by a mismatch between the Gradle version, Kotlin version, or JDK setup? What are the recommended kotlin and gradle version combinations for building React Native 0.76.9 with Expo SDK 52 that uses rnmapbox version 10.1.39?",
    "author_id":5295,
    "publication_date":1754403538000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"mrt_dev",
    "author_reputation":11.0,
    "tags":"react-native, expo, android-gradle-plugin",
    "text_length":3293,
    "title_length":91,
    "num_tags":3
  },
  {
    "id":5762,
    "title":"How can I overlay anti-aliased styled text on a Bitmap using Canvas in Android?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726096\/how-can-i-overlay-anti-aliased-styled-text-on-a-bitmap-using-canvas-in-android",
    "text":"I'm building an Android app where I need to overlay styled text on images using ``` Canvas ``` and ``` Paint ``` . I’ve successfully drawn basic text, but I'm running into two main issues: Text appears jagged or pixelated on high-resolution devices. Applying multiple effects like shadows, outlines, and gradients makes the rendering inconsistent. What I want to achieve: Smooth, anti-aliased text rendering. Multiple style effects (shadow, outline, gradient). Scalable text that looks clean on various screen densities. What I've tried: Using ``` Paint.ANTI_ALIAS_FLAG ``` Custom fonts via ``` Typeface.createFromAsset() ``` Adjusting canvas scaling Here is a minimal version of the code I’m using: ``` Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.my_image); Bitmap mutableBitmap = bitmap.copy(Bitmap.Config.ARGB_8888, true); Canvas canvas = new Canvas(mutableBitmap); Paint paint = new Paint(Paint.ANTI_ALIAS_FLAG); paint.setColor(Color.WHITE); paint.setTextSize(60); paint.setShadowLayer(10, 5, 5, Color.BLACK); canvas.drawText(\"Test Text\", 100, 100, paint); imageView.setImageBitmap(mutableBitmap);``` ```",
    "author_id":5294,
    "publication_date":1754403586000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Salu Tech",
    "author_reputation":11.0,
    "tags":"android, canvas, bitmap, custom-font, text-rendering",
    "text_length":1136,
    "title_length":79,
    "num_tags":5
  },
  {
    "id":5761,
    "title":"How to modify the inbuilt example and exercise environments in bookdown for LaTeX \/ PDF output",
    "link":"https:\/\/stackoverflow.com\/questions\/79726097\/how-to-modify-the-inbuilt-example-and-exercise-environments-in-bookdown-for-late",
    "text":"In my ``` style.css ``` file, I am able to introduce the following, which works for the HTML output: ``` .exercise { padding: 1em 1em 1em 1em; background: #bfe2e3 5px center\/3em no-repeat; border-radius: 15px; margin-bottom: 10px; } .example { padding: 1em 1em 1em 1em; background: #dadada 5px center\/3em no-repeat; border-radius: 15px; margin-bottom: 10px; } ``` Unfortunately, I cannot seem to find any guidance on replicating the above in ``` preamble.tex ``` for PDF output. I am able to happily add a simple colour box as follows: ``` \\definecolor{spr}{HTML}{FFC300} \\newtcolorbox{sprbox}{ colback=white, colframe=spr, coltext=black, boxsep=5pt, arc=4pt} ``` That obviously does not work for examples and exercises as they are \"built in\". Any help much appreciated.",
    "author_id":5293,
    "publication_date":1754403603000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"suknat",
    "author_reputation":331.0,
    "tags":"r-markdown, bookdown, latex",
    "text_length":770,
    "title_length":94,
    "num_tags":3
  },
  {
    "id":5760,
    "title":"Can&#39;t make https post requests with ESP32 using EthernetENC and SSLClient libraries",
    "link":"https:\/\/stackoverflow.com\/questions\/79726098\/cant-make-https-post-requests-with-esp32-using-ethernetenc-and-sslclient-librar",
    "text":"In my project I'm using my custom PCB with ESP32 pico-D4 and ENJ28J60 ethernet IC. I'm trying to make https post request to my Windows Server periodically. I'm coding on platformIO using Arduino framework. When I do https request on my own server with the same configuration and same code there is no problem. I send a request every 2 seconds without failure. But when I try to do the same thing with my customer's server there are only 4 succesful requests. After that I get connection failure. If I change UIP_CONF_MAX_CONNECTIONS setting (default 4 connections) in uipethernet-conf.h to another value, I can make that much succesful connections. - I have tried to use other ssl libraries with no success. - I have tried diffrent http keep-alive values with no success - I have tried using wifi and same result - I have tried to use http_client.setInsecure() and with using certificate but nothing changed. I think the problem is Windows server is not closing connection properly and I can't do more requests after number of UIP_CONF_MAX_CONNECTIONS. But I am not able to configure anything on my customers windows server. And I tried the same request with Postman on my windows pc, and there is no problem. Every request is returned with the value I expect. There are no connection errors. Libraries I use: jandrassy\/EthernetENC@^2.0.5 digitaldragon\/SSLClient@^1.3.2 arduino-libraries\/ArduinoHttpClient@^0.6.1 My code: ``` if (millis() - lastnnn >= 2000) { Serial.println(\"Making post request...\"); String body = \"{\"userName\": \"\" + qr_user + \"\",\"password\": \"\" + qr_password + \"\",\"idendityId\": \"\" + \"11223344\" + \"\",\"cihazId\": \"\" + EUI + \"\"}\"; http_client.sendHeader(\"Cache-Control\",\"no-cache\"); http_client.sendHeader(\"Connection\",\"close\"); http_client.post(\"\/api\/MobilApi\/BarkodeReader\",\"application\/json\",body); int status_code = http_client.responseStatusCode(); String response = http_client.responseBody(); Serial.print(\"Status code: \"); Serial.println(status_code); Serial.print(\"Response: \"); Serial.println(response); http_client.stop(); Ethernet.maintain(); lastnnn = millis(); } ``` Output: ``` Making post request... ``` ``` Status code: 200 ``` ``` Response: false ``` ``` Making post request... ``` ``` Status code: 200 ``` ``` Response: false ``` ``` Making post request... ``` ``` Status code: 200 ``` ``` Response: false ``` ``` Making post request... ``` ``` Status code: 200 ``` ``` Response: false ``` ``` [ 21685][E][ssl__client.cpp:400] init_tcp_connection(): Connection to server failed, is the signal good, server available at this address and timeout sufficient? kresapi.teias.gov.tr:443 ``` ``` [ 21702][E][ssl__client.cpp:45] _handle_error(): [start_ssl_client():370]: (-2) BIGNUM - An error occurred while reading from or writing to a file ``` ``` [ 21715][E][SSLClient.cpp:242] connect(): start_ssl_client failed: 0 ```",
    "author_id":5292,
    "publication_date":1754403688000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Kerem",
    "author_reputation":9.0,
    "tags":"arduino, esp32, https, platformio",
    "text_length":2849,
    "title_length":87,
    "num_tags":4
  },
  {
    "id":5759,
    "title":"Why is the script in the Cloudflare worker&#39;s online editor not updated after automatic deployment?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726099\/why-is-the-script-in-the-cloudflare-workers-online-editor-not-updated-after-aut",
    "text":"When using git to push a commit to online repo (Gitlab|Github) and have CI\/CD deploy the index.js file to the Cloudflare Worker automatically, after a few automatic deployments and manual edits in the Online Editor that provides live feedback to code-changes, eventually, the editor starts showing and older version of the code. (seemingly.) Update version number in comment. Save file. Commit File on proper branch linked to your CD. Push file. Wait 'build'\/deploy step at Cloudflare worker's interface \\ Deployments is complete. Open Online Editor. Review code and find something else than the code you JUST committed in git. ( Unexpected ) So the code in git after deployment is not the same as the code marked 'latest' in the online editor. It looks like an older version. This Q&A is added to prevent other users from running in the same circles. Even LLMs will send you in the wrong direction because so much could be configured incorrect in the pipeline. And LLMs are not great if the user does not realize yet, they are asking the wrong questions. The LLM was convinced there were two versions: a production environment version and a development environment version or convinced there was a button that was not there. Rephrased related questions that suffer a bit from not-yet-seeing-the-problem: How to make sure the Cloudflare worker script is also updated after deployment via git? ( ``` npx wrangler deploy ? ``` ) Why isn't the script online identical to the one in git after deployment? Why is the Cloudflare worker updated, but the online editor still showing an older version? Why am I not seeing the updated version number in comment in the online editor code after deployment via git? How to deploy to Cloudflare's online editor as well as production?",
    "author_id":5291,
    "publication_date":1754403715000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"A71",
    "author_reputation":167.0,
    "tags":"javascript, continuous-deployment, cloudflare-workers, esbuild, wrangler",
    "text_length":1769,
    "title_length":102,
    "num_tags":5
  },
  {
    "id":5758,
    "title":"How to reset a Choices.js select element on modal reopen?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726102\/how-to-reset-a-choices-js-select-element-on-modal-reopen",
    "text":"I'm using the Choices.js library inside a partial view that's rendered inside a Bootstrap modal. The first time I open the modal, everything works correctly. But when I close and reopen it, the dropdowns enhanced with Choices.js still display the previously selected values, even though the backend sends back empty values. It seems like Choices.js is caching the state or not fully resetting the UI. To fix this, I tried manually clearing the dropdowns each time the modal opens. For example, in the health care partial, I used: ``` var gsmSelect = document.querySelector('#healthCareModal select[name=\"GSMCountryCode\"]'); if (gsmSelect && gsmSelect._choicesInstance) { gsmSelect._choicesInstance.removeActiveItems(); gsmSelect._choicesInstance.setChoiceByValue(\"\"); } ``` Similarly, in the custom tour partial, I did: ``` var gsmSelect = document.querySelector('#customTourModal select[name=\"GSMCountryCode\"]'); if (gsmSelect && gsmSelect._choicesInstance) { gsmSelect._choicesInstance.removeActiveItems(); gsmSelect._choicesInstance.setChoiceByValue(\"\"); } var adultSelect = document.querySelector('#customTourModal select[name=\"NumberOfAdult\"]'); if (adultSelect && adultSelect._choicesInstance) { adultSelect._choicesInstance.removeActiveItems(); adultSelect._choicesInstance.setChoiceByValue(\"\"); } var howDidYouSelect = document.querySelector('#customTourModal select[name=\"HowDidYouFindUsID\"]'); if (howDidYouSelect && howDidYouSelect._choicesInstance) { howDidYouSelect._choicesInstance.removeActiveItems(); howDidYouSelect._choicesInstance.setChoiceByValue(\"\"); } ``` How can I fully reset Choices.js instances so that everything is visually clean every time the modal is reopened?",
    "author_id":5290,
    "publication_date":1754403780000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"miraykaragoz",
    "author_reputation":1.0,
    "tags":"choices.js",
    "text_length":1691,
    "title_length":57,
    "num_tags":1
  },
  {
    "id":5757,
    "title":"Detecting flipped YUV_420_888 buffers with Camera2 API",
    "link":"https:\/\/stackoverflow.com\/questions\/79726103\/detecting-flipped-yuv-420-888-buffers-with-camera2-api",
    "text":"I am having a strange issue where a single phone (Redmi Note 11 5G) has a flipped UV buffer on the front camera when I take an image. This causes the skin color to turn blue Currently I obtain the buffer via below ``` ImageReader.OnImageAvailableListener { it -> val image = it.acquireLatestImage() val yBuffer = image.planes[0].buffer val yPixelStride = image.planes[0].pixelStride val yRowStride = image.planes[0].rowStride val uBuffer = image.planes[1].buffer val uPixelStride = image.planes[1].pixelStride val uRowStride = image.planes[1].rowStride val vBuffer = image.planes[2].buffer val vPixelStride = image.planes[2].pixelStride val vRowStride = image.planes[2].rowStride } ``` However, on this phone, the ubuffer and v buffer are swapped on the front camera and causes skin color to turn blue as shown in the image. Blue skin . The back camera is perfectly fine using the same code. I need some help to figure out how to detect this issue as I must use YUV_420_888 for this. An idea suggested was: Get a JPEG frame Get a RAW format of the same frame Convert the RAW frame to I420, YV12, NV21, and NV12. a. Explanation of these formats here https:\/\/github.com\/android\/camera-samples\/blob\/ffa2a265197e8adc9d7cbfa3c1e4bbbca9f778e6\/CameraUtils\/lib\/src\/main\/java\/com\/example\/android\/camera\/utils\/Yuv.kt#L12 b. https:\/\/github.com\/gordinmitya\/yuv2buf?tab=readme-ov-file Compare to the JPEG frame and return whatever format that matches the JPEG color buffers Is the above possible? Will it take a lot of resources to do the conversion and comparisons?",
    "author_id":5289,
    "publication_date":1754403789000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Jonathan",
    "author_reputation":1.0,
    "tags":"android, kotlin, camera, android-camera2",
    "text_length":1553,
    "title_length":54,
    "num_tags":4
  },
  {
    "id":5756,
    "title":"Play Pac4j latest version overriding response headers",
    "link":"https:\/\/stackoverflow.com\/questions\/79726104\/play-pac4j-latest-version-overriding-response-headers",
    "text":"I have noticed that the Cache-Control header I set in the controller is getting overridden to ``` no-cache, no-store, max-age=0, must-revalidate ``` by the ``` org.pac4j.play.java.SecureAction#internalCall ``` This happens when I migrate from play-pac4j 12.0.0 to 12.0.2. The headers are being overridden to the headers set in the CacheControlMatcher. The matcher has excluded the below file types ``` `if (!url.endsWith(\".css\") && !url.endsWith(\".js\") && !url.endsWith(\".png\") && !url.endsWith(\".jpg\") && !url.endsWith(\".ico\") && !url.endsWith(\".jpeg\") && !url.endsWith(\".bmp\") && !url.endsWith(\".gif\")) { webContext.setResponseHeader(\"Cache-Control\", \"no-cache, no-store, max- age=0, must-revalidate\"); webContext.setResponseHeader(\"Pragma\", \"no-cache\"); webContext.setResponseHeader(\"Expires\", \"0\"); }` ``` Can I know why this is enforced? Why haven't they excluded certain file types like .svg, .pdf, etc.? How to stop my headers from getting updated?",
    "author_id":5288,
    "publication_date":1754403901000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"EwanthaUdagamaPronto",
    "author_reputation":11.0,
    "tags":"pac4j",
    "text_length":955,
    "title_length":53,
    "num_tags":1
  },
  {
    "id":5755,
    "title":"Custom XMP namespace appears in logs, but not visible in File Info → Raw Data",
    "link":"https:\/\/stackoverflow.com\/questions\/79726105\/custom-xmp-namespace-appears-in-logs-but-not-visible-in-file-info-%e2%86%92-raw-data",
    "text":"``` const { app } = require(\"indesign\"); const { XMPMeta } = require(\"uxp\").xmp; const doc = app.activeDocument; const xmp = new XMPMeta(doc.metadataPreferences.rawXMP); console.log(\"xmp\"); console.log(xmp); var namespaceURI = \"https:\/\/test.com\/\"; var prefix = \"test\"; \/\/ define new namespace XMPMeta.registerNamespace(namespaceURI, prefix); xmp.setProperty(namespaceURI, \"creator\", \"test\"); xmp.setProperty(namespaceURI, \"website\", \"https:\/\/test.com\/\"); xmp.setProperty(namespaceURI, \"email\", \"test@gmail.com\"); doc.metadataPreferences.rawXMP = xmp.serialize(); console.log(\"Updated rawXMP:\", xmp.serialize()); console.log(\"creator:\", xmp.getProperty(namespaceURI, \"creator\")); console.log(\"website:\", xmp.getProperty(namespaceURI, \"website\")); console.log(\"email:\", xmp.getProperty(namespaceURI, \"email\")); ``` But the custom namespace and fields don’t appear in raw data. Note: the custom namespace and fields are successfully serialized and logged, but they do not appear in raw data in the InDesign UI. Is this expected behavior for custom namespaces in UXP? How can I verify or expose custom XMP metadata in the InDesign UI (or is this restricted to standard namespaces only)? InDesign UXP does not run in a browser environment, so browser-specific features (like window, XML, etc.) will throw errors — see attached screenshot for the error: https:\/\/i.sstatic.net\/xFtJ00Yi.png",
    "author_id":5287,
    "publication_date":1754403949000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"ashwin lohiya",
    "author_reputation":11.0,
    "tags":"adobe-indesign, uxp-script",
    "text_length":1382,
    "title_length":77,
    "num_tags":2
  },
  {
    "id":5754,
    "title":"After browser back click, showing page content is still from previous route for 4 secs but url is correct. Its happening after upgrading nextjs 15.4.3",
    "link":"https:\/\/stackoverflow.com\/questions\/79726109\/after-browser-back-click-showing-page-content-is-still-from-previous-route-for",
    "text":"Issue Description: After upgrading Next.js from 15.3.1 to 15.4.3 and sitecore-jss from 22.7.0 to 22.8.0 (as recommended to resolve a critical vulnerability reported by Wiz in the form-data@4.0.3 dependency), we started facing the following issue: When navigating back using the browser history, especially from pages that include a search component (with API request\/response logic), the page content flickers or flashes before rendering the previous content. The issue was not present before the upgrade. This is a regression, and seems to be triggered by pages that perform client-side data fetching or conditional rendering based on search results. for example, After upgrading to next@15.4.3, I’ve noticed a significant issue when using the browser's Back button. ``` When navigating from Page A to Page B, and then pressing the Back button, ``` The URL updates correctly to Page A immediately. However, Page B’s content remains visible for ~4 seconds before Page A finally renders. Actual Behavior: Upon pressing the browser’s Back button: -Showing page content is still from previous route for 4 secs but current page url is correct. Expected Behavior: Upon pressing the browser’s Back button: -The URL and the visible content should both(pageUrl and pageContent) update immediately to match the previous route. -The page should restore the previous route without flickering and should be display relevant page content only. Versions Info: Next.js Version: 15.4.3 sitecore-jss Version: 22.8.0 Additional context: The upgrade was necessary due to a critical Wiz vulnerability in form-data@4.0.3, which was pulled via sitecore-jss. We tried multiple workarounds, but the issue persists. Can anyone help on this to proceed further",
    "author_id":5286,
    "publication_date":1754404139000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Sithu05",
    "author_reputation":146.0,
    "tags":"nextjs-dynamic-routing, nextjs-15",
    "text_length":1733,
    "title_length":150,
    "num_tags":2
  },
  {
    "id":5753,
    "title":"How do I check, using CMake, whether _Float16 is supported by my compiler?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726110\/how-do-i-check-using-cmake-whether-float16-is-supported-by-my-compiler",
    "text":"I have a C project configured with CMake. Some program within this project uses ``` _Float16 ``` (a \"half-precision\" type). I know how to determine, within the code, whether _Float16 is available: How to correctly determine at compile time that _Float16 is supported? but I want to use CMake and avoid trying to build that program if ``` _Float16 ``` cannot be used. How can I do that? I was thinking about a ``` CheckTypeSize() ``` -based solution, but that might not include relevant headers.",
    "author_id":4608,
    "publication_date":1754404150000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"einpoklum",
    "author_reputation":135725.0,
    "tags":"cmake, half-precision-float, buildconfiguration",
    "text_length":494,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":5752,
    "title":"Disable Codec2 logs when running a pipeline with qtic2venc on QRB5165",
    "link":"https:\/\/stackoverflow.com\/questions\/79726115\/disable-codec2-logs-when-running-a-pipeline-with-qtic2venc-on-qrb5165",
    "text":"When I run a pipeline with the ``` qtic2venc ``` When I run the encoder on the QRB5165 (Ubuntu 20.04), I receive numerous Codec2 logs in the console. I tried several methods to disable these logs, but none of them worked. Command: ``` gst-launch-1.0 videotestsrc ! video\/x-raw,format=NV12,width=1920,height=1080 ! qtic2venc ! fakesink ``` Log output: ``` 32:24.233 43601 43601 W QC2TargetSpec: vendor spec path: \/vendor\/etc\/video_system_specs.json 32:24.233 43601 43601 I QC2TargetSpec: Video Core Features: 32:24.233 43601 43601 I QC2TargetSpec: dec_secure_static_count : 0 32:24.233 43601 43601 I QC2TargetSpec: enc_hier_b_max_temporal_layer_count : 4 32:24.233 43601 43601 I QC2TargetSpec: Codec plugins: 32:24.233 43601 43601 I QC2TargetSpec: c2.qti.vp8.encoder 32:24.233 43601 43601 I QC2TargetSpec: c2.qti.heic.encoder 32:24.233 43601 43601 I QC2TargetSpec: c2.qti.avc.encoder 32:24.233 43601 43601 I QC2TargetSpec: c2.qti.vp9.decoder 32:24.233 43601 43601 I QC2TargetSpec: c2.qti.hevc.decoder.secure 32:24.234 43601 43601 I QC2Registry: Added Factory[0x55ac482010] for c2.qti.hevc.decoder.low_latency 32:24.234 43601 43601 I QC2Registry: Added codec c2.qti.hevc.decoder.low_latency 32:24.306 43601 43601 E QC2CompStore: Failed to get capabilities from component c2.qti.vp8.encoder 32:24.306 43601 43601 I QC2CompStore: initialized store (qti.c2.store) with 15 components Setting pipeline to PAUSED ... Pipeline is PREROLLING ... 32:24.318 43601 43618 I QC2Comp: Create: Allocated component[0] for name c2.qti.avc.encoder : [64 bit] 32:24.318 43601 43618 I QC2CompStore: Created component(c2.qti.avc.encoder) id(0) gbm_create_device(192): Info: backend name is: msm_drm 32:24.323 43601 43618 W StandardCaps: Preconditions for b-frame didn't meet. Disabling b-frame! 32:24.323 43601 43620 I QC2Registry: Build pipelined codec for session 32:24.323 43601 43620 I QC2Registry: Getting stages for pipelined codec 32:24.324 43601 43620 I QC2Registry: Pipelining not enabled 32:24.324 43601 43620 I QC2Registry: Doesn't support pipelining. Create standalone codec 32:24.324 43601 43620 I QC2V4l2Driver: [avcE_0] Device \/dev\/video33 opened with fd: 11 32:24.325 43601 43620 E QC2V4l2Driver: [avcE_0] failed to set buffer size limit to 4 32:24.325 43601 43620 E QC2V4l2Caps: c2 format not found for v4l2 format 0x34363248 32:24.325 43601 43620 E QC2V4l2Caps: c2 format not found for v4l2 format 0x43564548 32:24.373 43601 43620 E QC2Interface: Failed to query parameters 32:24.384 43601 43620 E QC2Interface: Failed to query parameters Pipeline is PREROLLED ... Setting pipeline to PLAYING ... New clock: GstSystemClock 32:24.827 43601 43620 I QC2Comp: [avcE_0] Stats: Pending(2) i\/p-done(1) Works: Q: 65\/Done 63|Work-Rate: Q(127.9\/s Avg=127.9\/s) Done(123.937\/s Avg=123.937\/s)| Stream: 30.00fps 21.3Mbps Mem-usage: [In-2D - 65 bufs 292.500 MB] [1D-0 - 65 bufs 194.238 MB] [1D-0 - 2 bufs 0.008 MB] Total Mem-usage: 486.746 MB ``` What I have tried: Setting ``` GST_DEBUG=0 ``` — the Codec2 logs are still shown. Switching to other encoder elements: ``` v4l2h264enc ``` — but it lacks hardware encoder support. ``` omxh264enc ``` — not available in Ubuntu 20.04. Setting environment variables as described in the Qualcomm documentation — did not work. Reference: Qualcomm Video Debug documentation Redirecting logs to ``` \/dev\/null ``` — works, but it hides both the Codec2 logs and my own application logs, which is not acceptable. Question: How can I disable only the Codec2 logs from ``` qtic2venc ``` without disabling or hiding my own application logs?",
    "author_id":5285,
    "publication_date":1754404454000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Ho&#224;ng Trần Minh",
    "author_reputation":9.0,
    "tags":"logging, linux, video-streaming, pipeline, gstreamer",
    "text_length":3553,
    "title_length":69,
    "num_tags":5
  },
  {
    "id":5751,
    "title":"How to run GA in pygad package?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726117\/how-to-run-ga-in-pygad-package",
    "text":"I am running pygad.GA and getting an error: ``` ValueError Traceback (most recent call last) Cell In[28], line 2 1 # Create GA instance ----> 2 ga_instance = pygad.GA( 3 gene_space=gene_space, 4 num_generations=100, 5 num_parents_mating=10, 6 fitness_func=fitness_func, 7 sol_per_pop=100, 8 num_genes=len(clients_products_selected_for_optimization), 9 parent_selection_type=\"sss\", 10 crossover_type=\"single_point\", 11 mutation_type=\"random\", 12 mutation_percent_genes=20, 13 ) File c:\\Users\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pygad\\pygad.py:1339, in GA.__init__(self, num_generations, num_parents_mating, fitness_func, fitness_batch_size, initial_population, sol_per_pop, num_genes, init_range_low, init_range_high, gene_type, parent_selection_type, keep_parents, keep_elitism, K_tournament, crossover_type, crossover_probability, mutation_type, mutation_probability, mutation_by_replacement, mutation_percent_genes, mutation_num_genes, random_mutation_min_val, random_mutation_max_val, gene_space, gene_constraint, sample_size, allow_duplicate_genes, on_start, on_fitness, on_parents, on_crossover, on_mutation, on_generation, on_stop, save_best_solutions, save_solutions, suppress_warnings, stop_criteria, parallel_processing, random_seed, logger) 1337 self.logger.exception(e) 1338 # sys.exit(-1) -> 1339 raise e File c:\\Users\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pygad\\pygad.py:431, in GA.__init__(self, num_generations, num_parents_mating, fitness_func, fitness_batch_size, initial_population, sol_per_pop, num_genes, init_range_low, init_range_high, gene_type, parent_selection_type, keep_parents, keep_elitism, K_tournament, crossover_type, crossover_probability, mutation_type, mutation_probability, mutation_by_replacement, mutation_percent_genes, mutation_num_genes, random_mutation_min_val, random_mutation_max_val, gene_space, gene_constraint, sample_size, allow_duplicate_genes, on_start, on_fitness, on_parents, on_crossover, on_mutation, on_generation, on_stop, save_best_solutions, save_solutions, suppress_warnings, stop_criteria, parallel_processing, random_seed, logger) 429 # Number of solutions in the population. 430 self.sol_per_pop = sol_per_pop ... --> 369 if not seq: 370 raise IndexError('Cannot choose from an empty sequence') 371 return seq[self._randbelow(len(seq))] ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() ``` ``` gene_space = [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]] ``` What is unusual, my friend can run it with the same script and output on his side.",
    "author_id":5284,
    "publication_date":1754404685000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Fendi",
    "author_reputation":91.0,
    "tags":"python, numpy, pygad",
    "text_length":2663,
    "title_length":31,
    "num_tags":3
  },
  {
    "id":5750,
    "title":"Django Pagination with DetailView",
    "link":"https:\/\/stackoverflow.com\/questions\/79726119\/django-pagination-with-detailview",
    "text":"I have a library website with a database of books and their copies. ``` Book ``` and ``` BookInstance ``` have a one-to-many relationship: each ``` BookInstance ``` has a foreign key to ``` Book ``` . I show detailed information about a specific book using: ``` class BookDetailView(generic.DetailView): model = Book ``` In ``` book_detail.html ``` , I display book details (Title, Author, Genre, ISBN) and all related copies: ``` {% for copy in book.bookinstance_set.all %} {{ copy.instance_id }} - {{ copy.imprint }} - {{ copy.status }} {% endfor %} ``` This works, but it shows all copies on one page, which can be slow if there are many. Django's built-in pagination works with ``` ListView ``` , but here I am using ``` DetailView ``` . Question: How can I paginate the related ``` BookInstance ``` objects ( ``` book.bookinstance_set.all ``` ) inside a ``` DetailView ``` so that they are shown in pages on the same ``` \/library\/book\/<id> ``` detail page? I tried creating a separate ``` BookInstanceView ``` but it shows copies of all books, not just the selected one. I want pagination only for the related instances of the current book.",
    "author_id":5283,
    "publication_date":1754404744000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Artem",
    "author_reputation":29.0,
    "tags":"django-views, django-pagination",
    "text_length":1145,
    "title_length":33,
    "num_tags":2
  },
  {
    "id":5749,
    "title":"Why do scheduled notifications arrive on the same day?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726120\/why-do-scheduled-notifications-arrive-on-the-same-day",
    "text":"I'm trying to schedule a notification for several days in advance, including today, but all scheduled notifications are coming today. This is my code: ``` import 'package:flutter_local_notifications\/flutter_local_notifications.dart'; import 'package:timezone\/timezone.dart' as tz; import 'package:timezone\/data\/latest.dart' as tz; import 'package:flutter_timezone\/flutter_timezone.dart'; class NotiService { final notificationsPlugin = FlutterLocalNotificationsPlugin(); Map<String, List<int>> scheduledNotificationIds = {}; bool _isInitialized = false; bool get isInitialized => _isInitialized; \/\/ INITIALIZE Future<void> initNotification() async { if (_isInitialized) return; \/\/ prevent re-initialization \/\/ init timezone handling tz.initializeTimeZones(); final String currentTimeZone = await FlutterTimezone.getLocalTimezone(); tz.setLocalLocation(tz.getLocation(currentTimeZone)); \/\/ prepare android init settings const initSettingsAndroid = AndroidInitializationSettings( '@mipmap\/ic_launcher', ); \/\/ prepare ios init settings const initSettingsIOS = DarwinInitializationSettings( requestAlertPermission: true, requestBadgePermission: true, requestSoundPermission: true, ); \/\/ init settings const initSettings = InitializationSettings( android: initSettingsAndroid, iOS: initSettingsIOS, ); \/\/ finally, initialize the plugin! await notificationsPlugin.initialize(initSettings); } \/\/ NOTIFICATIONS DETAIL SETUP NotificationDetails notificationDetails() { return const NotificationDetails( android: AndroidNotificationDetails( 'daily_channeLid', 'Daily Notifications', channelDescription: 'Daily Notification Channel', importance: Importance.max, priority: Priority.high, ), iOS: DarwinNotificationDetails(), ); } \/\/ SHOW NOTIFICATION Future<void> showNotification({ int id = 0, String? title, String? body, }) async { return notificationsPlugin.show( id, title, body, const NotificationDetails(), ); } \/* Schedule a notification at a specified time (e.g. 11pm) - hour (0-23) - minute (0-59) *\/ Future<void> scheduleNotification({ int ?id, required String title, required String body, required int hour, required int minute, required DateTime date, }) async { final now = tz.TZDateTime.now(tz.local); var scheduledDate = tz.TZDateTime( tz.local, date.year, date.month, date.day, hour, minute, ); if (scheduledDate.isBefore(now)) { scheduledDate = scheduledDate.add(Duration(days: 1)); } await notificationsPlugin.zonedSchedule( id!, title, body, scheduledDate, notificationDetails(), androidScheduleMode: AndroidScheduleMode.inexactAllowWhileIdle, matchDateTimeComponents: DateTimeComponents.time, ); } \/\/ Cancel all notifications Future<void> cancelAllNotifications() async { await notificationsPlugin.cancelAll(); } Future<void> scheduleNotifications(String medicineName, int count, DateTime nextDose) async { DateTime now = DateTime.now(); int baseId = 1; for (int i = 0; i < count; i++) { DateTime notificationDate = DateTime( nextDose.year, nextDose.month, nextDose.day + i, nextDose.hour, nextDose.minute, ); if (notificationDate.isBefore(now)) { continue; } int notificationId = baseId + i; scheduledNotificationIds[medicineName]?.add(notificationId); await scheduleNotification.( id: notificationId, title: 'Уведомление для $medicineName', body: 'Время принять $medicineName', hour: notificationDate.hour, minute: notificationDate.minute, date: notificationDate, ); } } Future<void> cancelAllScheduledNotifications(String medicineName) async { if (scheduledNotificationIds.containsKey(medicineName)) { for (int id in scheduledNotificationIds[medicineName]!) { await notificationsPlugin.cancel(id); } scheduledNotificationIds.remove(medicineName); } } } ``` When a notification is called at the scheduled time, all notifications are called. I expected the messages to be called on the scheduled date. Let's say if I create a scheduled notification for 12:20 every 10 days, then at 12:20 there will not be one, but all 10 scheduled for 10 days in advance. In this case, they are planned for the following days flutter: Planning notification for Cipralex: 2025-08-05 13:52:00.000 with ID: 1 flutter: Planning notification for Cipralex: 2025-08-06 13:52:00.000 with ID: 2 flutter: Planning notification for Cipralex: 2025-08-07 13:52:00.000 with ID: 3 flutter: Planning notification for Cipralex: 2025-08-08 13:52:00.000 with ID: 4 flutter: Planning notification for Cipralex: 2025-08-09 13:52:00.000 with ID: 5",
    "author_id":5282,
    "publication_date":1754404758000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Antoniy274",
    "author_reputation":31.0,
    "tags":"flutter, notifications, push-notification, flutter-local-notification",
    "text_length":4419,
    "title_length":54,
    "num_tags":4
  },
  {
    "id":5748,
    "title":"How to customize validation styles on inputs?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726121\/how-to-customize-validation-styles-on-inputs",
    "text":"I am using Bootstrap 5.3 and styling according to the designer's mockup. They have the input, upon failed validation, with a thick border to the left, with no border-radius. If I were doing this manually with regular css, then it would be a matter of writing: ``` .form-control.is-invalid{ border-radius: 0px; border-left-width: 4px; }; ``` However, I am interested in learning how to use Bootstrap better. Can this also be done through Bootstrap's mixins? When I read through Bootstrap's docs, it looks like there is only ways of using their validation mixin to overwrite what is already there, rather than add to it.",
    "author_id":5281,
    "publication_date":1754404868000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"cloudy day",
    "author_reputation":17.0,
    "tags":"css, bootstrap-5",
    "text_length":618,
    "title_length":45,
    "num_tags":2
  },
  {
    "id":5747,
    "title":"Azure Logic Apps: &quot;Specified blob does not exist&quot; error when using dynamic blob name with spaces",
    "link":"https:\/\/stackoverflow.com\/questions\/79726124\/azure-logic-apps-specified-blob-does-not-exist-error-when-using-dynamic-blob",
    "text":"I'm working in Azure Logic Apps and I'm trying to dynamically update a blob in Azure Blob Storage using Azure Logic Apps workflow action named 'Update Blob (V2)'. My goal is to write an integer value to a blob in a container. The blob name is based on a customer name retrieved during a For each loop. I am using 'Customer one' as an example name. When I use a customer name with just one word (e.g., Corp.json), it's works perfectly fine. But as soon as I want to dynamically fill in a customer name with spaces, it gives error. When I hardcode the blob name (e.g., Customer one.json), it works fine and updates the blob correctly. However, when I fill in the blob name dynamically like this: ``` encodeUriComponent(concat(trim(string(items('For_each')?['customerName'])), '.json')) ``` I consistently get the following error: ``` \"body\": { \"status\": 404, \"message\": \"Specified blob Customer one.json does not exist.\\r\\nclientRequestId: 11111111-1111-1111-1111-11111111111\", \"error\": { \"message\": \"Specified blob Customer one.json does not exist.\" }, \"source\": \"azureblob-we.azconn-we-223.p.azurewebsites.net\" } ``` What I've already tried: Wrapping the customer name with string() and trim() to prevent datatype or whitespace issues Using encodeUriComponent() to handle spaces Confirmed that the blob exists and can be updated. I did update the blob using hardcoded changes Verified that the blob is in the correct container and directory Used the Compose action to print the final blob name (= identical) Additional Info: The blob names do include spaces, like \"Van Der Dadels.json\". The blob is confirmed to exist and be accessible. Logic Apps is using the \"Update blob\" action. Container access level is private, but credentials and connection are configured correctly. This is where the error occurs: But hard coding the name works:",
    "author_id":5280,
    "publication_date":1754404970000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Denver",
    "author_reputation":11.0,
    "tags":"azure, azure-blob-storage, azure-logic-apps",
    "text_length":1838,
    "title_length":106,
    "num_tags":3
  },
  {
    "id":5746,
    "title":"@RecordApplicationEvents (still) not working for async events",
    "link":"https:\/\/stackoverflow.com\/questions\/79726127\/recordapplicationevents-still-not-working-for-async-events",
    "text":"I tried to switch our application to use Async Events. Now our event publisher tests fail. I found a few posts mentioning this https:\/\/github.com\/spring-projects\/spring-framework\/pull\/30020 So with current versions of Spring(Boot) this should be fixed, but it still doesn't work. We are using SpringBoot 3.4.1 ``` @SpringBootTest @RecordApplicationEvents class MyEventPublisherTest { @Autowired private ApplicationEvents events; @Autowired private MyEventPublisher testSubject; @BeforeEach void setUp() { events.clear(); } @Test void validPayload_publishEvent_eventCreated() { \/\/ given final MyId id = MyId.fromUuid(UUID.randomUUID()); final MyPayload payload = MyPayload.create(id); \/\/ when testSubject.publishEvent(payload); \/\/ then assertThat(events.stream(MyPayloadEvent.class).count()).isEqualTo(1); } } ``` Do I need to add some additional annotations or configurations to the test? This test fails with there were 0 events. This works with synchronous events.",
    "author_id":5279,
    "publication_date":1754405065000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Thomas",
    "author_reputation":7200.0,
    "tags":"spring-boot, events, asynchronous",
    "text_length":966,
    "title_length":61,
    "num_tags":3
  },
  {
    "id":5745,
    "title":"How to obtain full GraphQL specification without Introspection?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726128\/how-to-obtain-full-graphql-specification-without-introspection",
    "text":"Let's say administrator is hosting a Wiki.js instance and can distribute GraphQL API access to authorized developers. Consdering that introspection is disabled, the developers cannot obtain the full schema\/specification of the GraphQL API. The official documentation is also quite limited in terms of examples. What options do developers have to understand full specification of this API in this case? Considering that Introspection can't be enabled just for some people and not for others, is there a way for the administrator to provide a GraphQL schema via other means or how would it work?",
    "author_id":5278,
    "publication_date":1754405136000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"estranged",
    "author_reputation":432.0,
    "tags":"graphql, wiki",
    "text_length":593,
    "title_length":63,
    "num_tags":2
  },
  {
    "id":5744,
    "title":"Integrating a text input to blender",
    "link":"https:\/\/stackoverflow.com\/questions\/79726132\/integrating-a-text-input-to-blender",
    "text":"I am implementing an ``` .bpy ``` for an input module to ``` blender ``` using ``` pycharm ``` and am running into an issue with the ``` .bpy ``` not installing properly. I'll be bashing this for a bit, but wanted to start a thread involving font requirements in the system transposer while using a custom font install. There is an issue with they ``` .bpy ``` not initiating and I will post my updates in my progress. Used Junie to construct the passthrough for the base interface to input 2 lines of text. Utilized a custom font installed in a general location. Import text from UI to a base layer as objects converted to meshes. Create: Curve a ( 0, 0) Curve: Align text to curve. Text_1: Set rotation to X90 Text_2: Set Rotation to x90 Junie script compiled for module || output in blender compiler ``` classes = ( TextImporterProperties, TEXT_PT_importer_panel, TEXT_OT_import_to_blender, ) # Registration def register(): for cls in classes: bpy.utils.register_class(cls) bpy.types.Scene.text_importer_props = bpy.props.PointerProperty(type=TextImporterProperties) def unregister(): for cls in reversed(classes): bpy.utils.unregister_class(cls) del bpy.types.Scene.text_importer_props import bpy.ops.curve.primitive_bezier_circle_add (radius=1, enter_editmode+False, align_'WORLD', location=(0, 0, 0), scale=(1, 1, 1)) as bpy import os from bpy.props import StringProperty # Custom properties to store the text field values class TextImporterProperties(bpy.types.PropertyGroup): text_field_1: StringProperty( name=\"Text Field 1\", description=\"Enter text for field 1\", default=\"\" ) text_field_2: StringProperty( name=\"Text Field 2\", description=\"Enter text for field 2\", default=\"\" ) # Panel to display the text fields class TEXT_PT_importer_panel(bpy.types.Panel): bl_label = \"Text Importer\" bl_idname = \"TEXT_PT_importer\" bl_space_type = 'VIEW_3D' bl_region_type = 'UI' bl_category = 'Text Importer' def draw(self, context): layout = self.layout scene = context.scene text_props = scene.text_importer_props # Text Field 1 layout.label(text=\"Text Field 1:\") layout.prop(text_props, \"text_field_1\", text=\"\") # Text Field 2 layout.label(text=\"Text Field 2:\") layout.prop(text_props, \"text_field_2\", text=\"\") # Import button layout.separator() layout.operator(\"text.import_to_blender\", text=\"Import Text\") # Operator to handle the import action class TEXT_OT_import_to_blender(bpy.types.Operator): bl_idname = \"text.import_to_blender\" bl_label = \"Import Text to Blender\" bl_description = \"Import the text from both fields into Blender\" bl_options = {'REGISTER', 'UNDO'} def execute(self, context): scene = context.scene text_props = scene.text_importer_props text1 = text_props.text_field_1 text2 = text_props.text_field_2 # Process text from Field 1 if text1: self.process_field1_text(context, text1) # Process text from Field 2 if text2: self.process_field2_text(context, text2) self.report({'INFO'}, \"Text imported and processed successfully\") return {'FINISHED'} def create_text_object(self, context, text_content, name, location): # Create a new text curve data block text_curve = bpy.data.curves.new(type=\"FONT\", name=name) text_curve.body = text_content # Create a new object with the text curve text_obj = bpy.data.objects.new(name, text_curve) text_obj.location = location # Link the object to the current collection context.collection.objects.link(text_obj) return text_obj def create_cylinder(self, context, name): # Add a cylinder bpy.ops.mesh.primitive_cylinder_add(vertices=146, radius=2.5, depth=4.0) cylinder = context.active_object cylinder.name = name # Set cap fill type to \"Nothing\" bpy.ops.object.mode_set(mode='EDIT') bpy.ops.mesh.select_all(action='SELECT') bpy.ops.mesh.delete(type='ONLY_FACE') bpy.ops.object.mode_set(mode='OBJECT') # Set location X to 0 - Test set for offset if needed cylinder.location.x = 0.0 def process_field1_text(self, context, text_content): # Create text object text_obj = self.create_text_object(context, text_content, \"Field1Text\", (0, 0, 0)) # Select the text object bpy.ops.object.select_all(action='DESELECT') text_obj.select_set(True) context.view_layer.objects.active = text_obj ------- # Convert text to curve bpy.ops.object.convert(target='CURVE') # Create cylinder for text deformation cylinder = self.create_cylinder(context, \"Field1Cylinder\") # Select the text again bpy.ops.object.select_all(action='DESELECT') text_obj.select_set(True) context.view_layer.objects.active = text_obj # Set geometry taper object to cylinder text_obj.data.taper_object = cylinder # Add curve modifier curve_modifier = text_obj.modifiers.new(name=\"Curve\", type='CURVE') curve_modifier.object = cylinder # Set rotation X to -90 degrees text_obj.rotation_euler.x = -1.5708 # -90 degrees in radians return text_obj def process_field2_text(self, context, text_content): # Create text object text_obj = self.create_text_object(context, text_content, \"Field2Text\", (0, 0, 0)) # Select the text object bpy.ops.object.select_all(action='DESELECT') text_obj.select_set(True) context.view_layer.objects.active = text_obj ------- # Convert text to curve bpy.ops.object.convert(target='CURVE') # Create cylinder for text deformation cylinder = self.create_cylinder(context, \"Field2Cylinder\") # Select the text again bpy.ops.object.select_all(action='DESELECT') text_obj.select_set(True) context.view_layer.objects.active = text_obj # Set geometry taper object to cylinder text_obj.data.taper_object = cylinder # Add curve modifier curve_modifier = text_obj.modifiers.new(name=\"Curve\", type='CURVE') curve_modifier.object = cylinder # Set deform axis to -X curve_modifier.deform_axis = 'NEG_X' return text_obj if __name__ == \"__main__\": register() ``` ``` Python: Traceback (most recent call last): File \"C:\\ Foundation\\Blender\\4.1\\scripts\\addons\\_init4_.py\", line 75, in execute self.process_field1_text(context, text1) File \"C:\\ Foundation\\Blender\\4.1\\scripts\\addons\\_init4_.py\", line 132, in process_field1_text font_path = os.path.join(bpy.utils.user_resource('FONTS'), \"My decorative font.ttf\") ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C: 4.1\\4.1\\scripts\\modules\\bpy\\utils\\__init__.py\", line 749, in user_resource target_path = _user_resource(resource_type, path=path) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ValueError: expected a string in ('DATAFILES', 'CONFIG', 'SCRIPTS', 'AUTOSAVE'), got 'FONTS' ```",
    "author_id":5277,
    "publication_date":1754405224000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"William Rickard",
    "author_reputation":1.0,
    "tags":"python, blender",
    "text_length":6341,
    "title_length":35,
    "num_tags":2
  },
  {
    "id":5743,
    "title":"Unable to set up TypeScript in a .NET application that uses Bootstrap",
    "link":"https:\/\/stackoverflow.com\/questions\/79726133\/unable-to-set-up-typescript-in-a-net-application-that-uses-bootstrap",
    "text":"I am trying to set up a .NET web application that uses TypeScript along with Bootstrap. I am receiving the following error when I do a build of my application: ``` (TS) Cannot find module '@popperjs\/core'. Did you mean to set the 'moduleResolution' option to 'nodenext', or to add aliases to the 'paths' option? ``` ``` @popperjs\/core ``` does exist in my ``` node_modules ``` . The error is thrown in two different .ts files inside the ``` @types\/bootstrap\/* ``` types definition that I need to include. If I change the ``` modeResolution ``` option to ``` nodenext ``` as suggested, I run into a lot of issues down the line. So I was trying to add an alias to the paths option so that at compile time it would look in the ``` node_modules ``` folder. Here are the relevant parts of my app: ``` tsconfig.json ``` : ``` { \"compilerOptions\": { \"sourceMap\": true, \"target\": \"es2015\", \"allowJs\": true, \"outDir\": \"wwwroot\/js\/compiled\", \"alwaysStrict\": true, \"types\": [ \"bootstrap\" ], \"paths\": { \"@popperjs\/core\": [ \".\/node_modules\/@popperjs\/core\" ] } }, \"compileOnSave\": true, \"exclude\": [ \"node_modules\", \"wwwroot\" ], \"include\": [ \"javascript\/**\/*.ts\", \"node_modules\/@popperjs\/core\/index.d.ts\", \"node_modules\/@popperjs\/core\/lib\/*.ts\", \"node_modules\/@popperjs\/core\/lib\/**\/*.ts\" ] } ``` You can see above that I have tried adding ``` @popperjs\/core ``` to the paths section. And experimented with having it under the ``` include ``` section as well. ``` package.json ``` : ``` { \"version\": \"1.0.0\", \"name\": \"myapp\", \"private\": true, \"devDependencies\": { \"@types\/bootstrap\": \"5.2.10\", \"@popperjs\/core\": \"2.11.8\", \"bootstrap\": \"5.3.7\", \"typescript\": \"5.8.3\" }, \"dependencies\": { \"@types\/bootstrap\": \"5.2.10\", \"@popperjs\/core\": \"2.11.8\", \"bootstrap\": \"5.3.7\" } } ``` ``` libman.json ``` (to import bootstrap into wwwroot\/lib): ``` { \"version\": \"3.0\", \"defaultProvider\": \"cdnjs\", \"libraries\": [ { \"library\": \"bootstrap@5.3.7\", \"destination\": \"wwwroot\/lib\/bootstrap\/\" } ] } ``` And here is an example of how I am calling bootstrap in my app: ``` Javascript\/mysite.ts ``` ``` import { Modal } from 'bootstrap'; export function popupModal(someModalElement: HTMLElement): void { var mod = new Modal(someModalElement, { focus: true, keyboard: true }); } ``` Any ideas?",
    "author_id":5276,
    "publication_date":1754405244000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Mike Luken",
    "author_reputation":535.0,
    "tags":"typescript, .net, asp.net, bootstrap-5, popper.js",
    "text_length":2254,
    "title_length":69,
    "num_tags":5
  },
  {
    "id":5742,
    "title":"In Nesting timeline, how to make the start of one timeline coincide with a stage in another timeline?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726134\/in-nesting-timeline-how-to-make-the-start-of-one-timeline-coincide-with-a-stage",
    "text":"I want the overlay timeline to start animating from the text2 label. That is, when the text-2 animation starts, the overlay timeline will start running. ``` gsap.registerPlugin(ScrollTrigger) const tl = gsap.timeline({ scrollTrigger: { trigger: '.section', markers: true, pin: true, \/\/ pin the trigger element while active start: 'top top', \/\/ when the top of the trigger hits the top of the viewport end: 'bottom top', \/\/ end after scrolling 500px beyond the start scrub: true } }) tl.to('.text-1', { x: -400 }); tl.to('.text-2', { opacity: 1 }, \"<\"); tl.to('.text-2', { opacity: 0 }, \"text2\"); tl.to('.text-3', { opacity: 1 }, \"<\"); tl.to('.text-3', { opacity: 0 }); tl.to('.text-4', { opacity: 1 }, \"<\"); const tl2 = gsap.timeline({ scrollTrigger: { trigger: '.section', start: 'top top', \/\/ when the top of the trigger hits the top of the viewport end: 'bottom top', \/\/ end after scrolling 500px beyond the start scrub: true } }) tl2.to('.overlay', { opacity: 0.2 }) gsap.timeline() .add(tl) .add(tl2) ``` Schematic diagram",
    "author_id":5275,
    "publication_date":1754405274000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"user1434702",
    "author_reputation":839.0,
    "tags":"gsap",
    "text_length":1027,
    "title_length":101,
    "num_tags":1
  },
  {
    "id":5741,
    "title":"INVALID_RESOURCE_ID at https:\/\/www.paypal.com\/smart\/buttons",
    "link":"https:\/\/stackoverflow.com\/questions\/79726135\/invalid-resource-id-at-https-www-paypal-com-smart-buttons",
    "text":"I am trying to integrate Paypal into my laravel website, and i am seeing this error. help me as this project is due to submit within a 3 days. The error: ``` INVALID_RESOURCE_ID at https:\/\/www.paypal.com\/smart\/buttons click_initiate_payment_reject {err: 'Error: INVALID_RESOURCE_ID\\n at https:\/\/www.payp…false&supportsPopups=true&vault=false:1674:51923)', timestamp: '1754396869767', referer: 'www.paypal.com', sdkCorrelationID: 'prebuild', sessionID: 'uid_1b285093ff_mti6mjc6mtu'} INVALID_RESOURCE_ID at buttons?style.layout=...) ``` Button to activate the paypal buttons: ``` <button class=\"nav-link\" id=\"paypal-tab\" data-bs-toggle=\"pill\" data-bs-target=\"#paypal-payment\" type=\"button\" role=\"tab\"> <i class=\"fab fa-cc-paypal me-2\"><\/i> PayPal <\/button> <div class=\"tab-pane fade\" id=\"paypal-payment\" role=\"tabpanel\"> <!-- PayPal button will be inserted here --> <div id=\"paypal-button-container\" class=\"p-3 border rounded text-center\"> <div class=\"spinner-border text-primary\" role=\"status\"> <span class=\"visually-hidden\">Loading...<\/span> <\/div> <p class=\"mt-2 mb-0\">Loading PayPal...<\/p> <\/div> <\/div> ``` Javascript function to handle the Paypal payment ``` @if(config('paypal.mode') && config('paypal.' . config('paypal.mode') . '.client_id')) <script> \/\/ Enhanced PayPal integration document.getElementById('paypal-tab').addEventListener('shown.bs.tab', function() { \/\/ Clear previous errors document.getElementById('card-errors').textContent = ''; \/\/ Check if already loaded if (typeof paypal !== 'undefined' && typeof paypal.Buttons === 'function') { renderPayPalButton(); return; } \/\/ Show loading state document.getElementById('paypal-button-container').innerHTML = ` <div class=\"spinner-border text-primary\" role=\"status\"> <span class=\"visually-hidden\">Loading...<\/span> <\/div> <p class=\"mt-2 mb-0\">Loading PayPal...<\/p>`; \/\/ Load PayPal SDK const script = document.createElement('script'); script.src = `https:\/\/www.paypal.com\/sdk\/js?client-id={{ config('paypal.' . config('paypal.mode') . '.client_id') }}&currency=USD&components=buttons`; script.async = true; script.onload = function() { if (typeof paypal === 'undefined' || typeof paypal.Buttons !== 'function') { showPayPalError('PayPal failed to load properly'); return; } renderPayPalButton(); }; script.onerror = function() { showPayPalError('Failed to load PayPal SDK. Please refresh or try another method'); }; document.head.appendChild(script); \/\/ Add timeout for slow loading const paypalLoadTimeout = setTimeout(() => { if (!window.paypal) { showPayPalError('PayPal loading delayed. Please check your connection'); } }, 7000); }); function showPayPalError(message) { document.getElementById('paypal-button-container').innerHTML = ` <div class=\"alert alert-danger\"> <i class=\"fas fa-exclamation-triangle me-2\"><\/i> ${message} <\/div>`; } function renderPayPalButton() { try { \/\/ Clear the container document.getElementById('paypal-button-container').innerHTML = ''; \/\/ Render the button paypal.Buttons({ style: { layout: 'vertical', color: 'blue', shape: 'rect', height: 45 }, createOrder: function(data, actions) { if (!document.getElementById('agree-terms').checked) { return Promise.reject(new Error('You must agree to the terms and conditions')); } document.getElementById('button-text').textContent = 'Creating PayPal Order...'; document.getElementById('button-spinner').classList.remove('d-none'); return fetch('{{ route(\"paypal.create-order\") }}', { method: 'POST', headers: { 'Content-Type': 'application\/json', 'X-CSRF-TOKEN': '{{ csrf_token() }}' }, body: JSON.stringify({ plan_id: '{{ $plan->id }}' }) }) .then(function(res) { if (!res.ok) { throw new Error(`HTTP ${res.status}`); } return res.json(); }) .then(function(orderData) { if (!orderData.id || !orderData.status) { throw new Error('Invalid response from server'); } return orderData.id; }) .catch(function(error) { console.error('Create Order Error:', error); const errorDiv = document.getElementById('card-errors'); errorDiv.innerHTML = `<div class=\"alert alert-danger\">${error.message}<\/div>`; resetButtonState(); return Promise.reject(error); }); }, onApprove: function(data, actions) { document.getElementById('button-text').textContent = 'Processing PayPal Payment...'; document.getElementById('button-spinner').classList.remove('d-none'); return fetch('{{ route(\"paypal.capture-order\") }}', { method: 'POST', headers: { 'Content-Type': 'application\/json', 'X-CSRF-TOKEN': '{{ csrf_token() }}' }, body: JSON.stringify({ orderID: data.orderID, plan_id: '{{ $plan->id }}' }) }) .then(function(res) { if (!res.ok) { throw new Error(`HTTP ${res.status}`); } return res.json(); }) .then(function(details) { if (details.error) { throw new Error(details.error); } if (details.redirect_url) { window.location.href = details.redirect_url; } else { window.location.href = '#'; } }) .catch(function(error) { console.error('Payment Error:', error); const errorDiv = document.getElementById('card-errors'); errorDiv.innerHTML = ` <div class=\"alert alert-danger\"> <i class=\"fas fa-exclamation-triangle me-2\"><\/i> ${error.message || 'Payment processing failed'} <\/div>`; resetButtonState(); }); }, onError: function(err) { console.error('PayPal Error:', err); const errorDiv = document.getElementById('card-errors'); errorDiv.innerHTML = ` <div class=\"alert alert-danger\"> <i class=\"fas fa-exclamation-triangle me-2\"><\/i> ${err.message || 'An error occurred with PayPal. Please try again.'} <\/div>`; resetButtonState(); } }).render('#paypal-button-container'); } catch (err) { console.error('PayPal Button Initialization Error:', err); showPayPalError('Failed to initialize PayPal button'); } } function resetButtonState() { document.getElementById('button-text').textContent = `Complete Subscription for ${{ number_format($plan->price, 2) }}\/month`; document.getElementById('button-spinner').classList.add('d-none'); } \/\/ Global error handler window.addEventListener('unhandledrejection', event => { const errorDiv = document.getElementById('card-errors'); errorDiv.textContent = `System error: ${event.reason.message || event.reason}`; errorDiv.classList.add('alert', 'alert-danger'); resetButtonState(); }); <\/script> @else <script> document.getElementById('paypal-tab').addEventListener('shown.bs.tab', function() { document.getElementById('paypal-button-container').innerHTML = ` <div class=\"alert alert-danger\"> <i class=\"fas fa-exclamation-triangle me-2\"><\/i> PayPal is not configured. Please use another payment method. <\/div>`; }); <\/script> @endif ``` the controller to handle the subcription: ``` <?php namespace App\\Http\\Controllers; use Illuminate\\Http\\Request; use Srmklive\\PayPal\\Services\\PayPal as PayPalClient; use Illuminate\\Support\\Facades\\Log; use App\\Models\\Plan; use App\\Models\\MembershipSubscription; class PayPalController extends Controller { public function createOrder(Request $request) { $request->validate(['plan_id' => 'required|exists:membership_plans,id']); $plan = Plan::findOrFail($request->plan_id); try { $provider = new PayPalClient; $provider->setApiCredentials(config('paypal')); $provider->getAccessToken(); $response = $provider->createOrder([ \"intent\" => \"CAPTURE\", \"application_context\" => [ \"return_url\" => route('membership.success'), \"cancel_url\" => route('membership.checkout', $plan->id), \"brand_name\" => config('app.name'), ], \"purchase_units\" => [ [ \"reference_id\" => \"plan_\".$plan->id, \"amount\" => [ \"currency_code\" => \"USD\", \"value\" => $plan->price, ], \"description\" => \"Subscription for Plan: {$plan->name}\" ] ] ]); if (isset($response['id']) { Log::info('PayPal order created', ['order_id' => $response['id']]); return response()->json([ 'id' => $response['id'], 'status' => $response['status'], 'links' => $response['links'] \/\/ Important for PayPal JS SDK ]); } Log::error('PayPal order creation failed', ['response' => $response]); return response()->json([ 'error' => 'Could not create PayPal order', 'details' => $response ], 500); } catch (\\Exception $e) { Log::error('PayPal order creation exception', ['error' => $e->getMessage()]); return response()->json([ 'error' => $e->getMessage(), 'debug_id' => uniqid() \/\/ Important for debugging ], 500); } } public function captureOrder(Request $request) { $request->validate([ 'orderID' => 'required', 'plan_id' => 'required|exists:membership_plans,id' ]); $plan = Plan::findOrFail($request->plan_id); $user = auth()->user(); try { $provider = new PayPalClient; $provider->setApiCredentials(config('paypal')); $provider->getAccessToken(); $response = $provider->capturePaymentOrder($request->orderID); if (isset($response['status']) && $response['status'] === 'COMPLETED') { $subscription = MembershipSubscription::create([ 'user_id' => $user->id, 'plan_id' => $plan->id, 'payment_method' => 'paypal', 'payment_id' => $response['id'], 'amount' => $plan->price, 'currency' => 'USD', 'status' => 'active', 'starts_at' => now(), 'ends_at' => now()->addMonth(), 'is_recurring' => true ]); Log::info('PayPal payment captured', [ 'order_id' => $request->orderID, 'user_id' => $user->id, 'plan_id' => $plan->id, 'subscription_id' => $subscription->id, 'paypal_response' => $response ]); return response()->json([ 'status' => 'COMPLETED', 'id' => $response['id'], 'purchase_units' => [[ 'payments' => [ 'captures' => [[ 'id' => $response['id'], 'status' => 'COMPLETED', 'amount' => [ 'currency_code' => 'USD', 'value' => $plan->price ] ]] ] ]], 'debug_id' => uniqid() ]); } Log::error('PayPal payment capture failed', ['response' => $response]); return response()->json([ 'error' => 'Payment capture failed', 'details' => $response, 'debug_id' => uniqid() ], 500); } catch (\\Exception $e) { Log::error('PayPal payment capture exception', [ 'error' => $e->getMessage(), 'order_id' => $request->orderID ]); return response()->json([ 'error' => $e->getMessage(), 'debug_id' => uniqid() ], 500); } } } ``` the subscription database : ``` CREATE TABLE `membership_plans` ( `id` bigint(20) UNSIGNED NOT NULL, `name` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL, `slug` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL, `price` decimal(10,2) NOT NULL, `billing_cycle` enum('monthly','yearly') COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'monthly', `features` text COLLATE utf8mb4_unicode_ci COMMENT 'JSON array of features', `description` text COLLATE utf8mb4_unicode_ci, `is_active` tinyint(1) NOT NULL DEFAULT '1', `is_featured` tinyint(1) NOT NULL DEFAULT '0', `sort_order` int(11) NOT NULL DEFAULT '0', `created_at` timestamp NULL DEFAULT NULL, `updated_at` timestamp NULL DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; CREATE TABLE `membership_plan_features` ( `id` bigint(20) UNSIGNED NOT NULL, `plan_id` bigint(20) UNSIGNED NOT NULL, `name` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL, `code` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL, `description` text COLLATE utf8mb4_unicode_ci, `value` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL, `sort_order` int(11) NOT NULL DEFAULT '0', `created_at` timestamp NULL DEFAULT NULL, `updated_at` timestamp NULL DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; CREATE TABLE `membership_subscriptions` ( `id` bigint(20) UNSIGNED NOT NULL, `user_id` bigint(20) UNSIGNED NOT NULL, `plan_id` bigint(20) UNSIGNED NOT NULL, `payment_method` enum('stripe','paypal','card') COLLATE utf8mb4_unicode_ci NOT NULL, `payment_id` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL, `amount` decimal(10,2) NOT NULL, `currency` varchar(3) COLLATE utf8mb4_unicode_ci DEFAULT 'USD', `status` enum('active','pending','cancelled','expired','suspended') COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'pending', `starts_at` timestamp NULL DEFAULT NULL, `ends_at` timestamp NULL DEFAULT NULL, `cancelled_at` timestamp NULL DEFAULT NULL, `is_recurring` tinyint(1) NOT NULL DEFAULT '1', `trial_ends_at` timestamp NULL DEFAULT NULL, `created_at` timestamp NULL DEFAULT NULL, `updated_at` timestamp NULL DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; ``` config\/Service.php ``` 'paypal' => [ 'mode' => env('PAYPAL_MODE', 'live'), \/\/ Should be 'live' for production 'live' => [ 'client_id' => env('PAYPAL_LIVE_CLIENT_ID'), 'client_secret' => env('PAYPAL_LIVE_SECRET'), 'app_id' => env('PAYPAL_LIVE_APP_ID', ''), ], 'payment_action' => 'Sale', 'currency' => env('PAYPAL_CURRENCY', 'USD'), 'notify_url' => env('PAYPAL_NOTIFY_URL', ''), 'locale' => 'en_US', 'validate_ssl' => true, ], ```",
    "author_id":5274,
    "publication_date":1754405333000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Naimur Sharon",
    "author_reputation":85.0,
    "tags":"php, javascript, laravel, paypal, paypal-rest-sdk",
    "text_length":12450,
    "title_length":59,
    "num_tags":5
  },
  {
    "id":5740,
    "title":"Create object in Power BI",
    "link":"https:\/\/stackoverflow.com\/questions\/79726138\/create-object-in-power-bi",
    "text":"I have a dataset that contains information about availability. The availability can be null, 0, >= 1. Is it possible create in Power BI an object with the shape A and the middle the days have different colors? Example: Day 1 - 0 (green) Day 2 - 2 (red) Day 3 - null I have draft only 3 days but I need all month.",
    "author_id":5273,
    "publication_date":1754405632000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Giulia",
    "author_reputation":59.0,
    "tags":"powerbi",
    "text_length":312,
    "title_length":25,
    "num_tags":1
  },
  {
    "id":5739,
    "title":"How to create new GitLab token with scopes",
    "link":"https:\/\/stackoverflow.com\/questions\/79726141\/how-to-create-new-gitlab-token-with-scopes",
    "text":"When I try to create new token with GitLab API like this: ``` curl --request POST --header \"PRIVATE-TOKEN: glpat-\" --data \"name=test-token-short-ttl\" --data \"scopes[]=read_repository\" --data \"expires_at=2025-08-06\" --url \"https:\/\/gitlab.example.ru\/api\/v4\/user\/personal_access_tokens\" ``` I got the error ``` \"error\":\"scopes does not have a valid value\"} ``` But in documentation this scope is valid",
    "author_id":5272,
    "publication_date":1754405693000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Overlord",
    "author_reputation":11.0,
    "tags":"gitlab",
    "text_length":398,
    "title_length":42,
    "num_tags":1
  },
  {
    "id":5738,
    "title":"How to compare XMLs and their nested content?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726153\/how-to-compare-xmls-and-their-nested-content",
    "text":"I have the following structure that I need to compare: ``` <ROOT> <name> <name>string<\/name> <type>number<\/type> <server>string<\/server> <service>number<\/service> <user\/> <enabled>boolean<\/enabled> <\/name> <name> <name>string<\/name> <type>number<\/type> <server>string<\/server> <service>number<\/service> <user\/> <enabled>boolean<\/enabled> <\/name> <name> <name>string<\/name> <type>number<\/type> <server>string<\/server> <service>number<\/service> <user\/> <enabled>boolean<\/enabled> <\/name> ... <TOKEN> <generateDate>date<\/generateDate> <username>string<\/username> <role>string<\/role> <type value=\"string\" anotherValue=\"string\"\/> <attr value=\"string\"\/> <foo value=\"string1\"\/> <foo value=\"string2\"\/> <foo value=\"string3\"\/> <foo value=\"string4\"\/> ... <\/TOKEN> <\/ROOT> ``` Basically ``` <name> ``` is an array and a ``` <TOKEN> ``` at the end. The ``` <TOKEN> ``` contains an array of ``` <foo> ``` elements with only one attribute. I want to compare the ``` <name> ``` tag by matching all its nested fields and in the end compare the ``` <TOKEN> ``` by ignoring ``` <generateDate> ``` and matching the ``` <foo> ``` tag by name and its attribute. I tried many times, but it seems to fail: ``` Diff diff = DiffBuilder.compare(expected) .withTest(actual) .ignoreWhitespace() .ignoreComments() .checkForSimilar() .withNodeMatcher( new DefaultNodeMatcher( ElementSelectors.conditionalBuilder() .whenElementIsNamed(\"name\") .thenUse(ElementSelectors.and( ElementSelectors.byNameAndAllAttributes, ElementSelectors.byNameAndText )) .whenElementIsNamed(\"foo\") .thenUse(ElementSelectors.byNameAndAttributes(\"value\")) .elseUse(ElementSelectors.byNameAndText) .build() ) ) .withDifferenceEvaluator((comparison, outcome) -> { Node targetNode = comparison.getControlDetails().getTarget(); String nodeName = \"\"; if (targetNode != null) { nodeName = targetNode.getNodeName(); if (\"#text\".equals(nodeName) && targetNode.getParentNode() != null) { nodeName = targetNode.getParentNode().getNodeName(); } } if (nodeName.equals(\"generateDate\")) { return ComparisonResult.EQUAL; } return outcome; }) .build(); ``` However, i think it matches the right ``` <name> ``` tags, but when comparing its content it fails. Any idea? Im using ``` xmlunit-core 2.9.0 ``` if it matters",
    "author_id":5271,
    "publication_date":1754406617000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Andrei Manolache",
    "author_reputation":933.0,
    "tags":"xml, diff, xmlunit, xmlunit-2, xmldiff",
    "text_length":2244,
    "title_length":45,
    "num_tags":5
  },
  {
    "id":5737,
    "title":"Azure SDK for CPP Bearer token authentication",
    "link":"https:\/\/stackoverflow.com\/questions\/79726160\/azure-sdk-for-cpp-bearer-token-authentication",
    "text":"I have application that uses azure-sdk-for-cpp (new Azure SDK). And I want to rewrite old implemetation of authentication that uses bearer token. Previously we were using it like this. ``` RefreshToken(string accountName, string clientId, string refreshToken) : accountName(accountName), clientId(clientId), refreshToken(refreshToken) { using OAuthAccessToken = ::azure::storage::storage_credentials::bearer_token_credential; credentials =::azure::storage::storage_credentials(accountName_, OAuthAccessToken{})); } ``` Then when I wanted to update token, I would append this to the request. ``` query.append_query(oauth2_strings::refresh_token, web::uri::encode_data_string(refreshToken), false); ``` Can something like this be done via new SDK?",
    "author_id":5140,
    "publication_date":1754406789000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"john doe",
    "author_reputation":31.0,
    "tags":"azure-sdk, azure-identity",
    "text_length":745,
    "title_length":45,
    "num_tags":2
  },
  {
    "id":5736,
    "title":"How can I make selection slot work in Vue 3",
    "link":"https:\/\/stackoverflow.com\/questions\/79726163\/how-can-i-make-selection-slot-work-in-vue-3",
    "text":"In the following code snippet, using Vuetify 3, migrated from Vue 2 and Vuetify 2, the selection slot shows only \"undefined\" values, whereas the item slot is working fine. The item has a raw property, but it is undefined. It is also not possible to select an item from the list (as if read-only). ``` <v-autocomplete :label=\"label('clientManager', 'pdfExport')\" v-model=\"exportOptions.clientManager\" :items=\"getClientManager\" :item-value=\"userName\" :item-title=\"userName\" required :rules=\"[rules.required]\" :loading=\"loadingUserInfos\" clearable hide-details=\"auto\" > <template #selection=\"{ item }\"> <div> {{ `${item.raw.firstName} ${item.raw.lastName}` }} <span class=\"grey--text\">{{ `(${item.raw.userName})` }}<\/span> <\/div> <\/template> <template #item=\"{ props }\"> <div> {{ `${props.value.firstName} ${props.value.lastName}` }} <span class=\"grey--text\">{{ `(${props.value.userName})` }}<\/span> <\/div> <\/template> <\/v-autocomplete> ```",
    "author_id":5270,
    "publication_date":1754406976000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Oliver Henning",
    "author_reputation":61.0,
    "tags":"vuejs3, vuetifyjs3, vuejs-slots",
    "text_length":937,
    "title_length":43,
    "num_tags":3
  },
  {
    "id":5735,
    "title":"How to Customize Apache NiFi 2.4.0 API Logging to Include Custom Attributes?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726174\/how-to-customize-apache-nifi-2-4-0-api-logging-to-include-custom-attributes",
    "text":"I'm using Apache NiFi 2.4.0 and want to extend the default HTTP request logging (nifi-request.log) to include a custom attribute called logType with values like DATA_ACCESS, DATA_EXPORT, or INVALID_OBJECT based on the API endpoint or request parameters. The goal is to create a custom log file similar to nifi-request.log that captures these additional details for auditing and monitoring API interactions. For Example, When the endpoint GET \/nifi-api\/processors\/{id} is called: If the processor ID is invalid (does not exist or is incorrect), the log entry for this event should have: logType set to \"invalid_object\" Additional attributes relevant to this error scenario. If the processor information is successfully retrieved, the event should be logged with: logType set to \"DATA_ACCESS\" Other relevant attributes for data access auditing. I’ve explored logback.xml to add a custom appender but couldn’t figure out how to extract and categorize API requests dynamically. I considered using a custom Jetty filter to inspect requests and log them with a logType, but I’m unsure how to integrate it with NiFi’s Jetty server and Logback. I looked into the SiteToSiteBulletinReportingTask for bulletins, but it doesn’t cover HTTP request details or allow custom attributes like logType. Questions: How can I implement a custom Jetty filter in NiFi 2.4.0 to log API requests with a logType attribute based on the endpoint or parameters? What’s the best way to configure logback.xml to capture these logs in a separate file similar to nifi-request.log? Are there any NiFi OOB features (e.g., processors or reporting tasks) that can achieve this without custom code? Please suggest if any better way we can do this.",
    "author_id":5269,
    "publication_date":1754407369000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"vigneshwar reddy",
    "author_reputation":197.0,
    "tags":"logging, jetty, apache-nifi, logback",
    "text_length":1710,
    "title_length":76,
    "num_tags":4
  },
  {
    "id":5734,
    "title":"How to have access to globals injected with rollup in Vitest",
    "link":"https:\/\/stackoverflow.com\/questions\/79726184\/how-to-have-access-to-globals-injected-with-rollup-in-vitest",
    "text":"I'm currently working on an existing React app and have to add Vitest to it. The problem I'm facing is that globals are injected using the inject function from ``` @rollup\/plugin-inject ``` in my ``` vite.config.js ``` . ``` plugins: [ viteReact(), inject({ _: 'lodash', __: [path.join(path.resolve(), 'src\/app\/core'), 'evemit'], moment: 'moment', ..... }), viteCommonjs(), ], }), ``` I understand why it is not working, inject only runs on build not in the test env But I really need to have access to those globals. Is there a way to access them, or is the only way to mock all of them one by one ? For exemple : ``` __ ``` contains a lots of helpers, like ``` classNames ``` (under ``` __.cn() ``` ) So i have tried to mock a global ``` __ ``` like : ``` globalThis.__ = { cn: (...classes) => classes.filter(Boolean).join(' '), i18n: {t: (s) => s}, }; ``` But running my tests results in a ``` TypeError: cn is not a function ```",
    "author_id":5268,
    "publication_date":1754407799000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Ondo",
    "author_reputation":23.0,
    "tags":"reactjs, vite, vitest",
    "text_length":932,
    "title_length":60,
    "num_tags":3
  },
  {
    "id":5733,
    "title":"How to enable pgaudit to log RPC function queries in PostgreSQL running inside Docker (Supabase local)?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726186\/how-to-enable-pgaudit-to-log-rpc-function-queries-in-postgresql-running-inside-d",
    "text":"I'm running a local PostgreSQL database inside a Docker container as part of a Supabase self-hosted setup. I want to log all internal SQL statements(query mainly to see what genrated after RPC function call) (e.g., SELECT, UPDATE, etc.) executed from within an RPC function (PostgreSQL function), using the pgaudit extension. What I’ve done so far: Confirmed that pgaudit is installed via: Set audit options via ALTER ROLE: ``` ALTER ROLE postgres SET pgaudit.log = 'function, read'; ALTER ROLE postgres SET pgaudit.log_relation = 'on'; ``` also i don't want to use of 'all' for set pgaudit.log ``` ALTER ROLE postgres RESET pgaudit.log = 'all' ``` because it crashes docker. Called the function using Postman and browser via Supabase RPC (which works). ex : http:\/\/127.0.0.1:54321\/rest\/v1\/rpc\/rpc_function_name Ran docker logs -f supabase_db_name, but no AUDIT: logs appear related to function only some read,select ..... like thing print. also i am not raising any thing from function to log, i just want to see query that genrated by my rpc function. also watch particular docker container to see log in it, but nothing there. What I need help with: Is there a way to log the SQL executed inside a PostgreSQL function (RPC) using just SQL-level config (e.g., ALTER ROLE, SET), without editing postgresql.conf? Can I make pgaudit or any alternative log those queries. Any suggestions to make this work in a production-safe way would be really helpful. i want to avoid change in docker file, because when it come to production check there is no way to direct access docker file until self hosting use.",
    "author_id":5267,
    "publication_date":1754407843000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Mohit Sharma",
    "author_reputation":648.0,
    "tags":"postgresql, database, supabase, rpc",
    "text_length":1602,
    "title_length":103,
    "num_tags":4
  },
  {
    "id":5732,
    "title":"Django multi-app authentication: sharing custom token validation and permissions across separate codebases",
    "link":"https:\/\/stackoverflow.com\/questions\/79726187\/django-multi-app-authentication-sharing-custom-token-validation-and-permissions",
    "text":"I'm building a multi-platform system where I have one central authentication service (let's call it \"Auth\") and multiple other Django applications (App1, App2, App3) that need to authenticate users with custom tokens and permissions. Current Setup: Auth service: Handles user registration, login, token management App1, App2, App3: Separate Django projects with their own database and business logic All apps need to validate users authenticated through the Auth service Authentication Flow: User logs in through Auth service → receives token User makes requests to App1\/App2\/App3 with that token App1\/App2\/App3 need to validate the token and get user data Authentication Model: ``` class AuthToken(models.Model): token = models.CharField(max_length=48, unique=True) user = models.ForeignKey(User, on_delete=models.CASCADE) platform = models.ForeignKey(Platform, on_delete=models.CASCADE) expires_at = models.DateTimeField(null=True, blank=True) is_revoked = models.BooleanField(default=False) # other fields (ip, device_id, device_name, etc...) ``` Token Configuration: Settings.py ``` # settings.py REST_FRAMEWORK = { 'DEFAULT_AUTHENTICATION_CLASSES': [ 'path.to.authentication.AuthTokenAuthentication', ], 'DEFAULT_PERMISSION_CLASSES': [ 'rest_framework.permissions.IsAuthenticated', ], } ``` Authentication Class ``` # path\/to\/AuthTokenAuthentication.py class AuthTokenAuthentication(BaseAuthentication): \"\"\" Simple, clean AuthToken authentication. \"\"\" def authenticate(self, request): # Get Authorization header auth_header = request.META.get('HTTP_AUTHORIZATION') if not auth_header or not auth_header.startswith('Bearer '): return None # No token provided # rest of my code ``` The Problem: App1, App2, App3 need to use the same custom ``` AuthTokenAuthentication ``` and ``` permission classes ``` , but they don't have the ``` AuthToken ``` model or related authentication code.",
    "author_id":5266,
    "publication_date":1754407875000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"001",
    "author_reputation":2069.0,
    "tags":"microservices, django, authentication, architecture",
    "text_length":1887,
    "title_length":106,
    "num_tags":4
  },
  {
    "id":5731,
    "title":"Bugzilla set reporter on creating Bug via WebService",
    "link":"https:\/\/stackoverflow.com\/questions\/79726191\/bugzilla-set-reporter-on-creating-bug-via-webservice",
    "text":"I am creating a Bug on Bugzilla 5.2+ via rest-API. I try to achieve to set the reporter to a known Bugzilla user which differs from the used API-User. I already tried that by creating an ``` Extension.pm ``` like follows. ``` package Bugzilla::Extension::MyExtension; use strict; use base qw(Bugzilla::Extension); our $VERSION = '1.0'; use constant NAME => 'MyExtension'; sub object_end_of_create { my ($self, $args) = @_; my $object = $args->{'object'}; if ($object->isa('Bugzilla::Bug')) { my $user = new Bugzilla::User(3); $object->{'reporter'} = $user; $object->{'creator'} = $user; $object->{'url'} = $user->id; $object->update; } } __PACKAGE__->NAME; ``` The extension works, not throwing any errors, but it still does not change any values persistent, especially the reporter. It will, however, display the changed reporter after the bug got transmitted, but will switch back as soon as the page gets reloaded. Additionally, i would like to get the value for the reporter from my api-request-payload, as i have a relation between the sending client user and the bugzilla user.",
    "author_id":5265,
    "publication_date":1754408021000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"MSauer",
    "author_reputation":160.0,
    "tags":"perl, bugzilla",
    "text_length":1083,
    "title_length":52,
    "num_tags":2
  },
  {
    "id":5730,
    "title":"CMake error during fixup_bundle: target [library] is not absolute",
    "link":"https:\/\/stackoverflow.com\/questions\/79726193\/cmake-error-during-fixup-bundle-target-library-is-not-absolute",
    "text":"I'm using MSVC 2022 on windows, with ninja as a build generator and cmake 3.31.8. Here is the error output when running 'ninja package' when it gets to the fixup_bundle section: ``` CPack: - Install project: MYAPP[] Installing dependencies of myapplication. warning: target 'mylibrary.dll' is not absolute... CMake Error at C:\/Users\/ContainerUser\/scoop\/apps\/cmake\/3.31.8\/share\/cmake-3.31\/Modules\/GetPrerequisites.cmake:663 (file): file failed to open for reading (No such file or directory): \/mylibrary.dll Call Stack (most recent call first): C:\/Users\/ContainerUser\/scoop\/apps\/cmake\/3.31.8\/share\/cmake-3.31\/Modules\/GetPrerequisites.cmake:975 (get_prerequisites) C:\/Users\/ContainerUser\/scoop\/apps\/cmake\/3.31.8\/share\/cmake-3.31\/Modules\/BundleUtilities.cmake:647 (get_prerequisites) C:\/Users\/ContainerUser\/scoop\/apps\/cmake\/3.31.8\/share\/cmake-3.31\/Modules\/BundleUtilities.cmake:933 (get_bundle_keys) ``` There's nothing really special about how this DLL is built, and its not the only DLL in the project. I tried renaming it, removing OUTPUT_NAME, getting rid of VERSION & SOVERSION but none of these had any impact. ``` ADD_LIBRARY(mylibrary_shared SHARED ${BASE_SOURCES} ${DOC_SOURCES} ${HEADER_FILES}) TARGET_LINK_LIBRARIES(mylibrary_shared ${MY_LIB_DEPENDS}) SET_TARGET_PROPERTIES(mylibrary_shared PROPERTIES VERSION \"${mylibrary_LIBVERSION}\" SOVERSION \"${mylibrary_SOVERSION}\" CLEAN_DIRECT_OUTPUT 1 OUTPUT_NAME \"mylibrary\" COMPILE_FLAGS \"-DBUILDING_MYLIBRARY\" ) ``` Here is my BundleUtilities code: ``` SET(APPS \"\\${CMAKE_INSTALL_PREFIX}\/myapplication.exe\") INSTALL(CODE \" message(\\\"Installing dependencies of myapplication.\\\") file(GLOB_RECURSE MYPLUGINS \\\"\\${CMAKE_INSTALL_PREFIX}\/plugins\/*${CMAKE_SHARED_LIBRARY_SUFFIX}\\\") include(BundleUtilities) fixup_bundle(\\\"${APPS}\\\" \\\"\\${MYPLUGINS}\\\" \\\"${MY_BIN_DIR}\\\") verify_app(${APPS}) \" COMPONENT controller) ``` For some more context, this code works 100% fine when using msbuild. I'm trying to switch to the ninja generator and got this error. I'm at a loss for how to even debug it. I can't seem to get any diagnostic output from cmake's GetPrerequisites.cmake and I'm not sure how to determine why the path might be blank. Any assistance in this regard is appreciated. I also tried this with the latest cmake version 4.0.3 and this didn't fix it.",
    "author_id":5264,
    "publication_date":1754408124000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"KyleL",
    "author_reputation":1465.0,
    "tags":"c++, visual-studio-2022, cmake, windows-10, ninja",
    "text_length":2300,
    "title_length":65,
    "num_tags":5
  },
  {
    "id":5729,
    "title":"Is using Google Cloud Tasks to invoke an internal Django endpoint effectively asynchronous (even under WSGI)?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726197\/is-using-google-cloud-tasks-to-invoke-an-internal-django-endpoint-effectively-as",
    "text":"I'm running a Django application on Google Cloud Run using the default WSGI-based setup (e.g., Gunicorn\/ runserver for local dev). To avoid blocking during long-running operations like third-party API calls, I'm planning to use Google Cloud Tasks. Current design : A request comes in to a Django endpoint (e.g., a webhook or ping from an external service) Instead of processing the request inline, I enqueue a Cloud Task That task posts to another Django endpoint within the same service, which performs the third-party API call using data passed in the task payload This means: I'm not offloading the work to a separate Cloud Run service The fetch logic is still part of the same Django service\/container, just decoupled by the task My question : Does this setup allow the third-party call to be effectively asynchronous (i.e., non-blocking to the original request), despite using WSGI and not ASGI or Celery? When searching around, I mostly see articles and examples where Cloud Tasks are used to call a separate Cloud Run service, not another internal route in the same app. Is this internal invocation pattern valid and scalable under WSGI, or are there caveats I should be aware of?",
    "author_id":5263,
    "publication_date":1754408250000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"user26535132",
    "author_reputation":11.0,
    "tags":"google-cloud-platform, django, google-cloud-run, asynchronous",
    "text_length":1187,
    "title_length":109,
    "num_tags":4
  },
  {
    "id":5728,
    "title":"ExternalPurchaseCustomLink.token(for:) returns nil on one TestFlight device (while isEligible == true) — other device gets SERVICES token",
    "link":"https:\/\/stackoverflow.com\/questions\/79726205\/externalpurchasecustomlink-tokenfor-returns-nil-on-one-testflight-device-whi",
    "text":"I’m implementing StoreKit External Purchase Custom Links (EU) and so far it is really painful. I am running into a strange, device-specific issue. On 3\/4 devices it works. On one device I never get a token at launch nor before a transaction. isEligible is true everywhere. All devices have versions 18.5 and are located in Germany. Info.plist: SKExternalPurchaseCustomLinkRegions is set to EU storefront codes and I have followed every step in the documentation: https:\/\/developer.apple.com\/documentation\/storekit\/externalpurchasecustomlink Good device: At launch → ACQUISITION = nil, SERVICES = token present. Works consistently. Faulty device: At launch → ACQUISITION = nil, SERVICES = nil. Same before transaction. No token ever reaches my server from this device. isEligible is true on both devices. Any experts or help on the matter?",
    "author_id":5262,
    "publication_date":1754408667000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"CodeStallion",
    "author_reputation":62.0,
    "tags":"ios, swift, xcode",
    "text_length":838,
    "title_length":137,
    "num_tags":3
  },
  {
    "id":5727,
    "title":"How can I update Azure DevOps Datasource in Azure Managed Grafana?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726206\/how-can-i-update-azure-devops-datasource-in-azure-managed-grafana",
    "text":"Is there a way to update the Azure DevOps Datasource plugin in Azure Managed Grafana without waiting for Microsoft to update it? Or is the plugin version tied to the Grafana version that Azure provides? What I’ve tried: Checked the Grafana UI → Administration → Plugins and data → Plugins → Azure DevOps (no update option) Checked Azure Portal → Azure Managed Grafana → Settings → Plugin management → Azure DevOps (not found) Looked through Azure Managed Grafana documentation Environment: Azure Managed Grafana Azure DevOps Datasource plugin v0.9.0 (current)",
    "author_id":5261,
    "publication_date":1754408697000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"DevDaniels",
    "author_reputation":59.0,
    "tags":"azure, azure-devops, grafana, grafana-plugin, azure-managed-grafana",
    "text_length":559,
    "title_length":66,
    "num_tags":5
  },
  {
    "id":5726,
    "title":"How to enable WSS4j for SOAP-Webservices?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726209\/how-to-enable-wss4j-for-soap-webservices",
    "text":"I try to enable WSS4J for SOAP webservices running in Wildfly 36. My problem: I found several documents how to register message interceptors, but none of them worked. Wildfly documentation says, that I can forward security information to Elytron by interceptor, but not, how the interceptor is configured or enabled. The solutions I found: config file cxf.xml config file jbossws-cxf.xml config file jboss-endpoint-config.xml config file jboss-webservice.xml annotation @InInterceptors I tried also some custom interceptors (just logging something). The webservice method is called (breakpoint reached), but the interceptor is not used (breakpoint not reached). Is there any reliable documentation that describes how to configure WSS4J?",
    "author_id":5260,
    "publication_date":1754408747000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Markus",
    "author_reputation":185.0,
    "tags":"java, security, wildfly, cxf, wss4j",
    "text_length":736,
    "title_length":41,
    "num_tags":5
  },
  {
    "id":5725,
    "title":"How to manage large HTML email templates in .NET Core API?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726213\/how-to-manage-large-html-email-templates-in-net-core-api",
    "text":"I'm working on a .NET Core API where I need to send styled HTML emails. Currently, I'm storing the entire HTML template as a C# string in a service class and using string.Replace() to inject dynamic content. It's starting to get hard to manage and maintain. What is the best practice to handle large HTML email templates in a .NET Core application? Should I move the HTML to external files, use Razor, or is there a better templating engine for this use case? Any advice or examples would be appreciated.",
    "author_id":5259,
    "publication_date":1754408894000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Kafkaa",
    "author_reputation":81.0,
    "tags":"c#, asp.net-core, .net",
    "text_length":504,
    "title_length":58,
    "num_tags":3
  },
  {
    "id":5724,
    "title":"How to use soup to find partial links",
    "link":"https:\/\/stackoverflow.com\/questions\/79726220\/how-to-use-soup-to-find-partial-links",
    "text":"I have an ebay page in which I would like to formulate a list of all the item numbers on that page. I have executed and parsed the html using requests and BeautifulSoup but I can't figure out how to get exactly what I need. ``` <li id=“item247f10fa70” class=“s-card s-card—horizontal: data-viewport=“”{“trackableId”:”01K1XBEX0ATN4B5KW20VVB90ZN”}” data-view=“mi:1686|iid:3” data-listingid=“156760641776”> ``` But I am just trying to single out the number 156760641776",
    "author_id":5258,
    "publication_date":1754409253000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Travis Ward",
    "author_reputation":1.0,
    "tags":"html, search, beautifulsoup, find",
    "text_length":466,
    "title_length":37,
    "num_tags":4
  },
  {
    "id":5723,
    "title":"AWS CloudFront Signed URL throws ERR_OSSL_EVP_INVALID_DIGEST in Node.js 22",
    "link":"https:\/\/stackoverflow.com\/questions\/79726227\/aws-cloudfront-signed-url-throws-err-ossl-evp-invalid-digest-in-node-js-22",
    "text":"I'm trying to generate signed URLs for a CloudFront distribution using the AWS SDK v3 (@aws-sdk\/cloudfront-signer) in a Node.js project. I successfully uploaded my public key to CloudFront using: ``` aws cloudfront create-public-key --public-key-config ... ``` I have a matching RSA key pair (PEM format). When I try to sign a URL using the following code: ``` import { getSignedUrl } from \"@aws-sdk\/cloudfront-signer\"; import fs from \"fs\"; const privateKey = fs.readFileSync(\".\/private_key.pem\", \"utf8\"); const keyPairId = \"K3DOPF1CJHCTXX\"; (async () => { try { const signedUrl = await getSignedUrl({ url: \"https:\/\/dxxxxxxxx.cloudfront.net\/path\/to\/master.m3u8\", keyPairId, privateKey, dateLessThan: new Date(Date.now() + 60 * 60 * 1000), }); console.log(\"Signed URL:\", signedUrl); } catch (e) { console.error(e); } })(); ``` I'm running this with: ``` node test.js ``` My environment: Node.js: v22.x.x OS: Fedora Linux AWS SDK: @aws-sdk\/cloudfront-signer (latest) And I'm getting this error: ``` Error: error:03000098:digital envelope routines::invalid digest at Sign.sign (node:internal\/crypto\/sig:128:29) at CloudfrontSignBuilder.signData ... ... code: 'ERR_OSSL_EVP_INVALID_DIGEST' ``` What I've Tried Verified the public\/private key pair is correct using openssl md5 -modulus Tried running with --openssl-legacy-provider, but it no longer works in Node.js 22 Can't downgrade my whole stack easily What I Want to Know Is there a way to make this work in Node.js 22? Can I force cloudfront-signer to use a supported algorithm (e.g., SHA-256)?",
    "author_id":5257,
    "publication_date":1754409367000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Aqib Ansari",
    "author_reputation":9.0,
    "tags":"node.js, openssl, amazon-cloudfront",
    "text_length":1545,
    "title_length":74,
    "num_tags":3
  },
  {
    "id":5722,
    "title":"Why chrono::timezone and format are slower than localtime_s and stringstream?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726230\/why-chronotimezone-and-format-are-slower-than-localtime-s-and-stringstream",
    "text":"I'm generating a string representation of the current time in the local time zone for my logging system. I have an \"old\" version, and I wanted to see if I could improve its performance. Old version: ``` const auto now = std::chrono::system_clock::now(); const std::time_t t_c = std::chrono::system_clock::to_time_t(now); struct tm loc; localtime_s(&loc, &t_c); std::stringstream ss; ss << std::put_time(&loc, \"%F %T.\"); ss << std::setfill('0') << std::setw(6) << std::chrono::duration_cast<std::chrono::microseconds>(now.time_since_epoch()).count() % 1'000'000; return ss.str(); ``` New version: ``` const auto now = std::chrono::system_clock::now(); auto localNow = std::chrono::current_zone()->to_local(now); return std::format(\"{0:%F} {0:%R}:{0:%S}\", std::chrono::time_point_cast<std::chrono::microseconds>(localNow)); ``` It turns out the new version is ~5 times slower. I'm using Visual Studio 17.12.3 with ``` \/O2 ``` and Whole Program Optimization. My CPU is an Intel 12700K. Using the built-in performance profiler (sampling), it looks like both the ``` to_local call ``` and the ``` std::format ``` call are each slower than the entire old version. Why is the new version slower? How to further optimize the old version?",
    "author_id":5256,
    "publication_date":1754409505000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Carsten Kjaer",
    "author_reputation":33.0,
    "tags":"c++, performance, c++-chrono, stdformat",
    "text_length":1229,
    "title_length":77,
    "num_tags":4
  },
  {
    "id":5721,
    "title":"Searchstring to WikiData entity ID",
    "link":"https:\/\/stackoverflow.com\/questions\/79726232\/searchstring-to-wikidata-entity-id",
    "text":"Question How do I get from an animal name ( ``` tiger ``` ) to its WikiData entity id ( ``` Q19939 ``` ) in a static website? Context I am writing a small static website (no backend, just html and vanilla javascript) where the user enters the name of an animal ( ``` tiger ``` ) and the website displays information about that animal. I want to get the shown data from WikiData. (I am new to WikiData, so this is a good opportunity to learn). This works great if I have the WikiData entity id ( Q19939 for tiger ). With that I can query the wikidata-sparql-api and get all the data I need. How do I get the WikiData entity id ( ``` Q19939 ``` ) from the animal name ( ``` tiger ``` )? Here is a minimal example of how the final website might look like: ``` <!doctype html> <html> <body> <div id=\"output\"><\/div> <script> async function getWikidataId(entityName) { \/\/ TODO find id somehow return undefined; } async function tell_reader_about(animalName) { const wikidataId = await getWikidataId(animalName); document.getElementById(\"output\").innerText = `${animalName} has WikiData entity ID ${wikidataId}`; } tell_reader_about(\"tiger\"); <\/script> <\/body> <\/html> ``` What I tried I see mainly two ways here: Attempt A: Use the MediaWiki API The MediaWiki API seems perfect for my usecase. I can provide the animal name as an query parameter. https:\/\/www.wikidata.org\/w\/api.php?action=wbsearchentities&search=tiger&language=en It returns a list of fitting wikidata entities with a description. ``` { \"search\": [ { \"id\": \"Q79081545\", \"label\": \"Tiger\", \"description\": \"unisex given name\", ... }, { \"id\": \"Q19939\", \"label\": \"tiger\", \"description\": \"species of big cat\", ... }, { \"id\": \"Q16282104\", \"label\": \"Tiger\", \"description\": \"family name\", }, ... ], \"search-continue\": 7, \"success\": 1, ... } ``` This is awesome, from here I could choose which id is the right one and use it. But when I do this in my static website I get the following error: ``` Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at https:\/\/www.wikidata.org\/w\/api.php?action=wbsearchentities&search=apple&language=de&format=json. (Reason: CORS header ‘Access-Control-Allow-Origin’ missing). Status code: 200. ``` This sounds to me like that API is not available for me to use. Or is there a way to make this work? Attempt B: Use the WikiData sparql API I could try to do another sparql request. (The sparql API seemingly has that CORS header set) As far as I can see Wikidata doesn't like to match entities against strings, only against other entities. But it should work in theory. I tried a few queries, but they all timed out or didn't give me any results. ``` SELECT ?item ?itemLabel WHERE { ?item wdt:P31 wd:Q16521. FILTER(LCASE(?itemLabel) = LCASE(\"tiger\")) . SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],mul,en\". } } ``` try it How a solution could look like I can think of multiple solutions to this problem: Salvage attempt A: Perhaps I can do something clever so that calling that MediaWiki API is allowed? Salvage attempt B: Find a query that doesn't time out and produces the wished for entity id? Find another api that I can use to get to the id?",
    "author_id":5255,
    "publication_date":1754409568000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Kaligule",
    "author_reputation":808.0,
    "tags":"javascript, sparql, same-origin-policy, wikidata",
    "text_length":3199,
    "title_length":34,
    "num_tags":4
  },
  {
    "id":5720,
    "title":"Cross-account UNLOAD with AWS Timestream - AccessDenied",
    "link":"https:\/\/stackoverflow.com\/questions\/79726235\/cross-account-unload-with-aws-timestream-accessdenied",
    "text":"I'm attempting to solve following scenario: I have Timestream for LiveAnalytics placed in AWS Account A and S3 bucket placed in AWS Account B. I'm running following query: ``` UNLOAD( SELECT * FROM \"database\".\"table\" ) TO 's3:\/\/bucket-in-account-B\/unload' WITH ( format = 'PARQUET', encryption = 'SSE_S3' ) ``` Unfortunately hitting AccessDenied error. Based on my limited understanding of AWS and some research UNLOAD runs under Service ``` timestream.amazonaws.com ``` , so I've added following bucket policy to ``` bucket-in-account-B ``` ``` { \"Sid\": \"AllowWriteFromExternalAccount\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::AccountA:root\" }, \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::bucket-in-Account-B\", \"arn:aws:s3:::bucket-in-Account-B\/*\" ] }, { \"Sid\": \"AllowTimestreamServiceToWrite\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"timestream.amazonaws.com\" }, \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::bucket-in-Account-B\/*\", \"arn:aws:s3:::bucket-in-Account-B\" ] }, ``` ``` AllowWriteFromExternalAccount ``` is needed by different stream, and while maybe not required here per se, I figured it's better to show its existence to rule out. I tried with bucket with block public access on or off, with setting ACLs, but nothing seems to work. Same code replaced with bucket-in-account-A runs without any major issue, starting to think that this might be AWS limitation, but before jumping to conclusions and figuring other ways of doing such task (ex. query running Lambda and save to S3) would prefer a confirmation, as I was unable to find anything in AWS documentation regarding this",
    "author_id":5254,
    "publication_date":1754409722000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"glaeran",
    "author_reputation":426.0,
    "tags":"amazon-web-services, amazon-s3, amazon-timestream",
    "text_length":1615,
    "title_length":55,
    "num_tags":3
  },
  {
    "id":5719,
    "title":"SQLModel and emails",
    "link":"https:\/\/stackoverflow.com\/questions\/79726238\/sqlmodel-and-emails",
    "text":"I'm currently working on an api using fastapi and sqlmodel. I'm new to these two and I might have missed something. I have a user model with an e-mail field. So no email outside my organization is inserted inside the database I have set the email field as such: ``` class User(SQLModel, table=True): __tablename__ = 'users' id: UUID = Field(default_factory=uuid4, primary_key=True) username: str = Field(index=True, max_length=30) email: str = Field(max_length=50, schema_extra={'pattern': r'^email pattern$'}) ``` I've read in some github issues that the regex parameter is not working properlly with pydantic and the supposed way to do this is by using the schema_extra parameter. Unfortunately when I try to insert an e-mail that does not match the pattern it still does get inserted into the database. Am I missing something here? Have anyone encountered this same issue?",
    "author_id":5253,
    "publication_date":1754410107000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"somedev",
    "author_reputation":51.0,
    "tags":"python, validation, fastapi, pydantic, sqlmodel",
    "text_length":875,
    "title_length":19,
    "num_tags":5
  },
  {
    "id":5718,
    "title":"python gmsh: build 3D mesh from solid-split stl surface for openFOAM",
    "link":"https:\/\/stackoverflow.com\/questions\/79726242\/python-gmsh-build-3d-mesh-from-solid-split-stl-surface-for-openfoam",
    "text":"I have a simple task- I have a closed stl-surface, the whole surface is split into solids. I need to use python gmsh pack to build 3D mesh (tetra or hexa) with named-selections (lets call it this way) for BC patches according to solids from STL. I need to use this for openFOAM calculation. I am very new in gmsh. Are there any ways to do this? I tried this: ``` gmsh.initialize() input_file = r\"geometry\\geom_tut1\\geom.stl\" gmsh.merge(input_file) gmsh.model.mesh.createGeometry() gmsh.model.geo.removeAllDuplicates() loop = gmsh.model.geo.addSurfaceLoop([e[1] for e in gmsh.model.getEntities(2)]) gmsh.model.geo.addVolume([loop]) gmsh.model.geo.synchronize() gmsh.option.setNumber(\"Mesh.Algorithm\", 5) gmsh.option.setNumber(\"Mesh.Algorithm3D\", 10) gmsh.option.setNumber(\"Mesh.MeshSizeMin\", 0.0001) gmsh.option.setNumber(\"Mesh.MeshSizeMax\", 0.001) gmsh.model.mesh.generate(3) ``` I am not sure about using ``` gmsh.model.mesh.createTopology() ``` because I don't know exactly how it works. But I dont want to use ``` classifySurfaces ``` because it classifies surfaces by angle.",
    "author_id":5252,
    "publication_date":1754410215000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"IzaeDA",
    "author_reputation":439.0,
    "tags":"python, stl, gmsh",
    "text_length":1078,
    "title_length":68,
    "num_tags":3
  },
  {
    "id":5717,
    "title":"App Service sandbox blocks all forms of dynamic client certificate usage in HttpClient",
    "link":"https:\/\/stackoverflow.com\/questions\/79726244\/app-service-sandbox-blocks-all-forms-of-dynamic-client-certificate-usage-in-http",
    "text":"I’m trying to make TLS calls using HttpClient in an Azure App Service (Windows) with a certificate that is dynamically loaded at runtime (e.g., from blob storage or Key Vault as a byte array). A third-party service we must communicate with requires that we authenticate using a client certificate. Current production setup App Service (Windows), switched from 32-bit to 64-bit .NET 9 Premium v3 P0V3 (one of the many set-ups I tried) WEBSITE_LOAD_CERTIFICATES = 1 WEBSITE_LOAD_USER_PROFILE = 1 I’ve tried all relevant combinations of X509KeyStorageFlags, but none work in App Service: MachineKeySet throws \"The private key could not be loaded\" — maybe because of App Service sandbox restrictions. What’s strange is that this used to work for months, and then suddenly started failing across multiple App Services — with identical code and certificates. Nothing changed on our end. Our certificates are valid and correctly formatted. This makes the behavior feel unstable and opaque, which is extremely concerning in a production context. UserKeySet results in \"The system cannot find the file specified\" — likely because there’s no user profile loaded in the environment, even with a managed identity. EphemeralKeySet works during loading, but fails later with \"Authentication failed because the platform does not support ephemeral keys\" when calling HttpClient.GetAsync. Of course, because it's a Windows machine. Using a certificate from Key Vault still leaves me with the same problem — the certificate ends up in memory and needs to be converted to X509Certificate2, which runs into the same sandbox limitations. Uploading the certificate manually through App Service and using the WEBSITE_LOAD_CERTIFICATES setting does work, because the platform handles storage and injection into the certificate store. But that’s not an option for dynamic scenarios, such as user-provided certificates. One of my many attempts in code: ``` var limits = new Pkcs12LoaderLimits { PreserveStorageProvider = true, }; var certificate = X509CertificateLoader.LoadPkcs12(_certificateBytes, _password, keyStorageFlags: X509KeyStorageFlags.MachineKeySet, loaderLimits: limits); var handler = new HttpClientHandler { ClientCertificateOptions = ClientCertificateOption.Manual, UseDefaultCredentials = false }; handler.ClientCertificates.Add(certificate); using var httpClient = new HttpClient(handler) { BaseAddress = new Uri(_basePath) }; var response = await httpClient.GetAsync(requestUri); ``` Expected behavior I should be able to: Load a certificate at runtime (from blob, Key Vault, etc.) Use it with HttpClient for mutual TLS authentication → without needing to pre-upload the cert through the portal via WEBSITE_LOAD_CERTIFICATES. This should be possible in a stable and predictable way — not something that breaks seemingly at random. I need a solution I can rely on in production, not one that works one day and inexplicably fails the next. The issue concerns a single API call that needs to be made from 3 different App Services. Each of our customers has their own unique certificate that must be loaded dynamically at runtime. This one call literally represents less than 1% of what our entire platform does and can achieve — it’s a tiny detail in the bigger picture. It’s therefore absurd that we would have to move away from App Services altogether just because of this.",
    "author_id":5251,
    "publication_date":1754410257000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Ellen",
    "author_reputation":1.0,
    "tags":"azure, dotnet-httpclient, client-certificates, azure-appservice, x509certficiate2",
    "text_length":3366,
    "title_length":86,
    "num_tags":5
  },
  {
    "id":5716,
    "title":"Does http::response_parser inherits limits from source parser?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726249\/does-httpresponse-parser-inherits-limits-from-source-parser",
    "text":"I create ``` http::response_parser<http::buffer_body> ``` and set header_limit and body_limit like this ``` auto parser = std::make_shared<http::response_parser<http::buffer_body>>(); parser->header_limit(std::numeric_limits<std::uint32_t>::max()); parser->body_limit(std::numeric_limits<std::uint64_t>::max()); ``` Next, I transform the ``` http::response_parser<http::buffer_body> ``` parser into a parser of type ``` http::response_parser<http::string_body> ``` as follows ``` auto new_parser = std::make_shared<http::response_parser<http::string_body>>(std::move(*parser)); ``` My question is, will ``` new_parser ``` inherit header_limit and body_limit from the original parser or not? I didn't find any methods that could report the current values of the given limits.",
    "author_id":4345,
    "publication_date":1754410493000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Joe J",
    "author_reputation":1075.0,
    "tags":"c++, boost, boost-asio, boost-beast",
    "text_length":774,
    "title_length":62,
    "num_tags":4
  },
  {
    "id":5715,
    "title":"SQL Calculate Running Total Against Positive Transactions",
    "link":"https:\/\/stackoverflow.com\/questions\/79726254\/sql-calculate-running-total-against-positive-transactions",
    "text":"I have an instance where I need to be calculating the running total by positive transactions. This exercise is to determine which positive entries in our warehouse entries is still open as we need to determine the date of inventory based off of our item ledger entries. We are using NAV 2018 for reference. I already have the running total ( ``` [Qty (Base) Running Total] ``` ) per item and bin ( ``` [Item No_], [Location Code], [Bin Code] ``` ): ``` select we.[Entry No_], we.[Registering Date], we.[Item No_], we.[Location Code], we.[Bin Code], we.[Qty_ (Base)], sum(we.[Qty_ (Base)]) over (partition by we.[Item No_], we.[Location Code], we.[Bin Code] order by we.[Entry No_]) as [Qty (Base) Running Total], (select sum(we2.[Qty_ (Base)]) from [Warehouse Entry] we2 with(nolock) where we2.[Item No_] = we.[Item No_] and we2.[Location Code] = we.[Location Code] and we2.[Bin Code] = we.[Bin Code]) as [Total Qty Base In Bin] from [Warehouse Entry] we with(nolock) ``` In the example dataset below (where I have filtered the dataset down to 1 item and 1 bin already), I need to know when the initial 1280 (and all positive transactions) went to 0 so that I do not include that entry as \"still in inventory\" . In the dataset when I manually sum the ``` Qty (Base) ``` I can see that the initial 1280 was zeroed out by Entry No 927025. Similarly, Entry No 656541 for 600 is zeroed out by Entry No 1205470, etc. Dataset ( ``` Entry No., Registering Date, Qty (Base) ``` are my data, ``` Qty (Base) Running Total ``` is computed by the above query): Entry No. Registering Date Qty (Base) Qty (Base) Running Total 427752 4\/17\/2025 1280 1280 577708 4\/28\/2025 -100 1180 593215 4\/29\/2025 -130 1050 611579 4\/30\/2025 -130 920 616674 4\/30\/2025 -10 910 616682 4\/30\/2025 -10 900 616686 4\/30\/2025 -10 890 616758 4\/30\/2025 -10 880 616913 4\/30\/2025 -10 870 622854 4\/30\/2025 -20 850 622863 4\/30\/2025 -20 830 622875 4\/30\/2025 -20 810 622885 4\/30\/2025 -20 790 622902 4\/30\/2025 -20 770 633070 5\/1\/2025 -120 650 656541 5\/2\/2025 600 1250 671925 5\/5\/2025 -30 1220 704308 5\/7\/2025 -20 1200 736014 5\/8\/2025 -20 1180 853955 5\/19\/2025 -50 1130 884492 5\/21\/2025 -10 1120 884495 5\/21\/2025 -10 1110 884499 5\/21\/2025 -10 1100 884501 5\/21\/2025 -10 1090 884505 5\/21\/2025 -10 1080 884509 5\/21\/2025 -10 1070 884515 5\/21\/2025 -10 1060 884517 5\/21\/2025 -10 1050 884519 5\/21\/2025 -10 1040 884521 5\/21\/2025 -10 1030 887327 5\/21\/2025 -120 910 893571 5\/22\/2025 -10 900 893573 5\/22\/2025 -10 890 893578 5\/22\/2025 -10 880 893582 5\/22\/2025 -10 870 893584 5\/22\/2025 -10 860 893586 5\/22\/2025 -10 850 893591 5\/22\/2025 -10 840 893593 5\/22\/2025 -10 830 893598 5\/22\/2025 -10 820 893600 5\/22\/2025 -10 810 899565 5\/22\/2025 -160 650 913031 5\/23\/2025 640 1290 927025 5\/23\/2025 -60 1230 1042437 6\/6\/2025 -80 1150 1043369 6\/6\/2025 -60 1090 1120232 6\/16\/2025 -20 1070 1120236 6\/16\/2025 -20 1050 1125853 6\/16\/2025 -20 1030 1125858 6\/16\/2025 -20 1010 1125864 6\/16\/2025 -20 990 1125996 6\/16\/2025 -20 970 1126000 6\/16\/2025 -30 940 1205340 6\/24\/2025 -10 930 1205342 6\/24\/2025 -10 920 1205346 6\/24\/2025 -10 910 1205350 6\/24\/2025 -10 900 1205356 6\/24\/2025 -10 890 1205362 6\/24\/2025 -10 880 1205364 6\/24\/2025 -10 870 1205368 6\/24\/2025 -170 700 1205381 6\/24\/2025 -10 690 1205470 6\/24\/2025 -10 680 1205476 6\/24\/2025 -10 670 1222807 6\/25\/2025 -200 470 1225247 6\/25\/2025 -30 440 1225249 6\/25\/2025 -30 410 1225251 6\/25\/2025 -30 380 1225253 6\/25\/2025 -30 350 1225255 6\/25\/2025 -30 320 1225259 6\/25\/2025 -30 290 1225273 6\/25\/2025 -20 270 1241866 6\/27\/2025 -30 240 1241881 6\/27\/2025 -30 210 1242022 6\/27\/2025 -30 180 1242026 6\/27\/2025 -30 150 1242109 6\/27\/2025 -30 120 1242115 6\/27\/2025 -30 90 1242175 6\/27\/2025 -20 70 1246598 6\/27\/2025 -70 0 1265596 6\/30\/2025 320 320 1265604 6\/30\/2025 720 1040 1289533 7\/2\/2025 -100 940 1296320 7\/2\/2025 -90 850 1296323 7\/2\/2025 90 940 1324258 7\/7\/2025 220 1160 1396957 7\/14\/2025 -30 1130 1396964 7\/14\/2025 -30 1100 1396968 7\/14\/2025 -30 1070 1396973 7\/14\/2025 -30 1040 1467117 7\/21\/2025 -20 1020 1467161 7\/21\/2025 -100 920 1470236 7\/21\/2025 -10 910 1470348 7\/21\/2025 -10 900 1470422 7\/21\/2025 -10 890 1470497 7\/21\/2025 -10 880 1471788 7\/21\/2025 -10 870 1499118 7\/23\/2025 -300 570 1531689 7\/25\/2025 -20 550 1531692 7\/25\/2025 -20 530 1531697 7\/25\/2025 -20 510 1531699 7\/25\/2025 -20 490 1531703 7\/25\/2025 -20 470",
    "author_id":5250,
    "publication_date":1754410759000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Chimera111x",
    "author_reputation":11.0,
    "tags":"sql-server, sql",
    "text_length":4274,
    "title_length":57,
    "num_tags":2
  },
  {
    "id":5714,
    "title":"How to change the debugger &quot;background hints&quot; text color in VS Code?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726261\/how-to-change-the-debugger-background-hints-text-color-in-vs-code",
    "text":"How to change the \"background hints\" color in debug mode in vscode? In the image, I mean to the background yellow color of ``` y=1, x=1 ``` from the right (not the yellow background color of the current debug line in the import line). Colors example for reference:",
    "author_id":5249,
    "publication_date":1754410896000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"J. Doe",
    "author_reputation":303.0,
    "tags":"visual-studio-code",
    "text_length":264,
    "title_length":78,
    "num_tags":1
  },
  {
    "id":5713,
    "title":"How to fix offset column names in `addTable` request?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726262\/how-to-fix-offset-column-names-in-addtable-request",
    "text":"I'm trying to create a Table in a Google Sheet using the API in Python. I've constructed an API request that mimics the example one here . My request looks like this: ``` { \"requests\": [ { \"addTable\": { \"table\": { \"tableId\": \"xxxxxx\", \"name\": \"Foobar\", \"range\": { \"sheetId\": xxxxxx, \"startRowIndex\": 2, \"endRowIndex\": 4, \"startColumnIndex\": 1, \"endColumnIndex\": 3 }, \"columnProperties\": [ { \"columnIndex\": 0, \"columnName\": \"Column 1\", \"columnType\": \"PERCENT\" }, { \"columnIndex\": 1, \"columnName\": \"Column 2\", \"columnType\": \"DROPDOWN\", \"dataValidationRule\": { \"condition\": { \"type\": \"ONE_OF_LIST\", \"values\": [ { \"userEnteredValue\": \"Not Started\" }, { \"userEnteredValue\": \"In Progress\" }, { \"userEnteredValue\": \"Complete\" } ] } } } ] } } } ], \"includeSpreadsheetInResponse\": false } ``` For some reason, the column names are offset when this actually appears in the spreadsheet. This is what I get: And the response from the API matches this. Here's the ``` columnProperties ``` part of the response, which shows that my column names were moved around while preserving the data validation. And there's no explanation for why \"Column 2\" got written outside the table. ``` \"columnProperties\": [ { \"columnName\": \"Column 2\", \"columnType\": \"PERCENT\" }, { \"columnIndex\": 1, \"columnName\": \"Column 1\", \"columnType\": \"DROPDOWN\", \"dataValidationRule\": { \"condition\": { \"type\": \"ONE_OF_LIST\", \"values\": [ { \"userEnteredValue\": \"Not Started\" }, { \"userEnteredValue\": \"In Progress\" }, { \"userEnteredValue\": \"Complete\" } ] } } } ] ```",
    "author_id":5248,
    "publication_date":1754410908000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"kviLL",
    "author_reputation":408.0,
    "tags":"google-sheets, google-sheets-api",
    "text_length":1517,
    "title_length":53,
    "num_tags":2
  },
  {
    "id":5712,
    "title":"How do I link to the Ebay API from Azure Functions",
    "link":"https:\/\/stackoverflow.com\/questions\/79726268\/how-do-i-link-to-the-ebay-api-from-azure-functions",
    "text":"So I have an ebay import routine as part of a windows application I wrote a few years ago. It works well but I am thinking of converting this to an Azure Function App which will run every 2 weeks at a scheduled time. The original win app is written in C# so I am sticking with that. The app compiles and starts but then I hit a problem. I user the same eBayToken (context.ApiCredential.eBayToken) which works fine in the win app. So when I run the Function App in debug mode in VS I see this. ``` Functions: ebayPort: [GET,POST] http:\/\/localhost:7186\/api\/ebayPort [2025-08-04T20:01:52.815Z] Host lock lease acquired by instance ID '0000000000000000000000007A365188'. ``` I then place the localhost address into Chrome I see this. So if then I step thru this code in debug mode ``` ApiContext context = new(); \/\/set the User token context.ApiCredential.eBayToken = \"??????????????????????????????????????\"; \/\/set the server url context.SoapApiServerUrl = \"https:\/\/api.ebay.com\/wsapi\"; \/\/enable logging context.ApiLogManager = new ApiLogManager(); context.ApiLogManager.ApiLoggerList.Add(new FileLogger(\"log.txt\", true, true, true)); context.ApiLogManager.EnableLogging = true; context.Site = SiteCodeType.UK; DateTime CreateTimeFromPrev, CreateTimeFrom, CreateTimeTo; GetOrdersCall getOrders = new(context) { DetailLevelList = [] }; ``` Then I look at the output ``` For detailed output, run func with --verbose flag. [2025-08-04T20:04:41.312Z] Host lock lease acquired by instance ID '?????????????????. [2025-08-04T20:04:54.309Z] Executing 'Functions.ebayPort' (Reason='This function was programmatically called via the host APIs.', Id=????????????????????) [2025-08-04T20:05:13.859Z] Function 'ebayPort', Invocation id '02711137-45bb-4732-9a23-679094f81b63': An exception was thrown by the invocation. [2025-08-04T20:05:13.860Z] Result: Function 'ebayPort', Invocation id '02711137-45bb-4732-9a23-679094f81b63': An exception was thrown by the invocation. Exception: System.IO.FileNotFoundException: Could not load file or assembly 'System.Web.Services, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a'. The system cannot find the file specified. [2025-08-04T20:05:13.861Z] File name: 'System.Web.Services, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' ``` So the app is looking for 'System.Web.Services’. Has anyone got any useful suggestions on how to get this working or any useful links or sample code please.",
    "author_id":4594,
    "publication_date":1754411092000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Kev",
    "author_reputation":440.0,
    "tags":"c#, azure-functions",
    "text_length":2453,
    "title_length":50,
    "num_tags":2
  },
  {
    "id":5711,
    "title":"Excel Range IF fails with an AND",
    "link":"https:\/\/stackoverflow.com\/questions\/79726269\/excel-range-if-fails-with-an-and",
    "text":"Trying to aggregate a list of names that are overdue. This works great until I add an AND to the IF, then everything falls apart and no results are returned. I need to aggregate a list of names by each person that are overdue. ``` TEXTJOIN( \", \", TRUE, IF( AND( H2=$B$2:$B$8, $C$2:$C$8<NOW() ), $A$2:$A$8, \"\" ) ) ``` Any suggestions? Thanks",
    "author_id":5247,
    "publication_date":1754411190000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Brad",
    "author_reputation":1381.0,
    "tags":"excel, excel-formula, if-statement, textjoin",
    "text_length":340,
    "title_length":32,
    "num_tags":4
  },
  {
    "id":5710,
    "title":"How can I replicate this specific animated gradient (blue to purple) in HTML\/CSS, React, or Flutter?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726270\/how-can-i-replicate-this-specific-animated-gradient-blue-to-purple-in-html-css",
    "text":"I’m trying to replicate a specific animated gradient background similar to this image, but I can’t get the colors, direction, or animation quite right. I’ve tried using ChatGPT and Claude and experimenting with different gradients, but I still can’t match the look — my current version is only around 60% similar visually. Here’s my closest attempt using plain HTML\/CSS: ``` * { margin: 0; padding: 0; box-sizing: border-box; } body, html { height: 100%; overflow: hidden; } .gradient-container { width: 100vw; height: 100vh; background: linear-gradient(135deg, #1a237e 0%, #3f51b5 25%, #673ab7 50%, #9c27b0 75%, #e91e63 100%); background-size: 200% 200%; animation: gradientShift 8s ease-in-out infinite; } @keyframes gradientShift { 0%, 100% { background-position: 0% 50%; } 50% { background-position: 100% 50%; } } ``` ``` <div class=\"gradient-container\"><\/div> ``` I’d appreciate: Feedback on how to make the gradient match the image more closely Alternative implementations in React or Flutter if that’s easier Any tips on achieving smoother or more vibrant transitions",
    "author_id":5246,
    "publication_date":1754411302000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Juan Casas",
    "author_reputation":140.0,
    "tags":"next.js, reactjs, flutter, html, css",
    "text_length":1074,
    "title_length":100,
    "num_tags":5
  },
  {
    "id":5709,
    "title":"Is it possible to select the first row in a table with CSS?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726277\/is-it-possible-to-select-the-first-row-in-a-table-with-css",
    "text":"Given: ``` <table> <thead> <caption>Table Title<\/caption> <tr>...<\/tr> <\/thead> <tbody> <tr>...<\/tr> <\/tbody> <\/table> ``` Is there a css-way to select (in the meaning of: targeting with a css selector) only the first existing ``` tr ``` , knowing that: thead might exist, so it should be the ``` tr ``` inside the thead, if that exist caption might not exist thead might not exist or it might not contain any ``` tr ``` s, so it should be the first ``` tr ``` inside the tbody in that case.",
    "author_id":5245,
    "publication_date":1754411571000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"rhavin",
    "author_reputation":1738.0,
    "tags":"html, css, css-selectors",
    "text_length":491,
    "title_length":59,
    "num_tags":3
  },
  {
    "id":5708,
    "title":"Why element.querySelector matches a selector referencing an ancestor of element?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726278\/why-element-queryselector-matches-a-selector-referencing-an-ancestor-of-element",
    "text":"In this example I would expect that the ``` elements ``` list is empty, since we set the starting point of the query to the ``` div.bbb ``` element. Somehow it matches the span element even though none of the div elements have any siblings except for the ``` .aaa ``` parent element, which is an ancestor of our starting element. Why does this work like that? ``` const parent = document.querySelector(\"div.bbb\"); const elements = parent.querySelectorAll(\"div:nth-of-type(2) span\"); console.log(elements.length); ``` ``` div.bbb div:nth-of-type(2) span { border: 1px red solid; } ``` ``` <div class=\"000\"><\/div> <div class=\"aaa\"> <div class=\"bbb\"> <div class=\"ccc\"> <span>me<\/span> <\/div> <\/div> <\/div> ```",
    "author_id":5244,
    "publication_date":1754411586000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Martins Balodis",
    "author_reputation":2097.0,
    "tags":"javascript, html, dom",
    "text_length":706,
    "title_length":80,
    "num_tags":3
  },
  {
    "id":5707,
    "title":"Activating ADC in CubeMX forbids flashing of STM32-Nucleo-F303K8",
    "link":"https:\/\/stackoverflow.com\/questions\/79726279\/activating-adc-in-cubemx-forbids-flashing-of-stm32-nucleo-f303k8",
    "text":"I want to use the Nucleo-F303K8 to readout some ADC's. Now the weird problem I have found is: As soon as I am activating ADC1 or ADC2 I cannot flash my Nucleo anymore. This is the error message, but it is not helping me: ``` Info : clock speed 1000 kHz Info : STLINK V2J45M31 (API v2) VID:PID 0483:374B Info : Target voltage: 3.239293 Info : [stm32f3x.cpu] Cortex-M4 r0p1 processor detected Info : [stm32f3x.cpu] target has 6 breakpoints, 4 watchpoints Info : gdb port disabled Info : Unable to match requested speed 1000 kHz, using 950 kHz Info : Unable to match requested speed 1000 kHz, using 950 kHz [stm32f3x.cpu] halted due to debug-request, current mode: Thread xPSR: 0x01000000 pc: 0xfffffffe msp: 0xfffffffc Info : Unable to match requested speed 8000 kHz, using 4000 kHz Info : Unable to match requested speed 8000 kHz, using 4000 kHz ** Programming Started ** Info : device id = 0x10016438 Info : flash size = 64 KiB Warn : Adding extra erase range, 0x08005798 .. 0x080057ff Error: error writing to flash at address 0x08000000 at offset 0x00000000 embedded:startup.tcl:1516: Error: ** Programming Failed ** in procedure 'program' in procedure 'program_error' called at file \"embedded:startup.tcl\", line 1581 at file \"embedded:startup.tcl\", line 1516 ``` Even when I have nothing configured except the ADC it crashes. I am directly connected via usb to the nucleo and I am using clion with OCD to flash and debug.",
    "author_id":5243,
    "publication_date":1754411715000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Cats",
    "author_reputation":11.0,
    "tags":"c, clion, stm",
    "text_length":1423,
    "title_length":64,
    "num_tags":3
  },
  {
    "id":5706,
    "title":"How can I make the &quot;Figure XX:&quot; text bold and the description text regular font in a bookdown::html_document2?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726280\/how-can-i-make-the-figure-xx-text-bold-and-the-description-text-regular-font",
    "text":"For the output of the below Rmd I would like the \" Figure XX: \" part to be bold and the descriptive text to be in regular font. How can I achieve that? ``` --- title: \"Figure caption things\" output: bookdown::html_document2 --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE) library(ggplot2) library(plotly) plot <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() ``` ## My first ggplot2 figure TEXT TEXT TEXT ```{r fig-ggplot, fig.cap = \"My super caption for this ggplot figure here!\"} plot ``` TEXT TEXT TEXT ## My second plotly figure TEXT TEXT TEXT ```{r fig-plotly, fig.cap = \"My super caption for this plolty figure here?!\"} ggplotly(plot) ``` TEXT TEXT TEXT ```",
    "author_id":5031,
    "publication_date":1754411810000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Patrick",
    "author_reputation":1659.0,
    "tags":"r, r-markdown, bookdown",
    "text_length":722,
    "title_length":120,
    "num_tags":3
  },
  {
    "id":5705,
    "title":"Drop or cast zero default date in schema in sqlite-to-postgres migration using pgloader",
    "link":"https:\/\/stackoverflow.com\/questions\/79726283\/drop-or-cast-zero-default-date-in-schema-in-sqlite-to-postgres-migration-using-p",
    "text":"I use pgloader to upload 11 databases from BIRD-DEV dataset into Postgres. ``` pgloader \"$sqlite_path\" \"postgresql:\/\/\/$pg_db_name\" ``` 9 databases are uploaded correctly, but 2 give fatal error because of a zero default date in the schema: ``` 2025-08-05T14:30:41.188003Z ERROR Database error 22008: date\/time field value out of range: \"0000-00-00\" QUERY: CREATE TABLE races ( raceid bigserial, year bigint default '0', round bigint default '0', circuitid bigint default '0', name text default '', date date default '0000-00-00', time text, url text ); 2025-08-05T14:30:41.188003Z FATAL Failed to create the schema, see above. ``` Is it possible to solve this problem on the level of pgloader without editing the database file ?",
    "author_id":5242,
    "publication_date":1754411915000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Stanislav Kikot",
    "author_reputation":15.0,
    "tags":"postgresql, sqlite, pgloader",
    "text_length":728,
    "title_length":87,
    "num_tags":3
  },
  {
    "id":5704,
    "title":"What is defined to happen if SetConsoleCtrlHandler is used twice with the same handler routine?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726290\/what-is-defined-to-happen-if-setconsolectrlhandler-is-used-twice-with-the-same-h",
    "text":"SetConsoleCtrlHandler takes a function pointer and a boolean ( ``` true ``` to add another handler, ``` false ``` to remove a handler). When a ``` CTRL+C ``` event occurs, it will call the registered handler routines in reverse order (last-added first-called). What happens if my setup code is called a second time? I think these options sound plausible: It detects that it already does know this handler and does nothing It detects that it already knows this handler, removes the original entry, and adds it again in the new position (for ordering) It does not detect anything and the handler will be called twice It is undefined behavior and anything might happen, dependent on machine, wall-clock, and weather It is defined as unsupported usage and leads to termination of the process The possibility of undefined behaviour makes me wary of simply trying it out. Is there a documentation that explains what happens in this case? In case it matters, I am using c++23. Experimentation on my machine makes it seem that these are true: The handler routine will be called twice using ``` SetConsoleCtrlHandler(handler, FALSE) ``` before the setup code is okay to do even if there was no previous registration of the handler.",
    "author_id":5241,
    "publication_date":1754412135000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"lucidbrot",
    "author_reputation":6334.0,
    "tags":"c++, winapi",
    "text_length":1222,
    "title_length":95,
    "num_tags":2
  },
  {
    "id":5703,
    "title":"Keep row selection on mouse clicks on JTable",
    "link":"https:\/\/stackoverflow.com\/questions\/79726292\/keep-row-selection-on-mouse-clicks-on-jtable",
    "text":"How can Swing tables keep current selection when the user clicks on a row – without ``` Ctrl ``` pressed, but as if it is the case? A table you can experiment on: ``` import javax.swing.JFrame; import javax.swing.JPanel; import javax.swing.JScrollPane; import javax.swing.JTable; import javax.swing.table.DefaultTableModel; import java.awt.BorderLayout; public class LoadedTableDemo { private static JTable table; public static void main(String[] args) { JFrame frame = new JFrame(\"JTable Example\"); frame.setContentPane(createMainPanel()); frame.pack(); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.setLocationRelativeTo(null); frame.setVisible(true); } private static JPanel createMainPanel() { JPanel panel = new JPanel(new BorderLayout()); panel.add(createTableScroller()); return panel; } private static JScrollPane createTableScroller() { JTable table = createTable(); return new JScrollPane(table); } private static JTable createTable() { table = new JTable(); table.setModel(createTableModel()); return table; } private static DefaultTableModel createTableModel() { String[] columns = {\"Name\", \"Age\", \"City\"}; Object[][] data = { {\"Alice\", 25, \"New York\"}, {\"Bob\", 30, \"Los Angeles\"}, {\"Charlie\", 22, \"Chicago\"}, {\"Diana\", 28, \"Houston\"}, {\"Eve\", 35, \"Miami\"} }; DefaultTableModel model = new DefaultTableModel(data, columns) { @Override public boolean isCellEditable(int row, int column) { return false; } }; return model; } } ```",
    "author_id":5240,
    "publication_date":1754412232000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Sergey Zolotarev",
    "author_reputation":2487.0,
    "tags":"java, swing",
    "text_length":1454,
    "title_length":44,
    "num_tags":2
  },
  {
    "id":5702,
    "title":"Prometheus not differentiating data from different hostname or source with node-exporter",
    "link":"https:\/\/stackoverflow.com\/questions\/79726294\/prometheus-not-differentiating-data-from-different-hostname-or-source-with-node",
    "text":"I am trying to save data from node exporter to prometheus. But I am having a trouble. Although we are storing data from different nodes, still it's showing the same data. From the first node: From the second node: On the main server I have a compose file like this: ``` services: prometheus: image: prom\/prometheus:latest container_name: prometheus restart: unless-stopped network_mode: \"host\" # Use host networking for direct access ports: - \"9090:9090\" volumes: - .\/prometheus.yml:\/etc\/prometheus\/prometheus.yml:ro - prometheus_data:\/prometheus command: - \"--config.file=\/etc\/prometheus\/prometheus.yml\" - \"--storage.tsdb.path=\/prometheus\" - \"--web.console.libraries=\/etc\/prometheus\/console_libraries\" - \"--web.console.templates=\/etc\/prometheus\/consoles\" - \"--web.enable-remote-write-receiver\" - \"--web.enable-lifecycle\" grafana: image: grafana\/grafana:latest container_name: grafana restart: unless-stopped network_mode: \"host\" # Use host networking for direct access ports: - \"3000:3000\" environment: - GF_SECURITY_ADMIN_USER=admin - GF_SECURITY_ADMIN_PASSWORD=admin volumes: - grafana_data:\/var\/lib\/grafana depends_on: - prometheus volumes: prometheus_data: grafana_data: ``` Additional file: ``` global: scrape_interval: 15s scrape_configs: - job_name: \"self\" static_configs: - targets: [\"localhost:9090\"] ``` And in the client\/targeted device side, I have compose file like this: ``` services: node-exporter: image: prom\/node-exporter:latest container_name: node-exporter restart: unless-stopped network_mode: \"host\" command: - '--path.rootfs=\/host' - '--web.listen-address=:9100' volumes: - '\/:\/host:ro,rslave' prom-agent: image: prom\/prometheus:latest container_name: prom-agent restart: unless-stopped network_mode: \"host\" ports: - \"9091:9090\" # This will be ignored with host networking volumes: - .\/prometheus-agent.yml:\/etc\/prometheus\/prometheus.yml:ro - prom_agent_data:\/prometheus command: - \"--config.file=\/etc\/prometheus\/prometheus.yml\" - \"--storage.tsdb.path=\/prometheus\" - \"--web.console.libraries=\/etc\/prometheus\/console_libraries\" - \"--web.console.templates=\/etc\/prometheus\/consoles\" - \"--web.enable-lifecycle\" - \"--web.listen-address=:9091\" volumes: prom_agent_data: ``` And here is the related file: ``` global: scrape_interval: 15s scrape_configs: - job_name: 'node' static_configs: - targets: ['localhost:9100'] remote_write: - url: \"https:\/\/dhinkachika.wow\/api\/v1\/write\" ``` Do I need to configure more to uniquely identify each machines? NB: I even turned down that server for like hours to see if I am missing something or not, or there are some overlapping data or not. But no, it's plain non-distinguishable data issue.",
    "author_id":5239,
    "publication_date":1754412386000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Maifee Ul Asad",
    "author_reputation":4638.0,
    "tags":"grafana, prometheus-node-exporter, prometheus",
    "text_length":2648,
    "title_length":88,
    "num_tags":3
  },
  {
    "id":5701,
    "title":"What is Total Size and how can reduce it?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726304\/what-is-total-size-and-how-can-reduce-it",
    "text":"I'm developing a 2D game in Unity (version 6.1). After building and installing the APK on my Android device, I noticed that the app takes up about 170MB of space. However: The APK file is only 47MB. The Unity build report shows a Total Size of 887MB. I'm confused about these differences. Is this normal behavior for Unity builds? I'm especially concerned about the \"Total Size: 887MB\" in the build report — it feels way too large for a small 2D game. Here’s what I’d like help with: Is it normal for the build report to show such a large total size compared to the final APK? What parts of my project might be contributing to this? How can I reduce the build size effectively? Any advice or pointers on what to check would be greatly appreciated. Some images from the build report Buildreportimage-1 Buildreportimage-2 Buildreportimage-3",
    "author_id":5238,
    "publication_date":1754412908000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"zafer",
    "author_reputation":479.0,
    "tags":"android, unity-game-engine, apk",
    "text_length":838,
    "title_length":41,
    "num_tags":3
  },
  {
    "id":5700,
    "title":"How to correctly configure eslint-plugin-import in Next.js 15 with TypeScript and path aliases?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726307\/how-to-correctly-configure-eslint-plugin-import-in-next-js-15-with-typescript-an",
    "text":"I'm working with a Next.js 15 project using TypeScript, and I'm trying to properly configure ``` eslint-plugin-import ``` to work with path aliases (like ``` @\/ ``` ), as well as import sorting and unresolved path detection. I’ve already done the following: I created a ``` tsconfig.json ``` with this paths setup: ``` { \"compilerOptions\": { \"baseUrl\": \".\", \"paths\": { \"@\/*\": [\"src\/*\"] } } } ``` I installed all required packages: ``` npm install --save-dev eslint eslint-plugin-import eslint-import-resolver-typescript ``` ``` import importPlugin from 'eslint-plugin-import'; export default [ { files: ['**\/*.{js,ts,jsx,tsx}'], plugins: { import: importPlugin, }, settings: { 'import\/resolver': { typescript: { project: '.\/tsconfig.json', }, node: { extensions: ['.js', '.jsx', '.ts', '.tsx'], }, }, }, rules: { 'import\/no-unresolved': 'error', 'import\/order': [ 'error', { groups: ['builtin', 'external', 'internal'], pathGroups: [ { pattern: '@\/**', group: 'internal', position: 'after', }, ], 'newlines-between': 'always', alphabetize: { order: 'asc', caseInsensitive: true }, }, ], }, }, ]; ``` Even after all of this: ESLint does not report unresolved imports when I write something like ``` import x from '@\/not-existing-file' ``` . The ``` import\/order ``` rule doesn't recognize path groups correctly. Aliases like ``` @\/components\/Button ``` sometimes trigger ``` import\/no-unresolved ``` . I expect ``` import\/no-unresolved ``` to correctly validate all imports including aliases. ``` import\/order ``` to correctly group and sort imports (especially ``` @\/** ``` under \"internal\"). What are the correct steps to fully configure ``` eslint-plugin-import ``` in a Next.js 15 project using flat config, TypeScript, and path aliases?",
    "author_id":5237,
    "publication_date":1754413108000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Edgar",
    "author_reputation":6958.0,
    "tags":"typescript, eslint, next.js15",
    "text_length":1740,
    "title_length":95,
    "num_tags":3
  },
  {
    "id":5699,
    "title":"OAS Generator creates classes even for referenced schemas",
    "link":"https:\/\/stackoverflow.com\/questions\/79726312\/oas-generator-creates-classes-even-for-referenced-schemas",
    "text":"I have two yml files Common.yml and application.yml Both under same folder I have certain schemas like sortOrder that are defined in common schema And using $ref it is referred in application.yaml Problem is schemas that are referred are generated twice One under common folder and one under application folder E.g. SortOrder.java is generated twice..one under common folder and one under application folder Expected is Only one class should be generated under Common folder What I tried but NO luck: 1.Typemappings and importMappings 2.even checked the order of generation.. first is common followed by application 3.the $ref path is correct Anyway to resolve this duplicate class generation issue",
    "author_id":5236,
    "publication_date":1754413260000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"JayakumarSivasankar",
    "author_reputation":53.0,
    "tags":"java, swagger, yaml, oas",
    "text_length":698,
    "title_length":57,
    "num_tags":4
  },
  {
    "id":5698,
    "title":"Expo TaskManager not executing on scheduled notification trigger",
    "link":"https:\/\/stackoverflow.com\/questions\/79726321\/expo-taskmanager-not-executing-on-scheduled-notification-trigger",
    "text":"I'm building a sunrise alarm clock app in React Native using Expo (SDK 53) and I'm having trouble getting a background task to run when a scheduled notification is delivered. My Goal: When the user activates a new Alarm i want to program a notification and lunch the alarm sequence at a specified time that the user mentioned before. The Problem: The notification is being shown correctly in the background but I get no logs and none of the functions that i set are being lunched. The task does run correctly if the notification is received while the app is in the foreground. The failure is only when the app is in the background or terminated. What I've Tried: This task is defined globally in my root layout app\/_layout.jsx. and then i define the task (@\/components\/light\/setupApp.js) Requesting Permissions: I am successfully requesting and receiving granted status for notification permissions. \/\/ app.json snippet ``` { \"expo\": { \"name\": \"WakeUp\", \"slug\": \"WakeUp\", \"version\": \"1.0.0\", \"orientation\": \"portrait\", \"icon\": \".\/assets\/images\/icon.png\", \"scheme\": \"wakeup\", \"userInterfaceStyle\": \"automatic\", \"newArchEnabled\": true, \"ios\": { \"supportsTablet\": true }, \"android\": { \"adaptiveIcon\": { \"foregroundImage\": \".\/assets\/images\/adaptive-icon.png\", \"backgroundColor\": \"#1B1A2C\" }, \"edgeToEdgeEnabled\": true, \"package\": \"com.IvnoGood.WakeUp\" }, \"web\": { \"bundler\": \"metro\", \"output\": \"static\", \"favicon\": \".\/assets\/images\/favicon.png\" }, \"plugins\": [ \"expo-router\", [ \"expo-splash-screen\", { \"image\": \".\/assets\/images\/splash-icon.png\", \"imageWidth\": 200, \"resizeMode\": \"contain\", \"backgroundColor\": \"#fff8f4\", \"dark\": { \"backgroundColor\": \"#18120d\", \"image\": \".\/assets\/images\/splash-icon.png\" } } ], \"expo-font\", [ \"expo-notifications\", { \"color\": \"#1B1A2C\", \"icon\": \".\/assets\/images\/notificationIcon.png\", \"defaultChannel\": \"default\", \"enableBackgroundRemoteNotifications\": true } ], [ \"expo-task-manager\", { \"permissionMessage\": \"Allow WakeUp to run alarms and other background tasks.\" } ] ], \"experiments\": { \"typedRoutes\": true }, \"extra\": { \"router\": {}, \"eas\": { \"projectId\": \"d6e9fdf5-0dfa-47f0-982a-5aee7b7a5827\" } } } } ``` Here is the relevant code for my setup. utils\/setupApp.js (Task Definition and Setup) ``` import { startLightUpSequence } from '.\/lightControl'; import * as Notifications from 'expo-notifications'; import * as TaskManager from 'expo-task-manager'; const ALARM_WAKE_UP_TASK = 'ALARM_WAKE_UP_TASK'; export async function setupAllTasksAndPermissions() { \/\/ 1. Define the Task if (!TaskManager.isTaskDefined(ALARM_WAKE_UP_TASK)) { TaskManager.defineTask(ALARM_WAKE_UP_TASK, ({ data, error }) => { if (error) { console.error('--- TASK MANAGER ERROR ---', error); return; } if (data) { \/\/ This console.log is NEVER called when the app is in the background console.log('--- BACKGROUND TASK IS RUNNING ---'); const alarmData = data?.notification?.request?.content?.data; if (alarmData) { const { alarm, device } = alarmData; startLightUpSequence({ device, alarm }); } } }); } \/\/ 2. Define Notification Categories await Notifications.setNotificationCategoryAsync('alarm', [ { identifier: 'stop', buttonTitle: 'Stop' }, ]); \/\/ 3. Set Foreground Handler Notifications.setNotificationHandler({ handleNotification: async () => ({ shouldShowAlert: true, shouldPlaySound: true, shouldSetBadge: false, }), }); } ``` @\/components\/notifications.js (Scheduling and request logic) ``` import * as Notifications from 'expo-notifications'; import { Platform } from 'react-native'; export async function requestNotificationPermissions() { const { status: existingStatus } = await Notifications.getPermissionsAsync(); let finalStatus = existingStatus; if (existingStatus !== 'granted') { const { status } = await Notifications.requestPermissionsAsync(); finalStatus = status; } \/\/ Stop here if the user did not grant permissions if (finalStatus !== 'granted') { alert('Failed to get push token for push notification!'); return false; } \/\/ For Android, set a notification channel if (Platform.OS === 'android') { await Notifications.setNotificationChannelAsync('default', { name: 'default', importance: Notifications.AndroidImportance.MAX, vibrationPattern: [0, 250, 250, 250], lightColor: '#FF231F7C', }); } return true; } export async function scheduleAlarmNotification(alarm, device) { \/\/ ... (Your logic to calculate triggerDate is here) ... const now = new Date(); const [hours, minutes] = alarm.startTime.split(\":\"); let triggerDate = new Date(now.getFullYear(), now.getMonth(), now.getDate(), parseInt(hours), parseInt(minutes), 0); if (triggerDate < now) { triggerDate.setDate(triggerDate.getDate() + 1); } await Notifications.cancelScheduledNotificationAsync(alarm.id); await Notifications.scheduleNotificationAsync({ identifier: alarm.id, content: { title: 'Wake Up!', body: `Your alarm \"${alarm.title}\" is starting.`, sound: 'default', \/\/ --- THIS IS THE CRITICAL PART --- data: { \/\/ Pass the full alarm and device objects so the task can use them alarm: alarm, device: device, }, categoryIdentifier: 'alarm', }, \/\/ For Task Manager, the trigger can be the Date object directly. \/\/ It will still wake up the task. trigger: triggerDate, }); console.log(`Alarm scheduled for ${alarm.title} at ${triggerDate.toLocaleTimeString()}`); } ``` app\/_layout.jsx (Root Layout) ``` import { blink } from '@\/components\/light\/lightUp'; import { setupAllTasksAndPermissions } from '@\/components\/light\/setupApp'; import LightStateProvider from '@\/components\/provider\/LightStateProvider'; import ThemeProvider, { useAppTheme } from '@\/components\/provider\/ThemeProvider'; import AsyncStorage from \"@react-native-async-storage\/async-storage\"; import * as Notifications from 'expo-notifications'; import { Stack, useRouter } from 'expo-router'; import { StatusBar } from 'expo-status-bar'; import { useEffect } from 'react'; import 'react-native'; import { PaperProvider } from 'react-native-paper'; function Layout() { const { theme, setThemeName } = useAppTheme(); const router = useRouter() useEffect(() => { \/\/ --- LISTENER #1: Whe n a notification is RECEIVED while the app is in the foreground --- const notificationReceivedSubscription = Notifications.addNotificationReceivedListener(notification => { console.log(\"FOREGROUND: Notification received!\"); \/\/ Extract the data payload const { alarm, device } = notification.request.content.data; \/\/ Check if we have the data we need if (alarm && device) { console.log(`FOREGROUND: Manually starting light sequence for alarm \"${alarm.title}\"`); \/\/ Manually start your light sequence function blink(device, alarm, false) } else { console.warn(\"FOREGROUND: Notification received, but no alarm\/device data found in payload.\"); } }); return () => { notificationReceivedSubscription.remove(); subscription.remove(); }; }, []); return ( <PaperProvider theme={theme}> \/\/All the screens <\/PaperProvider> ); } export default function RootLayout() { useEffect(() => { setupAllTasksAndPermissions() }, []) return ( <LightStateProvider> <ThemeProvider> <Layout \/> <\/ThemeProvider> <\/LightStateProvider> ) } ``` You can also check the Github Repo for more details about my code Question: What am I missing that is preventing the defined TaskManager task from executing when a scheduled notification is delivered while the app is in the background or terminated? Is there a more direct way to link the notification to the task that I'm not using? Thank you",
    "author_id":5235,
    "publication_date":1754413898000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"IvnoGood",
    "author_reputation":3.0,
    "tags":"android, react-native, expo, notifications, background-process",
    "text_length":7399,
    "title_length":64,
    "num_tags":5
  },
  {
    "id":5697,
    "title":"Compile-time manipulation of std::source_location::current()",
    "link":"https:\/\/stackoverflow.com\/questions\/79726322\/compile-time-manipulation-of-stdsource-locationcurrent",
    "text":"I want to write a function that can obtain caller's function name, or at least its length, at compile time . I think obtaining ``` std::source_location::current() ``` is close to what I want and currently I'm able to do this: ``` consteval auto funcNameLength(const std::source_location &Loc = std::source_location::current()) { const char *Name = Loc.function_name(); std::size_t Length = 0; while (Name[Length] != '\\0') ++Length; return Length; } int main() { constexpr auto L = funcNameLength(); \/\/ L == 4 } ``` or this (with a constexpr strlen): ``` consteval auto funcNameLength(std::size_t Length = constexpr_strlen( std::source_location::current().function_name())) { return Length; } int main() { constexpr auto L = funcNameLength(); } ``` However, that is not enough for me. I need to use the length as a constant expression to do something before returning to caller, e.g. ``` consteval auto doSomething(std::size_t Length = constexpr_strlen( std::source_location::current().function_name())) { int A[Length]; \/\/ Error: Length is not a constant expression. } int main() { doSomething(); } ``` Is this achievable?",
    "author_id":5234,
    "publication_date":1754413996000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"GKxx",
    "author_reputation":403.0,
    "tags":"c++, constexpr, consteval, std-source-location",
    "text_length":1122,
    "title_length":60,
    "num_tags":4
  },
  {
    "id":5696,
    "title":"How to reclaim horizontal space made available by removing labels and ticks?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726327\/how-to-reclaim-horizontal-space-made-available-by-removing-labels-and-ticks",
    "text":"I have a row of subplots which start with a histogram and the rest are some ``` qqplot ``` s: I removed the y axis and ticks labels using ``` for ax in axqq[1:]: ax.set_ylabel(None) ax.tick_params(labelleft=False) # does not help ax.yaxis.set_visible(False) ax.sharey(axqq[0]) fig.tight_layout() ``` I would like to remove the horizontal whitespace between the QQ plots. Since my subplots are not homogeneous (the 1st subplot is a histogram), ``` fig.subplots_adjust ``` does not help.",
    "author_id":5233,
    "publication_date":1754414539000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"sds",
    "author_reputation":60318.0,
    "tags":"python, matplotlib, whitespace, subplot",
    "text_length":485,
    "title_length":76,
    "num_tags":4
  },
  {
    "id":5695,
    "title":"Why presenting ActivityView created from UIViewRepresentable is not loaded for the very first time?",
    "link":"https:\/\/stackoverflow.com\/questions\/79726331\/why-presenting-activityview-created-from-uiviewrepresentable-is-not-loaded-for-t",
    "text":"I am using the following library to create screenshot of my View: https:\/\/github.com\/RickeyBoy\/ScreenshotableView In code it is presented like this: ``` struct TreasureView: View { @State var activityItemsForShare: [Any]? @State var isActivityViewPresented = false @State var shot = false var body: some View { VStack { ScreenshotableView(shotting: $shot) { image in activityItemsForShare = [image] isActivityViewPresented = true } content: { style in homeView \/\/ view for screenshot, it might be anything. } .overlay(alignment: .bottomLeading) { Button { shot.toggle() } label: { Text(\"share\") } } } .sheet(isPresented: $isActivityViewPresented) { if let activityItemsForShare = activityItemsForShare { ActivityView(activityItems: activityItemsForShare) } } } } ``` And my ``` ActivityView ``` is very simple: ``` struct ActivityView: UIViewControllerRepresentable { var activityItems: [Any] var completion: (() -> Void)? = nil func makeUIViewController(context: UIViewControllerRepresentableContext<ActivityView>) -> UIActivityViewController { let controller = UIActivityViewController(activityItems: activityItems, applicationActivities: nil) controller.completionWithItemsHandler = { _, isDone, _, _ in if isDone { completion?() } } return controller } func updateUIViewController(_ uiViewController: UIActivityViewController, context: UIViewControllerRepresentableContext<ActivityView>) {} } ``` And for the very first tap when view is loaded, without any logs or errors (totally empty view without anything): and the second time, everything is fine (correct UIActivityViewController): What to do to make it working for the first time?",
    "author_id":5232,
    "publication_date":1754414821000,
    "scraped_at":1754660258000,
    "scrape_method":"api",
    "author_name":"Bartłomiej Semańczyk",
    "author_reputation":62279.0,
    "tags":"swiftui, screenshot, uiviewrepresentable",
    "text_length":1640,
    "title_length":99,
    "num_tags":3
  }
]