{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation and validation\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_if_missing(package):\n",
    "    \"\"\"Install package if not available in current environment\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Required packages for data analysis\n",
    "required_packages = ['pandas', 'matplotlib', 'seaborn', 'plotly', 'wordcloud', 'ipywidgets']\n",
    "for package in required_packages:\n",
    "    install_if_missing(package)\n",
    "\n",
    "print(\"Environment setup completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Project-specific modules\n",
    "from scraper_mariadb import StackOverflowScraperMariaDB\n",
    "from mariadb_crud import MariaDBCRUD\n",
    "from unified_scraper import StackOverflowScraper, QuestionData\n",
    "\n",
    "# Configure visualization settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All dependencies loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data Collection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collection system\n",
    "scraper = None\n",
    "scraping_results = {}\n",
    "\n",
    "def initialize_scraper():\n",
    "    \"\"\"Initialize MariaDB-backed scraper instance\"\"\"\n",
    "    global scraper\n",
    "    try:\n",
    "        scraper = StackOverflowScraperMariaDB()\n",
    "        print(\"Scraper initialized with MariaDB connection\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Scraper initialization failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# System initialization\n",
    "if initialize_scraper():\n",
    "    print(\"Data collection system ready\")\n",
    "else:\n",
    "    print(\"Please verify MariaDB connection and configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection interface\n",
    "method_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Stack Exchange API (Recommended - Most Reliable)', 'api'),\n",
    "        ('Beautiful Soup (HTML parsing)', 'beautifulsoup'),\n",
    "        ('Selenium (Browser automation - UNSTABLE)', 'selenium')\n",
    "    ],\n",
    "    value='api',  # Default to most reliable method\n",
    "    description='Collection Method:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Add method recommendation info with Selenium warning\n",
    "method_info = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style='padding: 10px; background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 5px; margin: 10px 0;'>\n",
    "        <strong>Recommendation:</strong> Use the <strong>Stack Exchange API</strong> for best results.<br>\n",
    "        <strong>‚ö†Ô∏è Selenium Issues:</strong> Currently experiencing timeout errors due to page loading issues.\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "num_questions_input = widgets.IntText(\n",
    "    value=20,\n",
    "    description='Questions to collect:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "avoid_duplicates_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Skip duplicate questions',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "delete_database_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Clear database before collection',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "warning_message = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style='padding: 10px; background-color: #fff3cd; border: 1px solid #ffeaa7; border-radius: 5px; margin: 10px 0;'>\n",
    "        <strong>Warning:</strong> This will permanently delete all existing data.\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    layout=widgets.Layout(display='none')\n",
    ")\n",
    "\n",
    "# Selenium-specific warning\n",
    "selenium_warning = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style='padding: 10px; background-color: #f8d7da; border: 1px solid #f5c6cb; border-radius: 5px; margin: 10px 0;'>\n",
    "        <strong>‚ö†Ô∏è Selenium Method Selected:</strong><br>\n",
    "        Selenium is currently experiencing issues with:<br>\n",
    "        ‚Ä¢ TimeoutExceptions (elements not loading in time)<br>\n",
    "        ‚Ä¢ Stale element reference errors<br>\n",
    "        ‚Ä¢ Chrome browser compatibility problems<br><br>\n",
    "        <strong>Strongly recommended:</strong> Use \"Stack Exchange API\" instead for reliable results.\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    layout=widgets.Layout(display='none')\n",
    ")\n",
    "\n",
    "def toggle_warning(change):\n",
    "    \"\"\"Toggle warning visibility based on delete option\"\"\"\n",
    "    warning_message.layout.display = 'block' if change['new'] else 'none'\n",
    "\n",
    "def toggle_selenium_warning(change):\n",
    "    \"\"\"Show/hide Selenium warning based on method selection\"\"\"\n",
    "    if change['new'] == 'selenium':\n",
    "        selenium_warning.layout.display = 'block'\n",
    "    else:\n",
    "        selenium_warning.layout.display = 'none'\n",
    "\n",
    "delete_database_checkbox.observe(toggle_warning, names='value')\n",
    "method_dropdown.observe(toggle_selenium_warning, names='value')\n",
    "\n",
    "start_button = widgets.Button(\n",
    "    description='Start Collection',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def clear_database_tables(scraper):\n",
    "    \"\"\"Clear all database tables in correct order\"\"\"\n",
    "    cursor = scraper.crud.connection.cursor()\n",
    "    try:\n",
    "        # Delete in correct order to avoid foreign key constraint issues\n",
    "        cursor.execute(\"DELETE FROM question_tags\")\n",
    "        cursor.execute(\"DELETE FROM questions\") \n",
    "        cursor.execute(\"DELETE FROM authors\")\n",
    "        cursor.execute(\"DELETE FROM tags\")\n",
    "        scraper.crud.connection.commit()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        scraper.crud.connection.rollback()\n",
    "        print(f\"Error clearing database: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def execute_collection(button):\n",
    "    \"\"\"Execute data collection with specified parameters\"\"\"\n",
    "    global scraping_results\n",
    "    \n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"Initiating data collection...\")\n",
    "        print(f\"Method: {method_dropdown.label}\")\n",
    "        print(f\"Target: {num_questions_input.value} questions\")\n",
    "        print(f\"Duplicate handling: {'Skip' if avoid_duplicates_checkbox.value else 'Allow'}\")\n",
    "        print(f\"Database reset: {'Yes' if delete_database_checkbox.value else 'No'}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Strong warning about Selenium method\n",
    "        if method_dropdown.value == 'selenium':\n",
    "            print(\"üö® CRITICAL WARNING: Selenium Method Selected\")\n",
    "            print(\"   Selenium is currently experiencing major stability issues:\")\n",
    "            print(\"   ‚Ä¢ TimeoutException errors (elements not loading)\")\n",
    "            print(\"   ‚Ä¢ Stale element reference errors\")\n",
    "            print(\"   ‚Ä¢ Chrome browser compatibility problems\")\n",
    "            print(\"   ‚Ä¢ High failure rate\")\n",
    "            print()\n",
    "            print(\"   RECOMMENDATION: Switch to 'Stack Exchange API' for reliable results\")\n",
    "            print(\"   Continuing with Selenium anyway...\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        if not scraper:\n",
    "            print(\"Error: Scraper not initialized\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Handle database reset if requested\n",
    "            if delete_database_checkbox.value:\n",
    "                print(\"Clearing existing database...\")\n",
    "                try:\n",
    "                    current_stats = scraper.get_database_stats()\n",
    "                    print(f\"Removing {current_stats['total_questions']:,} questions, {current_stats['total_authors']:,} authors, {current_stats['total_tags']:,} tags\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Clear database tables using the new function\n",
    "                if clear_database_tables(scraper):\n",
    "                    print(\"Database cleared successfully\\n\")\n",
    "                else:\n",
    "                    print(\"Warning: Database clearing had issues, continuing anyway\\n\")\n",
    "            \n",
    "            # Execute collection with enhanced error handling\n",
    "            print(\"Starting data collection...\")\n",
    "            result = scraper.scrape_and_store(\n",
    "                method=method_dropdown.value,\n",
    "                num_questions=num_questions_input.value,\n",
    "                avoid_duplicates=avoid_duplicates_checkbox.value\n",
    "            )\n",
    "            \n",
    "            scraping_results[method_dropdown.value] = result\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\nCOLLECTION COMPLETED\")\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Method: {result['method']}\")\n",
    "            print(f\"Retrieved: {result['scraped_count']} questions\")\n",
    "            print(f\"Stored: {result['stored_count']} new entries\")\n",
    "            print(f\"Duplicates: {result['duplicates_skipped']} skipped\")\n",
    "            print(f\"Execution time: {result['total_time']:.2f}s\")\n",
    "            \n",
    "            if result['stored_question_ids']:\n",
    "                print(f\"Sample IDs: {result['stored_question_ids'][:5]}\")\n",
    "            \n",
    "            # Success message with method-specific advice\n",
    "            if method_dropdown.value == 'selenium' and result['stored_count'] > 0:\n",
    "                print(\"\\nüéâ UNEXPECTED SUCCESS: Selenium worked despite known issues!\")\n",
    "                print(\"   Consider yourself lucky - this method is very unreliable\")\n",
    "            elif method_dropdown.value == 'api':\n",
    "                print(f\"\\n‚úÖ API collection completed successfully - reliable as expected\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Collection failed: {e}\")\n",
    "            \n",
    "            # Enhanced method-specific error advice\n",
    "            if method_dropdown.value == 'selenium':\n",
    "                print(\"\\nüö® SELENIUM FAILURE (Expected)\")\n",
    "                print(\"   This is a known issue with the Selenium scraper.\")\n",
    "                print(\"   The error indicates:\")\n",
    "                \n",
    "                error_str = str(e).lower()\n",
    "                if 'timeout' in error_str:\n",
    "                    print(\"   ‚Ä¢ TimeoutException: Browser couldn't find expected elements\")\n",
    "                    print(\"   ‚Ä¢ This usually means the page structure changed or loading is slow\")\n",
    "                elif 'stale' in error_str:\n",
    "                    print(\"   ‚Ä¢ Stale Element: Page elements became invalid during scraping\")\n",
    "                    print(\"   ‚Ä¢ This happens when the page DOM updates dynamically\")\n",
    "                else:\n",
    "                    print(\"   ‚Ä¢ General Selenium/Chrome browser error\")\n",
    "                \n",
    "                print(\"\\n   üí° SOLUTIONS:\")\n",
    "                print(\"   1. ‚úÖ Use 'Stack Exchange API' (RECOMMENDED)\")\n",
    "                print(\"   2. Try 'Beautiful Soup' as alternative\")\n",
    "                print(\"   3. Reduce number of questions if using Selenium\")\n",
    "                print(\"   4. Check Chrome browser version compatibility\")\n",
    "                \n",
    "            elif method_dropdown.value == 'beautifulsoup':\n",
    "                print(\"\\nüí° Beautiful Soup Troubleshooting:\")\n",
    "                print(\"   ‚Ä¢ Try using 'Stack Exchange API' for more reliable results\")\n",
    "                print(\"   ‚Ä¢ HTML structure may have changed\")\n",
    "                print(\"   ‚Ä¢ Network connectivity issues\")\n",
    "            else:\n",
    "                print(\"\\nüí° API Troubleshooting:\")\n",
    "                print(\"   ‚Ä¢ Check internet connection\")\n",
    "                print(\"   ‚Ä¢ API rate limits may apply\")\n",
    "                print(\"   ‚Ä¢ Stack Exchange API may be temporarily unavailable\")\n",
    "            \n",
    "            print(f\"\\nDetailed error information:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "start_button.on_click(execute_collection)\n",
    "\n",
    "# Display interface with enhanced warnings\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Collection Parameters</h3>\"),\n",
    "    method_info,\n",
    "    method_dropdown,\n",
    "    selenium_warning,\n",
    "    num_questions_input,\n",
    "    avoid_duplicates_checkbox,\n",
    "    delete_database_checkbox,\n",
    "    warning_message,\n",
    "    start_button,\n",
    "    output_area\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate database overview\n",
    "def get_database_overview():\n",
    "    \"\"\"Retrieve comprehensive database statistics\"\"\"\n",
    "    if not scraper:\n",
    "        print(\"Scraper not initialized\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        stats = scraper.get_database_stats()\n",
    "        \n",
    "        print(\"DATABASE OVERVIEW\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total Questions: {stats['total_questions']:,}\")\n",
    "        print(f\"Total Authors: {stats['total_authors']:,}\")\n",
    "        print(f\"Total Tags: {stats['total_tags']:,}\")\n",
    "        print(f\"Last Updated: {stats['last_scraped']}\")\n",
    "        \n",
    "        if stats['questions_by_method']:\n",
    "            print(\"\\nCollection Methods:\")\n",
    "            for method, count in stats['questions_by_method'].items():\n",
    "                print(f\"  ‚Ä¢ {method}: {count:,} questions\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving database statistics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Display current database state\n",
    "db_stats = get_database_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data for analysis\n",
    "def load_data_from_database(limit=1000):\n",
    "    \"\"\"Load questions from database into pandas DataFrame with preprocessing\"\"\"\n",
    "    if not scraper:\n",
    "        print(\"Scraper not initialized\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Retrieve questions from database\n",
    "        questions = scraper.crud.get_questions(limit=limit)\n",
    "        \n",
    "        if not questions:\n",
    "            print(\"No questions found in database\")\n",
    "            return None\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(questions)\n",
    "        \n",
    "        # Data preprocessing\n",
    "        if 'publication_date' in df.columns:\n",
    "            df['publication_date'] = pd.to_datetime(df['publication_date'])\n",
    "        \n",
    "        if 'scraped_at' in df.columns:\n",
    "            df['scraped_at'] = pd.to_datetime(df['scraped_at'])\n",
    "        \n",
    "        # Feature engineering\n",
    "        if 'text' in df.columns:\n",
    "            df['text_length'] = df['text'].str.len().fillna(0)\n",
    "        \n",
    "        if 'title' in df.columns:\n",
    "            df['title_length'] = df['title'].str.len().fillna(0)\n",
    "        \n",
    "        # Tag processing\n",
    "        if 'tags' in df.columns:\n",
    "            df['num_tags'] = df['tags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} questions from database\")\n",
    "        print(f\"Date range: {df['publication_date'].min()} to {df['publication_date'].max()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load dataset\n",
    "df = load_data_from_database(limit=1000)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display sample\n",
    "    display(df[['title', 'author_name', 'num_tags', 'publication_date', 'scrape_method']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Simple Tag Analysis\n",
    "\n",
    "This section helps you understand what programming topics are most popular in your Stack Overflow data. We'll look at:\n",
    "- Which tags appear most often\n",
    "- How many tags each question has\n",
    "- Simple charts to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to analyze tags\n",
    "def simple_tag_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze tags in a simple, easy-to-understand way\n",
    "    Perfect for beginners in data analysis!\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data available for analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"TAG ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Step 1: Count all tags\n",
    "    # We need to \"flatten\" the tag lists - combine all tags from all questions into one big list\n",
    "    all_tags = []\n",
    "    for question_tags in df['tags']:\n",
    "        if isinstance(question_tags, list):  # Make sure it's actually a list\n",
    "            all_tags.extend(question_tags)  # Add all tags from this question to our big list\n",
    "    \n",
    "    # Step 2: Count how many times each tag appears\n",
    "    from collections import Counter\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Step 3: Basic statistics\n",
    "    total_questions = len(df)\n",
    "    total_tag_uses = len(all_tags)\n",
    "    unique_tags = len(tag_counts)\n",
    "    avg_tags_per_question = df['num_tags'].mean()\n",
    "    \n",
    "    print(f\"Basic Numbers:\")\n",
    "    print(f\"   ‚Ä¢ Total questions: {total_questions:,}\")\n",
    "    print(f\"   ‚Ä¢ Total tag uses: {total_tag_uses:,}\")\n",
    "    print(f\"   ‚Ä¢ Unique tags: {unique_tags:,}\")\n",
    "    print(f\"   ‚Ä¢ Average tags per question: {avg_tags_per_question:.1f}\")\n",
    "    \n",
    "    # Step 4: Show top 10 most popular tags\n",
    "    top_10_tags = tag_counts.most_common(10)\n",
    "    print(f\"\\nTOP 10 MOST POPULAR TAGS:\")\n",
    "    for i, (tag, count) in enumerate(top_10_tags, 1):\n",
    "        percentage = (count / total_tag_uses) * 100\n",
    "        print(f\"   {i:2d}. {tag:<15} {count:3d} uses ({percentage:.1f}%)\")\n",
    "    \n",
    "    return tag_counts, top_10_tags\n",
    "\n",
    "# Run the analysis\n",
    "if df is not None:\n",
    "    tag_results = simple_tag_analysis(df)\n",
    "else:\n",
    "    print(\"Please load data first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple charts for tag analysis\n",
    "def create_simple_tag_charts(tag_results, df):\n",
    "    \"\"\"\n",
    "    Create easy-to-understand charts using matplotlib\n",
    "    Much simpler than the complex plotly version!\n",
    "    \"\"\"\n",
    "    if not tag_results or df is None:\n",
    "        print(\"No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    tag_counts, top_10_tags = tag_results\n",
    "    \n",
    "    # Set up the figure with 2 charts side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Tag Analysis Charts', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Chart 1: Bar chart of top 10 tags\n",
    "    tags = [tag for tag, count in top_10_tags]\n",
    "    counts = [count for tag, count in top_10_tags]\n",
    "    \n",
    "    bars = ax1.bar(tags, counts, color='skyblue', edgecolor='navy', linewidth=1)\n",
    "    ax1.set_title('Top 10 Most Popular Tags', fontweight='bold')\n",
    "    ax1.set_xlabel('Programming Tags')\n",
    "    ax1.set_ylabel('Number of Questions')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Chart 2: Histogram of tags per question\n",
    "    ax2.hist(df['num_tags'], bins=range(1, df['num_tags'].max() + 2), \n",
    "             color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "    ax2.set_title('How Many Tags Per Question?', fontweight='bold')\n",
    "    ax2.set_xlabel('Number of Tags')\n",
    "    ax2.set_ylabel('Number of Questions')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add some statistics text\n",
    "    avg_tags = df['num_tags'].mean()\n",
    "    most_common_tag_count = df['num_tags'].mode().iloc[0]\n",
    "    ax2.axvline(avg_tags, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Average: {avg_tags:.1f} tags')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some insights\n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(f\"   ‚Ä¢ Most questions have {most_common_tag_count} tags\")\n",
    "    print(f\"   ‚Ä¢ On average, questions have {avg_tags:.1f} tags\")\n",
    "    print(f\"   ‚Ä¢ '{top_10_tags[0][0]}' is by far the most popular topic!\")\n",
    "\n",
    "# Create the charts\n",
    "if df is not None and 'tag_results' in locals():\n",
    "    create_simple_tag_charts(tag_results, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Simple Time Analysis\n",
    "\n",
    "Let's look at **when** questions were posted! This helps us understand:\n",
    "- Which days of the week are most active\n",
    "- What time of day people ask questions\n",
    "- Basic patterns over time\n",
    "\n",
    "*Note: Time analysis works best when you have questions from different days/weeks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple time pattern analysis\n",
    "def simple_time_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze when questions are posted - simplified for beginners\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or 'publication_date' not in df.columns:\n",
    "        print(\"No time data available\")\n",
    "        return None\n",
    "    \n",
    "    # Remove questions without dates\n",
    "    df_with_dates = df.dropna(subset=['publication_date'])\n",
    "    \n",
    "    if df_with_dates.empty:\n",
    "        print(\"No valid dates found in the data\")\n",
    "        return None\n",
    "    \n",
    "    print(\"TIME ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic time info\n",
    "    earliest_question = df_with_dates['publication_date'].min()\n",
    "    latest_question = df_with_dates['publication_date'].max()\n",
    "    total_days = (latest_question - earliest_question).days + 1\n",
    "    \n",
    "    print(f\"Basic Time Info:\")\n",
    "    print(f\"   ‚Ä¢ First question: {earliest_question.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    print(f\"   ‚Ä¢ Last question: {latest_question.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    print(f\"   ‚Ä¢ Time span: {total_days} days\")\n",
    "    print(f\"   ‚Ä¢ Questions per day: {len(df_with_dates) / max(1, total_days):.1f}\")\n",
    "    \n",
    "    # Add simple time columns (easier to understand than complex datetime operations)\n",
    "    df_time = df_with_dates.copy()\n",
    "    df_time['day_name'] = df_time['publication_date'].dt.day_name()  # Monday, Tuesday, etc.\n",
    "    df_time['hour'] = df_time['publication_date'].dt.hour           # 0-23 hour of day\n",
    "    df_time['date'] = df_time['publication_date'].dt.date          # Just the date part\n",
    "    \n",
    "    # Count questions by day of week\n",
    "    print(f\"\\nQuestions by Day of Week:\")\n",
    "    day_counts = df_time['day_name'].value_counts()\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    for day in day_order:\n",
    "        if day in day_counts:\n",
    "            count = day_counts[day]\n",
    "            print(f\"   ‚Ä¢ {day:<10}: {count:3d} questions\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ {day:<10}:   0 questions\")\n",
    "    \n",
    "    # Find busiest day\n",
    "    if not day_counts.empty:\n",
    "        busiest_day = day_counts.index[0]\n",
    "        busiest_count = day_counts.iloc[0]\n",
    "        print(f\"\\nBusiest day: {busiest_day} ({busiest_count} questions)\")\n",
    "    \n",
    "    return df_time, day_counts\n",
    "\n",
    "# Run the analysis\n",
    "if df is not None:\n",
    "    time_results = simple_time_analysis(df)\n",
    "    if time_results:\n",
    "        df_time, day_counts = time_results\n",
    "else:\n",
    "    print(\"Please load data first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple time charts\n",
    "def create_simple_time_charts(df_time, day_counts):\n",
    "    \"\"\"\n",
    "    Create simple, easy-to-read time charts\n",
    "    \"\"\"\n",
    "    if df_time is None or day_counts is None:\n",
    "        print(\"No time data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure with 2 charts\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('When Do People Ask Questions?', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Chart 1: Questions by day of week\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    # Reorder the data to match our preferred order\n",
    "    ordered_counts = []\n",
    "    ordered_days = []\n",
    "    \n",
    "    for day in day_order:\n",
    "        if day in day_counts:\n",
    "            ordered_days.append(day[:3])  # Shorten to Mon, Tue, etc.\n",
    "            ordered_counts.append(day_counts[day])\n",
    "        else:\n",
    "            ordered_days.append(day[:3])\n",
    "            ordered_counts.append(0)\n",
    "    \n",
    "    # Create colorful bars - weekdays vs weekends\n",
    "    colors = ['lightblue' if day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri'] else 'lightcoral' \n",
    "              for day in ordered_days]\n",
    "    \n",
    "    bars1 = ax1.bar(ordered_days, ordered_counts, color=colors, edgecolor='navy', linewidth=1)\n",
    "    ax1.set_title('Questions by Day of Week', fontweight='bold')\n",
    "    ax1.set_xlabel('Day of Week')\n",
    "    ax1.set_ylabel('Number of Questions')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars1, ordered_counts):\n",
    "        if count > 0:  # Only show label if there are questions\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Chart 2: Questions by hour of day\n",
    "    hour_counts = df_time['hour'].value_counts().sort_index()\n",
    "    \n",
    "    # Fill in missing hours with 0\n",
    "    all_hours = range(24)\n",
    "    hour_data = [hour_counts.get(hour, 0) for hour in all_hours]\n",
    "    \n",
    "    bars2 = ax2.bar(all_hours, hour_data, color='lightgreen', edgecolor='darkgreen', linewidth=1)\n",
    "    ax2.set_title('Questions by Hour of Day', fontweight='bold')\n",
    "    ax2.set_xlabel('Hour (24h format)')\n",
    "    ax2.set_ylabel('Number of Questions')\n",
    "    ax2.set_xticks(range(0, 24, 4))  # Show every 4 hours\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight peak hour\n",
    "    if hour_data:\n",
    "        peak_hour = hour_data.index(max(hour_data))\n",
    "        bars2[peak_hour].set_color('orange')\n",
    "        ax2.text(peak_hour, max(hour_data) + 1, f'Peak: {peak_hour}:00', \n",
    "                ha='center', va='bottom', fontweight='bold', color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\nTime Insights:\")\n",
    "    if ordered_counts:\n",
    "        busiest_day_idx = ordered_counts.index(max(ordered_counts))\n",
    "        busiest_day = day_order[busiest_day_idx]\n",
    "        print(f\"   ‚Ä¢ Most active day: {busiest_day}\")\n",
    "        \n",
    "        weekday_total = sum(ordered_counts[:5])  # Mon-Fri\n",
    "        weekend_total = sum(ordered_counts[5:])   # Sat-Sun\n",
    "        if weekday_total + weekend_total > 0:\n",
    "            weekday_pct = (weekday_total / (weekday_total + weekend_total)) * 100\n",
    "            print(f\"   ‚Ä¢ Weekdays vs Weekends: {weekday_pct:.1f}% vs {100-weekday_pct:.1f}%\")\n",
    "    \n",
    "    if hour_data:\n",
    "        peak_hour = hour_data.index(max(hour_data))\n",
    "        print(f\"   ‚Ä¢ Most active hour: {peak_hour}:00 ({max(hour_data)} questions)\")\n",
    "\n",
    "# Create the charts\n",
    "if 'time_results' in locals() and time_results:\n",
    "    df_time, day_counts = time_results\n",
    "    create_simple_time_charts(df_time, day_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Simple Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple author analysis\n",
    "def simple_author_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze authors in a simple, beginner-friendly way\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data available for author analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"AUTHOR ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic author statistics\n",
    "    total_questions = len(df)\n",
    "    unique_authors = df['author_name'].nunique()\n",
    "    \n",
    "    print(f\"Basic Numbers:\")\n",
    "    print(f\"   ‚Ä¢ Total questions: {total_questions:,}\")\n",
    "    print(f\"   ‚Ä¢ Unique authors: {unique_authors:,}\")\n",
    "    print(f\"   ‚Ä¢ Questions per author: {total_questions / unique_authors:.1f}\")\n",
    "    \n",
    "    # Count how many questions each author has\n",
    "    author_question_counts = df['author_name'].value_counts()\n",
    "    \n",
    "    # Most active authors (top 10)\n",
    "    print(f\"\\nTOP 10 MOST ACTIVE AUTHORS:\")\n",
    "    for i, (author, question_count) in enumerate(author_question_counts.head(10).items(), 1):\n",
    "        # Get this author's reputation (if available)\n",
    "        author_rep = df[df['author_name'] == author]['author_reputation'].iloc[0]\n",
    "        rep_str = f\"{author_rep:,}\" if pd.notna(author_rep) else \"Unknown\"\n",
    "        print(f\"   {i:2d}. {author:<25} {question_count:2d} questions (Rep: {rep_str})\")\n",
    "    \n",
    "    # Author activity distribution\n",
    "    print(f\"\\nAuthor Activity Patterns:\")\n",
    "    one_question = (author_question_counts == 1).sum()\n",
    "    few_questions = ((author_question_counts >= 2) & (author_question_counts <= 3)).sum()\n",
    "    many_questions = (author_question_counts > 3).sum()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Authors with 1 question: {one_question} ({one_question/unique_authors*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Authors with 2-3 questions: {few_questions} ({few_questions/unique_authors*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Authors with 4+ questions: {many_questions} ({many_questions/unique_authors*100:.1f}%)\")\n",
    "    \n",
    "    # Reputation analysis (if available)\n",
    "    if 'author_reputation' in df.columns:\n",
    "        rep_data = df['author_reputation'].dropna()\n",
    "        if not rep_data.empty:\n",
    "            print(f\"\\nReputation Analysis:\")\n",
    "            print(f\"   ‚Ä¢ Average reputation: {rep_data.mean():,.0f} points\")\n",
    "            print(f\"   ‚Ä¢ Median reputation: {rep_data.median():,.0f} points\")\n",
    "            print(f\"   ‚Ä¢ Highest reputation: {rep_data.max():,.0f} points\")\n",
    "            print(f\"   ‚Ä¢ Lowest reputation: {rep_data.min():,.0f} points\")\n",
    "            \n",
    "            # Reputation categories\n",
    "            low_rep = (rep_data < 100).sum()\n",
    "            medium_rep = ((rep_data >= 100) & (rep_data < 1000)).sum()\n",
    "            high_rep = (rep_data >= 1000).sum()\n",
    "            \n",
    "            total_with_rep = len(rep_data)\n",
    "            print(f\"\\nReputation Categories:\")\n",
    "            print(f\"   ‚Ä¢ Low (< 100): {low_rep} authors ({low_rep/total_with_rep*100:.1f}%)\")\n",
    "            print(f\"   ‚Ä¢ Medium (100-999): {medium_rep} authors ({medium_rep/total_with_rep*100:.1f}%)\")\n",
    "            print(f\"   ‚Ä¢ High (1000+): {high_rep} authors ({high_rep/total_with_rep*100:.1f}%)\")\n",
    "    \n",
    "    return author_question_counts\n",
    "\n",
    "# Run the analysis\n",
    "if df is not None:\n",
    "    author_results = simple_author_analysis(df)\n",
    "else:\n",
    "    print(\"Please load data first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple author charts\n",
    "def create_simple_author_charts(df, author_question_counts):\n",
    "    \"\"\"\n",
    "    Create simple charts about authors\n",
    "    \"\"\"\n",
    "    if df is None or author_question_counts is None:\n",
    "        print(\"No author data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure with 2 charts\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Who Is Asking Questions?', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Chart 1: Top 10 most active authors\n",
    "    top_10_authors = author_question_counts.head(10)\n",
    "    author_names = [name[:20] + '...' if len(name) > 20 else name for name in top_10_authors.index]\n",
    "    question_counts = top_10_authors.values\n",
    "    \n",
    "    bars1 = ax1.bar(range(len(author_names)), question_counts, \n",
    "                    color='skyblue', edgecolor='navy', linewidth=1)\n",
    "    ax1.set_title('Top 10 Most Active Authors', fontweight='bold')\n",
    "    ax1.set_xlabel('Authors')\n",
    "    ax1.set_ylabel('Number of Questions')\n",
    "    ax1.set_xticks(range(len(author_names)))\n",
    "    ax1.set_xticklabels(author_names, rotation=45, ha='right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars1, question_counts)):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(i, height + 0.1, f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Chart 2: Author activity distribution\n",
    "    activity_labels = ['1 question', '2-3 questions', '4+ questions']\n",
    "    activity_counts = [\n",
    "        (author_question_counts == 1).sum(),\n",
    "        ((author_question_counts >= 2) & (author_question_counts <= 3)).sum(),\n",
    "        (author_question_counts > 3).sum()\n",
    "    ]\n",
    "    \n",
    "    colors = ['lightcoral', 'lightyellow', 'lightgreen']\n",
    "    bars2 = ax2.bar(activity_labels, activity_counts, color=colors, \n",
    "                    edgecolor='darkgreen', linewidth=1)\n",
    "    ax2.set_title('Author Activity Distribution', fontweight='bold')\n",
    "    ax2.set_xlabel('Activity Level')\n",
    "    ax2.set_ylabel('Number of Authors')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels and percentages on bars\n",
    "    total_authors = sum(activity_counts)\n",
    "    for bar, count in zip(bars2, activity_counts):\n",
    "        height = bar.get_height()\n",
    "        percentage = (count / total_authors) * 100 if total_authors > 0 else 0\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{count}\\n({percentage:.1f}%)', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\nAuthor Insights:\")\n",
    "    if len(author_question_counts) > 0:\n",
    "        most_active = author_question_counts.index[0]\n",
    "        most_active_count = author_question_counts.iloc[0]\n",
    "        print(f\"   ‚Ä¢ Most active author: '{most_active}' with {most_active_count} questions\")\n",
    "        \n",
    "        single_question_pct = (activity_counts[0] / total_authors) * 100\n",
    "        print(f\"   ‚Ä¢ {single_question_pct:.1f}% of authors ask only 1 question\")\n",
    "        \n",
    "        if activity_counts[2] > 0:\n",
    "            print(f\"   ‚Ä¢ {activity_counts[2]} authors are very active (4+ questions)\")\n",
    "\n",
    "# Create the charts\n",
    "if df is not None and 'author_results' in locals():\n",
    "    create_simple_author_charts(df, author_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Simple Content Analysis\n",
    "\n",
    "Let's examine the **content** of the questions! We'll look at:\n",
    "- How long questions are (title and text)\n",
    "- Common words in question titles  \n",
    "- Simple patterns in the text\n",
    "\n",
    "*This helps understand what kinds of questions people ask*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple content analysis\n",
    "def simple_content_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze question content in a beginner-friendly way\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data available for content analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"CONTENT ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic content statistics\n",
    "    print(f\"Basic Content Stats:\")\n",
    "    \n",
    "    # Title length analysis\n",
    "    if 'title_length' in df.columns:\n",
    "        avg_title_length = df['title_length'].mean()\n",
    "        min_title_length = df['title_length'].min()\n",
    "        max_title_length = df['title_length'].max()\n",
    "        \n",
    "        print(f\"   Title Length:\")\n",
    "        print(f\"      ‚Ä¢ Average: {avg_title_length:.0f} characters\")\n",
    "        print(f\"      ‚Ä¢ Shortest: {min_title_length:.0f} characters\")\n",
    "        print(f\"      ‚Ä¢ Longest: {max_title_length:.0f} characters\")\n",
    "    \n",
    "    # Question text length analysis\n",
    "    if 'text_length' in df.columns:\n",
    "        avg_text_length = df['text_length'].mean()\n",
    "        min_text_length = df['text_length'].min()\n",
    "        max_text_length = df['text_length'].max()\n",
    "        \n",
    "        print(f\"   Question Text Length:\")\n",
    "        print(f\"      ‚Ä¢ Average: {avg_text_length:.0f} characters\")\n",
    "        print(f\"      ‚Ä¢ Shortest: {min_text_length:.0f} characters\")\n",
    "        print(f\"      ‚Ä¢ Longest: {max_text_length:.0f} characters\")\n",
    "    \n",
    "    # Number of tags analysis\n",
    "    if 'num_tags' in df.columns:\n",
    "        avg_tags = df['num_tags'].mean()\n",
    "        min_tags = df['num_tags'].min()\n",
    "        max_tags = df['num_tags'].max()\n",
    "        \n",
    "        print(f\"   Tags per Question:\")\n",
    "        print(f\"      ‚Ä¢ Average: {avg_tags:.1f} tags\")\n",
    "        print(f\"      ‚Ä¢ Minimum: {min_tags:.0f} tags\")\n",
    "        print(f\"      ‚Ä¢ Maximum: {max_tags:.0f} tags\")\n",
    "    \n",
    "    # Find common words in titles (simplified approach)\n",
    "    if 'title' in df.columns:\n",
    "        print(f\"\\nMost Common Words in Titles:\")\n",
    "        \n",
    "        # Combine all titles into one big text\n",
    "        all_titles = ' '.join(df['title'].dropna().astype(str).str.lower())\n",
    "        \n",
    "        # Split into words and clean them up\n",
    "        words = all_titles.split()\n",
    "        \n",
    "        # Remove very common words (stop words) and short words\n",
    "        stop_words = {\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
    "            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'how', 'what', 'when',\n",
    "            'where', 'why', 'who', 'which', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she',\n",
    "            'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their'\n",
    "        }\n",
    "        \n",
    "        # Keep only meaningful words\n",
    "        meaningful_words = []\n",
    "        for word in words:\n",
    "            # Remove punctuation and keep only letters\n",
    "            clean_word = ''.join(char for char in word if char.isalpha())\n",
    "            # Keep words that are long enough and not stop words\n",
    "            if len(clean_word) >= 3 and clean_word not in stop_words:\n",
    "                meaningful_words.append(clean_word)\n",
    "        \n",
    "        # Count word frequency\n",
    "        from collections import Counter\n",
    "        word_counts = Counter(meaningful_words)\n",
    "        \n",
    "        # Show top 10 words\n",
    "        top_10_words = word_counts.most_common(10)\n",
    "        for i, (word, count) in enumerate(top_10_words, 1):\n",
    "            print(f\"      {i:2d}. {word:<15} appears {count:3d} times\")\n",
    "    \n",
    "    # Simple content categories based on length\n",
    "    if 'text_length' in df.columns:\n",
    "        print(f\"\\nQuestion Length Categories:\")\n",
    "        short_questions = (df['text_length'] < 500).sum()\n",
    "        medium_questions = ((df['text_length'] >= 500) & (df['text_length'] < 2000)).sum()\n",
    "        long_questions = (df['text_length'] >= 2000).sum()\n",
    "        \n",
    "        total = len(df)\n",
    "        print(f\"   ‚Ä¢ Short questions (< 500 chars): {short_questions} ({short_questions/total*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Medium questions (500-2000): {medium_questions} ({medium_questions/total*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Long questions (2000+ chars): {long_questions} ({long_questions/total*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'word_counts': top_10_words if 'top_10_words' in locals() else None,\n",
    "        'length_stats': {\n",
    "            'title_avg': avg_title_length if 'avg_title_length' in locals() else None,\n",
    "            'text_avg': avg_text_length if 'avg_text_length' in locals() else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "if df is not None:\n",
    "    content_results = simple_content_analysis(df)\n",
    "else:\n",
    "    print(\"Please load data first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple content charts\n",
    "def create_simple_content_charts(df, content_results):\n",
    "    \"\"\"\n",
    "    Create easy-to-understand charts about question content\n",
    "    \"\"\"\n",
    "    if df is None or content_results is None:\n",
    "        print(\"No content data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure with 2 charts\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('What Are Questions Like?', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Chart 1: Question length distribution (simple histogram)\n",
    "    if 'text_length' in df.columns:\n",
    "        # Create simple categories for easier understanding\n",
    "        length_categories = []\n",
    "        category_names = []\n",
    "        \n",
    "        short_q = (df['text_length'] < 500).sum()\n",
    "        medium_q = ((df['text_length'] >= 500) & (df['text_length'] < 2000)).sum()\n",
    "        long_q = (df['text_length'] >= 2000).sum()\n",
    "        \n",
    "        categories = ['Short\\n(< 500 chars)', 'Medium\\n(500-2000 chars)', 'Long\\n(2000+ chars)']\n",
    "        counts = [short_q, medium_q, long_q]\n",
    "        colors = ['lightcoral', 'lightyellow', 'lightgreen']\n",
    "        \n",
    "        bars1 = ax1.bar(categories, counts, color=colors, edgecolor='navy', linewidth=1)\n",
    "        ax1.set_title('Question Length Categories', fontweight='bold')\n",
    "        ax1.set_ylabel('Number of Questions')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars1, counts):\n",
    "            height = bar.get_height()\n",
    "            percentage = (count / len(df)) * 100 if len(df) > 0 else 0\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + len(df)*0.01,\n",
    "                    f'{count}\\n({percentage:.1f}%)', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Chart 2: Most common words in titles\n",
    "    if content_results['word_counts']:\n",
    "        top_words = content_results['word_counts'][:8]  # Show top 8 for better visibility\n",
    "        words = [word for word, count in top_words]\n",
    "        word_counts = [count for word, count in top_words]\n",
    "        \n",
    "        bars2 = ax2.bar(words, word_counts, color='skyblue', edgecolor='darkblue', linewidth=1)\n",
    "        ax2.set_title('Most Common Words in Titles', fontweight='bold')\n",
    "        ax2.set_xlabel('Words')\n",
    "        ax2.set_ylabel('Number of Times Used')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars2, word_counts):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print simple insights\n",
    "    print(\"\\nContent Insights:\")\n",
    "    \n",
    "    if 'text_length' in df.columns:\n",
    "        avg_length = df['text_length'].mean()\n",
    "        if avg_length < 800:\n",
    "            length_desc = \"short\"\n",
    "        elif avg_length < 2000:\n",
    "            length_desc = \"medium-length\"\n",
    "        else:\n",
    "            length_desc = \"long\"\n",
    "        print(f\"   ‚Ä¢ Questions are typically {length_desc} ({avg_length:.0f} characters on average)\")\n",
    "    \n",
    "    if content_results['word_counts']:\n",
    "        most_common_word = content_results['word_counts'][0][0]\n",
    "        most_common_count = content_results['word_counts'][0][1]\n",
    "        print(f\"   ‚Ä¢ Most common word in titles: '{most_common_word}' (appears {most_common_count} times)\")\n",
    "        \n",
    "        # Check if it's a programming language\n",
    "        prog_languages = ['python', 'javascript', 'java', 'php', 'css', 'html', 'sql', 'react', 'angular']\n",
    "        prog_words = [word for word, count in content_results['word_counts'] if word in prog_languages]\n",
    "        if prog_words:\n",
    "            print(f\"   ‚Ä¢ Programming languages mentioned: {', '.join(prog_words[:3])}\")\n",
    "\n",
    "# Create the charts\n",
    "if df is not None and 'content_results' in locals():\n",
    "    create_simple_content_charts(df, content_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Simple Summary Report\n",
    "\n",
    "**What you've learned so far:**\n",
    "This section creates a simple summary of all your analysis - perfect for sharing with others or keeping as notes!\n",
    "\n",
    "The summary includes:\n",
    "- Basic numbers about your dataset\n",
    "- Most popular topics (tags)\n",
    "- Most active authors\n",
    "- Key patterns and trends\n",
    "\n",
    "*Great for presenting your findings to others!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple, easy-to-read summary\n",
    "def create_beginner_summary():\n",
    "    \"\"\"\n",
    "    Create a simple summary report that's perfect for junior data analysts\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available - please load data first!\")\n",
    "        return\n",
    "    \n",
    "    print(\"SIMPLE ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Here's what we discovered about Stack Overflow questions:\\n\")\n",
    "    \n",
    "    # Dataset basics\n",
    "    print(\"DATASET OVERVIEW:\")\n",
    "    print(f\"   ‚Ä¢ We analyzed {len(df):,} questions\")\n",
    "    print(f\"   ‚Ä¢ From {df['author_name'].nunique():,} different authors\")\n",
    "    print(f\"   ‚Ä¢ Asked between {df['publication_date'].min().strftime('%Y-%m-%d')} and {df['publication_date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   ‚Ä¢ Data collected using: {', '.join(df['scrape_method'].dropna().unique())}\")\n",
    "    \n",
    "    # Top insights\n",
    "    print(f\"\\nKEY FINDINGS:\")\n",
    "    \n",
    "    # Most popular tag\n",
    "    if 'tag_results' in globals() and tag_results:\n",
    "        tag_counts, top_tags = tag_results\n",
    "        most_popular_tag = top_tags[0][0]\n",
    "        most_popular_count = top_tags[0][1]\n",
    "        print(f\"   ‚Ä¢ Most popular programming topic: '{most_popular_tag}' ({most_popular_count} questions)\")\n",
    "    \n",
    "    # Most active author\n",
    "    if 'author_results' in globals() and author_results is not None:\n",
    "        top_author = author_results.index[0]\n",
    "        author_question_count = author_results.iloc[0]\n",
    "        print(f\"   ‚Ä¢ Most active author: '{top_author}' ({author_question_count} questions)\")\n",
    "    \n",
    "    # Average question length\n",
    "    if 'text_length' in df.columns:\n",
    "        avg_length = df['text_length'].mean()\n",
    "        if avg_length < 800:\n",
    "            length_category = \"short and concise\"\n",
    "        elif avg_length < 2000:\n",
    "            length_category = \"medium-length\"\n",
    "        else:\n",
    "            length_category = \"detailed and comprehensive\"\n",
    "        print(f\"   ‚Ä¢ Questions are typically {length_category} ({avg_length:.0f} characters)\")\n",
    "    \n",
    "    # Average tags per question\n",
    "    if 'num_tags' in df.columns:\n",
    "        avg_tags = df['num_tags'].mean()\n",
    "        print(f\"   ‚Ä¢ Questions usually have {avg_tags:.1f} tags on average\")\n",
    "    \n",
    "    # Author reputation insights\n",
    "    if 'author_reputation' in df.columns:\n",
    "        rep_data = df['author_reputation'].dropna()\n",
    "        if not rep_data.empty:\n",
    "            avg_rep = rep_data.mean()\n",
    "            max_rep = rep_data.max()\n",
    "            print(f\"   ‚Ä¢ Average author reputation: {avg_rep:,.0f} points\")\n",
    "            print(f\"   ‚Ä¢ Highest reputation author has: {max_rep:,.0f} points\")\n",
    "    \n",
    "    # Time patterns\n",
    "    if 'time_results' in globals() and time_results:\n",
    "        df_time, day_counts = time_results\n",
    "        if not day_counts.empty:\n",
    "            busiest_day = day_counts.index[0]\n",
    "            busiest_count = day_counts.iloc[0]\n",
    "            print(f\"   ‚Ä¢ Most active day of the week: {busiest_day} ({busiest_count} questions)\")\n",
    "    \n",
    "    print(f\"\\nWHAT THIS MEANS:\")\n",
    "    print(f\"   ‚Ä¢ This data gives us insight into programming trends and community behavior\")\n",
    "    print(f\"   ‚Ä¢ Popular tags show what technologies people are working with\")\n",
    "    print(f\"   ‚Ä¢ Active authors indicate engaged community members\")\n",
    "    print(f\"   ‚Ä¢ Time patterns reveal when developers are most likely to seek help\")\n",
    "    \n",
    "    print(f\"\\nANALYSIS COMPLETED!\")\n",
    "    print(f\"   Report generated on: {datetime.now().strftime('%Y-%m-%d at %H:%M')}\")\n",
    "    print(f\"   Perfect for sharing with your team or keeping as notes!\")\n",
    "\n",
    "# Generate the summary\n",
    "create_beginner_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export options\n",
    "def create_export_interface():\n",
    "    \"\"\"Create interface for exporting data\"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data available for export\")\n",
    "        return\n",
    "    \n",
    "    print(\"Data Export Options\")\n",
    "    \n",
    "    # Export format selector\n",
    "    format_dropdown = widgets.Dropdown(\n",
    "        options=[('CSV', 'csv'), ('JSON', 'json'), ('Excel', 'xlsx')],\n",
    "        value='csv',\n",
    "        description='Format:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    filename_text = widgets.Text(\n",
    "        value=f'stackoverflow_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "        description='Filename:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    export_button = widgets.Button(\n",
    "        description='Export Data',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "    \n",
    "    export_output = widgets.Output()\n",
    "    \n",
    "    def export_data(b):\n",
    "        \"\"\"Export data in selected format\"\"\"\n",
    "        with export_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            filename = f\"{filename_text.value}.{format_dropdown.value}\"\n",
    "            \n",
    "            try:\n",
    "                # Prepare data for export (convert lists to strings)\n",
    "                export_df = df.copy()\n",
    "                export_df['tags'] = export_df['tags'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "                \n",
    "                if format_dropdown.value == 'csv':\n",
    "                    export_df.to_csv(filename, index=False, encoding='utf-8')\n",
    "                elif format_dropdown.value == 'json':\n",
    "                    export_df.to_json(filename, orient='records', indent=2, force_ascii=False)\n",
    "                elif format_dropdown.value == 'xlsx':\n",
    "                    export_df.to_excel(filename, index=False, engine='openpyxl')\n",
    "                \n",
    "                print(f\"Data exported successfully to '{filename}'\")\n",
    "                print(f\"Exported {len(export_df)} questions\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Export failed: {e}\")\n",
    "    \n",
    "    export_button.on_click(export_data)\n",
    "    \n",
    "    # Display export interface\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Export Data</h3>\"),\n",
    "        format_dropdown,\n",
    "        filename_text,\n",
    "        export_button,\n",
    "        export_output\n",
    "    ]))\n",
    "\n",
    "if df is not None:\n",
    "    create_export_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
